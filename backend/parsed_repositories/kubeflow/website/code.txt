Directory structure:
└── kubeflow-website/
    ├── README.md
    ├── LICENSE
    ├── OWNERS
    ├── config.toml
    ├── netlify.toml
    ├── package.json
    ├── prow_config.yaml
    ├── quick-github-guide.md
    ├── .hugo_build.lock
    ├── assets/
    │   ├── fonts/
    │   │   └── Inter-Medium.ttf
    │   ├── icons/
    │   ├── scss/
    │   │   ├── _styles_project.scss
    │   │   └── _variables_project.scss
    │   └── social/
    ├── content/
    │   └── en/
    │       ├── _index.html
    │       ├── _redirects
    │       ├── search.md
    │       ├── docs/
    │       │   ├── _index.md
    │       │   ├── about/
    │       │   │   ├── OWNERS
    │       │   │   ├── _index.md
    │       │   │   ├── community.md
    │       │   │   ├── contributing.md
    │       │   │   ├── events.md
    │       │   │   ├── membership.md
    │       │   │   ├── style-guide.md
    │       │   │   └── images/
    │       │   ├── components/
    │       │   │   ├── _index.md
    │       │   │   ├── central-dash/
    │       │   │   │   ├── OWNERS
    │       │   │   │   ├── _index.md
    │       │   │   │   ├── access.md
    │       │   │   │   ├── customize.md
    │       │   │   │   ├── overview.md
    │       │   │   │   └── profiles.md
    │       │   │   ├── katib/
    │       │   │   │   ├── OWNERS
    │       │   │   │   ├── _index.md
    │       │   │   │   ├── getting-started.md
    │       │   │   │   ├── installation.md
    │       │   │   │   ├── overview.md
    │       │   │   │   ├── images/
    │       │   │   │   ├── reference/
    │       │   │   │   │   ├── _index.md
    │       │   │   │   │   ├── architecture.md
    │       │   │   │   │   ├── experiment-cr.md
    │       │   │   │   │   └── nas-algorithms.md
    │       │   │   │   └── user-guides/
    │       │   │   │       ├── _index.md
    │       │   │   │       ├── early-stopping.md
    │       │   │   │       ├── env-variables.md
    │       │   │   │       ├── installation-options.md
    │       │   │   │       ├── katib-config.md
    │       │   │   │       ├── katib-ui.md
    │       │   │   │       ├── metrics-collector.md
    │       │   │   │       ├── resume-experiment.md
    │       │   │   │       ├── trial-template.md
    │       │   │   │       ├── hp-tuning/
    │       │   │   │       │   ├── _index.md
    │       │   │   │       │   ├── configure-algorithm.md
    │       │   │   │       │   └── configure-experiment.md
    │       │   │   │       └── nas/
    │       │   │   │           ├── _index.md
    │       │   │   │           ├── configure-algorithm.md
    │       │   │   │           └── configure-experiment.md
    │       │   │   ├── model-registry/
    │       │   │   │   ├── OWNERS
    │       │   │   │   ├── _index.md
    │       │   │   │   ├── getting-started.md
    │       │   │   │   ├── installation.md
    │       │   │   │   ├── overview.md
    │       │   │   │   ├── images/
    │       │   │   │   └── reference/
    │       │   │   │       ├── _index.md
    │       │   │   │       ├── architecture.md
    │       │   │   │       ├── python-client.md
    │       │   │   │       ├── rest-api.md
    │       │   │   │       └── images/
    │       │   │   ├── notebooks/
    │       │   │   │   ├── OWNERS
    │       │   │   │   ├── _index.md
    │       │   │   │   ├── container-images.md
    │       │   │   │   ├── jupyter-tensorflow-examples.md
    │       │   │   │   ├── overview.md
    │       │   │   │   ├── quickstart-guide.md
    │       │   │   │   ├── submit-kubernetes.md
    │       │   │   │   ├── troubleshooting.md
    │       │   │   │   └── api-reference/
    │       │   │   │       ├── _index.md
    │       │   │   │       └── notebook-v1.md
    │       │   │   ├── pipelines/
    │       │   │   │   ├── OWNERS
    │       │   │   │   ├── _index.md
    │       │   │   │   ├── getting-started.md
    │       │   │   │   ├── interfaces.md
    │       │   │   │   ├── overview.md
    │       │   │   │   ├── concepts/
    │       │   │   │   │   ├── _index.md
    │       │   │   │   │   ├── component.md
    │       │   │   │   │   ├── experiment.md
    │       │   │   │   │   ├── graph.md
    │       │   │   │   │   ├── metadata.md
    │       │   │   │   │   ├── output-artifact.md
    │       │   │   │   │   ├── pipeline-root.md
    │       │   │   │   │   ├── pipeline.md
    │       │   │   │   │   ├── run-trigger.md
    │       │   │   │   │   ├── run.md
    │       │   │   │   │   └── step.md
    │       │   │   │   ├── legacy-v1/
    │       │   │   │   │   ├── _index.md
    │       │   │   │   │   ├── introduction.md
    │       │   │   │   │   ├── troubleshooting.md
    │       │   │   │   │   ├── installation/
    │       │   │   │   │   │   ├── _index.md
    │       │   │   │   │   │   ├── choose-executor.md
    │       │   │   │   │   │   ├── compatibility-matrix.md
    │       │   │   │   │   │   ├── localcluster-deployment.md
    │       │   │   │   │   │   ├── overview.md
    │       │   │   │   │   │   ├── standalone-deployment.md
    │       │   │   │   │   │   └── upgrade.md
    │       │   │   │   │   ├── overview/
    │       │   │   │   │   │   ├── _index.md
    │       │   │   │   │   │   ├── caching.md
    │       │   │   │   │   │   └── quickstart.md
    │       │   │   │   │   ├── reference/
    │       │   │   │   │   │   ├── _index.md
    │       │   │   │   │   │   ├── sdk.md
    │       │   │   │   │   │   └── api/
    │       │   │   │   │   │       └── kubeflow-pipeline-api-spec.md
    │       │   │   │   │   ├── sdk/
    │       │   │   │   │   │   ├── _index.md
    │       │   │   │   │   │   ├── best-practices.md
    │       │   │   │   │   │   ├── build-pipeline.ipynb
    │       │   │   │   │   │   ├── build-pipeline.md
    │       │   │   │   │   │   ├── component-development.md
    │       │   │   │   │   │   ├── dsl-recursion.md
    │       │   │   │   │   │   ├── enviroment_variables.md
    │       │   │   │   │   │   ├── gcp.md
    │       │   │   │   │   │   ├── install-sdk.md
    │       │   │   │   │   │   ├── manipulate-resources.md
    │       │   │   │   │   │   ├── output-viewer.md
    │       │   │   │   │   │   ├── parameters.md
    │       │   │   │   │   │   ├── pipelines-with-tekton.md
    │       │   │   │   │   │   ├── python-based-visualizations.md
    │       │   │   │   │   │   ├── python-function-components.ipynb
    │       │   │   │   │   │   ├── python-function-components.md
    │       │   │   │   │   │   ├── sdk-overview.md
    │       │   │   │   │   │   └── static-type-checking.md
    │       │   │   │   │   └── tutorials/
    │       │   │   │   │       ├── _index.md
    │       │   │   │   │       ├── api-pipelines.md
    │       │   │   │   │       ├── benchmark-examples.md
    │       │   │   │   │       ├── build-pipeline.md
    │       │   │   │   │       ├── cloud-tutorials.md
    │       │   │   │   │       └── sdk-examples.md
    │       │   │   │   ├── operator-guides/
    │       │   │   │   │   ├── _index.md
    │       │   │   │   │   ├── configure-object-store.md
    │       │   │   │   │   ├── multi-user.md
    │       │   │   │   │   ├── server-config.md
    │       │   │   │   │   ├── images/
    │       │   │   │   │   └── installation/
    │       │   │   │   │       └── _index.md
    │       │   │   │   ├── reference/
    │       │   │   │   │   ├── _index.md
    │       │   │   │   │   ├── community-and-support.md
    │       │   │   │   │   ├── component-spec.md
    │       │   │   │   │   ├── kfp-kubernetes.md
    │       │   │   │   │   ├── sdk.md
    │       │   │   │   │   ├── version-compatibility.md
    │       │   │   │   │   └── api/
    │       │   │   │   │       └── kubeflow-pipeline-api-spec.md
    │       │   │   │   └── user-guides/
    │       │   │   │       ├── _index.md
    │       │   │   │       ├── migration.md
    │       │   │   │       ├── components/
    │       │   │   │       │   ├── _index.md
    │       │   │   │       │   ├── additional-functionality.md
    │       │   │   │       │   ├── compose-components-into-pipelines.md
    │       │   │   │       │   ├── container-components.md
    │       │   │   │       │   ├── containerized-python-components.md
    │       │   │   │       │   ├── importer-component.md
    │       │   │   │       │   ├── lightweight-python-components.md
    │       │   │   │       │   └── load-and-share-components.md
    │       │   │   │       ├── core-functions/
    │       │   │   │       │   ├── _index.md
    │       │   │   │       │   ├── build-advanced-pipeline.md
    │       │   │   │       │   ├── caching.md
    │       │   │   │       │   ├── cli.md
    │       │   │   │       │   ├── compile-a-pipeline.md
    │       │   │   │       │   ├── connect-api.md
    │       │   │   │       │   ├── control-flow.md
    │       │   │   │       │   ├── execute-kfp-pipelines-locally.md
    │       │   │   │       │   ├── platform-specific-features.md
    │       │   │   │       │   └── run-a-pipeline.md
    │       │   │   │       └── data-handling/
    │       │   │   │           ├── _index.md
    │       │   │   │           ├── artifacts.md
    │       │   │   │           ├── data-types.md
    │       │   │   │           └── parameters.md
    │       │   │   ├── spark-operator/
    │       │   │   │   ├── OWNERS
    │       │   │   │   ├── _index.md
    │       │   │   │   ├── developer-guide.md
    │       │   │   │   ├── getting-started.md
    │       │   │   │   ├── overview/
    │       │   │   │   │   └── _index.md
    │       │   │   │   ├── performance/
    │       │   │   │   │   ├── _index.md
    │       │   │   │   │   └── benchmarking.md
    │       │   │   │   ├── reference/
    │       │   │   │   │   ├── _index.md
    │       │   │   │   │   └── api-docs.md
    │       │   │   │   └── user-guide/
    │       │   │   │       ├── _index.md
    │       │   │   │       ├── customizing-spark-operator.md
    │       │   │   │       ├── gcp.md
    │       │   │   │       ├── leader-election.md
    │       │   │   │       ├── resource-quota-enforcement.md
    │       │   │   │       ├── running-multiple-instances-of-the-operator.md
    │       │   │   │       ├── running-sparkapplication-on-schedule.md
    │       │   │   │       ├── using-sparkapplication.md
    │       │   │   │       ├── volcano-integration.md
    │       │   │   │       ├── working-with-sparkapplication.md
    │       │   │   │       ├── writing-sparkapplication.md
    │       │   │   │       └── yunikorn-integration.md
    │       │   │   └── trainer/
    │       │   │       ├── OWNERS
    │       │   │       ├── _index.md
    │       │   │       ├── getting-started.md
    │       │   │       ├── overview.md
    │       │   │       ├── contributor-guides/
    │       │   │       │   ├── _index.md
    │       │   │       │   ├── community.md
    │       │   │       │   └── contributing.md
    │       │   │       ├── images/
    │       │   │       ├── legacy-v1/
    │       │   │       │   ├── _index.md
    │       │   │       │   ├── getting-started.md
    │       │   │       │   ├── installation.md
    │       │   │       │   ├── overview.md
    │       │   │       │   ├── explanation/
    │       │   │       │   │   ├── _index.md
    │       │   │       │   │   └── fine-tuning.md
    │       │   │       │   ├── images/
    │       │   │       │   ├── reference/
    │       │   │       │   │   ├── _index.md
    │       │   │       │   │   ├── architecture.md
    │       │   │       │   │   ├── distributed-training.md
    │       │   │       │   │   └── fine-tuning.md
    │       │   │       │   └── user-guides/
    │       │   │       │       ├── _index.md
    │       │   │       │       ├── fine-tuning.md
    │       │   │       │       ├── jax.md
    │       │   │       │       ├── job-scheduling.md
    │       │   │       │       ├── mpi.md
    │       │   │       │       ├── paddle.md
    │       │   │       │       ├── prometheus.md
    │       │   │       │       ├── pytorch.md
    │       │   │       │       ├── tensorflow.md
    │       │   │       │       └── xgboost.md
    │       │   │       ├── operator-guides/
    │       │   │       │   ├── _index.md
    │       │   │       │   ├── installation.md
    │       │   │       │   └── migration.md
    │       │   │       └── user-guides/
    │       │   │           ├── _index.md
    │       │   │           └── pytorch.md
    │       │   ├── distributions/
    │       │   │   ├── _index.md
    │       │   │   └── list.md
    │       │   ├── external-add-ons/
    │       │   │   ├── _index.md
    │       │   │   ├── elyra/
    │       │   │   │   ├── _index.md
    │       │   │   │   ├── github.md
    │       │   │   │   ├── introduction.md
    │       │   │   │   ├── website.md
    │       │   │   │   └── images/
    │       │   │   ├── feast/
    │       │   │   │   ├── _index.md
    │       │   │   │   ├── github.md
    │       │   │   │   ├── introduction.md
    │       │   │   │   ├── website.md
    │       │   │   │   └── images/
    │       │   │   └── kserve/
    │       │   │       ├── _index.md
    │       │   │       ├── github.md
    │       │   │       ├── introduction.md
    │       │   │       ├── webapp.md
    │       │   │       ├── website.md
    │       │   │       └── pics/
    │       │   ├── images/
    │       │   │   ├── dashboard/
    │       │   │   ├── logos/
    │       │   │   ├── pipelines/
    │       │   │   │   ├── v1/
    │       │   │   │   │   └── v2-compatible/
    │       │   │   │   └── v2/
    │       │   │   │       └── run-comparison/
    │       │   │   └── v2/
    │       │   ├── releases/
    │       │   │   ├── OWNERS
    │       │   │   ├── _index.md
    │       │   │   ├── kubeflow-0.6.md
    │       │   │   ├── kubeflow-0.7.md
    │       │   │   ├── kubeflow-1.0.md
    │       │   │   ├── kubeflow-1.1.md
    │       │   │   ├── kubeflow-1.2.md
    │       │   │   ├── kubeflow-1.3.md
    │       │   │   ├── kubeflow-1.4.md
    │       │   │   ├── kubeflow-1.5.md
    │       │   │   ├── kubeflow-1.6.md
    │       │   │   ├── kubeflow-1.7.md
    │       │   │   ├── kubeflow-1.8.md
    │       │   │   └── kubeflow-1.9.md
    │       │   ├── started/
    │       │   │   ├── OWNERS
    │       │   │   ├── _index.md
    │       │   │   ├── architecture.md
    │       │   │   ├── introduction.md
    │       │   │   ├── kubeflow-examples.md
    │       │   │   ├── support.md
    │       │   │   ├── images/
    │       │   │   └── installing-kubeflow/
    │       │   │       ├── get_new_releases.sh
    │       │   │       ├── index.md
    │       │   │       └── release-info/
    │       │   │           ├── latest.json
    │       │   │           ├── v0.6.0.json
    │       │   │           ├── v0.6.1.json
    │       │   │           ├── v0.6.2.json
    │       │   │           ├── v1.0.1.json
    │       │   │           ├── v1.0.2.json
    │       │   │           ├── v1.1.0.json
    │       │   │           ├── v1.2.0.json
    │       │   │           ├── v1.3.0.json
    │       │   │           ├── v1.3.1.json
    │       │   │           ├── v1.4.0.json
    │       │   │           ├── v1.4.1.json
    │       │   │           ├── v1.5.0.json
    │       │   │           ├── v1.5.1.json
    │       │   │           ├── v1.6.0.json
    │       │   │           ├── v1.6.1.json
    │       │   │           ├── v1.7.0.json
    │       │   │           ├── v1.8.0.json
    │       │   │           ├── v1.8.1.json
    │       │   │           ├── v1.9.0.json
    │       │   │           └── v1.9.1.json
    │       │   └── videos/
    │       │       ├── taxi_custom_visualization.webm
    │       │       └── tfdv_example_with_taxi_pipeline.webm
    │       └── events/
    │           ├── OWNERS
    │           ├── _index.md
    │           ├── past-events/
    │           │   ├── _index.md
    │           │   ├── 2023/
    │           │   │   ├── _index.md
    │           │   │   ├── kubeflow-summit-2023.md
    │           │   │   └── watch--kubeflow-summit-2023.md
    │           │   └── 2024/
    │           │       ├── _index.md
    │           │       ├── gsoc-2024.md
    │           │       ├── watch--kubeflow-summit-2024.md
    │           │       └── watch--kubernetes-ai-day-2024.md
    │           └── upcoming-events/
    │               ├── _index.md
    │               ├── gsoc-2025.md
    │               └── kubeflow-summit-2025.md
    ├── i18n/
    │   └── en.toml
    ├── layouts/
    │   ├── 404.html
    │   ├── sitemap.xml
    │   ├── _default/
    │   │   ├── content.html
    │   │   └── _markup/
    │   │       └── render-codeblock-mermaid.html
    │   ├── _internal/
    │   │   ├── opengraph.html
    │   │   └── twitter_cards.html
    │   ├── docs/
    │   │   ├── baseof.html
    │   │   └── list.html
    │   ├── partials/
    │   │   ├── favicons.html
    │   │   ├── feedback.html
    │   │   ├── footer.html
    │   │   ├── head.html
    │   │   ├── math.html
    │   │   ├── mermaid.html
    │   │   ├── navbar-version-selector.html
    │   │   ├── navbar.html
    │   │   ├── page-meta-links.html
    │   │   ├── popper.html
    │   │   ├── scripts.html
    │   │   ├── seo_schema.html
    │   │   ├── sidebar-tree.html
    │   │   ├── social_image_generator.html
    │   │   └── swaggerui.html
    │   └── shortcodes/
    │       ├── alpha-status.html
    │       ├── beta-status.html
    │       ├── config-file-anthos.html
    │       ├── config-file-gcp-basic-auth.html
    │       ├── config-file-gcp-iap.html
    │       ├── config-file-ibm.html
    │       ├── config-file-istio-dex.html
    │       ├── config-file-k8s-istio.html
    │       ├── config-file-openshift.html
    │       ├── config-uri-anthos.html
    │       ├── config-uri-gcp-basic-auth.html
    │       ├── config-uri-gcp-iap.html
    │       ├── config-uri-ibm.html
    │       ├── config-uri-istio-dex.html
    │       ├── config-uri-k8s-istio.html
    │       ├── config-uri-openshift.html
    │       ├── kf-deployment-ui-version.html
    │       ├── kf-latest-version.html
    │       ├── kf-version-notice.html
    │       ├── kfp-v2-keywords.html
    │       ├── kubernetes-incompatible-versions.html
    │       ├── kubernetes-min-version.html
    │       ├── kubernetes-tested-version.html
    │       ├── kustomize-min-version.html
    │       ├── needs-update.html
    │       ├── no-index.html
    │       ├── note.html
    │       ├── oss-be-unsupported.html
    │       ├── params.html
    │       ├── pipelines-compatibility.html
    │       ├── stable-status.html
    │       ├── swaggerui-inline.html
    │       ├── tf-serving-version.html
    │       ├── aws/
    │       │   ├── OWNERS
    │       │   ├── kfctl-aws.html
    │       │   └── latest-version.html
    │       ├── azure/
    │       │   ├── config-uri-azure-oidc.html
    │       │   ├── config-uri-azure.html
    │       │   └── latest-version.html
    │       ├── blocks/
    │       │   ├── content-item.html
    │       │   ├── content-section.html
    │       │   ├── link-down.html
    │       │   ├── sample-section.html
    │       │   ├── tab.html
    │       │   └── tabs.html
    │       ├── canonical/
    │       │   ├── OWNERS
    │       │   └── latest-version.html
    │       ├── deploykf/
    │       │   ├── OWNERS
    │       │   └── latest-version.html
    │       ├── gke/
    │       │   ├── OWNERS
    │       │   └── latest-version.html
    │       ├── iks/
    │       │   ├── OWNERS
    │       │   └── latest-version.html
    │       ├── model-registry/
    │       │   ├── OWNERS
    │       │   └── latest-version.html
    │       ├── nutanix/
    │       │   ├── OWNERS
    │       │   └── latest-version.html
    │       ├── oracle/
    │       │   └── latest-version.html
    │       ├── pipelines/
    │       │   ├── OWNERS
    │       │   └── latest-version.html
    │       ├── qbo/
    │       │   └── latest-version.html
    │       ├── redhat/
    │       │   ├── OWNERS
    │       │   └── latest-version.html
    │       └── vmware/
    │           ├── OWNERS
    │           └── latest-version.html
    ├── scripts/
    │   ├── add_outdated_banner.py
    │   ├── kfp_nb_to_md.sh
    │   ├── nb_to_md.py
    │   ├── nb_to_md_test.py
    │   └── validate-urls.py
    ├── static/
    │   ├── browserconfig.xml
    │   ├── google65401334ad4c38b1.html
    │   ├── site.webmanifest
    │   └── images/
    ├── themes/
    │   └── docsy/
    └── .github/
        ├── issue_label_bot.yaml
        ├── pull_request_template.md
        ├── ISSUE_TEMPLATE/
        │   ├── BUG_REPORT.md
        │   ├── CHORE.md
        │   ├── FEATURE_REQUEST.md
        │   └── SUPPORT_REPORT.md
        └── workflows/
            ├── pr_title_check.yaml
            ├── stale.yaml
            └── triage_issues.yaml

================================================
File: README.md
================================================
# Kubeflow Website

[![Netlify Status](https://api.netlify.com/api/v1/badges/80644d22-0685-44d0-83bd-187d55464321/deploy-status)](https://app.netlify.com/sites/competent-brattain-de2d6d/deploys)

Welcome to the GitHub repository for Kubeflow's public website!

The docs website is hosted at https://www.kubeflow.org.

We use [Hugo](https://gohugo.io/) with the [google/docsy](https://github.com/google/docsy)
theme for styling and site structure, and [Netlify](https://www.netlify.com/) to manage the deployment of the site.

## Quickstart

Here's a quick guide to updating the docs:

1. Fork the [kubeflow/website repository](https://github.com/kubeflow/website) on GitHub.

2. Make your changes and send a pull request (PR).

3. If you're not yet ready for a review, add "WIP" to the PR name to indicate it's a work in progress. 
   Alternatively, you use the `/hold` [prow command](https://prow.k8s.io/command-help) in a comment to mark the PR as not ready for merge.

4. Wait for the automated PR workflow to do some checks. 
   When it's ready, you should see a comment like this: `deploy/netlify — Deploy preview ready!`

5. Click **Details** to the right of "Deploy preview ready" to see a preview of your updates.

6. Continue updating your doc and pushing your changes until you're happy with the content.

7. When you're ready for a review, add a comment to the PR, remove any holds or "WIP" markers, and assign a reviewer/approver. 
   See the [Kubeflow contributor guide](https://www.kubeflow.org/docs/about/contributing/).

If you need more help with the GitHub workflow, follow
this [guide to a standard GitHub workflow](https://github.com/kubeflow/website/blob/master/quick-github-guide.md).

## Local development

This section will show you how to develop the website locally, by running a local Hugo server.

### Install Hugo

To install Hugo, follow the [instructions for your system type](https://gohugo.io/getting-started/installing/).

**NOTE:** we recommend that you use Hugo version `0.124.1`, as this is currently the version we deploy to Netlify.

For example, using homebrew to install hugo on macOS or linux:

```bash
# WARNING: using `brew install hugo` will install the latest version of hugo
#          which may not be compatible with the website

# TIP: to install hugo run the following commands
HOMEBREW_COMMIT="9d025105a8be086b2eeb3b1b2697974f848dbaac" # 0.124.1
curl -fL -o "hugo.rb" "https://raw.githubusercontent.com/Homebrew/homebrew-core/${HOMEBREW_COMMIT}/Formula/h/hugo.rb"
brew install ./hugo.rb
brew pin hugo
```

### Install Node Packages

If you plan to make changes to the site styling, you need to install some **node libraries** as well.
(See the [Docsy setup guide](https://www.docsy.dev/docs/getting-started/#install-postcss) for more information)

You can install the same versions we use in Netlify (defined in `package.json`) with the following command:

```bash
npm install -D
```

### Run local hugo server

Follow the usual GitHub workflow of forking the repository on GitHub and then cloning your fork to your local machine.

1. **Fork** the [kubeflow/website repository](https://github.com/kubeflow/website) in the GitHub UI.

2. Clone your fork locally:

    ```bash
    git clone git@github.com:<your-github-username>/website.git
    cd website/
    ```

3. Initialize the Docsy submodule:

    ```bash
    git submodule update --init --recursive
    ```

4. Install Docsy dependencies:

    ```bash
    # NOTE: ensure you have node 18 installed
    (cd themes/docsy/ && npm install)
    ```

5. Start your local Hugo server:

    ```bash
    # NOTE: You should ensure that you are in the root directory of the repository.
    hugo server -D
    ```

6. You can access your website at [http://localhost:1313/](http://localhost:1313/)

### Useful docs

* [User guide for the Docsy theme](https://www.docsy.dev/docs/getting-started/)
* [Hugo installation guide](https://gohugo.io/getting-started/installing/)
* [Hugo basic usage](https://gohugo.io/getting-started/usage/)
* [Hugo site directory structure](https://gohugo.io/getting-started/directory-structure/)
* [hugo server reference](https://gohugo.io/commands/hugo_server/)

## Menu structure

The site theme has one Hugo menu (`main`), which defines the top navigation bar. You can find and adjust the definition
of the menu in the [site configuration file](https://github.com/kubeflow/website/blob/master/config.toml).

The left-hand navigation panel is defined by the directory structure under the [`docs` directory](https://github.com/kubeflow/website/tree/master/content/en/docs).

A `weight` property in the _front matter_ of each page determines the position of the page relative to the others in the same directory.
The lower the weight, the earlier the page appears in the section.

Here is an example `_index.md` file:

```md
+++
title = "Getting Started with Kubeflow"
description = "Overview"
weight = 1
+++
```

## Docsy Theme

We use the [Docsy](https://www.docsy.dev/) theme for the website. 
The theme files are managed with a [git submodule](https://git-scm.com/book/en/v2/Git-Tools-Submodules) in the `themes/docsy` directory.

**Do not change these files**, they are not actually inside this repo, but are part of the [google/docsy](https://github.com/google/docsy) repo.

To update referenced docsy commit, run the following command at the root of the repo:

```bash
# for example, to update docsy to v0.6.0
# WARNING: updating the docsy version will require you to update our overrides
#          check under: `layouts/partials` and `assets/scss`
git -C themes/docsy fetch --tags
git -C themes/docsy checkout tags/v0.6.0
```

## Documentation style guide

For guidance on writing effective documentation, see
the [style guide for the Kubeflow docs](https://kubeflow.org/docs/about/style-guide/).

## Styling your content

The theme holds its styles in the [`assets/scss` directory](https://github.com/kubeflow/website/tree/master/themes/docsy/assets/scss).

**Do not change these files**, they are not actually inside this repo, but are part of the [google/docsy](https://github.com/google/docsy) repo.

You can override the default styles and add new ones:

* In general, put your files in the project directory structure under `website` rather than in the theme directory. 
  Use the same file name as the theme does, and put the file in the same relative position.
  Hugo looks first at the file in the main project directories, if present, then at the files under the theme directory. 
  For example, the Kubeflow website's [`layouts/partials/navbar.html`](https://github.com/kubeflow/website/blob/master/layouts/partials/navbar.html)
  overrides the theme's [`layouts/partials/navbar.html`](https://github.com/kubeflow/website/blob/master/themes/docsy/layouts/partials/navbar.html)

* You can update the Kubeflow website's project variables in the [`_variables_project.scss` file](https://github.com/kubeflow/website/blob/master/assets/scss/_variables_project.scss).
  Values in that file override the [Docsy variables](https://github.com/kubeflow/website/blob/master/themes/docsy/assets/scss/_variables.scss). 
  You can also use `_variables_project.scss` to specify your own values for any of the default [Bootstrap 4 variables](https://getbootstrap.com/docs/4.0/getting-started/theming/).

* Custom styles [`_styles_project` file](https://github.com/kubeflow/website/blob/master/assets/scss/_styles_project.scss)

Styling of images:

* To see some examples of styled images, take a look at the [OAuth setup page](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/deploy/oauth-setup/) in the Kubeflow docs. 
  Search for `.png` in the [page source](https://raw.githubusercontent.com/GoogleCloudPlatform/kubeflow-gke-docs/main/content/en/docs/deploy/oauth-setup.md).

* For more help, see the guide to
  [Bootstrap image styling](https://getbootstrap.com/docs/4.0/content/images/).

* Also see the Bootstrap utilities, such as
  [borders](https://getbootstrap.com/docs/4.0/utilities/borders/).

The site's [front page](https://www.kubeflow.org/):

* See the [page source](https://github.com/kubeflow/website/blob/master/content/en/_index.html).

* The CSS styles are in the [project variables file](https://github.com/kubeflow/website/blob/master/assets/scss/_variables_project.scss).

* The page uses the [cover block](https://www.docsy.dev/docs/adding-content/shortcodes/#blocks-cover) defined by the theme.

* The page also uses the [linkdown block](https://www.docsy.dev/docs/adding-content/shortcodes/#blocks-link-down).

## Using Hugo shortcodes

Sometimes it's useful to define a snippet of information in one place and reuse it wherever we need it. 
For example, we want to be able to refer to the minimum version of various frameworks/libraries throughout the docs, 
without causing a maintenance nightmare.

For this purpose, we use Hugo's "shortcodes". 
Shortcodes are similar to Django variables. You define a shortcode in a file, then use a specific markup 
to invoke the shortcode in the docs. That markup is replaced by the content of the shortcode file when the page is built.

To create a shortcode:

1. Add an HTML file in the `/website/layouts/shortcodes/` directory. 
   The file name must be short and meaningful, as it determines the shortcode you and others use in the docs.

2. For the file content, add the text and HTML markup that should replace the shortcode markup when the web page is built.

To use a shortcode in a document, wrap the name of the shortcode in braces and percent signs like this:

  ```
  {{% shortcode-name %}}
  ```

The shortcode name is the file name minus the `.html` file extension.

**Example:** The following shortcode defines the minimum required version of Kubernetes:

* File name of the shortcode:

  ```
  kubernetes-min-version.html
  ```

* Content of the shortcode:

  ```
  1.8
  ```

* Usage in a document:

  ```
  You need Kubernetes version {{% kubernetes-min-version %}} or later.
  ```

Useful Hugo docs:

* [Shortcode templates](https://gohugo.io/templates/shortcode-templates/)
* [Shortcodes](https://gohugo.io/content-management/shortcodes/)

## Versioning of the docs

For each stable release, we create a new branch for the relevant documentation. 
For example, the documentation for the v0.2 stable release is maintained in the [v0.2-branch](https://github.com/kubeflow/website/tree/v0.2-branch).
Each branch has a corresponding Netlify website that automatically syncs each merged PR.

The versioned sites follow this convention:

* `www.kubeflow.org` always points to the current *master branch*
* `master.kubeflow.org` always points to GitHub head
* `vXXX-YYY.kubeflow.org` points to the release at vXXX.YYY-branch

We also hook up each version to the dropdown on the website menu bar. 


Whenever any documents reference any source code, you should use the version shortcode in the links, like so:

```
https://github.com/kubeflow/kubeflow/blob/{{< params "githubbranch" >}}/scripts/gke/deploy.sh
```

This ensures that all the links in a versioned webpage point to the correct branch.



================================================
File: LICENSE
================================================
Attribution 4.0 International

=======================================================================

Creative Commons Corporation ("Creative Commons") is not a law firm and
does not provide legal services or legal advice. Distribution of
Creative Commons public licenses does not create a lawyer-client or
other relationship. Creative Commons makes its licenses and related
information available on an "as-is" basis. Creative Commons gives no
warranties regarding its licenses, any material licensed under their
terms and conditions, or any related information. Creative Commons
disclaims all liability for damages resulting from their use to the
fullest extent possible.

Using Creative Commons Public Licenses

Creative Commons public licenses provide a standard set of terms and
conditions that creators and other rights holders may use to share
original works of authorship and other material subject to copyright
and certain other rights specified in the public license below. The
following considerations are for informational purposes only, are not
exhaustive, and do not form part of our licenses.

     Considerations for licensors: Our public licenses are
     intended for use by those authorized to give the public
     permission to use material in ways otherwise restricted by
     copyright and certain other rights. Our licenses are
     irrevocable. Licensors should read and understand the terms
     and conditions of the license they choose before applying it.
     Licensors should also secure all rights necessary before
     applying our licenses so that the public can reuse the
     material as expected. Licensors should clearly mark any
     material not subject to the license. This includes other CC-
     licensed material, or material used under an exception or
     limitation to copyright. More considerations for licensors:
	wiki.creativecommons.org/Considerations_for_licensors

     Considerations for the public: By using one of our public
     licenses, a licensor grants the public permission to use the
     licensed material under specified terms and conditions. If
     the licensor's permission is not necessary for any reason--for
     example, because of any applicable exception or limitation to
     copyright--then that use is not regulated by the license. Our
     licenses grant only permissions under copyright and certain
     other rights that a licensor has authority to grant. Use of
     the licensed material may still be restricted for other
     reasons, including because others have copyright or other
     rights in the material. A licensor may make special requests,
     such as asking that all changes be marked or described.
     Although not required by our licenses, you are encouraged to
     respect those requests where reasonable. More_considerations
     for the public: 
	wiki.creativecommons.org/Considerations_for_licensees

=======================================================================

Creative Commons Attribution 4.0 International Public License

By exercising the Licensed Rights (defined below), You accept and agree
to be bound by the terms and conditions of this Creative Commons
Attribution 4.0 International Public License ("Public License"). To the
extent this Public License may be interpreted as a contract, You are
granted the Licensed Rights in consideration of Your acceptance of
these terms and conditions, and the Licensor grants You such rights in
consideration of benefits the Licensor receives from making the
Licensed Material available under these terms and conditions.


Section 1 -- Definitions.

  a. Adapted Material means material subject to Copyright and Similar
     Rights that is derived from or based upon the Licensed Material
     and in which the Licensed Material is translated, altered,
     arranged, transformed, or otherwise modified in a manner requiring
     permission under the Copyright and Similar Rights held by the
     Licensor. For purposes of this Public License, where the Licensed
     Material is a musical work, performance, or sound recording,
     Adapted Material is always produced where the Licensed Material is
     synched in timed relation with a moving image.

  b. Adapter's License means the license You apply to Your Copyright
     and Similar Rights in Your contributions to Adapted Material in
     accordance with the terms and conditions of this Public License.

  c. Copyright and Similar Rights means copyright and/or similar rights
     closely related to copyright including, without limitation,
     performance, broadcast, sound recording, and Sui Generis Database
     Rights, without regard to how the rights are labeled or
     categorized. For purposes of this Public License, the rights
     specified in Section 2(b)(1)-(2) are not Copyright and Similar
     Rights.

  d. Effective Technological Measures means those measures that, in the
     absence of proper authority, may not be circumvented under laws
     fulfilling obligations under Article 11 of the WIPO Copyright
     Treaty adopted on December 20, 1996, and/or similar international
     agreements.

  e. Exceptions and Limitations means fair use, fair dealing, and/or
     any other exception or limitation to Copyright and Similar Rights
     that applies to Your use of the Licensed Material.

  f. Licensed Material means the artistic or literary work, database,
     or other material to which the Licensor applied this Public
     License.

  g. Licensed Rights means the rights granted to You subject to the
     terms and conditions of this Public License, which are limited to
     all Copyright and Similar Rights that apply to Your use of the
     Licensed Material and that the Licensor has authority to license.

  h. Licensor means the individual(s) or entity(ies) granting rights
     under this Public License.

  i. Share means to provide material to the public by any means or
     process that requires permission under the Licensed Rights, such
     as reproduction, public display, public performance, distribution,
     dissemination, communication, or importation, and to make material
     available to the public including in ways that members of the
     public may access the material from a place and at a time
     individually chosen by them.

  j. Sui Generis Database Rights means rights other than copyright
     resulting from Directive 96/9/EC of the European Parliament and of
     the Council of 11 March 1996 on the legal protection of databases,
     as amended and/or succeeded, as well as other essentially
     equivalent rights anywhere in the world.

  k. You means the individual or entity exercising the Licensed Rights
     under this Public License. Your has a corresponding meaning.


Section 2 -- Scope.

  a. License grant.

       1. Subject to the terms and conditions of this Public License,
          the Licensor hereby grants You a worldwide, royalty-free,
          non-sublicensable, non-exclusive, irrevocable license to
          exercise the Licensed Rights in the Licensed Material to:

            a. reproduce and Share the Licensed Material, in whole or
               in part; and

            b. produce, reproduce, and Share Adapted Material.

       2. Exceptions and Limitations. For the avoidance of doubt, where
          Exceptions and Limitations apply to Your use, this Public
          License does not apply, and You do not need to comply with
          its terms and conditions.

       3. Term. The term of this Public License is specified in Section
          6(a).

       4. Media and formats; technical modifications allowed. The
          Licensor authorizes You to exercise the Licensed Rights in
          all media and formats whether now known or hereafter created,
          and to make technical modifications necessary to do so. The
          Licensor waives and/or agrees not to assert any right or
          authority to forbid You from making technical modifications
          necessary to exercise the Licensed Rights, including
          technical modifications necessary to circumvent Effective
          Technological Measures. For purposes of this Public License,
          simply making modifications authorized by this Section 2(a)
          (4) never produces Adapted Material.

       5. Downstream recipients.

            a. Offer from the Licensor -- Licensed Material. Every
               recipient of the Licensed Material automatically
               receives an offer from the Licensor to exercise the
               Licensed Rights under the terms and conditions of this
               Public License.

            b. No downstream restrictions. You may not offer or impose
               any additional or different terms or conditions on, or
               apply any Effective Technological Measures to, the
               Licensed Material if doing so restricts exercise of the
               Licensed Rights by any recipient of the Licensed
               Material.

       6. No endorsement. Nothing in this Public License constitutes or
          may be construed as permission to assert or imply that You
          are, or that Your use of the Licensed Material is, connected
          with, or sponsored, endorsed, or granted official status by,
          the Licensor or others designated to receive attribution as
          provided in Section 3(a)(1)(A)(i).

  b. Other rights.

       1. Moral rights, such as the right of integrity, are not
          licensed under this Public License, nor are publicity,
          privacy, and/or other similar personality rights; however, to
          the extent possible, the Licensor waives and/or agrees not to
          assert any such rights held by the Licensor to the limited
          extent necessary to allow You to exercise the Licensed
          Rights, but not otherwise.

       2. Patent and trademark rights are not licensed under this
          Public License.

       3. To the extent possible, the Licensor waives any right to
          collect royalties from You for the exercise of the Licensed
          Rights, whether directly or through a collecting society
          under any voluntary or waivable statutory or compulsory
          licensing scheme. In all other cases the Licensor expressly
          reserves any right to collect such royalties.


Section 3 -- License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the
following conditions.

  a. Attribution.

       1. If You Share the Licensed Material (including in modified
          form), You must:

            a. retain the following if it is supplied by the Licensor
               with the Licensed Material:

                 i. identification of the creator(s) of the Licensed
                    Material and any others designated to receive
                    attribution, in any reasonable manner requested by
                    the Licensor (including by pseudonym if
                    designated);

                ii. a copyright notice;

               iii. a notice that refers to this Public License;

                iv. a notice that refers to the disclaimer of
                    warranties;

                 v. a URI or hyperlink to the Licensed Material to the
                    extent reasonably practicable;

            b. indicate if You modified the Licensed Material and
               retain an indication of any previous modifications; and

            c. indicate the Licensed Material is licensed under this
               Public License, and include the text of, or the URI or
               hyperlink to, this Public License.

       2. You may satisfy the conditions in Section 3(a)(1) in any
          reasonable manner based on the medium, means, and context in
          which You Share the Licensed Material. For example, it may be
          reasonable to satisfy the conditions by providing a URI or
          hyperlink to a resource that includes the required
          information.

       3. If requested by the Licensor, You must remove any of the
          information required by Section 3(a)(1)(A) to the extent
          reasonably practicable.

       4. If You Share Adapted Material You produce, the Adapter's
          License You apply must not prevent recipients of the Adapted
          Material from complying with this Public License.


Section 4 -- Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that
apply to Your use of the Licensed Material:

  a. for the avoidance of doubt, Section 2(a)(1) grants You the right
     to extract, reuse, reproduce, and Share all or a substantial
     portion of the contents of the database;

  b. if You include all or a substantial portion of the database
     contents in a database in which You have Sui Generis Database
     Rights, then the database in which You have Sui Generis Database
     Rights (but not its individual contents) is Adapted Material; and

  c. You must comply with the conditions in Section 3(a) if You Share
     all or a substantial portion of the contents of the database.

For the avoidance of doubt, this Section 4 supplements and does not
replace Your obligations under this Public License where the Licensed
Rights include other Copyright and Similar Rights.


Section 5 -- Disclaimer of Warranties and Limitation of Liability.

  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE
     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS
     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF
     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,
     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,
     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR
     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,
     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT
     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT
     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.

  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE
     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,
     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,
     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,
     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR
     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN
     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR
     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR
     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.

  c. The disclaimer of warranties and limitation of liability provided
     above shall be interpreted in a manner that, to the extent
     possible, most closely approximates an absolute disclaimer and
     waiver of all liability.


Section 6 -- Term and Termination.

  a. This Public License applies for the term of the Copyright and
     Similar Rights licensed here. However, if You fail to comply with
     this Public License, then Your rights under this Public License
     terminate automatically.

  b. Where Your right to use the Licensed Material has terminated under
     Section 6(a), it reinstates:

       1. automatically as of the date the violation is cured, provided
          it is cured within 30 days of Your discovery of the
          violation; or

       2. upon express reinstatement by the Licensor.

     For the avoidance of doubt, this Section 6(b) does not affect any
     right the Licensor may have to seek remedies for Your violations
     of this Public License.

  c. For the avoidance of doubt, the Licensor may also offer the
     Licensed Material under separate terms or conditions or stop
     distributing the Licensed Material at any time; however, doing so
     will not terminate this Public License.

  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public
     License.


Section 7 -- Other Terms and Conditions.

  a. The Licensor shall not be bound by any additional or different
     terms or conditions communicated by You unless expressly agreed.

  b. Any arrangements, understandings, or agreements regarding the
     Licensed Material not stated herein are separate from and
     independent of the terms and conditions of this Public License.


Section 8 -- Interpretation.

  a. For the avoidance of doubt, this Public License does not, and
     shall not be interpreted to, reduce, limit, restrict, or impose
     conditions on any use of the Licensed Material that could lawfully
     be made without permission under this Public License.

  b. To the extent possible, if any provision of this Public License is
     deemed unenforceable, it shall be automatically reformed to the
     minimum extent necessary to make it enforceable. If the provision
     cannot be reformed, it shall be severed from this Public License
     without affecting the enforceability of the remaining terms and
     conditions.

  c. No term or condition of this Public License will be waived and no
     failure to comply consented to unless expressly agreed to by the
     Licensor.

  d. Nothing in this Public License constitutes or may be interpreted
     as a limitation upon, or waiver of, any privileges and immunities
     that apply to the Licensor or You, including from the legal
     processes of any jurisdiction or authority.


=======================================================================

Creative Commons is not a party to its public
licenses. Notwithstanding, Creative Commons may elect to apply one of
its public licenses to material it publishes and in those instances
will be considered the "Licensor." The text of the Creative Commons
public licenses is dedicated to the public domain under the CC0 Public
Domain Dedication. Except for the limited purpose of indicating that
material is shared under a Creative Commons public license or as
otherwise permitted by the Creative Commons policies published at
creativecommons.org/policies, Creative Commons does not authorize the
use of the trademark "Creative Commons" or any other trademark or logo
of Creative Commons without its prior written consent including,
without limitation, in connection with any unauthorized modifications
to any of its public licenses or any other arrangements,
understandings, or agreements concerning use of licensed material. For
the avoidance of doubt, this paragraph does not form part of the
public licenses.

Creative Commons may be contacted at creativecommons.org.



================================================
File: OWNERS
================================================
approvers:
  - andreyvelich
  - franciscojavierarceo
  - juliusvonkohout 
  - johnugeorge
  - terrytangyuan
  - zijianjoy

reviewers:
  - varodrig

emeritus_approvers:
  - james-jwu
  - jbottum




================================================
File: config.toml
================================================
baseURL = "/"
title = "Kubeflow"
description = "Kubeflow makes deployment of ML Workflows on Kubernetes straightforward and automated"

enableRobotsTXT = true

theme = ["docsy"]

###############################################################################
# Docsy
###############################################################################
enableGitInfo = true

# language settings
contentDir = "content/en"
defaultContentLanguage = "en"
# tell Hugo not to include the /en/ element in the URL path for English docs
defaultContentLanguageInSubdir = false
# useful when translating
enableMissingTranslationPlaceholders = true
# disable taxonomies
disableKinds = ["taxonomy"]
# deprecated directories
ignoreFiles = []

###############################################################################
# Hugo - Top-level navigation (horizontal)
###############################################################################
[menu]
  [[menu.main]]
    name = "Kubeflow Summit"
    weight = -1000
    pre = "<i class='fas fa-calendar pr-2' style='color: #FFC107'></i>"
    post = "<br><span class='badge badge-warning'>April 1st, 2025</span> <span class='badge badge-warning'>London, England</span> "
    url = "https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/co-located-events/kubeflow-summit/"
  [[menu.main]]
    name = "GSoC 2025"
    weight = -900
    pre = "<i class='fas pr-2'><img src='/docs/images/logos/gsoc.svg' style='height: 1.22em;'></i>"
      post = "<br><span class='badge badge-info'>Coming Soon</span>"
    url = "/events/gsoc-2025/"
  [[menu.main]]
    name = "Docs"
    weight = -102
    pre = "<i class='fas fa-book pr-2'></i>"
    url = "/docs/"
  [[menu.main]]
    name = "Events"
    weight = -101
    pre = "<i class='fas fa-calendar pr-2'></i>"
    url = "/events/"
  [[menu.main]]
    name = "Blog"
    weight = -100
    pre = "<i class='fas fa-rss pr-2'></i>"
    url = "https://blog.kubeflow.org/"
  [[menu.main]]
    name = "GitHub"
    weight = -99
    pre = "<i class='fab fa-github pr-2'></i>"
    url = "https://github.com/kubeflow/"

###############################################################################
# Docsy - Output Formats
###############################################################################
[outputs]
section = [ "HTML" ]

###############################################################################
# Docsy - Goldmark markdown parser
###############################################################################
[markup]
  [markup.goldmark]
    [markup.goldmark.renderer]
      unsafe = true

    # For MathJax
    # https://gohugo.io/content-management/mathematics/#setup
    [markup.goldmark.extensions]
      [markup.goldmark.extensions.passthrough]
        enable = true
        [markup.goldmark.extensions.passthrough.delimiters]
          block = [['\[', '\]'], ['$$', '$$']]
          inline = [['\(', '\)']]

  [markup.highlight]
    # See a complete list of available styles at https://xyproto.github.io/splash/docs/all.html
    style = "tango"
    guessSyntax = "true"

###############################################################################
# Docsy - DrawIO configuration
###############################################################################
[params.drawio]
enable = true

###############################################################################
# Docsy - Image processing configuration
###############################################################################
[imaging]
  resampleFilter = "CatmullRom"
  quality = 75
  anchor = "smart"

###############################################################################
# Docsy - Services configuration
###############################################################################
[services]
  [services.googleAnalytics]
  id = "G-Y2KDEK0998"

###############################################################################
# Docsy - Language configuration
###############################################################################
[languages]
  [languages.en]
    title = "Kubeflow"
    languageName ="English"
    weight = 1

    [languages.en.params]
      description = "Kubeflow makes deployment of ML Workflows on Kubernetes straightforward and automated"

###############################################################################
# Docsy - Site Parameters
###############################################################################
[params]
  github_repo = "https://github.com/kubeflow/website"
  github_project_repo = "https://github.com/kubeflow/kubeflow"

  RSSLink = "/index.xml"
  author = "kubeflow.org"
  github = "kubeflow"

  copyright = "The Kubeflow Authors."
  privacy_policy = "https://policies.google.com/privacy"
  trademark = "https://www.linuxfoundation.org/trademark-usage/"

  # Google Custom Search Engine ID.
  gcs_engine_id = "007239566369470735695:624rglujm-w"

  # Text label for the version menu in the top bar of the website.
  version_menu = "Version"

  # The major.minor version tag for the version of the docs represented in this
  # branch of the repository. Used in the "version-banner" partial to display a
  # version number for this doc set.
  version = "master"

  # Flag used in the "version-banner" partial to decide whether to display a
  # banner on every page indicating that this is an archived version of the docs.
  archived_version = false

  # A link to latest version of the docs. Used in the "version-banner" partial to
  # point people to the main doc site.
  url_latest_version = "https://kubeflow.org/docs/"

  # A variable used in various docs to determine URLs for config files etc.
  # To find occurrences, search the repo for 'params "githubbranch"'.
  github_branch = "master"

  # Disable MathJax by default
  # NOTE: enable it per-page with `mathjax: true` in front matter
  mathjax = false

  # Disable Swagger UI by default
  # NOTE: enable it per-page with `swaggerui: true` in front matter
  swaggerui = false

  # Social media accounts
  [params.social]

    # Twitter account (used to set `twitter:site` in the SEO partial)
    twitter = "kubeflow"

  # These entries appear in the drop-down menu at the top of the website.
  [[params.versions]]
    version = "master"
    githubbranch = "master"
    url = "https://master.kubeflow.org"
  [[params.versions]]
    version = "v1.9"
    githubbranch = "v1.9-branch"
    url = "https://v1-9-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.8"
    githubbranch = "v1.8-branch"
    url = "https://v1-8-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.7"
    githubbranch = "v1.7-branch"
    url = "https://v1-7-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.6"
    githubbranch = "v1.6-branch"
    url = "https://v1-6-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.5"
    githubbranch = "v1.5-branch"
    url = "https://v1-5-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.4"
    githubbranch = "v1.4-branch"
    url = "https://v1-4-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.3"
    githubbranch = "v1.3-branch"
    url = "https://v1-3-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.2"
    githubbranch = "v1.2-branch"
    url = "https://v1-2-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.1"
    githubbranch = "v1.1-branch"
    url = "https://v1-1-branch.kubeflow.org"
  [[params.versions]]
    version = "v1.0"
    githubbranch = "v1.0-branch"
    url = "https://v1-0-branch.kubeflow.org"
  [[params.versions]]
    version = "v0.7"
    githubbranch = "v0.7-branch"
    url = "https://v0-7.kubeflow.org"
  [[params.versions]]
    version = "v0.6"
    githubbranch = "v0.6-branch"
    url = "https://v0-6.kubeflow.org"
  [[params.versions]]
    version = "v0.5"
    githubbranch = "v0.5-branch"
    url = "https://v0-5.kubeflow.org"
  [[params.versions]]
    version = "v0.4"
    githubbranch = "v0.4-branch"
    url = "https://v0-4.kubeflow.org"
  [[params.versions]]
    version = "v0.3"
    githubbranch = "v0.3-branch"
    url = "https://v0-3.kubeflow.org"

  # User interface configuration
  [params.ui]
    # Enable the logo
    navbar_logo = true
    # Enable to show the side bar menu in its compact state.
    sidebar_menu_compact = true
    # Enable the search box in the side bar.
    sidebar_search_disable = false
    # Set to true to disable breadcrumb navigation.
    breadcrumb_disable = false
    # Show expand/collapse icon for sidebar sections
    sidebar_menu_foldable = true
    # Disable about button in footer
    footer_about_disable = true

      # Adds a H2 section titled "Feedback" to the bottom of each doc. The responses are sent to Google Analytics as events.
      # This feature depends on [services.googleAnalytics] and will be disabled if "services.googleAnalytics.id" is not set.
      # If you want this feature, but occasionally need to remove the "Feedback" section from a single page,
      # add "hide_feedback: true" to the page's front matter.
      [params.ui.feedback]
        enable = true
        # The responses that the user sees after clicking "yes" (the page was helpful) or "no" (the page was not helpful).
        # NOTE: the actual content of the responses is set in the "layouts/partials/feedback.html" file.
        yes = ""
        no = ""

  # Links in footer
  [params.links]
    [[params.links.user]]
      name = "Twitter"
      url = "https://twitter.com/kubeflow/"
      icon = "fab fa-twitter"
      desc = "Follow us on Twitter to get the latest news!"
    [[params.links.user]]
      name = "Kubeflow Slack"
      url = "/docs/about/community/#kubeflow-slack-channels"
      icon = "fab fa-slack"
      desc = "Join the Kubeflow Slack Workspace!"
    [[params.links.user]]
      name = "Kubeflow Mailing List"
      url = "/docs/about/community/#kubeflow-mailing-list"
      icon = "fa fa-envelope"
      desc = "Join the Kubeflow Mailing List!"
    [[params.links.user]]
      name = "Kubeflow Community Calendars"
      url = "/docs/about/community/#kubeflow-community-meetings"
      icon = "fa fa-calendar"
      desc = "View the Kubeflow Community Calendars!"



================================================
File: netlify.toml
================================================
[build]
  publish = "public"
  # setting baseURL is required for `PAGE.Permalink` to correctly include the full `https://DOMAIN/` prefix
  # which is required for things like `twitter:image` meta tags to work correctly
  command = "cd themes/docsy && git submodule update -f --init && npm install && cd ../.. && hugo --gc --minify"

[context.deploy-preview]
  # for deploy previews, the domain will be `https://deploy-preview-PR_NUMBER--competent-brattain-de2d6d.netlify.app/`
  # which can be read from the `DEPLOY_PRIME_URL` environment variable in Netlify
  command = "cd themes/docsy && git submodule update -f --init && npm install && cd ../.. && hugo --gc --minify --baseURL $DEPLOY_PRIME_URL"

[context.production]
  # for production, the domain will be `https://www.kubeflow.org/`
  command = "cd themes/docsy && git submodule update -f --init && npm install && cd ../.. && hugo --gc --minify --baseURL https://www.kubeflow.org/"

[[context.deploy-preview.plugins]]
package = "netlify-plugin-checklinks"

  [context.deploy-preview.plugins.inputs]

  # If a link contains these patterns it will be ignored.
  skipPatterns = [
    # ignore absolute links, which are only found in tags like `<meta property="og:url" content="URL_HERE">`
    # https://github.com/Munter/netlify-plugin-checklinks/issues/388
    "competent-brattain-de2d6d.netlify.app",
    "public/docs/examples/shared-resources",
  ]

[context.deploy-preview.environment]
  HUGO_ENV = "development"
  HUGO_VERSION = "0.124.1"
  NODE_VERSION = "18"

[context.production.environment]
  HUGO_ENV = "production"
  HUGO_VERSION = "0.124.1"
  NODE_VERSION = "18"

[context.branch-deploy.environment]
  HUGO_ENV = "development"
  HUGO_VERSION = "0.124.1"
  NODE_VERSION = "18"



================================================
File: package.json
================================================
{
  "name": "website",
  "devDependencies": {
    "autoprefixer": "^10.4.0",
    "netlify-plugin-checklinks": "^4.1.1",
    "postcss": "^8.4.1",
    "postcss-cli": "^9.1.0"
  }
}



================================================
File: prow_config.yaml
================================================
workflows: []


================================================
File: quick-github-guide.md
================================================
# Quick guide to working with a GitHub repository

This page is intended for people who want to update the
[Kubeflow docs](https://www.kubeflow.org/docs/) and
who don't use Git or GitHub often. The page gives you a quick guide to
get going with a GitHub repository, using either the GitHub web user interface 
(UI) or Git on the command line.

## Using the GitHub web UI

**Note:** The GitHub web UI is suitable for quick updates to a single file. If
your update is more complex or you need to update more than one file within one
pull request (PR), then the command line provides a better experience.

Follow these steps to edit a page using the GitHub UI:

1. Sign in to GitHub if you haven't yet done so.

1. Go to the page that you want to edit on the
  [Kubeflow website](https://www.kubeflow.org/docs/).

1. Click **Edit this page**.

1. If this is the first time you're updating a file in the Kubeflow
  website repository, a screen opens asking you to *fork* the repository. A
  fork is a copy of the repository where you can make your updates before
  submitting them for review. You only have to fork the repository once:

    * Click **Fork this repository**.
    * If GitHub asks you **Where should we fork website** and offers your
      username as an option, click the link on your username.	
    * Wait a few seconds while GitHub makes a copy of the repository at	
     `https://github.com/yourusername/website`. This copy is your *fork*	
     of the `kubeflow/website` repository.

1. The GitHub editor interface opens for the selected page.
  Make your updates to the content.

1. Click **Preview changes** at the top of the editing area to see the effect of your changes.

1. If you need to make more changes, click **Edit file** at the top of the preview area.

1. When you are ready to submit your changes, scroll down to the 
  **Propose file change**
   section at the bottom of the editing area.

      * Enter a short description of your update. This short description becomes the 
        title of your pull request (PR).

      * In the second text box (for the extended description), enter a more detailed
        description.

1. Click **Propose file change**. A new screen appears, offering you the 
  opportunity to open a pull request.

1. Click **Create pull request**. 

1. Optionally, edit the pull request title and description.

1. Make sure **Allow edits from maintainers** remains checked.

1. Click **Create pull request** again. You have now sent a request to the repository
  maintainers to review your change.

1. Check the online preview of your changes:

      * Wait for the automated PR workflow to do some checks. When it's ready,
        you should see a comment like this: **deploy/netlify — Deploy preview ready!**
      * Click **Details** to the right of "Deploy preview ready" to see a preview
        of your updates.

## Using the command line

Here's a quick guide to a fairly standard GitHub workflow using Git on the command line:

1. Fork the kubeflow/website repository:

    * Go to the [kubeflow/website 
      repository](https://github.com/kubeflow/website) on GitHub.
    * Click **Fork** to make your own copy of the repository. GitHub creates a 
      copy at `https://github.com/<your-github-username>/website`.

1. Open a command window on your local machine.

1. Clone your forked repository, to copy the files down to your local machine.
  This example creates a directory called `kubeflow` and uses SSH cloning to
  download the files:

    ```
    mkdir kubeflow
    cd kubeflow/
    git clone git@github.com:<your-github-username>/website.git
    cd website/
    ```

1. Add the upstream repository as a Git remote repository:

    ```
    git remote add upstream https://github.com/kubeflow/website.git
    ```

1. Check your remotes:

    ```
    git remote -vv
    ```

    You should have 2 remote repositories:

      -  `origin` - points to your own fork of the repository on gitHub -
         that is, the one you cloned your local repository from.
      -  `upstream` - points to the actual repository on gitHub.

1. Create a branch. In this example, replace `doc-updates` with any branch name
  you like. Choose a branch name that helps you recognize the updates you plan
  to make in that branch:

    ```
    git checkout -b doc-updates
    ```

1. Add and edit the files as you like. The doc pages are in the
  `/website/content/docs/` directory.

1. Run `git status` at any time, to check the status of your local files.
  Git tells you which files need adding or committing to your local repository.

1. Commit your updated files to your local Git repository. Example commit:

    ```
    git commit -a -m "Fixed some doc errors."
    ```

    Or:

    ```
    git add add-this-doc.md
    git commit -a -m "Added a shiny new doc."
    ```

1. Push from your branch (for example, `doc-updates`) to **the relevant branch
  on your fork on GitHub:**

    ```
    git checkout doc-updates
    git push origin doc-updates
    ```

1. When you're ready to start the review process, create a pull request (PR)
  **in the branch** on **your fork** on the GitHub UI, based on the above push.
  The PR is auto-sent to the upstream repository - that is, the one you forked 
  from.

1. If you need to change the files in your PR, continue changing them
  locally in the same branch, then push them again in the same way. GitHub
  automatically sends them through to the same PR on the upstream repository!

1. **Hint:** If you're authenticating to GitHub via SSH, use `ssh-add` to add
  your SSH key passphrase to the managing agent, so that you don't have to
  keep authenticating to GitHub. You need to do this again after every reboot.

## More information

For further information about the GitHub workflow, refer to the
[GitHub guide to pull requests](https://help.github.com/en/articles/creating-a-pull-request).


================================================
File: .hugo_build.lock
================================================



================================================
File: assets/fonts/Inter-Medium.ttf
================================================
[Non-text file]



================================================
File: assets/scss/_styles_project.scss
================================================
// --------------------------------------------------
// popover styles
// --------------------------------------------------
.popover-header {
  color: #fff;
  font-weight: 600;
  background-color: $dark;
  padding: 0.75rem;
}

// --------------------------------------------------
// reduce padding for small screens
// --------------------------------------------------
@include media-breakpoint-down(md) {
  .td-main {
    main {
      padding-top: 1rem;
    }
  }
}

// --------------------------------------------------
// increase 80% content width limit for large screens
// --------------------------------------------------
.td-max-width-on-larger-screens {
    @include media-breakpoint-up(lg) {
        max-width: 90%;
    }
}

// --------------------------------------------------
// color the sidebars darker than main content area
// --------------------------------------------------
@include media-breakpoint-down(sm) {
  .td-main {
    padding-left: 0;
    padding-right: 0;
  }
  .td-sidebar {
    background-color: $td-sidebar-bg-color;
  }
}
@include media-breakpoint-up(md) {
  .td-outer {
    background-color: $td-sidebar-bg-color;
    height: unset;
  }
  .td-sidebar, .td-sidebar-toc {
    background-color: unset;
  }
  .td-main {
    padding-left: 0;
    padding-right: 0;
  }
  main {
    background-color: #ffffff;
  }
}

// --------------------------------------------------
// add border below search on small devices
// --------------------------------------------------
@include media-breakpoint-down(md) {
   .td-sidebar {
     border-bottom: 0.1rem solid $border-color;
   }
}

// --------------------------------------------------
// fix positions
// --------------------------------------------------
@include media-breakpoint-up(md) {
  .td-main {
    .td-main_inner {
      // note, this is required to ensure the footer not visible for short pages
      // which is important to prevent the "search" and "toc" sidebars from being
      // underneath the the header
      min-height: 100vh;

      // ensure the background color is not visible because of left-right padding
      // note that there is a default left-right margin of -15px, which we are
      // selectively overwriting depending on the screen width
      //  - there is a left sidebar for md and above
      //  - there is a right sidebar for xl and above
      margin-left: 0;
      @include media-breakpoint-up(xl) {
        margin-right: 0;
      }
    }

    // left-hand sidebar
    .td-sidebar {
      padding-left: 0;
      padding-right: 0.25rem;
      padding-top: 7rem;
      padding-bottom: 0;

      .td-sidebar__inner {
        position: sticky;
        top: 7rem;
        height: calc(100vh - 7rem);

        padding-left: 0;
        padding-right: 0;
      }
    }

    // main content area
    main {
      padding-top: 7rem;
      padding-bottom: 3rem;
    }

    // right hand sidebar
    .td-sidebar-toc {
      height: unset;
      padding-left: 0;
      padding-right: 0;
      padding-top: 7rem;
      padding-bottom: 0;

      // the default theme makes the outer div stickey, but we use the inner div
      position: relative;
      top: unset;
      overflow-y: unset;

      .td-sidebar-toc__inner {
        position: sticky;
        top: 7rem;
        height: calc(100vh - 7rem);

        padding-left: 1rem;
        padding-right: 1.5rem;

        // allow the toc to scroll
        overflow-y: auto;
      }

      // add separator line before toc list
      .td-toc {
        padding-top: 1rem;
        border-top: 0.1rem solid $border-color;

        // fix the padding of the toc list
        ul {
          margin-bottom: 0;
        }
        a {
          padding-top: 0.2rem;
          padding-bottom: 0.2rem;
        }
      }
    }
  }
}

// --------------------------------------------------
// prevent sidebar from appearing over the footer
// --------------------------------------------------
footer {
  z-index: 1000;
}

// --------------------------------------------------
// remove whitespace after footer
// --------------------------------------------------
footer {
  min-height: auto;

  p {
    margin-bottom: 0.4rem;
    line-height: 1.0rem;
  }

  small {
    font-size: 0.7rem;
  }
}

// --------------------------------------------------
// sidebar styling
// --------------------------------------------------
.td-sidebar {
  padding-left: 0.5rem;
  padding-right: 0.5rem;
  padding-bottom: 0.5rem;

  @include media-breakpoint-down(md) {
    padding-top: 2rem;
  }
}

// sidebar search styling
.td-sidebar__search {
  padding-top: 0;
  padding-bottom: 0.5rem;

  margin: 0;

  .td-search {
    width: 100%;
  }

  @include media-breakpoint-down(md) {
    padding-left: 0.75rem;
    padding-right: 0.75rem;
    .td-search {
      padding-right: 0.25rem;
    }
  }

  @include media-breakpoint-up(md) {
    padding-left: 0;
    padding-right: 0;
    .td-search {
      padding-right: 0.5rem;
    }
  }
}

.td-sidebar-nav {
  padding-left: 0;
  padding-right: 0.75rem;
  padding-top: 0;
  padding-bottom: 2rem;

  margin-left: 0;
  margin-right: 0;
  margin-top: 1rem;
  margin-bottom: 0;

  // extra padding on mobile (in collapsible menu)
  @include media-breakpoint-down(sm) {
    padding-left: 1rem;
    padding-right: 1rem;
    padding-bottom: 1.5rem;

    margin-top: 0.5rem;
  }
}

// reduce padding for the root level of the sidebar
nav.foldable-nav .with-child, nav.foldable-nav .without-child {
  padding-left: 0;
  position: unset;
}

// style the sidebar like a folder tree
nav.foldable-nav {

  // apply styles to the main sidebar sections
  .ul-1 {

    // reduce padding of the sidebar links
    .with-child {
      padding-left: 1.5rem;
      position: relative;
    }
    .without-child {
      padding-left: 0.7rem;
      position: relative;
    }

    // make the active link bold
    a.active {
      color: $primary !important;
      font-weight: 700 !important;
    }

    // bold the chain of pages leading to the active page
    .with-child.active-path > label a {
      font-weight: 700;
    }

    // show a border on the left of open sections
    .with-child > input:checked ~ ul {
      border-left: .1rem dotted $gray-400;

      // round the bottom left corner
      border-bottom-left-radius: 1rem;

      // position the border correctly
      padding-bottom: 0.25rem;
      margin-bottom: 0.5rem;
      margin-left: -0.77rem;
    }

    // color the border for the active path
    .with-child.active-path > ul {
      border-left: .1rem solid rgb(190, 212, 255) !important;
    }

    // make the arrow for open non-active sections gray
    .with-child > input:checked ~ label:before {
      color: $gray-900;
    }

    // make the arrow for open active sections blue
    .with-child.active-path > input:checked ~ label:before {
      color: $primary;
    }
  }
}

// prevent overflow of long names in sidebar
.td-sidebar-nav__section > ul {
  overflow-wrap: break-word;
}

// --------------------------------------------------
// fix the arrow transform animation on desktop
// --------------------------------------------------
@media (hover: hover) and (pointer: fine) {
    nav.foldable-nav {
        .ul-1 .with-child > label:hover:before {
            transition: transform 0.3s;
        }

        .ul-1 .with-child > input:checked ~ label:hover:before {
            transition: transform 0.3s;
        }
    }
}

// --------------------------------------------------
// custom navbar with larger logo, dropdown on mobile
// --------------------------------------------------
.td-navbar {
  min-height: auto;

  .navbar-brand {
    margin: 0;
    padding: 0;

    .text-uppercase {
      display: none;
    }

    .navbar-logo {
      svg {
        display: inline-block;
        position: absolute;
        top: 0;
        z-index: 33;
        padding: 10px;
        margin: 0;
        height: 95px;
        background: white;
        border: 2px solid #4279f4;
        border-top: none;

        @include media-breakpoint-down(md) {
          width: 80px;
          height: auto;
          padding: 6px;
        }
      }
    }
  }

  // this filler ensures that the navbar links are on the right
  // and the links do not go under the logo (which uses absolute positioning)
  .filler {
      flex-grow: 1;
      min-width: 120px;

      @include media-breakpoint-down(md) {
        min-width: 100px;
      }
  }

  // extra padding on mobile (in collapsible menu)
  @include media-breakpoint-down(sm) {
    .navbar-collapse {
      padding-left: 0.5rem;
      padding-right: 0.5rem;
    }
  }

  .navbar-nav {
    // allow wrapping of navbar links
    white-space: normal;
    .dropdown-toggle {
      white-space: normal;
    }

    // prevent long links from taking up too much space
    @include media-breakpoint-between(sm, lg) {
      .nav-item.long-content {
        font-size: 70%;
      }
    }

    @include media-breakpoint-down(md) {
      font-size: .875rem;
      .dropdown {
        min-width: inherit;
      }
    }
  }
}

// --------------------------------------------------
// Home page
// --------------------------------------------------
.td-home {

  // dont color the outer div with the sidebar color
  // as there is no sidebar on the main page
  .td-outer {
      background-color: unset;
  }

  // animate the background color change of navbar
  .td-navbar {
    transition: background-color 75ms linear;
  }

  .card-img-top {
    object-fit: scale-down;
  }

  .text-white {
    font-weight: 400;
  }

  .bg-primary-dark {
    background-color: $dark;

    a {
      border-bottom: 0.1em dotted paleturquoise;

      color: paleturquoise !important;
      font-weight: 600;
      padding: 0 0.2em 0.05em 0.2em;
      text-decoration: none;

      &:hover {
        border-bottom: 1px dotted #fff;

        color: #fff !important;
      }
    }
  }

  .border-primary-dark {
    border-color: $info !important;
  }

  .section-head {
    color: $primary;
    font-weight: bold;
    padding: 0 0 1.9rem 0;
  }

  .image {
    display: block;
  }

  .image img {
    display: block;
    width: 100%;
    height: auto;
  }

  .image.left, .image.right {
    max-width: 45%;
  }

  .image.left::after, .image.right::after {
    clear: both;
    content: "";
    display: block;
  }

  .image.left {
    float: left;
    margin: 0 1.5em 1.5em 0;
  }

  .image.right {
    float: right;
    margin: 0 0 1.5em 1.5em;
  }

  #overview, #pageContent, #cncf, #community {
    padding: 4rem 0 4rem 0 !important;
  }

  #overview p, #community p {
    font-size: 1.125em;
  }

  #overview {
    border-bottom: 0.1rem solid #b6d0ff;
  }

  #pageContent .container {
    max-width: 70rem;
  }

  #community {
    border-top: 0.1rem solid #b6d0ff;
    border-bottom: 0.1rem solid #b6d0ff;
  }

  #cncf {
    margin-top: 1.5rem;
    margin-bottom: 1.5rem;

    .cncf-title {
      color: #6b7281;
      font-weight: bold;
      padding-top: 1.6rem;
    }
  }

  #pageContent .lead {
    margin: 0 1.5em 3em 1.5em;
  }

  #pageContent .lead > .image {
    padding: 0 1.5em;
    max-width: 60%;
    margin-bottom: 1.5em;
  }

  #pageContent .lead > .text p {
    font-size: 1em;
  }

  @include media-breakpoint-up(sm) {
    #overview, #community {
      font-size: 1.125em;
    }
    #pageContent .lead > .image {
      max-width: 35%;
      margin-bottom: 3em;
    }
    #pageContent .lead > .text p {
      font-size: 1em;
    }
  }

  @include media-breakpoint-up(md) {
    #pageContent .lead {
      margin-top: 1em;
      display: -ms-flexbox;
      display: flex;
      -ms-flex-direction: row;
      flex-direction: row;
      -ms-flex-wrap: nowrap;
      flex-wrap: nowrap;
      -ms-flex-pack: justify;
      justify-content: space-between;
      -ms-flex-align: stretch;
      align-items: stretch;
    }
    #pageContent .lead > * {
      -ms-flex: 0 1 auto;
      flex: 0 1 auto;
    }
    #pageContent .lead > .image {
      display: block;
      margin: 0 auto;
      max-width: 100%;
    }
    #pageContent .lead > .image > img {
      max-width: 80%;
      margin: 0 auto;
    }
    #pageContent .lead > .text {
      -ms-flex-preferred-size: 70%;
      flex-basis: 70%;
    }
    #pageContent .lead:nth-child(2n+0) > .image {
      -ms-flex-order: 2;
      order: 2;
    }
    #pageContent .lead:nth-child(2n+0) > .text {
      -ms-flex-order: 1;
      order: 1;
    }
  }
}

// --------------------------------------------------
// Installing Kubeflow page
// --------------------------------------------------
.distributions-table thead {
  background-color: $primary;
  color: $secondary;
}

// --------------------------------------------------
// 404 page
// --------------------------------------------------
.error-page {
  margin-top: 120px;

  ul {
    margin-bottom: 50px;
    list-style-type: none;
    display: flex;
    align-items: center;
    justify-content: center;
    flex-wrap: wrap;
    padding-left: 0;
  }

  li {
    margin-left: 10px;
    margin-right: 10px;
  }

  h1 {
    text-align: center;
    margin-bottom: 20px;
  }

  @include media-breakpoint-between(md, lg) {
    margin-top: 100px;
  }

  @include media-breakpoint-down(md) {
    margin-top: 50px;
  }
}

// --------------------------------------------------
// for tabbed code blocks
// https://github.com/kubeflow/website/pull/2779
// --------------------------------------------------
.nav-tabs {
  border-bottom: none !important;
}

.td-content > ul li,
.td-content > ol li.nav-item {
  margin-bottom: 0px;
}

.td-content .tab-content .highlight {
  margin: 0;
}

.tab-pane {
  border-radius: 0.25rem;
  padding: 0 16px 16px;

  border: 1px solid #dee2e6;

  &:first-of-type.active {
    border-top-left-radius: 0;
  }
}


================================================
File: assets/scss/_variables_project.scss
================================================
/*
Add styles or override the theme's variables here.
*/

html.smooth-scroll {
  scroll-behavior: smooth;
}

// Bootstrap container-max-widths
// NOTE: we increase the max-widths to make the content wider compared to the Bootstrap defaults
$container-max-widths: (
  sm: 768px, // default: 540px, breakpoint: 576px
  md: 992px, // default: 720px, breakpoint: 768px
  lg: 1200px, // default: 960px, breakpoint: 992px
  xl: 1440px // default: 1140px, breakpoint: 1200px
);

// Theme colors
$primary: #4279f4;
$secondary: #fff;
$dark: #213d7a;
$info: #adb5bd;
$light: #dee2e6;

// Nav bar colors
$white: #fff;
$navbar-dark-color: rgba($white, 1);
$navbar-dark-hover-color: rgba($white, 0.75);
$navbar-dark-active-color: $navbar-dark-color;

// Fonts
$google_font_family: "Open+Sans:300,300i,400,400i,600,600i,700,700i&display=swap" !default;



================================================
File: content/en/_index.html
================================================
+++
title = "Kubeflow"
+++

<!-- Hero section -->
{{<blocks/cover title="Kubeflow" image_anchor="center" color="dark">}}
<div class="px-2">
  <p class="lead mt-5 mb-3">The Machine Learning Toolkit for Kubernetes</p>
  <div class="mx-auto">
    <a class="btn btn-lg btn-primary mr-1 mb-4" href="/docs/started/">
      Get Started <i class="fas fa-arrow-alt-circle-right ml-2"></i>
    </a>
    <a class="btn btn-lg btn-secondary ml-1 mb-4" href="/docs/about/contributing/">
      Contribute <i class="fas fa-pencil-alt ml-2"></i>
    </a>
  </div>
  <div class="mt-5">
    {{<blocks/link-down color="light">}}
  </div>
</div>
{{</blocks/cover>}}

<div id="overview" class="text-center">
  <h3 class="section-head">What is Kubeflow?</h3>
  <div class="container">
    <p class="mx-auto col-11 col-xl-7 px-0">
      Kubeflow makes artificial intelligence and machine learning simple, portable, and scalable.
      We are an <i>ecosystem</i> of <a href="https://kubernetes.io/" target="_blank">Kubernetes</a>
      based components for each stage in
      <a href="/docs/started/architecture/#kubeflow-components-in-the-ml-lifecycle" target="_blank">the AI/ML Lifecycle</a>
      with support for best-in-class open source <a href="/docs/started/architecture/#kubeflow-ecosystem" target="_blank">tools and frameworks</a>.
      <br><br>
      <a href="/docs/started/installing-kubeflow/" target="_blank">Deploy Kubeflow</a> anywhere you run Kubernetes.
    </p>
  </div>
</div>

<section id="pageContent">
  <h3 class="section-head text-center">Kubeflow Components</h3>
  <div class="container">
    <div class="card-deck">
      <div class="card border-primary-dark">
        <a href="/docs/components/pipelines/overview/" target="_blank" rel="noopener" >
          <img
            src="/docs/images/logos/kubeflow.png"
            class="card-img-top"
            draggable="false"
            style="padding: 2rem;"
            alt="Kubeflow Pipelines Logo"
          />
        </a>
        <div class="card-body bg-primary-dark">
          <h5 class="card-title text-white section-head">Pipelines</h5>
          <p class="card-text text-white">
            <a target="_blank" rel="noopener" href="/docs/components/pipelines/overview/">Kubeflow Pipelines</a> (KFP) is a platform for building then deploying portable and scalable machine learning workflows using Kubernetes.
          </p>
        </div>
      </div>
      <div class="card border-primary-dark">
        <a href="/docs/components/notebooks/overview/" target="_blank" rel="noopener" >
          <img
            src="/docs/images/logos/jupyter-vscode-rlang.png"
            class="card-img-top"
            draggable="false"
            style="padding: 2rem;"
            alt="Jupyter + VSCode + RLang Logo"
          />
        </a>
        <div class="card-body bg-primary-dark">
          <h5 class="card-title text-white section-head">Notebooks</h5>
          <p class="card-text text-white">
            <a target="_blank" rel="noopener" href="/docs/components/notebooks/overview/">Kubeflow Notebooks</a> lets you run web-based development environments on your Kubernetes cluster by running them inside Pods.
          </p>
        </div>
      </div>
      <div class="card border-primary-dark">
        <a href="/docs/components/central-dash/overview/" target="_blank" rel="noopener" >
          <img
            src="/docs/images/logos/dashboard.png"
            class="card-img-top"
            draggable="false"
            style="padding: 2rem;"
            alt="People Icon"
          />
        </a>
        <div class="card-body bg-primary-dark">
          <h5 class="card-title text-white section-head">Dashboard</h5>
          <p class="card-text text-white">
            <a href="/docs/components/central-dash/overview/" target="_blank" rel="noopener">Kubeflow Central Dashboard</a> is our hub which connects the authenticated web interfaces of Kubeflow and other ecosystem components.
          </p>
        </div>
      </div>
    </div>
    <br />
    <div class="card-deck">
      <div class="card border-primary-dark">
         <a href="/docs/components/trainer/overview/" target="_blank" rel="noopener" >
           <img
             src="/docs/images/logos/kubeflow-trainer.png"
             class="card-img-top"
             draggable="false"
             style="padding: 2rem;"
             alt="Kubeflow Trainer logo"
           />
         </a>
         <div class="card-body bg-primary-dark">
           <h5 class="card-title text-white section-head">Model Training</h5>
           <p class="card-text text-white">
             <a href="/docs/components/trainer/overview/" target="_blank" rel="noopener" >Kubeflow Trainer</a> is a Kubernetes-native project
             designed for LLMs fine-tuning and enabling scalable, distributed training of ML models across
             various frameworks, including PyTorch, JAX, TensorFlow, and others.
           </p>
         </div>
       </div>
      <div class="card border-primary-dark">
        <a href="/docs/components/katib/overview/" target="_blank" rel="noopener" >
          <img
            src="/docs/images/logos/katib.png"
            class="card-img-top"
            draggable="false"
            style="padding: 2rem;"
            alt="Katib Logo"
          />
        </a>
        <div class="card-body bg-primary-dark">
          <h5 class="card-title text-white section-head">AutoML</h5>
          <p class="card-text text-white">
            <a target="_blank" rel="noopener" href="/docs/components/katib/overview/">Katib</a> is a Kubernetes-native project for automated machine learning (AutoML) with support for hyperparameter tuning, early stopping and neural architecture search.
          </p>
        </div>
      </div>
      <div class="card border-primary-dark">
        <a href="https://kserve.github.io/website/" target="_blank" rel="noopener" >
          <img
            src="/docs/images/logos/kserve.png"
            class="card-img-top"
            draggable="false"
            style="padding: 2rem;"
            alt="KServe Logo"
          />
        </a>
        <div class="card-body bg-primary-dark">
          <h5 class="card-title text-white section-head">Model Serving</h5>
          <p class="card-text text-white">
            <a href="/docs/external-add-ons/kserve/introduction/" target="_blank" rel="noopener">KServe</a> <small>(previously <em>KFServing</em>)</small> solves production model serving on Kubernetes.
            It delivers high-abstraction and performant interfaces for frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX.
          </p>
        </div>
      </div>
    </div>
    <br />
    <!-- For future editors: If you are adding cards not in multiples of 3, wrap them in a 'card-columns' div instead of card-deck.
         It will look more pleasing to the eye. -->
  </div>
</section>

<div id="community" class="text-center">
  <h3 class="section-head">Join our Community</h3>
  <div class="container">
    <p class="mx-auto col-md-7 px-0">
      We are an open and welcoming <a href="/docs/about/community/" target="_blank" rel="noopener">community</a> of software developers, data scientists, and organizations!
      Check out the <a href="/docs/about/community/#list-of-available-meetings">weekly community calls</a>, get involved in discussions on the <a href="/docs/about/community/#kubeflow-mailing-list">mailing list</a> or chat with others on the <a href="/docs/about/community/#kubeflow-slack-channels">Slack Workspace</a>!
    </p>
  </div>
</div>

<div id="cncf" class="text-center">
  <div class="container">
    <div class="mx-auto col-md-4">
      <img
          src="/docs/images/logos/cncf.svg"
          draggable="false"
          class="img-fluid"
          alt="Cloud Native Computing Foundation Logo"
      />
    </div>
    <h5 class="cncf-title">We are a Cloud Native Computing Foundation project.</h5>
  </div>
</div>


================================================
File: content/en/_redirects
================================================
# Redirects from what the browser requests to what we serve
# Info: https://www.netlify.com/docs/redirects/

/version/stable/gke/scripts/deploy.sh  https://raw.githubusercontent.com/kubeflow/kubeflow/v0.2.2/scripts/gke/deploy.sh
/version/stable/scripts/deploy.sh      https://raw.githubusercontent.com/kubeflow/kubeflow/v0.2.2/scripts/deploy.sh
/version/stable/source.tar.gz          https://github.com/kubeflow/kubeflow/archive/v0.2.2.tar.gz
/blog/                                 https://blog.kubeflow.org
/blog/why_kubeflow/                    https://medium.com/kubeflow/why-kubeflow-in-your-infrastructure-56b8fabf1f3e
/blog/announcing_kubeflow_0.3/         https://medium.com/kubeflow/kubeflow-0-3-simplifies-setup-improves-ml-development-98b8ca10bd69
/blog/blog_posts/                      https://medium.com/kubeflow/blog-posts-about-kubeflow-7068688359f3
/blog/nvidia_tensorrt/                 https://medium.com/kubeflow/gpu-accelerated-inference-for-kubernetes-with-the-nvidia-tensorrt-inference-server-and-kubeflow-63061305fff2
/blog/kaggle_on_kubeflow/              https://medium.com/kubeflow/kaggle-on-kubeflow-185a29e27c53
/blog/announcing_kubeflow_0.2/         https://medium.com/kubeflow/kubeflow-0-2-offers-new-components-and-simplified-setup-735e4c56988d

/docs/guides/components/components     /docs/components/
/docs/guides/pipelines/deploy-pipelines-service/  /docs/components/pipelines/pipelines-quickstart/

# https://github.com/kubeflow/website/pull/3786
/docs/components/pipelines/legacy-v1/overview/multi-user    /docs/components/pipelines/operator-guides/multi-user

# Merged duplicate page pipelines-samples.md into build-pipeline.md
/docs/pipelines/pipelines-samples/	   /docs/components/pipelines/legacy-v1/sdk/build-pipeline/

# Removed redundant UI guide. Quickstart is a better destination.
/docs/pipelines/pipelines-ui/	         /docs/components/pipelines/pipelines-quickstart/

# Restructured the pipelines docs.
/docs/pipelines/                                    /docs/components/pipelines
/docs/pipelines/output-viewer/                      /docs/components/pipelines/legacy-v1/sdk/output-viewer/
/docs/pipelines/pipelines-metrics/                  /docs/components/pipelines/legacy-v1/sdk/output-viewer/#v2-sdk-use-sdk-visualization-apis
/docs/pipelines/build-component/                    /docs/components/pipelines/legacy-v1/sdk/component-development/
/docs/pipelines/install-sdk/                        /docs/components/pipelines/legacy-v1/sdk/install-sdk/
/docs/pipelines/lightweight-python-components/      /docs/components/pipelines/legacy-v1/sdk/python-function-components/
/docs/pipelines/sdk/lightweight-python-components/  /docs/components/pipelines/legacy-v1/sdk/python-function-components/
/docs/pipelines/build-pipeline/                     /docs/components/pipelines/legacy-v1/tutorials/build-pipeline/
/docs/pipelines/pipelines-tutorial/                 /docs/components/pipelines/legacy-v1/tutorials/cloud-tutorials/
/docs/pipelines/tutorials/pipelines-tutorial/       /docs/components/pipelines/legacy-v1/tutorials/cloud-tutorials/
/docs/gke/pipelines-tutorial/                       /docs/components/pipelines/legacy-v1/tutorials/cloud-tutorials/
/docs/gke/pipelines/pipelines-tutorial/             /docs/components/pipelines/legacy-v1/tutorials/cloud-tutorials/
/docs/gke/authentication-pipelines/                 /docs/distributions/gke/pipelines/authentication-pipelines/

/docs/pipelines/metrics/                            /docs/components/pipelines/legacy-v1/sdk/output-viewer/#v2-sdk-use-sdk-visualization-apis
/docs/pipelines/metrics/pipelines-metrics/          /docs/components/pipelines/legacy-v1/sdk/output-viewer/#v2-sdk-use-sdk-visualization-apis
/docs/pipelines/metrics/output-viewer/              /docs/components/pipelines/legacy-v1/sdk/output-viewer/
/docs/pipelines/pipelines-overview/                 /docs/components/pipelines/overview/pipelines-overview/
/docs/pipelines/enable-gpu-and-tpu/                 /docs/distributions/gke/pipelines/enable-gpu-and-tpu/
/docs/pipelines/sdk/enable-gpu-and-tpu/             /docs/distributions/gke/pipelines/enable-gpu-and-tpu/
/docs/pipelines/sdk/gcp/enable-gpu-and-tpu/         /docs/distributions/gke/pipelines/enable-gpu-and-tpu/
/docs/pipelines/preemptible/                        /docs/distributions/gke/pipelines/preemptible/
/docs/pipelines/sdk/gcp/preemptible/                /docs/distributions/gke/pipelines/preemptible/
/docs/pipelines/reusable-components/                /docs/examples/shared-resources/
/docs/pipelines/sdk/reusable-components/            /docs/examples/shared-resources/
/docs/pipelines/sdk/build-component/                /docs/components/pipelines/legacy-v1/sdk/build-pipeline/
/docs/components/pipelines/sdk/build-component/     /docs/components/pipelines/legacy-v1/sdk/build-pipeline/
/docs/components/pipelines/upgrade/                 /docs/components/pipelines/legacy-v1/installation/upgrade/

# Moved the guide to monitoring GKE deployments.
/docs/other-guides/monitoring/                 /docs/distributions/gke/monitoring/

# Created a new section for pipeline concepts.
/docs/pipelines/pipelines-concepts/            /docs/components/pipelines/concepts/

# Replaces Pipelines DSL overview with SDK overview
/docs/pipelines/sdk/dsl-overview/              /docs/components/pipelines/legacy-v1/sdk/sdk-overview/

# Created a new section for pipelines installation.
/docs/pipelines/standalone-deployment-gcp/    /docs/components/pipelines/operator-guides/installation/

# Removed the downloads page from Reference to Getting Started with Kubeflow
/docs/reference/downloads/                     /docs/started/getting-started/

# Adding the content of Requirements to Getting Started and removing Requirements
docs/started/requirements/                    /docs/started/getting-started/

# Created a new components layout
/docs/components/chainer                       /docs/components/training/chainer
/docs/components/modeldb                       /docs/components/misc/modeldb
/docs/components/mpi                           /docs/components/training/mpi
/docs/components/mxnet                         /docs/components/training/mxnet
/docs/components/pytorch                       /docs/components/training/pytorch
/docs/components/nuclio                        /docs/components/misc/nuclio
/docs/components/seldon                        /docs/external-add-ons/serving/seldon
/docs/components/trtinferenceserver            /docs/external-add-ons/serving/tritoninferenceserver
/docs/components/tfbatchpredict                /docs/external-add-ons/serving/tfbatchpredict
/docs/components/tftraining                    /docs/components/training/user-guides/tensorflow
/docs/components/training/pytorch              /docs/components/training/user-guides/pytorch
/docs/components/training/mpi                  /docs/components/training/user-guides/mpi
/docs/components/training/mxnet                /docs/components/training/user-guides/mxnet
/docs/components/training/paddlepaddle         /docs/components/training/user-guides/paddle
/docs/components/training/xgboost              /docs/components/training/user-guides/xgboost
/docs/components/training/tftraining           /docs/components/training/user-guides/tensorflow
/docs/components/tfserving_new                 /docs/external-add-ons/serving/tfserving_new

# Deleted the PyTorch serving page
/docs/components/pytorchserving/               /docs/external-add-ons/serving/overview/
/docs/components/serving/pytorchserving/       /docs/external-add-ons/serving/overview/

# Restructured the getting-started and other-guides sections.
/docs/started/getting-started-k8s/             /docs/started/k8s/
/docs/started/getting-started-minikube/                    /docs/started/distributions/kfctl/minikube/
/docs/use-cases/kubeflow-on-multinode-cluster/ /docs/other-guides/kubeflow-on-multinode-cluster/
/docs/use-cases/job-scheduling/                /docs/other-guides/job-scheduling/

# Remove examples docs
/docs/examples/*                                /docs/started/kubeflow-examples

# Move the kustomize guide to the config section
/docs/components/misc/kustomize/              /docs/distributions/kfctl/kustomize/

# Merged the UIs page with the new central dashboard page
/docs/other-guides/accessing-uis/             /docs/components/central-dash/overview/

# Refactored multi-user docs
/docs/other-guides/multi-user-overview/       /docs/components/multi-tenancy/

# Rename TensorRT Inference Server to Triton Inference Server
/docs/components/serving/trtinferenceserver   /docs/external-add-ons/serving/tritoninferenceserver

# Kubeflow Operator move to under distributions
/docs/operator                      /docs/distributions/operator
/docs/operator/introduction         /docs/distributions/operator/introduction
/docs/operator/install-operator     /docs/distributions/operator/install-operator
/docs/operator/install-kubeflow     /docs/distributions/operator/install-kubeflow
/docs/operator/uninstall-kubeflow   /docs/distributions/operator/uninstall-kubeflow
/docs/operator/uninstall-operator   /docs/distributions/operator/uninstall-operator
/docs/operator/troubleshooting      /docs/distributions/operator/troubleshooting

# kfctl move to under distributions
/docs/started/workstation/minikube-linux        /docs/distributions/kfctl/minikube
/docs/other-guides/kustomize                    /docs/distributions/kfctl/kustomize
/docs/started/k8s/kfctl-istio-dex               /docs/distributions/kfctl/multi-user
/docs/started/k8s/kfctl-k8s-istio               /docs/distributions/kfctl/deployment

# Moved Job scheduling under Training
/docs/other-guides/job-scheduling/    /docs/components/training/job-scheduling/

# Moved KFServing
/docs/components/serving/kfserving/         /docs/components/kfserving

# Moved KServe/KFserving Docs to External Addons
/docs/components/kfserving/ /docs/external-add-ons/kserve
/docs/components/kfserving/kfserving/ /docs/external-add-ons/kserve/kserve
/docs/components/kfserving/webapp/ /docs/external-add-ons/kserve/webapp

# Moved MicroK8s to distributions
/docs/started/workstation/kubeflow-on-microk8s    /docs/distributions/microk8s/kubeflow-on-microk8s

# Moved K8s deployment overview to under kfctl
/docs/started/k8s/overview              /docs/distributions/kfctl/overview

# Moved MiniKF to distributions
/docs/started/workstation/getting-started-minikf        /docs/distributions/minikf/getting-started-minikf
/docs/started/workstation/minikf-aws                    /docs/distributions/minikf/minikf-aws
/docs/started/workstation/minikf-gcp                    /docs/distributions/minikf/minikf-gcp

# Distinguish components from external add-ons
/docs/components/fairing                       /docs/external-add-ons/fairing
/docs/components/istio                         /docs/external-add-ons/istio
/docs/components/feature-store                 /docs/external-add-ons/feature-store
/docs/components/serving                       /docs/external-add-ons/serving
/docs/components/metadata                      /docs

# Rename Getting Started to Installing Kubeflow
/docs/started/getting-started           /docs/started/installing-kubeflow

# Remove Reference section
/docs/reference/notebook/*         /docs/components/notebooks/api-reference
/docs/reference/mpijob/*           /docs/components/training
/docs/reference/pytorchjob/*       /docs/components/training
/docs/reference/tfjob/*            /docs/components/training

# Cleanup of Notebooks Docs
/docs/components/notebooks/custom-notebook                /docs/components/notebooks/container-images
/docs/components/notebooks/setup                          /docs/components/notebooks/quickstart-guide
/docs/components/notebooks/troubleshoot                   /docs/components/notebooks/troubleshooting
/docs/components/notebooks/why-use-jupyter-notebook       /docs/components/notebooks/overview

# Refactor Pipelines section
/docs/components/pipelines/caching                            /docs/components/pipelines/legacy-v1/overview/caching
/docs/components/pipelines/caching-v2                         /docs/components/pipelines/user-guides/core-functions/caching
/docs/components/pipelines/multi-user                         /docs/components/pipelines/operator-guides/multi-user
/docs/components/pipelines/pipeline-root                      /docs/components/pipelines/concepts/pipeline-root
/docs/components/pipelines/overview/pipelines-overview        /docs/components/pipelines/overview
/docs/components/pipelines/pipelines-quickstart               /docs/components/pipelines/overview/quickstart

# Restructure About section
/docs/about/kubeflow             /docs/started/introduction
/docs/started/kubeflow-overview  /docs/started/architecture

# move istio from external-add-ons to multi-tenancy component
/docs/external-add-ons/istio/*    /docs/components/multi-tenancy/istio

# merge multi-tenancy/getting-started with profiles
/docs/components/multi-tenancy/getting-started /docs/components/central-dash/profiles

# move multi-tenancy to concepts
/docs/components/multi-tenancy/design          /docs/concepts/multi-tenancy/design
/docs/components/multi-tenancy/istio           /docs/concepts/multi-tenancy/istio
/docs/components/multi-tenancy/overview        /docs/concepts/multi-tenancy/overview
/docs/components/multi-tenancy                 /docs/concepts/multi-tenancy

# remove registration-flow page
/docs/components/central-dash/registration-flow /docs/components/central-dash/profiles

# rename customize dashboard page
/docs/components/central-dash/customizing-menu /docs/components/central-dash/customize

# rename feature-store to feast
/docs/external-add-ons/feature-store/overview/        /docs/external-add-ons/feast/introduction
/docs/external-add-ons/feature-store/getting-started/ /docs/external-add-ons/feast/introduction
/docs/external-add-ons/feature-store/                 /docs/external-add-ons/feast

# rename kserve/kserve to kserve/introduction
/docs/external-add-ons/kserve/kserve/ /docs/external-add-ons/kserve/introduction

# redirect kserve pages to kserve website
/docs/external-add-ons/kserve/first_isvc_kserve/ https://kserve.github.io/website/latest/get_started/first_isvc/
/docs/external-add-ons/kserve/migration/         https://kserve.github.io/website/latest/admin/migration/

# ===============
# IMPORTANT NOTE:
# Catch-all redirects should be added at the end of this file as redirects happen from top to bottom
# ===============
/docs/guides/*                      /docs/:splat
/docs/pipelines/concepts/*          /docs/components/pipelines/concepts
/docs/pipelines/*                   /docs/components/pipelines
/docs/aws/*                         /docs/distributions/aws/
/docs/azure/*                       /docs/distributions/azure/:splat
/docs/gke/*                         /docs/distributions/gke/:splat
/docs/ibm/*                         /docs/distributions/ibm/:splat
/docs/openshift/*                   /docs/distributions/openshift/:splat
/docs/components/fairing/*          /docs/external-add-ons/fairing/:splat
/docs/components/istio/*            /docs/external-add-ons/istio/:splat
/docs/components/feature-store/*    /docs/external-add-ons/feature-store/:splat
/docs/components/serving/*          /docs/external-add-ons/serving/:splat

# redirect distribution pages to external websites
/docs/distributions/aws/*           https://awslabs.github.io/kubeflow-manifests/
/docs/distributions/azure/*         https://azure.github.io/kubeflow-aks/main/
/docs/distributions/charmed/*       https://charmed-kubeflow.io/
/docs/distributions/ekf/*           https://www.arrikto.com/enterprise-kubeflow/
/docs/distributions/gke/*           https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/:splat
/docs/distributions/ibm/*           https://ibm.github.io/manifests/
/docs/distributions/nutanix/*       https://nutanix.github.io/kubeflow-manifests/

# Refactor Katib docs
/docs/components/hyperparameter                /docs/components/katib/overview
/docs/components/katib/hyperparameter          /docs/components/katib/getting-started
/docs/components/katib/experiment              /docs/components/katib/user-guides/hp-tuning/configure-experiment
/docs/components/katib/early-stopping          /docs/components/katib/user-guides/early-stopping
/docs/components/katib/resume-experiment       /docs/components/katib/user-guides/resume-experiment
/docs/components/katib/trial-template          /docs/components/katib/user-guides/trial-template

# Restructured the pipeline docs (https://github.com/kubeflow/website/issues/3716)
/docs/components/pipelines/overview/quickstart/                             /docs/components/pipelines/overview/
/docs/components/pipelines/v1/                                              /docs/components/pipelines/legacy-v1/
/docs/components/pipelines/v1/concepts/                                     /docs/components/pipelines/concepts/
/docs/components/pipelines/v1/concepts/component/                           /docs/components/pipelines/concepts/component/
/docs/components/pipelines/v1/concepts/experiment/                          /docs/components/pipelines/concepts/experiment/
/docs/components/pipelines/v1/concepts/graph/                               /docs/components/pipelines/concepts/graph/
/docs/components/pipelines/v1/concepts/metadata/                            /docs/components/pipelines/concepts/metadata/
/docs/components/pipelines/v1/concepts/output-artifact/                     /docs/components/pipelines/concepts/output-artifact/
/docs/components/pipelines/v1/concepts/pipeline/                            /docs/components/pipelines/concepts/pipeline/
/docs/components/pipelines/v1/concepts/run-trigger/                         /docs/components/pipelines/concepts/run-trigger/
/docs/components/pipelines/v1/concepts/run/                                 /docs/components/pipelines/concepts/run/
/docs/components/pipelines/v1/concepts/step/                                /docs/components/pipelines/concepts/step/
/docs/components/pipelines/v1/installation/                                 /docs/components/pipelines/legacy-v1/installation/
/docs/components/pipelines/v1/installation/choose-executor/                 /docs/components/pipelines/legacy-v1/installation/choose-executor/
/docs/components/pipelines/v1/installation/compatibility-matrix/            /docs/components/pipelines/legacy-v1/installation/compatibility-matrix/
/docs/components/pipelines/v1/installation/localcluster-deployment/         /docs/components/pipelines/legacy-v1/installation/localcluster-deployment/
/docs/components/pipelines/v1/installation/overview/                        /docs/components/pipelines/legacy-v1/installation/overview/
/docs/components/pipelines/v1/installation/standalone-deployment/           /docs/components/pipelines/legacy-v1/installation/standalone-deployment/
/docs/components/pipelines/v1/installation/upgrade/                         /docs/components/pipelines/legacy-v1/installation/upgrade/
/docs/components/pipelines/v1/introduction/                                 /docs/components/pipelines/legacy-v1/introduction/
/docs/components/pipelines/v1/overview/                                     /docs/components/pipelines/legacy-v1/overview/
/docs/components/pipelines/v1/overview/caching/                             /docs/components/pipelines/legacy-v1/overview/caching/
/docs/components/pipelines/v1/overview/interfaces/                          /docs/components/pipelines/interfaces/
/docs/components/pipelines/v1/overview/multi-user/                          /docs/components/pipelines/operator-guides/multi-user/
/docs/components/pipelines/v1/overview/pipeline-root/                       /docs/components/pipelines/concepts/pipeline-root/
/docs/components/pipelines/v1/overview/quickstart/                          /docs/components/pipelines/legacy-v1/overview/quickstart/
/docs/components/pipelines/v1/reference/                                    /docs/components/pipelines/legacy-v1/reference/
/docs/components/pipelines/v1/reference/api/kubeflow-pipeline-api-spec/     /docs/components/pipelines/legacy-v1/reference/api/kubeflow-pipeline-api-spec/
/docs/components/pipelines/v1/reference/component-spec/                     /docs/components/pipelines/reference/component-spec/
/docs/components/pipelines/v1/reference/sdk/                                /docs/components/pipelines/legacy-v1/reference/sdk/
/docs/components/pipelines/v1/sdk/                                          /docs/components/pipelines/legacy-v1/sdk/
/docs/components/pipelines/v1/sdk/best-practices/                           /docs/components/pipelines/legacy-v1/sdk/best-practices/
/docs/components/pipelines/v1/sdk/build-pipeline/                           /docs/components/pipelines/legacy-v1/sdk/build-pipeline/
/docs/components/pipelines/v1/sdk/component-development/                    /docs/components/pipelines/legacy-v1/sdk/component-development/
/docs/components/pipelines/v1/sdk/connect-api/                              /docs/components/pipelines/user-guides/core-functions/connect-api/
/docs/components/pipelines/v1/sdk/dsl-recursion/                            /docs/components/pipelines/legacy-v1/sdk/dsl-recursion/
/docs/components/pipelines/v1/sdk/enviroment_variables/                     /docs/components/pipelines/legacy-v1/sdk/enviroment_variables/
/docs/components/pipelines/v1/sdk/gcp/                                      /docs/components/pipelines/legacy-v1/sdk/gcp/
/docs/components/pipelines/v1/sdk/install-sdk/                              /docs/components/pipelines/legacy-v1/sdk/install-sdk/
/docs/components/pipelines/v1/sdk/manipulate-resources/                     /docs/components/pipelines/legacy-v1/sdk/manipulate-resources/
/docs/components/pipelines/v1/sdk/output-viewer/                            /docs/components/pipelines/legacy-v1/sdk/output-viewer/
/docs/components/pipelines/v1/sdk/parameters/                               /docs/components/pipelines/legacy-v1/sdk/parameters/
/docs/components/pipelines/v1/sdk/pipelines-with-tekton/                    /docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton/
/docs/components/pipelines/v1/sdk/python-based-visualizations/              /docs/components/pipelines/legacy-v1/sdk/python-based-visualizations/
/docs/components/pipelines/v1/sdk/python-function-components/               /docs/components/pipelines/legacy-v1/sdk/python-function-components/
/docs/components/pipelines/v1/sdk/sdk-overview/                             /docs/components/pipelines/legacy-v1/sdk/sdk-overview/
/docs/components/pipelines/v1/sdk/static-type-checking/                     /docs/components/pipelines/legacy-v1/sdk/static-type-checking/
/docs/components/pipelines/v1/troubleshooting/                              /docs/components/pipelines/legacy-v1/troubleshooting/
/docs/components/pipelines/v1/tutorials/                                    /docs/components/pipelines/legacy-v1/tutorials/
/docs/components/pipelines/v1/tutorials/api-pipelines/                      /docs/components/pipelines/legacy-v1/tutorials/api-pipelines/
/docs/components/pipelines/v1/tutorials/benchmark-examples/                 /docs/components/pipelines/legacy-v1/tutorials/benchmark-examples/
/docs/components/pipelines/v1/tutorials/build-pipeline/                     /docs/components/pipelines/legacy-v1/tutorials/build-pipeline/
/docs/components/pipelines/v1/tutorials/cloud-tutorials/                    /docs/components/pipelines/legacy-v1/tutorials/cloud-tutorials/
/docs/components/pipelines/v1/tutorials/sdk-examples/                       /docs/components/pipelines/legacy-v1/tutorials/sdk-examples/
/docs/components/pipelines/v2/                                              /docs/components/pipelines/
/docs/components/pipelines/v2/administration/                               /docs/components/pipelines/operator-guides/
/docs/components/pipelines/v2/administration/server-config/                 /docs/components/pipelines/operator-guides/server-config/
/docs/components/pipelines/v2/caching/                                      /docs/components/pipelines/user-guides/core-functions/caching/
/docs/components/pipelines/v2/cli/                                          /docs/components/pipelines/user-guides/core-functions/cli/
/docs/components/pipelines/v2/community-and-support/                        /docs/components/pipelines/reference/community-and-support/
/docs/components/pipelines/v2/compile-a-pipeline/                           /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline/
/docs/components/pipelines/v2/components/                                   /docs/components/pipelines/user-guides/components/
/docs/components/pipelines/v2/components/additional-functionality/          /docs/components/pipelines/user-guides/components/additional-functionality/
/docs/components/pipelines/v2/components/container-components/              /docs/components/pipelines/user-guides/components/container-components/
/docs/components/pipelines/v2/components/containerized-python-components/   /docs/components/pipelines/user-guides/components/containerized-python-components/
/docs/components/pipelines/v2/components/importer-component/                /docs/components/pipelines/user-guides/components/importer-component/
/docs/components/pipelines/v2/components/lightweight-python-components/     /docs/components/pipelines/user-guides/components/lightweight-python-components/
/docs/components/pipelines/v2/data-types/                                   /docs/components/pipelines/user-guides/data-handling/data-types/
/docs/components/pipelines/v2/data-types/artifacts/                         /docs/components/pipelines/user-guides/data-handling/artifacts/
/docs/components/pipelines/v2/data-types/parameters/                        /docs/components/pipelines/user-guides/data-handling/parameters/
/docs/components/pipelines/v2/hello-world/                                  /docs/components/pipelines/getting-started/
/docs/components/pipelines/v2/installation/                                 /docs/components/pipelines/operator-guides/installation/
/docs/components/pipelines/v2/installation/quickstart/                      /docs/components/pipelines/operator-guides/installation/
/docs/components/pipelines/v2/introduction/                                 /docs/components/pipelines/overview/
/docs/components/pipelines/v2/load-and-share-components/                    /docs/components/pipelines/user-guides/components/load-and-share-components/
/docs/components/pipelines/v2/local-execution/                              /docs/components/pipelines/user-guides/core-functions/execute-kfp-pipelines-locally/
/docs/components/pipelines/v2/migration/                                    /docs/components/pipelines/user-guides/migration/
/docs/components/pipelines/v2/pipelines/                                    /docs/components/pipelines/user-guides/core-functions/
/docs/components/pipelines/v2/pipelines/control-flow/                       /docs/components/pipelines/user-guides/core-functions/control-flow/
/docs/components/pipelines/v2/pipelines/pipeline-basics/                    /docs/components/pipelines/user-guides/components/compose-components-into-pipelines/
/docs/components/pipelines/v2/platform-specific-features/                   /docs/components/pipelines/user-guides/core-functions/platform-specific-features/
/docs/components/pipelines/v2/reference/                                    /docs/components/pipelines/reference/
/docs/components/pipelines/v2/reference/api/kubeflow-pipeline-api-spec/     /docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec/
/docs/components/pipelines/v2/reference/sdk/                                /docs/components/pipelines/reference/sdk/
/docs/components/pipelines/v2/run-a-pipeline/                               /docs/components/pipelines/user-guides/core-functions/run-a-pipeline/
/docs/components/pipelines/v2/version-compatibility/                        /docs/components/pipelines/reference/version-compatibility/

# Remove Components / Multi-Tenancy section
/docs/concepts/multi-tenancy/                /docs/components/central-dash/profiles/
/docs/concepts/multi-tenancy/overview/       /docs/components/central-dash/profiles/
/docs/concepts/multi-tenancy/design/         /docs/components/central-dash/profiles/
/docs/concepts/multi-tenancy/istio/          /docs/components/central-dash/profiles/

# Kubeflow Trainer V2 (https://github.com/kubeflow/trainer/issues/2214)
/docs/components/training/installation/                       /docs/components/trainer/legacy-v1/installation/
/docs/components/training/explanation/                        /docs/components/trainer/legacy-v1/explanation/
/docs/components/training/explanation/fine-tuning/            /docs/components/trainer/legacy-v1/explanation/fine-tuning/
/docs/components/training/reference/                          /docs/components/trainer/legacy-v1/reference/
/docs/components/training/reference/architecture/             /docs/components/trainer/legacy-v1/reference/architecture/
/docs/components/training/reference/distributed-training/     /docs/components/trainer/legacy-v1/reference/distributed-training/
/docs/components/training/reference/fine-tuning/              /docs/components/trainer/legacy-v1/reference/fine-tuning/
/docs/components/training/user-guides/                        /docs/components/trainer/legacy-v1/user-guides/
/docs/components/training/user-guides/fine-tuning/            /docs/components/trainer/legacy-v1/user-guides/fine-tuning/
/docs/components/training/user-guides/jax/                    /docs/components/trainer/legacy-v1/user-guides/jax/
/docs/components/training/user-guides/job-scheduling/         /docs/components/trainer/legacy-v1/user-guides/job-scheduling/
/docs/components/training/user-guides/mpi/                    /docs/components/trainer/legacy-v1/user-guides/mpi/
/docs/components/training/user-guides/paddle/                 /docs/components/trainer/legacy-v1/user-guides/paddle/
/docs/components/training/user-guides/prometheus/             /docs/components/trainer/legacy-v1/user-guides/prometheus/
/docs/components/training/user-guides/tensorflow/             /docs/components/trainer/legacy-v1/user-guides/tensorflow/
/docs/components/training/user-guides/xgboost/                /docs/components/trainer/legacy-v1/user-guides/xgboost/
/docs/components/training/                                    /docs/components/trainer/
/docs/components/training/user-guides/pytorch/                /docs/components/trainer/legacy-v1/user-guides/pytorch/
/docs/components/training/overview/                           /docs/components/trainer/legacy-v1/overview/
/docs/components/training/getting-started/                    /docs/components/trainer/legacy-v1/getting-started/



================================================
File: content/en/search.md
================================================
---
title: Search results
layout: search
---



================================================
File: content/en/docs/_index.md
================================================
+++
title = "Documentation"
description = "All of Kubeflow documentation"
+++



================================================
File: content/en/docs/about/OWNERS
================================================
approvers:
  - 8bitmp3
  - andreyvelich
  - gaocegege
  - Jeffwan
  - johnugeorge
  - terrytangyuan



================================================
File: content/en/docs/about/_index.md
================================================
+++
title = "About"
description = "About Kubeflow and its community"
weight = 10
+++



================================================
File: content/en/docs/about/community.md
================================================
+++
title =  "Community"
description = "About the Kubeflow community"
weight = 10
aliases = ["/docs/community/"]
+++

## Contributing

If you are interested in learning more about how to participate in and contribute to the Kubeflow community, take a look at [Contributing](/docs/about/contributing/)!

## Kubeflow Slack Channels

Kubeflow is part of the vibrant CNCF community, we use the [CNCF Slack](https://slack.cncf.io/) for informal discussions among users and contributors.
Please join the [Kubeflow channels](#slack-channels) to join the conversation and get help from the community.

<a href="https://slack.cncf.io/">
  <button class="btn btn-primary py-2 px-5 mb-3">Click to join:<br><b>CNCF Slack</b></button>
</a>

### Slack Channels

The following table lists official Kubeflow channels which are hosted on the **CNCF Slack**:

| Description                             | Link                                                                              |
| --------------------------------------- | --------------------------------------------------------------------------------- |
| Announcements                           | [#kubeflow-announcements](https://app.slack.com/client/T08PSQ7BQ/C01EV0FV154)     |
| Contributors                            | [#kubeflow-contributors](https://app.slack.com/client/T08PSQ7BQ/C0742LBR5BM) |
| Katib                                   | [#kubeflow-katib](https://app.slack.com/client/T08PSQ7BQ/C073N7AS48P)             |
| Model Registry                          | [#kubeflow-model-registry](https://app.slack.com/client/T08PSQ7BQ/C073N7B6K3R)    |
| Notebooks                               | [#kubeflow-notebooks](https://app.slack.com/client/T08PSQ7BQ/C073W562HFY)         |
| Pipelines                               | [#kubeflow-pipelines](https://app.slack.com/client/T08PSQ7BQ/C073N7BMLB1)         |
| Platform Manifests and Release Planning | [#kubeflow-platform](https://app.slack.com/client/T08PSQ7BQ/C073W572LA2)          |
| Spark Operator                          | [#kubeflow-spark-operator](https://app.slack.com/client/T08PSQ7BQ/C074588U7EG)    |
| Kubeflow Trainer and MPI Operator       | [#kubeflow-training](https://app.slack.com/client/T08PSQ7BQ/C0742LDFZ4K)          |
| KServe                                  | [#kserve](https://app.slack.com/client/T08PSQ7BQ/C06AH2C3K8B)                     |

## Kubeflow Mailing List

The official Kubeflow mailing list is a Google Group called [kubeflow-discuss](https://groups.google.com/g/kubeflow-discuss).

<a href="https://groups.google.com/g/kubeflow-discuss">
  <button class="btn btn-primary py-2 px-5">Click to join:<br>Kubeflow Mailing List</button>
</a>

## Kubeflow Community Meetings

The Kubeflow community holds various meetings to all users and contributors to discus
issues/proposals and present demos/products.

### Subscribe to the Kubeflow Calendar

Joining the [kubeflow-discuss mailing list](#kubeflow-mailing-list) should automatically add
the Kubeflow community meetings to your Google calendar. If you still can't see the invites,
manually add [the Kubeflow calendar using this name](https://calendar.google.com/calendar/u/0/r/settings/addcalendar):

```shell
kubeflow.org_7l5vnbn8suj2se10sen81d9428@group.calendar.google.com
```

<img src="/docs/about/images/google-calendar.png"
      alt="Google Calendar"
      class="mt-3 mb-3">

### List of Available Meetings

The following list shows available Kubeflow community meetings with the corresponding meeting notes and recordings.

| Meeting Name                    | Meeting Notes                                        | Recordings                                                                                   |
| ------------------------------- | ---------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| Kubeflow community call         | [Google Doc](https://bit.ly/kf-meeting-notes)        | [YouTube playlist](https://www.youtube.com/playlist?list=PLmzRWLV1CK_ypvsQu10SGRmhf2S7mbYL5) |
| Kubeflow AutoML and Training WG | [Google Doc](https://bit.ly/2PWVCkV)                 | [YouTube playlist](https://www.youtube.com/playlist?list=PLmzRWLV1CK_xAiAY-3Vw94lrUs4xeNZ3j) |
| Kubeflow Model Registry call    | [Google Doc](https://bit.ly/kf-model-registry-notes) | [YouTube playlist](https://www.youtube.com/playlist?list=PLmzRWLV1CK_ymLhMu0UMeaWPsLDPIjNnW) |
| Kubeflow Notebooks WG           | [Google Doc](https://bit.ly/kf-notebooks-wg-notes)   |                                                                                              |
| Kubeflow Platform WG            | [Google Doc](https://bit.ly/kf-wg-manifests-notes)   |                                                                                              |
| Kubeflow Pipelines WG           | [Google Doc](http://bit.ly/kfp-meeting-notes)        |                                                                                              |
| Kubeflow Release team call      | [Google Doc](https://bit.ly/kf-release-team-notes)   |                                                                                              |
| Kubeflow Spark Operator call    | [Google Doc](https://bit.ly/3VGzP4n)                 | [YouTube playlist](https://www.youtube.com/playlist?list=PLmzRWLV1CK_xXuM6gALgBG8vDZHFCNxce) |
| KServe call                     | [Google Doc](https://bit.ly/3NlKFb3)                 |                                                                                              |

### Kubeflow Community Calendar

This is an aggregated view of the Kubeflow community calendar and should be displayed in your
device's timezone.

<style>
#calendar-container {
   overflow: auto;
}
</style>
<div id="calendar-container"></div>
<script type="text/javascript">
const timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
const calender_src_list = [
  // Kubeflow Community
  "kubeflow.org_7l5vnbn8suj2se10sen81d9428%40group.calendar.google.com",
];
let calender_src = calender_src_list.map(src => `&src=${src}&color=%23A79B8E`).join('');
const html = `<iframe src="https://calendar.google.com/calendar/embed?ctz=${timezone}&height=600&wkst=1&bgcolor=%23ffffff&showPrint=0&showDate=1&mode=AGENDA&showTitle=0${calender_src}" style="border:solid 1px #777" width="800" height="600" frameborder="0" scrolling="no"></iframe>`;
document.getElementById('calendar-container').innerHTML = html;
</script>

## Kubeflow on Social Media

- [Official Kubeflow YouTube Channel](https://www.youtube.com/@Kubeflow) for the
  announcements and project updates.
- [Kubeflow Community YouTube Channel](https://www.youtube.com/@KubeflowCommunity) for the working
  group and community meeting recordings.
- Join [LinkedIn](https://www.linkedin.com/company/kubeflow/) for latest news in Kubeflow.
- Follow us on X formerly known as [Twitter](https://twitter.com/kubeflow) for latest news on Kubeflow.

## Kubeflow Blog and Other Resources

The Kubeflow project maintains an official blog that can be [found here](https://blog.kubeflow.org).

{{% alert title="Tip" color="info" %}}
To contribute an article for the blog, please raise an issue on the [kubeflow/community](https://github.com/kubeflow/community) GitHub repo or create a thread on the [mailing list](#kubeflow-mailing-list).
Note, articles are published using the [kubeflow/blog](https://github.com/kubeflow/blog) GitHub repo.
{{% /alert %}}

In addition, please check out the community-curated [awesome list of projects and resources related to Kubeflow](https://github.com/terrytangyuan/awesome-kubeflow).

## Kubeflow Trademark

The Kubeflow trademark and logos are registered trademarks of The Linux Foundation® (TLF), please review the [Kubeflow Brand Guidelines](https://www.linuxfoundation.org/legal/trademark-usage) for more information.

## Kubeflow Steering Committee

The [Kubeflow Steering Committee (KSC)](https://github.com/kubeflow/community/blob/master/KUBEFLOW-STEERING-COMMITTEE.md) is the governing body of the Kubeflow project, providing decision-making and oversight pertaining to the Kubeflow project policies, sub-organizations, and financial planning, and defines the project values and structure.

## Kubeflow Working Groups

The Kubeflow project has a number of Working Groups (WGs) who each maintain some aspect of the Kubeflow project.
The following table outlines which components are maintained by each Working Group.

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Working Group</th>
        <th>Maintained Components</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">
          <a href="https://github.com/kubeflow/community/tree/master/wg-automl">AutoML</a>
        </td>
        <td>
          <a href="https://github.com/kubeflow/katib">Katib</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Manifests Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">
          <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Manifests</a>
        </td>
        <td>
          <a href="https://github.com/kubeflow/manifests">Manifests Repository</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">
          <a href="https://github.com/kubeflow/community/tree/master/wg-notebooks">Notebooks</a>
        </td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/admission-webhook">Admission Webhook (PodDefaults)</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/centraldashboard">Central Dashboard</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/jupyter">Jupyter Web App</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/access-management">Kubeflow Access Management API (KFAM)</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/notebook-controller">Notebook Controller</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/profile-controller">Profile Controller</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/tensorboard-controller">Tensorboard Controller</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/tensorboards">Tensorboard Web App</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/volumes">Volumes Web App</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">
          <a href="https://github.com/kubeflow/community/tree/master/wg-pipelines">Pipelines</a>
        </td>
        <td>
          <a href="https://github.com/kubeflow/pipelines">Kubeflow Pipelines</a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton">Kubeflow Pipelines on Tekton</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">
          <a href="https://github.com/kubeflow/community/tree/master/wg-serving">Serving</a>
        </td>
        <td>
          <a href="https://github.com/kserve/kserve">KServe (formerly KFServing)</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">
          <a href="https://github.com/kubeflow/community/tree/master/wg-training">Training</a>
        </td>
        <td>
          <a href="https://github.com/kubeflow/trainer">Training Operator</a>
        </td>
      </tr>
  </tbody>
</table>
</div>



================================================
File: content/en/docs/about/contributing.md
================================================
+++
title = "Contributing"
description = "Guidelines for contributing to Kubeflow"
weight = 20
aliases = ["/docs/contributing/"]
+++

This document is the single source of truth for how to contribute to the code base.
We'd love to accept your patches and contributions to this project.
There are just a few small guidelines you need to follow.

## Getting Started

### Sign off your commits

Kubeflow uses Developer Certificate of Origin ([DCO](https://github.com/apps/dco/)).

Check <https://github.com/kubeflow/community/tree/master/dco-signoff-hook#signing-off-commits> to learn how to sign off your contributions.

### Follow the code of conduct

Please make sure to read and observe our [Code of Conduct](https://github.com/kubeflow/community/blob/master/CODE_OF_CONDUCT.md)
and [inclusivity document](https://github.com/kubeflow/community/blob/master/INCLUSIVITY.md).

## Membership

Details about the different types of Kubeflow members as well as membership criteria can be found at [Community Membership](/docs/about/membership/)

**Note**: Anyone can contribute to Kubeflow, joining the Kubeflow organization is not a mandatory step.

### Companies/organizations

If you would like your company or organization to be acknowledged for contributing to Kubeflow, or participating in the community (being a user counts), please send a PR adding the relevant info to [member_organizations.yaml](https://github.com/kubeflow/community/blob/master/member_organizations.yaml).

Additionally, if your company has adopted Kubeflow internally, we encouraage you to add yourself to [ADOPTERS.md](https://github.com/kubeflow/community/blob/master/ADOPTERS.md)!

If you want your employee's GitHub contributions to be attributed to your company,
please ask them to set the company field in their GitHub profile.

## Your first contribution

### Find something to work on

Help is always welcome!
For example, documentation (like the text you are reading now) can always use improvement.
There's always code that can be clarified and variables or functions that can be renamed or commented.
There's always a need for more test coverage.
You get the idea - if you ever see something you think should be fixed, you should own it.
Here is how you get started.

### Starter issues

To find Kubeflow issues that make good entry points:

- Start with issues labeled **good first issue**. For example, see the good first issues in the [kubeflow/website repository](https://github.com/kubeflow/website/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) for documentation updates
- If you're looking for good first issues for code, check out some of the following repositories:
  - [kubeflow/pipelines](https://github.com/kubeflow/pipelines/issues?q=is:open+is:issue+label:%22good+first+issue%22)
  - [kubeflow/trainer](https://github.com/kubeflow/trainer/issues?q=is:open+is:issue+label:%22good+first+issue%22)
  - [kubeflow/model-registry](https://github.com/kubeflow/model-registry/issues?q=is:issue+label:%22good+first+issue%22+is:open)
  - [kubeflow/notebooks](https://github.com/kubeflow/notebooks/issues?q=is:issue+label:%22good+first+issue%22+is:open)
- For issues that require deeper knowledge of one or more technical aspects, look at issues labeled **help wanted**.
  For example, see these issues in the [kubeflow/kubeflow repository](https://github.com/kubeflow/kubeflow/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
- Examine the issues in any of the [Kubeflow repositories](https://github.com/kubeflow).

## Owners files and PR workflow

Our PR workflow is nearly identical to Kubernetes'.
Most of these instructions are a modified version of Kubernetes' [contributors](https://github.com/kubernetes/community/blob/master/contributors/guide/README.md)
and [owners](https://github.com/kubernetes/community/blob/master/contributors/guide/owners.md#code-review-using-owners-files) guides.

### Overview of OWNERS files

OWNERS files are used to designate responsibility over different parts of the Kubeflow codebase.
Today, we use them to assign the **reviewer** and **approver** roles used in our two-phase code review process.
Our OWNERS files were inspired by [Chromium OWNERS files](https://chromium.googlesource.com/chromium/src/+/master/docs/code_reviews.md),
which in turn inspired [GitHub's CODEOWNERS files](https://help.github.com/articles/about-codeowners/).

The velocity of a project that uses code review is limited by the number of people capable of reviewing code.
The quality of a person's code review is limited by their familiarity with the code under review.
Our goal is to address both of these concerns through the prudent use and maintenance of OWNERS files

<a name="owners-1"></a>

### OWNERS

Each directory that contains a unit of independent code or content may also contain an OWNERS file.
This file applies to everything within the directory, including the OWNERS file itself, sibling files, and child directories.

OWNERS files are in YAML format and support the following keys:

- `approvers`: a list of GitHub usernames or aliases that can `/approve` a PR
- `labels`: a list of GitHub labels to automatically apply to a PR
- `options`: a map of options for how to interpret this OWNERS file, currently only one:
  - `no_parent_owners`: defaults to `false` if not present; if `true`, exclude parent OWNERS files.
    Allows the use case where `a/deep/nested/OWNERS` file prevents `a/OWNERS` file from having any
    effect on `a/deep/nested/bit/of/code`
- `reviewers`: a list of GitHub usernames or aliases that are good candidates to `/lgtm` a PR
- `emeritus_approvers` a list of GitHub usernames of folks who were previously in the `approvers`
  section, but are no longer actively approving code. Please see [below](#emeritus) for more details.

All users are expected to be assignable.
In GitHub terms, this means they are either collaborators of the repo, or members of the organization to which the repo belongs.

A typical OWNERS file looks like:

```yaml
approvers:
  - alice
  - bob # this is a comment
reviewers:
  - alice
  - carol # this is another comment
  - sig-foo # this is an alias
```

#### Emeritus

It is inevitable, but there are times when someone may shift focuses, change jobs or step away from
a specific area in the project for a time. These people may be domain experts over certain areas of
the codebase, but can no longer dedicate the time needed to handle the responsibilities of
reviewing and approving changes. They are encouraged to add themselves as an “emeritus”
approver under the `emeritus_approvers` key.

GitHub usernames listed under the `emeritus_approvers` key can no longer approve code
(use the /approve command) and will be ignored by prow for assignment. However, it can still be
referenced by a person looking at the OWNERS file for a possible second or more informed opinion.

When a contributor returns to being more active in that area, they may be promoted back to a
regular approver at the discretion of the current approvers.

```yaml
emeritus_approvers:
  - david
  - emily
```

#### OWNERS_ALIASES

Each repo may contain at its root an OWNERS_ALIAS file.

OWNERS_ALIAS files are in YAML format and support the following keys:

- `aliases`: a mapping of alias name to a list of GitHub usernames

We use aliases for groups instead of GitHub Teams, because changes to GitHub Teams are not publicly auditable.

A sample OWNERS_ALIASES file looks like:

```yaml
aliases:
  sig-foo:
    - david
    - erin
  sig-bar:
    - bob
    - frank
```

GitHub usernames and aliases listed in OWNERS files are case-insensitive.

### The code review process

- The **author** submits a PR
- Phase 0: Automation suggests **reviewers** and **approvers** for the PR
  - Determine the set of OWNERS files nearest to the code being changed
  - Choose at least two suggested **reviewers**, trying to find a unique reviewer for every leaf
    OWNERS file, and request their reviews on the PR
  - Choose suggested **approvers**, one from each OWNERS file, and list them in a comment on the PR
- Phase 1: Humans review the PR
  - **Reviewers** look for general code quality, correctness, sane software engineering, style, etc.
  - Anyone in the organization can act as a **reviewer** with the exception of the individual who
    opened the PR
  - If the code changes look good to them, a **reviewer** types `/lgtm` in a PR comment or review;
    if they change their mind, they `/lgtm cancel`
  - Once a **reviewer** has `/lgtm`'ed, [prow](https://prow.k8s.io)
    ([@k8s-ci-robot](https://github.com/k8s-ci-robot/)) applies an `lgtm` label to the PR
- Phase 2: Humans approve the PR
  - The PR **author** `/assign`'s all suggested **approvers** to the PR, and optionally notifies
    them (eg: "pinging @foo for approval")
  - Only people listed in the relevant OWNERS files, either directly or through an alias, can act
    as **approvers**, including the individual who opened the PR
  - **Approvers** look for holistic acceptance criteria, including dependencies with other features,
    forwards/backwards compatibility, API and flag definitions, etc
  - If the code changes look good to them, an **approver** types `/approve` in a PR comment or
    review; if they change their mind, they `/approve cancel`
  - [prow](https://prow.k8s.io) ([@k8s-ci-robot](https://github.com/k8s-ci-robot/)) updates its
    comment in the PR to indicate which **approvers** still need to approve
  - Once all **approvers** (one from each of the previously identified OWNERS files) have approved,
    [prow](https://prow.k8s.io) ([@k8s-ci-robot](https://github.com/k8s-ci-robot/)) applies an
    `approved` label
- Phase 3: Automation merges the PR:

  - If all of the following are true:

    - All required labels are present (eg: `lgtm`, `approved`)
    - Any blocking labels are missing (eg: there is no `do-not-merge/hold`, `needs-rebase`)

  - And if any of the following are true:

    - there are no presubmit prow jobs configured for this repo
    - there are presubmit prow jobs configured for this repo, and they all pass after automatically
      being re-run one last time

  - Then the PR will automatically be merged

### Quirks of the process

There are a number of behaviors we've observed that while _possible_ are discouraged, as they go
against the intent of this review process. Some of these could be prevented in the future, but this
is the state of today.

- An **approver**'s `/lgtm` is simultaneously interpreted as an `/approve`
  - While a convenient shortcut for some, it can be surprising that the same command is interpreted
    in one of two ways depending on who the commenter is
  - Instead, explicitly write out `/lgtm` and `/approve` to help observers, or save the `/lgtm` for
    a **reviewer**
  - This goes against the idea of having at least two sets of eyes on a PR, and may be a sign that
    there are too few **reviewers** (who aren't also **approver**)
- Technically, anyone who is a member of the Kubeflow GitHub organization can drive-by `/lgtm` a
  PR
  - Drive-by reviews from non-members are encouraged as a way of demonstrating experience and
    intent to become a collaborator or reviewer
  - Drive-by `/lgtm`'s from members may be a sign that our OWNERS files are too small, or that the
    existing **reviewers** are too unresponsive
  - This goes against the idea of specifying **reviewers** in the first place, to ensure that
    **author** is getting actionable feedback from people knowledgeable with the code
- **Reviewers**, and **approvers** are unresponsive
  - This causes a lot of frustration for **authors** who often have little visibility into why their
    PR is being ignored
  - Many **reviewers** and **approvers** are so overloaded by GitHub notifications that @mention'ing
    is unlikely to get a quick response
  - If an **author** `/assign`'s a PR, **reviewers** and **approvers** will be made aware of it on
    their [PR dashboard](https://k8s-gubernator.appspot.com/pr)
  - An **author** can work around this by manually reading the relevant OWNERS files,
    `/unassign`'ing unresponsive individuals, and `/assign`'ing others
  - This is a sign that our OWNERS files are stale; pruning the **reviewers** and **approvers** lists
    would help with this
  - It is the PR **authors** responsibility to drive a PR to resolution. This means if the PR **reviewers** are unresponsive they should escalate as noted below
    - e.g ping **reviewers** in a timely manner to get it reviewed
    - If the **reviewers** don't respond look at the OWNERs file in root and ping **approvers** listed there
- **Authors** are unresponsive
  - This costs a tremendous amount of attention as context for an individual PR is lost over time
  - This hurts the project in general as its general noise level increases over time
  - Instead, close PR's that are untouched after too long (we currently have a bot do this after 90 days)

## Automation using OWNERS files

### [`prow`](https://git.k8s.io/test-infra/prow)

Prow receives events from GitHub, and reacts to them. It is effectively stateless. The following
pieces of prow are used to implement the code review process above.

- [cmd: tide](https://git.k8s.io/test-infra/prow/cmd/tide)
  - per-repo configuration:
    - `labels`: list of labels required to be present for merge (eg: `lgtm`)
    - `missingLabels`: list of labels required to be missing for merge (eg: `do-not-merge/hold`)
    - `reviewApprovedRequired`: defaults to `false`; when true, require that there must be at least
      one [approved pull request review](https://help.github.com/articles/about-pull-request-reviews/)
      present for merge
    - `merge_method`: defaults to `merge`; when `squash` or `rebase`, use that merge method instead
      when clicking a PR's merge button
  - merges PR's once they meet the appropriate criteria as configured above
  - if there are any presubmit prow jobs for the repo the PR is against, they will be re-run one
    final time just prior to merge
- [plugin: assign](https://git.k8s.io/test-infra/prow/plugins/assign)
  - assigns GitHub users in response to `/assign` comments on a PR
  - unassigns GitHub users in response to `/unassign` comments on a PR
- [plugin: approve](https://git.k8s.io/test-infra/prow/plugins/approve)
  - per-repo configuration:
    - `issue_required`: defaults to `false`; when `true`, require that the PR description link to
      an issue, or that at least one **approver** issues a `/approve no-issue`
    - `implicit_self_approve`: defaults to `false`; when `true`, if the PR author is in relevant
      OWNERS files, act as if they have implicitly `/approve`'d
  - adds the `approved` label once an **approver** for each of the required
    OWNERS files has `/approve`'d
  - comments as required OWNERS files are satisfied
  - removes outdated approval status comments
- [plugin: blunderbuss](https://git.k8s.io/test-infra/prow/plugins/blunderbuss)
  - determines **reviewers** and requests their reviews on PR's
- [plugin: lgtm](https://git.k8s.io/test-infra/prow/plugins/lgtm)
  - adds the `lgtm` label when a **reviewer** comments `/lgtm` on a PR
  - the **PR author** may not `/lgtm` their own PR
- [pkg: k8s.io/test-infra/prow/repoowners](https://git.k8s.io/test-infra/prow/repoowners/repoowners.go)
  - parses OWNERS and OWNERS_ALIAS files
  - if the `no_parent_owners` option is encountered, parent owners are excluded from having
    any influence over files adjacent to or underneath of the current OWNERS file

## Maintaining OWNERS files

OWNERS files should be regularly maintained.

We encourage people to self-nominate or self-remove from OWNERS files via PR's. Ideally in the future
we could use metrics-driven automation to assist in this process.

We should strive to:

- grow the number of OWNERS files
- add new people to OWNERS files
- ensure OWNERS files only contain org members and repo collaborators
- ensure OWNERS files only contain people are actively contributing to or reviewing the code they own
- remove inactive people from OWNERS files

Bad examples of OWNERS usage:

- directories that lack OWNERS files, resulting in too many hitting root OWNERS
- OWNERS files that have a single person as both approver and reviewer
- OWNERS files that haven't been touched in over 6 months
- OWNERS files that have non-collaborators present

Good examples of OWNERS usage:

- there are more `reviewers` than `approvers`
- the `approvers` are not in the `reviewers` section
- OWNERS files that are regularly updated (at least once per release)



================================================
File: content/en/docs/about/events.md
================================================
+++
title =  "Events"
description = "Kubeflow Community Events and Meetups"
weight = 15
manualLink = "/events/"
icon = "fa-solid fa-arrow-up-right-from-square"
+++


================================================
File: content/en/docs/about/membership.md
================================================
+++
title = "Community Membership"
description = "Guidelines for contributing to Kubeflow"
weight = 30
aliases = ["/docs/membership/"]
+++

This document outlines the various responsibilities of contributor roles in Kubeflow. Kubeflow is divided into working groups that have stewardship over different subprojects/repositories

Responsibilities for most roles are scoped to these repositories.

| Role | Responsibilities | Requirements | Defined by |
| -----| ---------------- | ------------ | -------|
| Member | Active contributor in the community | Sponsored by 2 Kubeflow members and multiple contributions to the project | Kubeflow GitHub org member|
| Reviewer | Review contributions from other members | History of review and authorship in a repository | [OWNERS](/docs/about/contributing/#owners) file reviewer entry |
| Approver | Contributions acceptance approval| Highly experienced active reviewer and contributor to a repository | [OWNERS](/docs/about/contributing/#owners) file approver entry|
| WG Lead  | Provides technical leadership for a Working Group | Have sufficient domain knowledge to provide effective technical leadership | [wgs.yaml] entry |
| WG Chair | Provides overall leadership for a Working Group | Have sufficient domain knowledge to provide effective leadership | [wgs.yaml] entry |
| Kubeflow Steering Commitee Member | The KSC provides leadership for the overall Kubeflow project | [Details](https://github.com/kubeflow/community/blob/master/KUBEFLOW-STEERING-COMMITTEE.md#charter) | [Members](https://github.com/kubeflow/community/blob/master/KUBEFLOW-STEERING-COMMITTEE.md#charter) |

{{< note >}}
Detailed documentation for Working Group structure and responsibilities can be found at [wg-governance.md](https://github.com/kubeflow/community/blob/master/wgs/wg-governance.md)
{{< /note >}}

## New contributors

[New contributors] should be welcomed to the community by existing members, helped with PR workflow, and directed to the relevant documentation and communication channels.

## Established community members

Established community members are expected to demonstrate their adherence to the principles in this document, familiarity with project organization, roles, policies, procedures, conventions, etc., and technical and/or writing ability. Role-specific expectations, responsibilities, and requirements are enumerated below.

## Member

Members are *[continuously active]* contributors in the community. They can have issues and PRs assigned to them and tests are automatically run for their PRs. Members are expected to remain active contributors to the community.

**Defined by:** Member of the Kubeflow GitHub organization

### Requirements

- Enabled two-factor authentication on their GitHub account
- Have made **at least** 2-3 [code contributions] or [non-code contributions] to the project or community.
- Have read the [contributor guide].
- Sponsored by 2 Kubeflow members. **Note the following requirements for sponsors**:
- **[Open an issue with the membership template][membership template] against the kubeflow/internal-acls repo**
  - Ensure your sponsors are @mentioned on the issue
- **Open a pull request against the kubeflow/internal-acls repo**
  - Complete every item on the checklist ([preview the current version of the template][membership template])
  - Make sure that the list of contributions included is representative of your work on the project.
- Have your sponsoring reviewers reply confirmation of sponsorship
- Once your sponsors have responded, your request will be reviewed by the Kubeflow team. Any missing information will be requested
- After your PR is merged, you will get an email (to your GitHub-associated email address) inviting you to the Kubeflow GitHub org. Follow the instructions to accept your membership.
- To confirm that the membership acceptance process has completed, you can search for your GitHub username at https://github.com/orgs/kubeflow/people.

### Responsibilities

- Responsive to issues and PRs assigned to them
- Active participants in the Kubeflow community by participating in:
  - Working Group Meetings
  - Slack Discussions
  - Project Discussions
- Responsive to mentions of any teams they may be members of
- Active owner of code they have contributed (unless ownership is explicitly transferred)
  - Code is well tested
  - Tests consistently pass
  - Addresses bugs or issues discovered after code is accepted
- Subscribed to <https://groups.google.com/g/kubeflow-discuss>

{{< note >}}
Members who frequently contribute code are expected to proactively perform code reviews and work towards becoming a primary *reviewer* for the subproject that they are active in.
{{< /note >}}

### Privileges

- Members can do `/lgtm` on open PRs.
- They can be assigned to issues and PRs, and people can ask members for reviews with a `/cc @username`.
- They are eligible to be appointed as a Kubeflow release manager
- Tests can be run against their PRs automatically. No `/ok-to-test` needed.
- Members can do `/ok-to-test` for PRs that have a `needs-ok-to-test` label, and use commands like `/close` to close PRs as well. A complete list of commands can be found in [the Prow documentation](https://prow.k8s.io/command-help)

## Reviewer

Reviewers are able to review code for quality and correctness on some part of a subproject. They are knowledgeable about both the codebase and software engineering principles.

**Defined by:** *reviewers* entry in an `OWNERS` file in a repo owned by the Kubeflow organization.

Reviewer status can be scoped to either parts of the codebase or the root directory for the entire codebase.

{{< note >}}
Acceptance of code contributions requires at least one approver in addition to the assigned reviewers.
{{< /note >}}

### Requirements

The following apply to the part of codebase for which one would be a reviewer in an [OWNERS](/docs/about/contributing/#owners) file.

- member for at least 3 months
- Primary reviewer for at least 5 PRs to the codebase
- Reviewed or merged at least 15 substantial PRs to the codebase
- Knowledgeable about the codebase
- Active engagement with the commmunity by answering user questions in GitHub issues and Slack
- Sponsored by a subproject approver
  - With no objections from other approvers
  - Done through PR to update the OWNERS file
- May either self-nominate or be nominated by an approver in this subproject

{{< note >}}
Working Group Leads may nominate and approve `Reviewers` that don't meet these requirements due to exceptional circumstances. While acceptable in the short term, Working Group Leads should ensure that these `Reviewers` eventually meet the requirements
{{< /note >}}

The following apply to the part of codebase for which one would be a reviewer in an [OWNERS](/docs/about/contributing/#owners) file.

### Responsibilities

- All responsiblities that community members have
- Responsible for project quality control via code reviews
  - Focus on code quality and correctness, including testing and factoring
  - May also review for more holistic issues, but not a requirement
- Expected to be responsive to review requests
- Expected to actively engage with the community by answering questions in GitHub issues and Slack
- Assigned PRs to review related to subproject of expertise
- Assigned test bugs related to subproject of expertise

### Privileges

- All Privileges that community members have
- Code reviewer status may be a precondition to accepting large code contributions
- May get a badge on PR and issue comments

## Approver

Code approvers are able to both review and approve code contributions. While
code review is focused on code quality and correctness, approval is focused on
holistic acceptance of a contribution including: backwards / forwards
compatibility, adhering to API and flag conventions, subtle performance and
correctness issues, interactions with other parts of the system, overall code test coverage, etc.

**Defined by:** *approvers* entry in an OWNERS file in a repo owned by the Kubeflow organization.

Approver status can be scoped to either parts of the codebase or the root directory for the entire codebase.

### Requirements

The following apply to the part of codebase for which one would be an approver in an [OWNERS](/docs/about/contributing/#owners) file.

- Have met the responsibilities of the `Reviewer` role (as defined above) of the codebase for at least 3 months
- Primary reviewer for at least 10 substantial PRs to the codebase
- Reviewed or merged at least 30 PRs to the codebase
- Nominated by a WG Lead or Chair
  - With no objections from other Leads or Chairs
  - Done through PR to update the relevant OWNERS file

{{< note >}}
Working Group Leads may nominate and approve `Approvers` that don't meet these requirements due to exceptional circumstances. While acceptable in the short term, Working Group Leads should ensure that these `Approvers` eventually meet the requirements
{{< /note >}}

### Responsibilities

The following apply to the part of codebase for which one would be an approver in an [OWNERS](/docs/about/contributing/#owners) file.

- All responsibilities that reviewers have
- Approver status may be a precondition to accepting large architectural contributions
- Demonstrate sound technical judgement
- Responsible for project quality control via code reviews
  - Focus on holistic acceptance of contribution such as dependencies with other features, backwards / forwards
    compatibility, API and flag definitions, etc
- Expected to be responsive to review requests
- Expected to be responsive to merge requests for pull requests when reviewed
- Mentor contributors and reviewers

### Privileges

- All privileges that reviewers have
- May approve code contributions for acceptance

## Inactive members

*Members are continuously active contributors in the community.*

A core principle in maintaining a healthy community is encouraging active
participation. It is inevitable that people's focuses will change over time and
they are not expected to be actively contributing forever.

However, being a member of one of the Kubeflow GitHub organizations comes with
an elevated set of permissions. These capabilities should not be used by those
that are not familiar with the current state of the Kubeflow organization.

Therefore members with an extended period (1 year) away from the organization with no activity
will be removed from the Kubeflow GitHub Organizations and will be required to
go through the org membership process again after re-familiarizing themselves
with the current state.

If anyone listed in OWNERS files should become inactive, here is what we will do:

- If the person is in reviewers section, their GitHub id will be removed from the section.
- If the person is in approvers section, their GitHub id will be moved
  [the `emeritus_approvers` section](/docs/about/contributing/#emeritus).

### How inactivity is measured

Inactive members are defined as members of one of the Kubeflow Organizations with **no** technical and non-technical contributions across any organization within 12 months. [DevStats](https://kubeflow.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&var-period_name=Last%20year&var-metric=contributions&var-repogroup_name=All&var-country_name=All&var-companies=All) offers an easy way to determine contributions to Kubeflow

After an extended period away from the project with no activity those members would need to re-familiarize themselves with the current state before being able to contribute effectively.

## Credit

This set of guidelines is heavily inspired by the [Kubernetes membership guidelines](https://github.com/kubernetes/community/blob/master/community-membership.md?plain=1#community-membership).

[code contributions]: https://contribute.cncf.io/contributors/getting-started/#code-contributors
[non-code contributions]: https://contribute.cncf.io/contributors/getting-started/#non-code-contributors
[contributor guide]: https://www.kubeflow.org/docs/about/contributing/
[membership template]: https://github.com/kubeflow/internal-acls/blob/master/.github/ISSUE_TEMPLATE/join_org.md
[New contributors]: /docs/about/contributing/
[continuously active]: #inactive-members
[wgs.yaml]: https://github.com/kubeflow/community/blob/master/wgs.yaml



================================================
File: content/en/docs/about/style-guide.md
================================================
+++
title =  "Documentation Style Guide"
description = "Style guide for writing Kubeflow documentation"
weight = 90
+++

This style guide is for the [Kubeflow documentation](/docs/).
The style guide helps contributors to write documentation that readers can understand quickly and correctly. 

The Kubeflow docs aim for:

- Consistency in style and terminology, so that readers can expect certain
  structures and conventions. Readers don't have to keep re-learning how to use
  the documentation or questioning whether they've understood something
  correctly.
- Clear, concise writing so that readers can quickly find and understand the
  information they need.

## Use standard American spelling

Use American spelling rather than Commonwealth or British spelling.
Refer to [Merriam-Webster's Collegiate Dictionary, Eleventh Edition](https://www.merriam-webster.com/).

## Use capital letters sparingly

Some hints:

- Capitalize only the first letter of each heading within the page. (That is, use sentence case.)
- Capitalize (almost) every word in page titles. (That is, use title case.) 
  The little words like "and", "in", etc, don't get a capital letter.
- In page content, use capitals only for brand names, like Kubeflow, Kubernetes, and so on. 
  See more about brand names [below](#use-full-correct-brand-names).
- Don't use capital letters to emphasize words.

## Spell out abbreviations and acronyms on first use

Always spell out the full term for every abbreviation or acronym the first time you use it on the page. 
Don't assume people know what an abbreviation or acronym means, even if it seems like common knowledge.

Example: "To run Kubernetes locally in a virtual machine (VM)"

## Use contractions if you want to

For example, it's fine to write "it's" instead of "it is".

<a id="brand-names"></a>

## Use full, correct brand names

When referring to a product or brand, use the full name. 
Capitalize the name as the product owners do in the product documentation. 
Do not use abbreviations even if they're in common use, unless the product owner has sanctioned the abbreviation.

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Use this</th>
        <th>Instead of this</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Kubeflow</td>
        <td>kubeflow</td>
      </tr>
      <tr>
        <td>Kubernetes</td>
        <td>k8s</td>
      </tr>
      <tr>
        <td>ksonnet</td>
        <td>Ksonnet</td>
      </tr>
    </tbody>
  </table>
</div>

## Be consistent with punctuation

Use punctuation consistently within a page. 
For example, if you use a period (full stop) after every item in list, then use a period on all other lists on the page.

Check the other pages if you're unsure about a particular convention.

Examples:

- Most pages in the Kubeflow docs use a period at the end of every list item.
- There is no period at the end of the page subtitle and the subtitle need not be a full sentence. 
  (The subtitle comes from the `description` in the front matter of each page.)

## Use active voice rather than passive voice

Passive voice is often confusing, as it's not clear who should perform the action.

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Use active voice</th>
        <th>Instead of passive voice</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>You can configure Kubeflow to</td>
        <td>Kubeflow can be configured to</td>
      </tr>
      <tr>
        <td>Add the directory to your path</td>
        <td>The directory should be added to your path</td>
      </tr>
    </tbody>
  </table>
</div>

## Use simple present tense

Avoid future tense ("will") and complex syntax such as conjunctive mood ("would", "should").

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Use simple present tense</th>
        <th>Instead of future tense or complex syntax</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>The following command provisions a virtual machine</td>
        <td>The following command will provision a virtual machine</td>
      </tr>
      <tr>
        <td>If you add this configuration element, the system is open to
          the Internet</td>
        <td>If you added this configuration element, the system would be open to
          the Internet</td>
      </tr>
    </tbody>
  </table>
</div>

**Exception:** Use future tense if it's necessary to convey the correct meaning. This requirement is rare.

## Address the audience directly

Using "we" in a sentence can be confusing, because the reader may not know whether they're part of the "we" you're describing. 

For example, compare the following two statements:

- "In this release we've added many new features."
- "In this tutorial we build a flying saucer."

The words "the developer" or "the user" can be ambiguous. 
For example, if the reader is building a product that also has users, 
then the reader does not know whether you're referring to the reader or the users of their product.

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Address the reader directly</th>
        <th>Instead of "we", "the user", or "the developer"</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Include the directory in your path</td>
        <td>The user must make sure that the directory is included in their path
        </td>
      </tr>
      <tr>
        <td>In this tutorial you build a flying saucer</td>
        <td>In this tutorial we build a flying saucer</td>
      </tr>
    </tbody>
  </table>
</div>

## Use short, simple sentences

Keep sentences short. Short sentences are easier to read than long ones. 
Below are some tips for writing short sentences.

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th colspan="2">Use fewer words instead of many words that convey the same meaning</th>
      </tr>
      <tr>
        <th>Use this</th>
        <th>Instead of this</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>You can use</td>
        <td>It is also possible to use</td>
      </tr>
      <tr>
        <td>You can</td>
        <td>You are able to</td>
      </tr>
    </tbody>
  </table>
</div>

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th colspan="2">Split a single long sentence into two or more shorter ones</th>
      </tr>
      <tr>
        <th>Use this</th>
        <th>Instead of this</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>You do not need a running GKE cluster. The deployment process
          creates a cluster for you</td>
        <td>You do not need a running GKE cluster, because the deployment 
          process creates a cluster for you</td>
      </tr>
    </tbody>
  </table>
</div>

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th colspan="2">Use a list instead of a long sentence showing various options</th>
      </tr>
      <tr>
        <th>Use this</th>
        <th>Instead of this</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>
          <p>To train a model:</p>
          <ol>
            <li>Package your program in a Kubernetes container.</li>
            <li>Upload the container to an online registry.</li>
            <li>Submit your training job.</li>
          </ol>
        </td>
        <td>To train a model, you must package your program in a Kubernetes 
          container, upload the container to an online registry, and submit your 
          training job.</td>
      </tr>
    </tbody>
  </table>
</div>

## Avoid too much text styling

Use **bold text** when referring to UI controls or other UI elements.

Use `code style` for:

- filenames, directories, and paths
- inline code and commands
- object field names

Avoid using bold text or capital letters for emphasis. 
If a page has too much textual highlighting it becomes confusing and even annoying.

## Use angle brackets for placeholders

For example:

- `export KUBEFLOW_USERNAME=<your username>`
- `--email <your email address>`

## Style your images

The Kubeflow docs recognise Bootstrap classes to style images and other content.

The following code snippet shows the typical styling that makes an image show up nicely on the page:

```
<img src="/docs/images/my-image.png"
  alt="My image"
  class="mt-3 mb-3 p-3 border border-info rounded">
```

To see some examples of styled images, take a look at the [OAuth setup page](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/deploy/oauth-setup/).

For more help, see the guide to [Bootstrap image styling](https://getbootstrap.com/docs/4.6/content/images/) and the Bootstrap utilities, such as [borders](https://getbootstrap.com/docs/4.6/utilities/borders/).

## A detailed style guide

The [Google Developer Documentation Style Guide](https://developers.google.com/style/) contains detailed information about specific aspects of writing clear, readable, succinct documentation for a developer audience.

## Next steps

- Take a look at the [documentation README](https://github.com/kubeflow/website/blob/master/README.md) for guidance on contributing to the Kubeflow docs.




================================================
File: content/en/docs/components/_index.md
================================================
+++
title = "Components"
description = "Logical components that make up Kubeflow"
weight = 30
+++



================================================
File: content/en/docs/components/central-dash/OWNERS
================================================
approvers:
  - kimwnasptd
  - thesuperzapper


================================================
File: content/en/docs/components/central-dash/_index.md
================================================
+++
title = "Central Dashboard"
description = "The central user interface (UI) in Kubeflow"
weight = 5
+++



================================================
File: content/en/docs/components/central-dash/access.md
================================================
+++
title = "Access the Dashboard"
description = "How to access the Kubeflow Central Dashboard via the Istio Gateway"
weight = 15
+++

## How to access the Kubeflow Central Dashboard?

To access the Kubeflow central dashboard, you need to connect to the [Istio Gateway](https://istio.io/docs/concepts/traffic-management/#gateways) that provides access to the Kubeflow [service mesh](https://istio.io/docs/concepts/what-is-istio/#what-is-a-service-mesh).
How you access the Istio gateway varies depending on how you've configured it.

## Packaged Distributions

Each [packaged distribution of Kubeflow](/docs/started/installing-kubeflow/#packaged-distributions) will have its own way of accessing the central dashboard.

For more information, please see the documentation of the distribution you are using.

## Raw Manifests

If you are using the default [Kubeflow Manifests](/docs/started/installing-kubeflow/#kubeflow-manifests), you may access the Istio gateway with `kubectl` port-forwarding or another method.

### kubectl port-forwarding

To access the central dashboard using `kubectl` port-forwarding:

1. [Install `kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if you haven't already done so.
2. Use the following command to set up port forwarding on your local machine:

    ```bash
    export ISTIO_NAMESPACE=istio-system
    kubectl port-forward svc/istio-ingressgateway -n ${ISTIO_NAMESPACE} 8080:80
    ```

3. Open a browser and navigate to: [http://localhost:8080](http://localhost:8080)
4. If you have not changed the default username and password, you may log in with:
    - Username: `user@example.com`
    - Password: `12341234`

## Next steps

- Learn about [Profiles and Namespaces](/docs/components/central-dash/profiles/).


================================================
File: content/en/docs/components/central-dash/customize.md
================================================
+++
title = "Customize the Dashboard"
description = "Customize the Kubeflow Central Dashboard menu items and integrate third-party apps"
weight = 90
+++

## How to customize the Kubeflow Central Dashboard?

The Kubeflow Central Dashboard provides a way to customize the menu items and integrate third-party apps.

For example, the below image shows the Kubeflow Central Dashboard with a custom **"My App"** menu item:

<img src="/docs/images/dashboard/custom-menu-item.png" 
     alt="Kubeflow Central Dashboard - Customize Menu Items"
     class="mt-3 mb-3 border border-info rounded">
</img>

## Central Dashboard ConfigMap

The Kubeflow Central Dashboard is configured using a Kubernetes [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/).

The `CD_CONFIGMAP_NAME` environment variable on the central-dashboard Deployment specifies the name of the ConfigMap (`centraldashboard-config` by default).

You can find examples of the ConfigMap in the following locations:

- [Default](https://github.com/kubeflow/kubeflow/blob/v1.9.0/components/centraldashboard/manifests/base/configmap.yaml)
- [Default + KServe](https://github.com/kubeflow/kubeflow/blob/v1.9.0/components/centraldashboard/manifests/overlays/kserve/patches/configmap.yaml)

## External Links

The `externalLinks` section of the ConfigMap adds links to the sidebar for external sites (not hosted on the Kubernetes cluster).

Each element of `externalLinks` is a JSON object with the following fields:

- `type`: must be set to `"item"`
- `iframe`: must be set to `false`
- `text`: the text to display for the link
- `link`: the URL to open when the link is clicked
- `icon`: an [iron-icon](https://www.webcomponents.org/element/@polymer/iron-icons/demo/demo/index.html) name to display for the link.
    - Note, you must exclude the `icons:` prefix
    - For example, to use `icons:launch` you would set `"launch"`
    - For example, to use `social:mood` you would set `"social:mood"`

For example, the below ConfigMap adds a link to the Kubeflow website:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: centraldashboard-config
  namespace: kubeflow
data:
  settings: |-
    ...
  links: |-
    {
      "menuLinks": [
        ...
      ],
      "externalLinks": [
        {
          "type": "item",
          "iframe": false,
          "text": "Kubeflow Website",
          "link": "https://www.kubeflow.org/",
          "icon": "launch"
        }
      ],
      "quickLinks": [
        ...
      ],
      "documentationItems": [
        ...
      ]
    }
```

## Documentation Links

The `documentationItems` section of the ConfigMap adds links to the "Documentation" section of the Home page.

Each element of `documentationItems` is a JSON object with the following fields:

- `text`: the text to display for the link
- `desc`: the description to display below the link
- `link`: the URL to open when the link is clicked

For example, the below ConfigMap adds a link to the Kubeflow website documentation:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: centraldashboard-config
  namespace: kubeflow
data:
  settings: |-
    ...
  links: |-
    {
      "menuLinks": [
        ...
      ],
      "externalLinks": [
        ...
      ],
      "quickLinks": [
        ...
      ],
      "documentationItems": [
        {
          "text": "Kubeflow Website",
          "desc": "Kubeflow website documentation",
          "link": "https://www.kubeflow.org/docs/"
        }
      ]
    }
```

## In-Cluster Links

### Create VirtualService

If you have a non-Kubeflow application running on the cluster, you may expose it through the Kubeflow Central Dashboard by creating a [`VirtualService`](https://istio.io/latest/docs/reference/config/networking/virtual-service/) on the Kubeflow Istio Gateway.
To do this, your app must have an injected Istio sidecar and be exposed as a Kubernetes Service.

For example, the below `VirtualService` exposes `Service/my-app` from the `my-namespace` namespace on the Kubeflow Istio Gateway under the path `/my-app/`:

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: my-custom-app
  namespace: <MY_APP_NAMESPACE>
spec:
  gateways:
    ## the istio gateway which is serving kubeflow
    ## TEMPLATE: <KUBEFLOW_GATEWAY_NAMESPACE>/<KUBEFLOW_GATEWAY_NAME>
    - kubeflow/kubeflow-gateway
  hosts:
    - '*'
  http:
    - headers:
        request:
          add:
            x-forwarded-prefix: /my-app
      match:
        - uri:
            prefix: /my-app/
      rewrite:
        uri: /
      route:
        - destination:
            host: <MY_APP_SERVICE_NAME>.<MY_APP_NAMESPACE>.svc.cluster.local
            port:
              number: 80
```

Creating this `VirtualService` should make the application available at the `/_/my-app/` path on the Kubeflow Istio Gateway.

```text
http(s)://<KUBEFLOW_ISTIO_GATEWAY>/_/my-app/
```

{{% alert title="UserID Header Authentication" color="info" %}}
Each request to the application will have a header named `kubeflow-userid` with the user's email address, which may be used for authentication.

To ensure that this header is not spoofed, you should ensure that the application is ONLY accessible from the Kubeflow Istio Gateway.
This could be achieved by:

- creating an ALLOW `AuthorizationPolicy` which requires the `from[].source[].principals[]` to be `cluster.local/ns/<ISTIO_GATEWAY_NAMESPACE>/sa/<ISTIO_GATEWAY_SERVICE_ACCOUNT>`
- ensuring that out-of-mesh traffic is blocked by the sidecar using a `DestinationRule` with `trafficPolicy.tls.mode` set to `ISTIO_MUTUAL` for the `Service` backing the application
{{% /alert %}}

### Add In-Cluster Link

The `menuLinks` section of the ConfigMap adds links to the sidebar for __in-cluster__ applications.

Each element of `menuLinks` is a JSON object with the following fields:

- `type`: must be set to `"item"`
- `link`: the path to open when the link is clicked
- `text`: the text to display for the link
- `icon`: an [iron-icon](https://www.webcomponents.org/element/@polymer/iron-icons/demo/demo/index.html) name to display for the link.
    - Note, you must exclude the `icons:` prefix
    - For example, to use `icons:launch` you would set `"launch"`
    - For example, to use `social:mood` you would set `"social:mood"`

For example, the below ConfigMap adds the ["my-app" application](#create-virtualservice) from above:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: centraldashboard-config
  namespace: kubeflow
data:
  settings: |-
    ...
  links: |-
    {
      "menuLinks": [
        ...
        {
          "type": "item",
          "link": "/my-app/",
          "text": "My App",
          "icon": "social:mood"
        },
        ...
      ],
      "externalLinks": [
        ...
      ],
      "quickLinks": [
        ...
      ],
      "documentationItems": [
        ...
      ]
    }
```

{{% alert title="Namespaced Applications" color="info" %}}
If you have instances of your application in each profile namespace, you can use `{ns}` in the `link` field to dynamically insert the active profile namespace into the link.

For example, if you have an instance of the application in the `profile1` namespace and another instance in the `profile2` namespace.
You may configure your `VirtualService` to expose the application under the path `/my-app/{ns}/`:

```text
http(s)://<KUBEFLOW_ISTIO_GATEWAY>/_/my-app/profile1/
http(s)://<KUBEFLOW_ISTIO_GATEWAY>/_/my-app/profile2/
```

The `menuLinks` element for such and app might look like this:

```json
{
  "type": "item",
  "link": "/my-app/{ns}/",
  "text": "My App",
  "icon": "social:mood"
}
```

Because the application pods are within the profile namespaces, existing Kubeflow AuthorizationPolicies should restrict the application to profile contributors.
For example, if the user is a contributor of the `profile1` namespace (but not `profile2`), they will be able to access `http(s)://<KUBEFLOW_ISTIO_GATEWAY>/_/my-app/profile1/` but not `http(s)://<KUBEFLOW_ISTIO_GATEWAY>/_/my-app/profile2/`.
{{% /alert %}}


================================================
File: content/en/docs/components/central-dash/overview.md
================================================
+++
title = "Overview"
description = "Overview of the Kubeflow Central Dashboard"
weight = 10
+++

{{% stable-status %}}

## What is the Kubeflow Central Dashboard?

The _Kubeflow Central Dashboard_ provides an authenticated web interface for Kubeflow and ecosystem components. 
It acts as a hub for your machine learning platform and tools by exposing the UIs of components running in the cluster.

Some core features of the central dashboard include:

- Authentication and authorization based on [Profiles and Namespaces](/docs/components/central-dash/profiles/).
- Access to the [user interface's](#navigation) of Kubeflow components.
- The ability to [customize and include links](/docs/components/central-dash/customize/) to third-party applications.

## Screenshots

Here is a screenshot of the Kubeflow Central Dashboard:

<img src="/docs/images/dashboard/homepage.png" 
     alt="Kubeflow Central Dashboard - Homepage" 
     class="mt-3 mb-3 border border-info rounded">
</img>

## Navigation

Kubeflow and its components have a number of user interfaces which you access from the central dashboard.

Here is a list of the main pages, grouped by component.

### Core Sections

The following sections are available in all Kubeflow deployments:

- **Home**: landing page for Kubeflow Central Dashboard
- **Manage Contributors**: manage contributors of profiles (namespaces) that you own

### Kubeflow Notebooks

The following sections are available when [Kubeflow Notebooks](/docs/components/notebooks/) is installed:

- **Notebooks**: manage Kubeflow Notebooks
- **TensorBoards**: manage TensorBoard instances
- **Volumes**: manage Kubernetes PVC Volumes

### Kubeflow Katib

The following sections are available when [Katib](/docs/components/katib/) is installed:

- **Katib Experiments**: manage Katib AutoML experiments

### KServe

The following sections are available when [KServe](/docs/external-add-ons/kserve/) is installed:

- **KServe Endpoints**: manage deployed KServe model endpoints

### Kubeflow Pipelines

When [Kubeflow Pipelines](/docs/components/pipelines/) is installed, you can select **Pipelines** from the sidebar:

<img src="/docs/images/dashboard/pipelines-runs.png" 
     alt="Kubeflow Central Dashboard - Pipelines - Runs" 
     class="mt-3 mb-3 border border-info rounded">
</img>

In the **Pipelines** section, you can access the following pages:

- **Pipelines**: manage pipeline definitions
- **Experiments**: manage pipeline experiments
- **Runs**: manage pipeline runs
- **Recurring Runs**: manage recurring pipeline runs
- **Artifacts**: track artifacts produced by pipelines stored in MLMD
- **Executions**: track executions of pipeline components stored in MLMD

## Next steps

- Learn how to [Access the Central Dashboard](/docs/components/central-dash/access/).
- Learn about [Profiles and Namespaces](/docs/components/central-dash/profiles/).


================================================
File: content/en/docs/components/central-dash/profiles.md
================================================
+++
title = "Profiles and Namespaces"
description = "About Kubeflow Profiles and Namespaces for multi-user isolation"
weight = 20
+++

## What is a Kubeflow Profile?

A Kubeflow Profile is a [Kubernetes CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions) introduced by Kubeflow that wraps a Kubernetes [Namespace](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).
Profiles are owned by a single user, and can have multiple contributors with view or modify access.
The owner of a profile can add and remove contributors (this can also be done by the cluster administrator).

Profiles and their child Namespaces are reconciled by the [Kubeflow Profile Controller](https://github.com/kubeflow/kubeflow/tree/master/components/profile-controller) and contributors (not owners) are managed by the [Kubeflow Access Management API (KFAM)](https://github.com/kubeflow/kubeflow/tree/master/components/access-management).

## Profiles in the Central Dashboard

Select the active profile with the drop-down found in the top bar of Kubeflow Central Dashboard.
Most Kubeflow components use the active profile to determine which resources to display, and what permissions to grant.

Users can only see profiles to which they have owner, contributor (read + write), or viewer (read) access.

<img src="/docs/images/dashboard/homepage-profile-selector.png" 
     alt="Kubeflow Central Dashboard - Profile Selector" 
     class="mt-3 mb-3 border border-info rounded">
</img>

## Automatic Profile Creation

Kubeflow supports automatic profile creation for users who log into Kubeflow for the first time.

The `CD_REGISTRATION_FLOW` environment variable on the central-dashboard Deployment controls whether automatic profile creation is enabled.
By default, automatic profile creation is disabled.
When `CD_REGISTRATION_FLOW` is `true`, if a user logs into Kubeflow, and is not already a profile owner, they will be prompted to create a profile.

{{% alert title="Warning" color="warning" %}}
Automatic profile creation may not be suitable for all use cases.
<br>
Users become owners of the automatically created profile, so can add/remove contributors.

Cluster administrators may choose to disable automatic profile creation and [manually create profiles](#create-a-profile) for users and/or teams.
Typically, in these cases, users are only given view or modify access to profiles (not made owners).
{{% /alert %}}

Here is an example of the automatic profile creation flow:

1. A new user logs into Kubeflow for the first time:

<img src="/docs/images/dashboard/auto-profile-step-1.png"
     alt="Kubeflow Central Dashboard - Automatic Profile Creation - Step 1"
     class="mt-3 mb-3 border border-info rounded"
     style="width: 100%; max-width: 30em">
</img>

2. The user can name their profile and click *Finish*: 

<img src="/docs/images/dashboard/auto-profile-step-2.png"
     alt="Kubeflow Central Dashboard - Automatic Profile Creation - Step 2"
     class="mt-3 mb-3 border border-info rounded"
     style="width: 100%; max-width: 30em">
</img>

## Profile Resources

The following resources are created for each profile:

- A Kubernetes Namespace that shares the same name as the profile.
- Kubernetes [RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) for Users:
    - For profile owner, a `RoleBinding` named `namespaceAdmin` to `ClusterRole/kubeflow-admin`
    - For each contributor, a `RoleBinding` named `user-{EMAIL}-clusterrole-{ROLE}` to `ClusterRole/kubeflow-{ROLE}`
       - `{EMAIL}` is the email of the contributor, special characters replaced with `-`, cast to lowercase.
       - `{ROLE}` is the role of the contributor, either `edit` or `view`
- Kubernetes [RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) for ServiceAccounts:
    - For `ServiceAcount/default-editor`, a `RoleBinding` named `default-editor` to `ClusterRole/kubeflow-edit`
    - For `ServiceAcount/default-viewer`, a `RoleBinding` named `default-viewer` to `ClusterRole/kubeflow-view`
- Istio [AuthorizationPolicies](https://istio.io/latest/docs/reference/config/security/authorization-policy/):
    - For the profile owner, an `AuthorizationPolicy` named `ns-owner-access-istio`
    - For each contributor, an `AuthorizationPolicy` named `user-{EMAIL}-clusterrole-{ROLE}`
       - `{EMAIL}` is the email of the contributor, special characters replaced with `-`, cast to lowercase
       - `{ROLE}` is the role of the contributor, either `edit` or `view`

## Manage Profiles 

Because a Profile is a Kubernetes CRD, a cluster administrator can use `kubectl` commands to manage profiles.

### Create a Profile

A cluster administrator can create a new profile with `kubectl` commands.

First, create a file named `my-profile.yaml` with the following structure:

```yaml
apiVersion: kubeflow.org/v1
kind: Profile
metadata:
  ## the profile name will be the namespace name
  ## WARNING: unexpected behavior may occur if the namespace already exists
  name: my-profile
spec:
  ## the owner of the profile
  ## NOTE: you may wish to make a global super-admin the owner of all profiles
  ##       and only give end-users view or modify access to profiles to prevent
  ##       them from adding/removing contributors
  owner:
    kind: User
    name: admin@example.com

  ## plugins extend the functionality of the profile
  ## https://github.com/kubeflow/kubeflow/tree/master/components/profile-controller#plugins
  plugins: []
  
  ## optionally create a ResourceQuota for the profile
  ## https://github.com/kubeflow/kubeflow/tree/master/components/profile-controller#resourcequotaspec
  ## https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/#ResourceQuotaSpec
  resourceQuotaSpec: {}
```

Next, run the following command to create the profile:

```bash
kubectl apply -f my-profile.yaml
```

### List all Profiles

A cluster administrator can list existing profiles using the following command:

```bash
kubectl get profiles
```

### Describe a Profile

A cluster administrator can describe a specific profile using the following command:

```bash
kubectl describe profile MY_PROFILE_NAME
```

### Delete a Profile

A cluster administrator can delete an existing profile using the following command:

```bash
kubectl delete profile MY_PROFILE_NAME
```

{{% alert title="Warning" color="warning" %}}
Deleting a profile also deletes the corresponding Namespace from the cluster.
<br>
All resources created in the profile namespace will be deleted.
{{% /alert %}}

## Manage Profile Contributors

Profile contributors are defined by the __presence__ of [specific `RoleBinding` and `AuthorizationPolicy` resources](#profile-resources) in the profile namespace.

{{% alert title="Note" color="info" %}}
The [central dashboard method](#manage-contributors-with-central-dashboard) ONLY allows you to add contributors with "edit" access.
<br>
To add contributors with "view" access, you must use the [manual method](#manage-contributors-manually).
{{% /alert %}}

### Manage Contributors with Central Dashboard

The __owner__ of a profile can use the __Manage Contributors__ tab in the Kubeflow Central Dashboard to add or remove contributors.

<img src="/docs/images/dashboard/homepage-manage-contributors.png" 
     alt="Kubeflow Central Dashboard - Manage Contributors Link"
     class="mt-3 mb-3 border border-info rounded">
</img>

Contributors are managed with the "Contributors to your namespace" field.

<img src="/docs/images/dashboard/manage-contributors.png" 
     alt="Kubeflow Central Dashboard - Manage Contributors"
     class="mt-3 mb-3 border border-info rounded"
     style="width: 100%; max-width: 40em">
</img>

### Manage Contributors Manually

An administrator can manually add contributors to an existing profile by creating the [required `RoleBinding` and `AuthorizationPolicy` resources](#profile-resources) in the profile namespace.

#### Create Contributor RoleBinding

The `RoleBinding` which grants a user access to a profile is structured as follows:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: user-<SAFE_USER_EMAIL>-clusterrole-<USER_ROLE>
  namespace: <PROFILE_NAME>
  annotations:
    role: <USER_ROLE>
    user: <RAW_USER_EMAIL>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubeflow-<USER_ROLE>
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: <RAW_USER_EMAIL>
```

Where the following variables are replaced with the appropriate values:

- `<RAW_USER_EMAIL>` the email of the user (case-sensitive)
- `<SAFE_USER_EMAIL>` the email of the user (special characters replaced with `-`, and cast to lowercase)
- `<USER_ROLE>` the role of the user, either `edit` or `view`
- `<PROFILE_NAME>` the name of the profile

#### Create Contributor AuthorizationPolicy

The `AuthorizationPolicy` which grants a user access to a profile is structured as follows:

```yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: user-<SAFE_USER_EMAIL>-clusterrole-<USER_ROLE>
  namespace: <PROFILE_NAME>
  annotations:
    role: <USER_ROLE>
    user: <RAW_USER_EMAIL>
spec:
  rules:
    - from:
        - source:
            ## for more information see the KFAM code:
            ## https://github.com/kubeflow/kubeflow/blob/v1.8.0/components/access-management/kfam/bindings.go#L79-L110
            principals:
              ## required for kubeflow notebooks
              ## TEMPLATE: "cluster.local/ns/<ISTIO_GATEWAY_NAMESPACE>/sa/<ISTIO_GATEWAY_SERVICE_ACCOUNT>"
              - "cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"

              ## required for kubeflow pipelines
              ## TEMPLATE: "cluster.local/ns/<KUBEFLOW_NAMESPACE>/sa/<KFP_UI_SERVICE_ACCOUNT>"
              - "cluster.local/ns/kubeflow/sa/ml-pipeline-ui"
      when:
        - key: request.headers[kubeflow-userid]
          values:
            - <RAW_USER_EMAIL>
```

Where the following variables are replaced with the appropriate values:

- `<RAW_USER_EMAIL>` the email of the user (case-sensitive)
- `<SAFE_USER_EMAIL>` the email of the user (special characters replaced with `-`, and cast to lowercase)
- `<USER_ROLE>` the role of the user, either `edit` or `view`
- `<PROFILE_NAME>` the name of the profile
- `<KUBEFLOW_NAMESPACE>` the namespace where Kubeflow is installed
- `<KFP_UI_SERVICE_ACCOUNT>` the name of the ServiceAccount used by `ml-pipeline-ui` Pod
- `<ISTIO_GATEWAY_NAMESPACE>` the namespace containing the Istio Gateway Deployment
- `<ISTIO_GATEWAY_SERVICE_ACCOUNT>` the name of the ServiceAccount used by the Istio Gateway Pods


================================================
File: content/en/docs/components/katib/OWNERS
================================================
approvers:
  - andreyvelich
  - gaocegege
  - johnugeorge
  - sperlingxx



================================================
File: content/en/docs/components/katib/_index.md
================================================
+++
title = "Katib"
description = "Documentation for Kubeflow Katib"
weight = 70
+++



================================================
File: content/en/docs/components/katib/getting-started.md
================================================
+++
title = "Getting Started"
description = "Get started with Katib"
weight = 30

mathjax = true
+++

This guide describes how to get started with Katib and run a few examples.

## Prerequisites

You need to install the following Katib components to run examples:

- Katib control plane [installed](/docs/components/katib/installation/#installing-control-plane).
- Katib Python SDK [installed](/docs/components/katib/installation/#installing-python-sdk).

## Getting Started with Katib Python SDK

You can run your first hyperparameter tuning Katib Experiment using Python SDK.

In the following example we are going to maximize a simple objective function:

<p>
$$
F(a,b) = 4a - b^2
$$
</p>

The bigger \(a\) and the lesser \(b\) value, the bigger the function value \(F\).

If you install Katib standalone, make sure that you
[configure local `kubeconfig`](https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#programmatic-access-to-the-api)
to access your Kubernetes cluster where you installed Katib control plane.

If you install Katib as part of Kubeflow Platform, you can open a new
[Kubeflow Notebook](/docs/components/notebooks/quickstart-guide/) to run this script.

**Note**. If you use Katib within Kubeflow Platform to run this example, you need to use this
namespace: `KatibClient(namespace="kubeflow-user-example-com")`.

**Note**. The `kubeflow` namespace is pre-configured with the required label 
`katib.kubeflow.org/metrics-collector-injection: enabled` for metrics collection. If you want to use pull-based metrics collector 
in other namespaces, you can attach this label following the instructions in the 
[Metrics Collector](/docs/components/katib/user-guides/metrics-collector/#prerequisites).

```python
# [1] Create an objective function.
def objective(parameters):
    # Import required packages.
    import time
    time.sleep(5)
    # Calculate objective function.
    result = 4 * int(parameters["a"]) - float(parameters["b"]) ** 2
    # Katib parses metrics in this format: <metric-name>=<metric-value>.
    print(f"result={result}")

import kubeflow.katib as katib

# [2] Create hyperparameter search space.
parameters = {
    "a": katib.search.int(min=10, max=20),
    "b": katib.search.double(min=0.1, max=0.2)
}

# [3] Create Katib Experiment with 12 Trials and 2 CPUs per Trial.
katib_client = katib.KatibClient(namespace="kubeflow")

name = "tune-experiment"
katib_client.tune(
    name=name,
    objective=objective,
    parameters=parameters,
    objective_metric_name="result",
    max_trial_count=12,
    resources_per_trial={"cpu": "2"},
)

# [4] Wait until Katib Experiment is complete
katib_client.wait_for_experiment_condition(name=name)

# [5] Get the best hyperparameters.
print(katib_client.get_optimal_hyperparameters(name))
```

You should get similar output for the most optimal Trial, hyperparameters, and observation metrics:

```json
{
  "best_trial_name": "tune-experiment-nmggpxx2",
  "parameter_assignments": [
    {
      "name": "a",
      "value": "19"
    },
    {
      "name": "b",
      "value": "0.13546396192975868"
    }
  ],
  "observation": {
    "metrics": [
      {
        "latest": "75.98164951501829",
        "max": "75.98164951501829",
        "min": "75.98164951501829",
        "name": "result"
      }
    ]
  }
}
```

In [the Katib UI](/docs/components/katib/user-guides/katib-ui/) you should see list of all
completed Trials with results:

<img src="/docs/components/katib/images/getting-started-example.png"
  alt="Getting Started Example"
  class="mt-3 mb-3">

## Next steps

- Check [the Katib UI guide](/docs/components/katib/user-guides/katib-ui/) to get more information
  about your Katib Experiments.

- Run Katib hyperparameter tuning [Experiment using YAML](/docs/components/katib/user-guides/hp-tuning/configure-experiment/#running-the-experiment).

- Learn how to configure [Katib Experiment parameters](/docs/components/katib/user-guides/hp-tuning/configure-experiment).

- Check more [Katib Examples](https://github.com/kubeflow/katib/tree/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/examples/v1beta1).



================================================
File: content/en/docs/components/katib/installation.md
================================================
+++
title = "Installation"
description = "How to install Katib"
weight = 20
+++

This guide describes how to install Katib on your Kubernetes cluster.

## Prerequisites

These are minimal requirements to install Katib.

- Kubernetes >= 1.27
- `kubectl` >= 1.27
- Python >= 3.7

## Installing Katib

You need to install Katib control plane and Python SDK to create Katib Experiments.

### Installing Control Plane

You can skip these steps if you have already
[installed Kubeflow platform](/docs/started/installing-kubeflow/)
using manifests or package distributions. Kubeflow platform includes Katib.

You can install Katib as a standalone component.

Run the following command to install the stable release of Katib control plane: `v0.17.0`

```shell
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone?ref=v0.17.0"
```

Run the following command to install the latest changes of Katib control plane:

```shell
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone?ref=master"
```

After installing it, you can verify that all
[Katib control plane components](/docs/components/katib/reference/architecture/#katib-control-plane-components)
are running:

```shell
$ kubectl get pods -n kubeflow

NAME                                READY   STATUS      RESTARTS   AGE
katib-controller-566595bdd8-8w7sx   1/1     Running     0          82s
katib-db-manager-57cd769cdb-vt7zs   1/1     Running     0          82s
katib-mysql-7894994f88-djp7m        1/1     Running     0          81s
katib-ui-5767cfccdc-v9fcs           1/1     Running     0          80s
```

**Note**. Your Kubernetes cluster must have `StorageClass` for dynamic volume provisioning for Katib DB.
For more information, check the Kubernetes documentation on
[dynamic provisioning](https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/).
If your cluster doesn't have dynamic volume provisioning, you must manually deploy
[PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes)
to bind [PVC](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/mysql/pvc.yaml)
for the Katib DB component.

### Installing Python SDK

Katib [implements Python SDK](https://pypi.org/project/kubeflow-katib/)
to simplify creation of Katib Experiments for Data Scientists.

Run the following command to install the latest stable release of Katib SDK:

```shell
pip install -U kubeflow-katib
```

Run the following command to install the latest changes of Katib SDK:

```shell
pip install git+https://github.com/kubeflow/katib.git@master#subdirectory=sdk/python/v1beta1
```

Otherwise, you can also install the Katib SDK using the specific GitHub commit, for example:

```shell
pip install git+https://github.com/kubeflow/katib.git@ea46a7f2b73b2d316b6b7619f99eb440ede1909b#subdirectory=sdk/python/v1beta1
```

## Next steps

- Run your first Katib Experiment by following the [Getting Started guide](/docs/components/katib/getting-started/).

- Learn about various options to [install Katib control plane components](/docs/components/katib/user-guides/installation-options/).



================================================
File: content/en/docs/components/katib/overview.md
================================================
+++
title = "Overview"
description = "An overview for Katib"
weight = 10
+++

{{% beta-status
  feedbacklink="https://github.com/kubeflow/katib/issues" %}}

## What is Katib ?

Katib is a Kubernetes-native project for automated machine learning (AutoML).
Katib supports hyperparameter tuning, early stopping and
neural architecture search (NAS).
Learn more about AutoML at [fast.ai](https://www.fast.ai/2018/07/16/auto-ml2/),
[Google Cloud](https://cloud.google.com/automl),
[Microsoft Azure](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#automl-in-azure-machine-learning) or
[Amazon SageMaker](https://aws.amazon.com/blogs/aws/amazon-sagemaker-autopilot-fully-managed-automatic-machine-learning/).

Katib is the project which is agnostic to machine learning (ML) frameworks.
It can tune hyperparameters of applications written in any language
of the users' choice and natively supports many ML frameworks,
such as TensorFlow, MXNet, PyTorch, XGBoost, and others.

Katib supports a lot of various AutoML algorithms, such as
[Bayesian optimization](https://arxiv.org/pdf/1012.2599.pdf),
[Tree of Parzen Estimators](https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf),
[Random Search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search),
[Covariance Matrix Adaptation Evolution Strategy](https://en.wikipedia.org/wiki/CMA-ES),
[Hyperband](https://arxiv.org/pdf/1603.06560.pdf),
[Efficient Neural Architecture Search](https://arxiv.org/abs/1802.03268),
[Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)
and many more. Additional algorithm support is coming soon.

The [Katib project](https://github.com/kubeflow/katib) is open source.
The [developer guide](https://github.com/kubeflow/katib/blob/master/docs/developer-guide.md)
is a good starting point for developers who want to contribute to the project.

<img src="/docs/components/katib/images/katib-overview.drawio.png"
  alt="Katib Overview"
  class="mt-3 mb-3">

## Why Katib ?

Katib addresses AutoML step for hyperparameter optimization or Neural Architecture Search
in AI/ML lifecycle as shown on that diagram:

<img src="/docs/components/katib/images/ml-lifecycle-katib.drawio.svg"
  alt="AI/ML Lifecycle Katib"
  class="mt-3 mb-3">

- **Katib can orchestrate multi-node & multi-GPU [distributed training workloads](/docs/components/katib/user-guides/trial-template)**.

Katib is integrated with Kubeflow Training Operator jobs such as PyTorchJob, which allows to
optimize hyperparameters for large models of any size.

In addition to that, Katib can orchestrate workflows such as Argo Workflows and Tekton Pipelines
for more advanced optimization use-cases.

- **Katib is extensible and portable.**

Katib runs Kubernetes containers to [perform hyperparameter tuning job](/docs/components/katib/reference/architecture),
which allows to use Katib with any ML training framework.

Users can even use Katib to optimize non-ML tasks as long as optimization metrics can be collected.

- **Katib has rich support of optimization algorithms.**

Katib is integrated with many optimization frameworks such as [Hyperopt](https://hyperopt.github.io/hyperopt/) and
[Optuna](https://optuna.org/) which implements most of the state of the art optimization algorithms.

Users can leverage Katib control plane to implement and benchmark [their own optimization algorithms](/docs/components/katib/user-guides/hp-tuning/configure-algorithm/#use-custom-algorithm-in-katib)

## Next steps

- Follow [the installation guide](/docs/components/katib/installation/) to deploy Katib.

- Run examples from [getting started guide](/docs/components/katib/getting-started/).




================================================
File: content/en/docs/components/katib/reference/_index.md
================================================
+++
title = "Reference"
description = "Reference docs for Katib"
weight = 50
+++



================================================
File: content/en/docs/components/katib/reference/architecture.md
================================================
+++
title = "Katib Architecture"
description = "How does Katib work?"
weight = 10
+++

This page describes Katib concepts and architectures.

## Hyperparameter Tuning

_Hyperparameters_ are the variables that control the model training process. They include:

- The learning rate.
- The number of layers in a neural network.
- The number of training epochs.

Hyperparameter values are not _learned_. In other words, in contrast to the
model weights and biases the model training process does not adjust the hyperparameter values.

_Hyperparameter tuning_ is the process of optimizing the hyperparameter values to maximize the
predictive accuracy of the model. If you don't use Katib or a similar system for hyperparameter
tuning, you need to run many training jobs yourself, manually adjusting the hyperparameters
to find the optimal values.

You can improve your hyperparameter tuning Experiments by using
[early stopping](https://en.wikipedia.org/wiki/Early_stopping) techniques.
Follow the [early stopping guide](/docs/components/katib/user-guides/early-stopping/) for the details.

## Katib Architecture for Hyperparameter Tuning

This diagram shows how Katib performs Hyperparameter tuning:

<img src="/docs/components/katib/images/katib-architecture.drawio.svg"
  alt="Katib Overview"
  class="mt-3 mb-3">

First of all, users need to write ML training code which will be evaluated on every Katib Trial
with different hyperparameters. Then, using Katib Python SDK users should set the objective, search
space, search algorithm, Trial resources, and create the Katib Experiment.

Katib implements the following
[Kubernetes Custom Resource Definitions (CRDs)](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
to tune Hyperparameters.

### Experiment

An _Experiment_ is a single tuning run, also called an optimization run.

You specify configuration settings to define the Experiment. The following are the main configurations:

- **Objective**: What you want to optimize. This is the objective metric, also called the target
  variable. A common metric is the model's accuracy in the validation pass of the training job
  (e.g. validation accuracy). You also specify whether you want the hyperparameter tuning job
  to _maximize_ or _minimize_ the metric.

- **Search space**: The set of all possible hyperparameter values that the hyperparameter tuning job
  should consider for optimization, and the constraints for each hyperparameter. Other names for
  search space include _feasible set_ and _solution space_. For example, you may provide the
  names of the hyperparameters that you want to optimize. For each hyperparameter, you may
  provide a _minimum_ and _maximum_ value or a _list_ of allowable values.

- **Search algorithm**: The algorithm to use when searching for the optimal hyperparameter values.
  For example, Bayesian Optimization or Random Search.

For details of how to define your Experiment, follow [this guide](/docs/components/katib/user-guides/hp-tuning/configure-experiment/)

### Suggestion

A _Suggestion_ is a set of hyperparameter values that the hyperparameter tuning process has proposed.
Katib creates a Trial to evaluate the suggested set of values.

### Trial

A _Trial_ is one iteration of the hyperparameter tuning process. A Trial corresponds to one
worker job instance with a list of parameter assignments. The list of parameter assignments
corresponds to a Suggestion.

Each Experiment runs several Trials. The Experiment runs the Trials until it
reaches either the objective or the configured maximum number of Trials.

### Worker

The _Worker_ is the process that runs to evaluate a Trial and calculate its objective value.

The Worker can be any type of Kubernetes resource or
[Kubernetes CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/).
Follow the [Trial template guide](/docs/components/katib/user-guides/trial-template/)
to check how to support your own Kubernetes resource in Katib.

## Neural Architecture Search

{{% alert title="Alpha version" color="warning" %}}
NAS is currently in <b>alpha</b> with limited support. The Kubeflow team is
interested in any feedback you may have, in particular with regards to usability
of the feature. You can log issues and comments in
the [Katib issue tracker](https://github.com/kubeflow/katib/issues).
{{% /alert %}}

In addition to hyperparameter tuning, Katib offers a _neural architecture
search_ feature. You can use the NAS to design your artificial neural network, with a goal of
maximizing the predictive accuracy and performance of your model.

NAS is closely related to hyperparameter tuning. Both are subsets of AutoML. While hyperparameter
tuning optimizes the model's hyperparameters, a NAS system optimizes the model's structure,
node weights and hyperparameters.

NAS technology in general uses various techniques to find the optimal neural network design.

Learn more about various NAS algorithms in
[Differentiable Architecture Search](/docs/components/katib/reference/nas-algorithms/#differentiable-architecture-search-darts)
and [Efficient Neural Architecture Search](/docs/components/katib/reference/nas-algorithms/#efficient-neural-architecture-search-enas)
guides.

## Katib Control Plane Components

Katib has the following components on the control plane to run Experiments:

- `katib-controller` - the controller to manage Katib Kubernetes CRDs:
  [`Experiment`](#experiment),
  [`Suggestion`](#suggestion),
  [`Trial`](#trial).

  - (Optional) If certificate generator is enabled in
    [Katib Config](/docs/components/katib/user-guides/katib-config/), Katib controller deployment will create
    self-signed certificate for the Katib webhooks. Learn more about the cert generator in the
    [developer guide](https://github.com/kubeflow/katib/blob/master/docs/developer-guide.md#katib-cert-generator).

- `katib-ui` - the Katib user interface.

- `katib-db-manager` - the gRPC API server to control Katib DB interface.

- `katib-mysql` - the MySQL DB backend to store Katib Experiments metrics.



================================================
File: content/en/docs/components/katib/reference/experiment-cr.md
================================================
+++
title = "Katib Experiment Lifecycle"
description = "What happens after an Experiment is created"
weight = 10
+++

## Katib Experiment Lifecycle

When user creates an Experiment, Katib Experiment controller,
Suggestion controller and Trial controller is working together to achieve
hyperparameters tuning for user's Machine learning model. The Experiment
workflow looks as follows:

<img src="/docs/components/katib/images/katib-workflow.png" alt="Katib Workflow" class="mt-3 mb-3">

1. The Experiment is submitted to the Kubernetes API server. Katib
   Experiment mutating and validating webhook is called to set the default
   values for the Experiment and validate the CR separately.

1. The Experiment controller creates the Suggestion.

1. The Suggestion controller creates the algorithm deployment and service
   based on the new Suggestion.

1. When the Suggestion controller verifies that the algorithm service is
   ready, it calls the service to generate
   `spec.request - len(status.suggestions)` sets of hyperparameters and append
   them into `status.suggestions`.

1. The Experiment controller finds that Suggestion had been updated and
   generates each Trial for the each new hyperparameters set.

1. The Trial controller generates `Worker Job` based on the `runSpec`
   from the Trial with the new hyperparameters set.

1. The related job controller
   (Kubernetes batch Job, Kubeflow TFJob, Tekton Pipeline, etc.) generates
   Kubernetes Pods.

1. Katib Pod mutating webhook is called to inject the metrics collector sidecar
   container to the candidate Pods.

1. During the ML model container runs, the metrics collector container
   collects metrics from the injected pod and persists metrics to the Katib
   DB backend.

1. When the ML model training ends, the Trial controller updates status
   of the corresponding Trial.

1. When the Trial goes to end, the Experiment controller increases
   `request` field of the corresponding Suggestion if it is needed,
   then everything goes to `step 4` again.
   Of course, if the Trial meet one of `end` condition
   (exceeds `maxTrialCount`, `maxFailedTrialCount` or `goal`),
   the Experiment controller takes everything done.


================================================
File: content/en/docs/components/katib/reference/nas-algorithms.md
================================================
+++
title = "Neural Architecture Search Algorithms"
description = "Overview for the Neural Architecture Search algorithms in Katib"
weight = 20
+++

This page describes how Neural Architecture Search (NAS) algorithms work in Katib.

## Efficient Neural Architecture Search (ENAS)

The algorithm follows the idea proposed in _Efficient Neural Architecture Search via Parameter Sharing_
by Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le and Jeff Dean (https://arxiv.org/abs/1802.03268)
and _Neural Architecture Search with Reinforcement Learning_ by Barret Zoph and
Quoc V. Le (https://arxiv.org/abs/1611.01578) .

The implementation is based on
[the GitHub of _Efﬁcient Neural Architecture Search via Parameter Sharing_](https://github.com/melodyguan/enas)
and
[Google Implementation for ENAS](https://github.com/google-research/google-research/tree/master/enas_lm).
It uses a recurrent neural network with LSTM cells as controller to generate neural architecture candidates.
And this controller network is updated by policy gradients. However, it currently does not support parameter sharing.

### Katib Implementation

Katib represents neural network in the specific format. If number of layers (n) = 12 and number
of possible operations (m) = 6, the definition of an architecture will be like:

```
[2]
[0 0]
[1 1 0]
[5 1 0 1]
[1 1 1 0 1]
[5 0 0 1 0 1]
[1 1 1 0 0 1 0]
[2 0 0 0 1 1 0 1]
[0 0 0 1 1 1 1 1 0]
[2 0 1 0 1 1 1 0 0 0]
[3 1 1 1 1 1 1 0 0 1 1]
[0 1 1 1 1 0 0 1 1 1 1 0]
```

There are n rows, the i<sup>th</sup> row has i elements and describes the i<sup>th</sup>
layer. Please notice that layer 0 is the input and is not included in this definition.

In each row, the first integer ranges from 0 to m-1 and indicates the operation in this layer.
Starting from the second position, the k<sup>th</sup> integer is a boolean value that indicates
whether (k-2)<sup>th</sup> layer has a skip connection with this layer.
(There will always be a connection from (k-1)<sup>th</sup> layer to k<sup>th</sup> layer)

#### Output of `GetSuggestion()`

The output of `GetSuggestion()` from the algorithm service consists of two parts:
`architecture` and `nn_config`.

`architecture` is a json string of the definition of a neural architecture. The format is as stated
above. One example is:

```
[[27], [29, 0], [22, 1, 0], [13, 0, 0, 0], [26, 1, 1, 0, 0], [30, 1, 0, 1, 0, 0], [11, 0, 1, 1, 0, 1, 1], [9, 1, 0, 0, 1, 0, 0, 0]]
```

`nn_config` is a json string of the detailed description of what is the num of layers,
input size, output size and what each operation index stands for. A `nn_config` corresponding to
the architecture above can be:

```
{
    "num_layers": 8,
    "input_sizes": [32, 32, 3],
    "output_sizes": [10],
    "embedding": {
        "27": {
            "opt_id": 27,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "7",
                "num_filter": "96",
                "stride": "2"
            }
        },
        "29": {
            "opt_id": 29,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "7",
                "num_filter": "128",
                "stride": "2"
            }
        },
        "22": {
            "opt_id": 22,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "7",
                "num_filter": "48",
                "stride": "1"
            }
        },
        "13": {
            "opt_id": 13,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "5",
                "num_filter": "48",
                "stride": "2"
            }
        },
        "26": {
            "opt_id": 26,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "7",
                "num_filter": "96",
                "stride": "1"
            }
        },
        "30": {
            "opt_id": 30,
            "opt_type": "reduction",
            "opt_params": {
                "reduction_type": "max_pooling",
                "pool_size": 2
            }
        },
        "11": {
            "opt_id": 11,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "5",
                "num_filter": "32",
                "stride": "2"
            }
        },
        "9": {
            "opt_id": 9,
            "opt_type": "convolution",
            "opt_params": {
                "filter_size": "3",
                "num_filter": "128",
                "stride": "2"
            }
        }
    }
}
```

This neural architecture can be visualized as:

<img src="/docs/components/katib/images/nas-example.png"
  alt="Example of NAS network"
  class="mt-3 mb-3">

The following items can be implemented in Katib to better support ENAS:

1. Add 'micro' mode, which means searching for a neural cell instead of the whole neural network.
1. Add support for recurrent neural networks and build a training container for the Penn Treebank task.
1. Add parameter sharing, if possible.
1. Change LSTM cell from self defined functions in LSTM.py to `tf.nn.rnn_cell.LSTMCell`
1. Store the Suggestion checkpoint to PVC to protect against unexpected enas service pod restarts
1. Add `RequestCount` into API so that the Suggestion can clean the information of completed studies.

## Differentiable Architecture Search (DARTS)

The algorithm follows the idea proposed in _DARTS: Differentiable Architecture Search_ by Hanxiao Liu,
Karen Simonyan, Yiming Yang: https://arxiv.org/abs/1806.09055.
The implementation is based on [official github implementation](https://github.com/quark0/darts) and
[popular repository](https://github.com/khanrc/pt.darts).

The algorithm addresses the scalability challenge of architecture search by formulating the task in
a differentiable manner. It is based on continuous relaxation and gradient descent in the search space.
It is able to efficiently design high-performance convolutional architectures for image classification
(on CIFAR-10 and ImageNet) and recurrent architectures for language modeling (on Penn Treebank and WikiText-2).

### Katib Implementation

To support DARTS in current Katib functionality the implementation follows this way:

1. [DARTS Suggestion service](https://github.com/kubeflow/katib/blob/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/pkg/suggestion/v1beta1/nas/darts/service.py)
   creates set of primitive operations from the Experiment search space.
   For example:

   ```
   ['separable_convolution_3x3', 'dilated_convolution_3x3', 'dilated_convolution_5x5', 'avg_pooling_3x3', 'max_pooling_3x3', 'skip_connection']
   ```

1. Suggestion returns algorithm settings, number of layers and set of primitives to Katib Controller

1. Katib controller starts
   [DARTS training container](https://github.com/kubeflow/katib/tree/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/examples/v1beta1/trial-images/darts-cnn-cifar10)
   with the appropriate settings and all possible operations.

1. Training container runs DARTS algorithm.

1. Metrics collector saves Best Genotype from the training container log.

#### Best Genotype representation

Best Genotype is the best cell for each neural network layer. Cells are generated by DARTS algorithm.
Here is an example of the Best Genotype:

```
Genotype(
  normal=[
      [('max_pooling_3x3',0),('max_pooling_3x3',1)],
      [('max_pooling_3x3',0),('max_pooling_3x3',1)],
      [('max_pooling_3x3',0),('dilated_convolution_3x3',3)],
      [('max_pooling_3x3',0),('max_pooling_3x3',1)]
    ],
    normal_concat=range(2,6),
  reduce=[
      [('dilated_convolution_5x5',1),('separable_convolution_3x3',0)],
      [('max_pooling_3x3',2),('dilated_convolution_5x5',1)],
      [('dilated_convolution_5x5',3),('dilated_convolution_5x5',2)],
      [('dilated_convolution_5x5',3),('dilated_convolution_5x5',4)]
    ],
    reduce_concat=range(2,6)
)
```

In this example you can see 4 DARTS nodes with indexes: 2,3,4,5.

`reduce` parameter is the cells which located at the 1/3 and 2/3 of the total neural network layers.
They represent reduction cells in which all the operations adjacent to the input nodes are of stride two.

`normal` parameter is the cells which is located at the rest neural network layers.
They represent normal cell.

In CNN all reduce and normal intermediate nodes are concatenated and each node has 2 edges.

Each element in `normal` array is the node which has 2 edges. First element is the operation on
the edge and second element is the node index connection. Note that index 0 is the `C_{k-2}` node
and index 1 is the `C_{k-1}` node.

For example `[('max_pooling_3x3',0),('max_pooling_3x3',1)]` means that `C_{k-2}` node connects to
the first node with `max_pooling_3x3` operation (Max Pooling with filter size 3) and `C_{k-1}`
node connects to the first node with `max_pooling_3x3` operation.

`reduce` array follows the same way as `normal` array.

`normal_concat` and `reduce_concat` means concatenation between intermediate nodes.

Currently, it supports running only on single GPU and second-order approximation, which produced
better results than first-order.

The following items can be implemented in DARTS:

- Support multi GPU training. Add functionality to select GPU for training.

- Support DARTS in Katib UI.

- Think about better representation of Best Genotype.

- Add more dataset for CNN. Currently, it supports only CIFAR-10.

- Support RNN in addition to CNN.

- Support micro mode, which means searching for a particular neural network cell.



================================================
File: content/en/docs/components/katib/user-guides/_index.md
================================================
+++
title = "User Guides"
description = "User guides for Katib"
weight = 40
+++



================================================
File: content/en/docs/components/katib/user-guides/early-stopping.md
================================================
+++
title = "How to Configure Early Stopping"
description = "Early Stopping overview for Katib Experiments"
weight = 50
+++

This guide shows how you can use [early stopping](https://en.wikipedia.org/wiki/Early_stopping)
to optimize cost for your Katib Experiments. Early stopping allows you to avoid overfitting when you
train your model during Katib Experiments. It also helps by saving computing resources and reducing
Experiment execution time by stopping the Experiment's Trials when the target metric(s) no
longer improves before the training process is complete.

The major advantage of using early stopping in Katib is that you don't
need to modify your
[training container package](/docs/components/katib/user-guides/hp-tuning/configure-experiment/#create-image-for-training-code).
All you have to do is make necessary changes to your Experiment's YAML file.

Early stopping works in the same way as Katib's
[metrics collector](/docs/components/katib/user-guides/metrics-collector). It analyses required
metrics from the `StdOut` or from the arbitrary output file and an early stopping algorithm makes
the decision if the Trial needs to be stopped. Currently, early stopping works only with
`StdOut` or `File` metrics collectors.

**Note**: Your training container must print training logs with the timestamp,
because early stopping algorithms need to know the sequence of reported metrics.
Check the
[`PyTorch` example](https://github.com/kubeflow/katib/blob/399340418a84b96804a9f304cea841b6497796f4/examples/v1beta1/trial-images/pytorch-mnist/mnist.py#L139-L142)
to learn how to add a date format to your logs.

## Configure the Experiment with early stopping

As a reference, you can use the YAML file of the
[early stopping example](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/early-stopping/median-stop.yaml).

1. Follow the
   [guide](/docs/components/katib/user-guides/hp-tuning/configure-experiment/#configuring-the-experiment)
   to configure your Katib Experiment.

2. Next, to apply early stopping for your Experiment, specify the `.spec.earlyStopping`
   parameter, similar to the `.spec.algorithm`.

   - `.earlyStopping.algorithmName` - the name of the early stopping algorithm.

   - `.earlyStopping.algorithmSettings`- the settings for the early stopping algorithm.

What happens is your Experiment's Suggestion produces new Trials. After that, the early stopping
algorithm generates early stopping rules for the created Trials. Once the Trial reaches all the rules,
it is stopped and the Trial status is changed to the `EarlyStopped`. Then, Katib calls the Suggestion again to
ask for the new Trials.

## Early Stopping Algorithms

Katib currently supports several algorithms for early stopping:

- [Median Stopping Rule](#median-stopping-rule)

More algorithms are under development.

### Median Stopping Rule

The early stopping algorithm name in Katib is `medianstop`.

The median stopping rule stops a pending Trial `X` at step `S` if the Trial's best objective value
by step `S` is worse than the median value of the running averages of all completed Trials objectives
reported up to step `S`.

To learn more about it, check
[Google Vizier: A Service for Black-Box Optimization](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46180.pdf).

Katib supports the following early stopping settings:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting Name</th>
        <th>Description</th>
        <th>Default Value</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>min_trials_required</td>
        <td>Minimal number of successful Trials to compute median value</td>
        <td>3</td>
      </tr>
      <tr>
        <td>start_step</td>
        <td>Number of reported intermediate results before stopping the Trial</td>
        <td>4</td>
      </tr>
    </tbody>
  </table>
</div>

## Next steps

- How to use Katib Experiment Trial templates(/docs/components/katib/user-guides/trial-template).

- How to [restart your Experiment and use the resume policies](/docs/components/katib/user-guides/resume-experiment/).



================================================
File: content/en/docs/components/katib/user-guides/env-variables.md
================================================
+++
title = "How to configure env variables"
description = "List of environment variables accepted by Katib components"
weight = 80
+++

This guide describes environment variables for each Katib component. If you want to change your
Katib installation, you can modify some of these variables.

In the tables below you can find descriptions, default values and mandatory properties for all
environment variables in each Katib component. If a variable has a mandatory property, you need to
set the relevant environment variable in an appropriate Katib component's manifest.

## Katib Controller

Bellow are the environment variables for the
[Katib Controller](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/controller/controller.yaml)
deployment:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Variable</th>
        <th>Description</th>
        <th>Default Value</th>
        <th>Mandatory</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>KATIB_CORE_NAMESPACE</code></td>
        <td>Base Namespace for all Katib components and default Experiment</td>
        <td><code>metadata.namespace</code></td>
        <td>Yes</td>
      </tr>
      <tr>
        <td><code>KATIB_SUGGESTION_COMPOSER</code></td>
        <td> <a href="https://github.com/kubeflow/katib/blob/master/pkg/controller.v1beta1/suggestion/composer/composer.go">Composer</a>
          for the Katib Suggestions. You can use your own Composer</td>
        <td>general</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_DB_MANAGER_SERVICE_NAMESPACE</code></td>
        <td>Katib DB Manager Namespace</td>
        <td><code>KATIB_CORE_NAMESPACE</code> env variable</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_DB_MANAGER_SERVICE_IP</code></td>
        <td>Katib DB Manager IP</td>
        <td>katib-db-manager</td>
        <td>No</td>
      </tr>
       <tr>
        <td><code>KATIB_DB_MANAGER_SERVICE_PORT</code></td>
        <td>Katib DB Manager Port</td>
        <td>6789</td>
        <td>No</td>
      </tr>
    </tbody>
  </table>
</div>

Katib Controller calls Katib DB Manager with this address expression:

**`KATIB_DB_MANAGER_SERVICE_IP.KATIB_DB_MANAGER_SERVICE_NAMESPACE:KATIB_DB_MANAGER_SERVICE_PORT`**

If you set `KATIB_DB_MANAGER_SERVICE_NAMESPACE=""`, Katib Controller calls Katib DB Manager with this address:

**`KATIB_DB_MANAGER_SERVICE_IP:KATIB_DB_MANAGER_SERVICE_PORT`**

If you want to use your own DB Manager to report Katib metrics, you can change `KATIB_DB_MANAGER_SERVICE_NAMESPACE`, `KATIB_DB_MANAGER_SERVICE_IP` and `KATIB_DB_MANAGER_SERVICE_PORT` variables.

## Katib UI

Below are the environment variables for the
[Katib UI](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/ui/ui.yaml)
deployment:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Variable</th>
        <th>Description</th>
        <th>Default Value</th>
        <th>Mandatory</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>KATIB_CORE_NAMESPACE</code></td>
        <td>Base Namespace for all Katib components and default Experiment</td>
        <td><code>metadata.namespace</code></td>
        <td>Yes</td>
      </tr>
      <tr>
        <td><code>KATIB_DB_MANAGER_SERVICE_NAMESPACE</code></td>
        <td>Katib DB Manager Namespace</td>
        <td><code>KATIB_CORE_NAMESPACE</code> env variable</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_DB_MANAGER_SERVICE_IP</code></td>
        <td>Katib DB Manager IP</td>
        <td>katib-db-manager</td>
        <td>No</td>
      </tr>
       <tr>
        <td><code>KATIB_DB_MANAGER_SERVICE_PORT</code></td>
        <td>Katib DB Manager Port</td>
        <td>6789</td>
        <td>No</td>
      </tr>
    </tbody>
  </table>
</div>

Katib UI calls Katib DB Manager with the same address expression as Katib Controller.

## Katib DB Manager

Bellow are the environment variables for the
[Katib DB Manager](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/db-manager/db-manager.yaml)
deployment:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Variable</th>
        <th>Description</th>
        <th>Default Value</th>
        <th>Mandatory</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>DB_NAME</code></td>
        <td>Katib DB Name: 'mysql' or 'postgres'</td>
        <td>  </td> 
        <td>Yes</td>
      </tr>
      <tr>
        <td><code>DB_PASSWORD</code></td>
        <td>Katib DB Password</td>
        <td>test (MySQL)<br>katib (Postgres)</td>
        <td>Yes</td>
      </tr>
      <tr>
        <td><code>DB_USER</code></td>
        <td>Katib DB User</td>
        <td>root (MySQL)<br>katib (Postgres)</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_MYSQL_DB_HOST</code></td>
        <td>Katib MySQL Host</td>
        <td>katib-mysql</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_MYSQL_DB_PORT</code></td>
        <td>Katib MySQL Port</td>
        <td>3306</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_MYSQL_DB_DATABASE</code></td>
        <td>Katib MySQL Database name</td>
        <td>katib</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_POSTGRESQL_DB_HOST</code></td>
        <td>Katib Postgres Host</td>
        <td>katib-postgres</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_POSTGRESQL_DB_PORT</code></td>
        <td>Katib Postgres Port</td>
        <td>5432</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_POSTGRESQL_DB_DATABASE</code></td>
        <td>Katib Postgres Database name</td>
        <td>katib</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>KATIB_POSTGRESQL_SSL_MODE</code></td>
        <td>Katib Postgres SSL mode</td>
        <td>disable</td>
        <td>No</td>
      </tr>
      <tr>
        <td><code>SKIP_DB_INITIALIZATION</code></td>
        <td>Option to skip DB table initialization</td>
        <td>false</td>
        <td>No</td>
      </tr>
    </tbody>
  </table>
</div>

Currently, Katib DB Manager supports only **MySQL** and **Postgres** database. (`DB_NAME` env
variable must be filled with one of `mysql` or `postgres`). However, you can use your own DB Manager
and Database to report metrics by implements the
[katib db interface](https://github.com/kubeflow/katib/blob/master/pkg/db/v1beta1/common/kdb.go).

For the [Katib DB Manager](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/db-manager/db-manager.yaml#L25)
you can change `DB_PASSWORD` to your own DB password.

Katib DB Manager creates DB connection to the DB by the type of DB.  
If `DB_NAME=mysql`, it uses `mysql` driver and this data source name:  
**`DB_USER:DB_PASSWORD@tcp(KATIB_MYSQL_DB_HOST:KATIB_MYSQL_DB_PORT)/KATIB_MYSQL_DB_DATABASE?timeout=5s`**

If `DB_NAME=postgres`, it uses `pq` driver and this data source name:  
**`postgresql://[DB_USER[:DB_PASSWORD]@][KATIB_POSTGRESQL_DB_HOST][:KATIB_POSTGRESQL_DB_PORT][/KATIB_POSTGRESQL_DB_DATABASE]`**

## Katib DB

Katib DB components supports MySQL and Postgres.

### Katib MySQL

For the [Katib MySQL](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/mysql/mysql.yaml)
you need to set these environment variables:

- `MYSQL_ROOT_PASSWORD` to a value from [katib-mysql-secrets](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/mysql/secret.yaml),
  which is equal to "test".
- `MYSQL_ALLOW_EMPTY_PASSWORD` as `true`
- `MYSQL_DATABASE` as `katib`.

You can refer to the list of
[all environment variables](https://github.com/docker-library/docs/tree/master/mysql#environment-variables)
for the MySQL Docker image.

Katib MySQL environment variables must be matched with the Katib DB Manager environment variables, it means:

1. `MYSQL_ROOT_PASSWORD` = `DB_PASSWORD`
1. `MYSQL_DATABASE` = `KATIB_MYSQL_DB_DATABASE`

### Katib Postgres

For the [Katib Postgres](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/postgres/postgres.yaml)
you need to set these environment variables:

- `POSTGRES_USER`, `POSTGRES_PASSWORD` and `POSTGRES_DB` to a value from [katib-postgres-secrets](https://github.com/kubeflow/katib/blob/master/manifests/v1beta1/components/postgres/secret.yaml),
  which are equal to "katib".

You can refer to the list of
[all environment variables](https://github.com/docker-library/docs/blob/master/postgres/README.md#environment-variables)
for the Postgres Docker image.

Katib Postgres environment variables must be matched with the Katib DB Manager environment variables, it means:

1. `POSTGRES_USER` = `DB_USER`
1. `POSTGRES_PASSWORD` = `DB_PASSWORD`
1. `POSTGRES_DB` = `KATIB_POSTGRESQL_DB_DATABASE`

## Next steps

- Learn about different [options to install Katib](/docs/components/katib/user-guides/installation-options/).



================================================
File: content/en/docs/components/katib/user-guides/installation-options.md
================================================
+++
title = "Katib Installation Options"
description = "Overview of the ways to install Katib control plane"
weight = 90
+++

Katib offers a few installation options to install control plane. This page describes the options
and the features available with each option. Check
[the installation guide](/docs/components/katib/installation/#installing-control-plane) to
understand the Katib control plane components.

## The Default Katib Standalone Installation

Follow [the installation guide](/docs/components/katib/installation/#installing-katib) to install
the default version of Katib control plane.

### Katib with Controller Leader Election

Run the following command to deploy Katib with
[Controller Leader Election](https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/) support:

```shell
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-leader-election?ref=master"
```

This installation is almost the same as Katib Standalone installation, although you can make
`katib-controller` Highly Available (HA) using leader election. If you plan to use Katib in an
environment where high Service Level Agreements (SLAs) and Service Level Objectives (SLOs)
are required, such as a production environment, consider choosing this installation.

### Katib with PostgreSQL Database

Run the following command to deploy Katib with PostgreSQL database (DB) instead of MySQL:

```shell
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone-postgres?ref=master"
```

### Katib with External DB

Run the following command to deploy Katib with custom DB backend:

```shell
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-external-db?ref=master"
```

This installation allows to use custom instance of MySQL DB instead `katib-mysql`.
You have to modify the appropriate environment variables for `katib-db-manager` in the
[secrets.env](https://github.com/kubeflow/katib/blob/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/manifests/v1beta1/installs/katib-external-db/secrets.env)
with your MySQL DB values.

### Katib with Cert Manager

Run the following command to deploy Katib with [Cert Manager](https://cert-manager.io/docs/releases/)
requirement:

```shell
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-cert-manager?ref=master"
```

This installation uses Cert Manager instead of Katib certificate generator to provision Katib
webhooks certificates. You have to deploy Cert Manager on your Kubernetes cluster before
deploying Katib using this installation.

### Katib on OpenShift

Run the following command to deploy Katib on [OpenShift](https://docs.openshift.com/) v4.4+:

```
kubectl apply -k "github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-openshift?ref=master"
```

This installation uses OpenShift service controller instead of Katib certificate generator to
provision Katib webhooks certificates.

## Next Steps

- How to [set up environment variables](/docs/components/katib/user-guides/env-variables/) for
  various Katib component.



================================================
File: content/en/docs/components/katib/user-guides/katib-config.md
================================================
+++
title = "How to use Katib Config"
description = "Katib configuration overview and how to update values"
weight = 80
+++

This guide describes
[the Katib Config](https://github.com/kubeflow/katib/blob/19268062f1b187dde48114628e527a2a35b01d64/manifests/v1beta1/installs/katib-standalone/katib-config.yaml) —
the main configuration file for every Katib component. We use Kubernetes
[ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) to
fetch that config into [the Katib control plane components](/docs/components/katib/reference/architecture/#katib-control-plane-components).

The ConfigMap must be deployed in the
[`KATIB_CORE_NAMESPACE`](/docs/components/katib/user-guides/env-variables/#katib-controller)
namespace with the `katib-config` name.

Katib config has the initialization: `init` and the runtime: `runtime` parameters. You can modify
these parameters by editing the `katib-config` ConfigMap:

```shell
kubectl edit configMap katib-config -n kubeflow
```

## Initialization Parameters

Katib Config parameters set in `init` represent initialization settings for
the Katib control plane. These parameters can be modified before Katib control plane is deployed.

```yaml
apiVersion: config.kubeflow.org/v1beta1
kind: KatibConfig
init:
  certGenerator:
    enable: true
    ...
  controller:
    trialResources:
      - Job.v1.batch
      - TFJob.v1.kubeflow.org
    ...
```

It has settings for the following Katib components:

1. Katib certificate generator: `certGenerator`

1. Katib controller: `controller`

### Katib Certificate Generator Parameters

The following parameters set in `.init.certGenerator` configure the Katib certificate generator:

- `enable` - whether to enable Katib certificate generator.

  The default value is `false`

- `webhookServiceName` - a service name for the Katib webhooks. If it is set, Katib certificate
  generator is forcefully enabled.

  The default value is `katib-controller`

- `webhookSecretName` - a secret name to store Katib webhooks certificates. If it is set, Katib
  certificate generator is forcefully enabled.

  The default value is `katib-webhook-cert`

### Katib Controller Parameters

The following parameters set in `.init.controller` configure the Katib controller:

- `experimentSuggestionName` - the implementation of Suggestion interface for
  Experiment controller.

  The default value is `default`

- `metricsAddr` - a TCP address that the Katib controller should bind to
  for serving prometheus metrics.

  The default value is `8080`

- `healthzAddr` - a TCP address that the Katib controller should bind to
  for health probes.

  The default value is `18080`

- `injectSecurityContext` - whether to inject security context to Katib metrics collector sidecar
  container from Katib Trial training container.

  The default value is `false`

- `trialResources` - list of resources that can be used as a Trial template. The Trial resources
  must be in this format: Kind.version.group (e.g. `TFJob.v1.kubeflow.org`).
  Follow [this guide](/docs/components/katib/user-guides/trial-template/#use-crds-with-trial-template)
  to understand how to make Katib Trial work with your Kubernetes CRDs.

  The default value is `[Job.v1.batch]`

- `webhookPort` - a port number for Katib admission webhooks.

  The default value is `8443`

- `enableLeaderElection` - whether to enable leader election for Katib controller. If this value
  is true only single Katib controller Pod is active.

  The default value is `false`

- `leaderElectionID` - an ID for the Katib controller leader election.

  The default value is `3fbc96e9.katib.kubeflow.org`

## Runtime Parameters

Katib Config parameters set in `runtime` represent runtime settings for
the Katib Experiment. These parameters can be modified before Katib Experiment is created. When
Katib Experiment is created Katib controller fetches the latest configuration from the
`katib-config` ConfigMap.

```yaml
apiVersion: config.kubeflow.org/v1beta1
kind: KatibConfig
runtime:
  metricsCollectors:
    - kind: StdOut
      image: docker.io/kubeflowkatib/file-metrics-collector:latest
    ...
  suggestions:
    - algorithmName: random
      image: docker.io/kubeflowkatib/suggestion-hyperopt:latest
    ...
  earlyStoppings:
    - algorithmName: medianstop
      image: docker.io/kubeflowkatib/earlystopping-medianstop:latest
    ...
```

### Metrics Collectors Parameters

Parameters set in `.runtime.metricsCollectors` configure container for
[the Katib metrics collector](/docs/components/katib/user-guides/metrics-collector).
The following settings are **required** for each Katib metrics collector that you want to use in your Katib Experiments:

- `kind` - one of the Katib metrics collector types.

- `image` - a Docker image for the metrics collector's container.

The following settings are **optional**:

- `imagePullPolicy` - an [image pull policy](https://kubernetes.io/docs/concepts/configuration/overview/#container-images)
  for the metrics collector's container.

  The default value is `IfNotPresent`

- `resources` - [resources](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-requests-and-limits-of-pod-and-container)
  for the metrics collector's container.

  The default values for the `resources` are:

  ```yaml
  metricsCollectors:
    - kind: StdOut
      image: docker.io/kubeflowkatib/file-metrics-collector:latest
      resources:
        requests:
          cpu: 50m
          memory: 10Mi
          ephemeral-storage: 500Mi
        limits:
          cpu: 500m
          memory: 100Mi
          ephemeral-storage: 5Gi
  ```

  You can run your metrics collector's container without requesting
  the `cpu`, `memory`, or `ephemeral-storage` resource from the Kubernetes cluster.
  For instance, you have to remove `ephemeral-storage` from the container resources to use the
  [Google Kubernetes Engine cluster autoscaler](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#limitations).

  To remove specific resources from the metrics collector's container set the
  negative values in requests and limits in your Katib config as follows:

  ```yaml
  resources:
    requests:
      cpu: -1
      memory: -1
      ephemeral-storage: -1
    limits:
      cpu: -1
      memory: -1
      ephemeral-storage: -1
  ```

- `waitAllProcesses` - a flag to define whether the metrics collector should wait until all
  processes in the Trial's training container are finished before start to collect metrics.

  The default value is `false`

### Suggestions Parameters

Parameters set in `.runtime.suggestions` configure Deployment for
[the Katib Suggestions](/docs/components/katib/reference/architecture/#suggestion). Every Suggestion represents
one of the AutoML algorithms that you can use in Katib Experiments.
The following settings are **required** for Suggestion Deployment:

- `algorithmName` - one of the Katib algorithm names. For example: `tpe`

- `image` - a Docker image for the Suggestion Deployment's container. Image
  example: `docker.io/kubeflowkatib/<suggestion-name>`

  For each algorithm you can specify one of the following Suggestion names in the Docker image:

  <div class="table-responsive">
    <table class="table table-bordered">
      <thead class="thead-light">
        <tr>
          <th>Suggestion name</th>
          <th>List of supported algorithms</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code>suggestion-hyperopt</code></td>
          <td><code>random</code>, <code>tpe</code></td>
          <td><a href="https://github.com/hyperopt/hyperopt">Hyperopt</a> optimization framework</td>
        </tr>
        <tr>
          <td><code>suggestion-skopt</code></td>
          <td><code>bayesianoptimization</code></td>
          <td><a href="https://github.com/scikit-optimize/scikit-optimize">Scikit-optimize</a> optimization framework</td>
        </tr>
        <tr>
          <td><code>suggestion-goptuna</code></td>
          <td><code>cmaes</code>, <code>random</code>, <code>tpe</code>, <code>sobol</code></td>
          <td><a href="https://github.com/c-bata/goptuna">Goptuna</a> optimization framework</td>
        </tr>
        <tr>
          <td><code>suggestion-optuna</code></td>
          <td><code>multivariate-tpe</code>, <code>tpe</code>, <code>cmaes</code>, <code>random</code>, <code>grid</code></td>
          <td><a href="https://github.com/optuna/optuna">Optuna</a> optimization framework</td>
        </tr>
        <tr>
          <td><code>suggestion-hyperband</code></td>
          <td><code>hyperband</code></td>
          <td><a href="https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1beta1/hyperband">Katib
            Hyperband</a> implementation</td>
        </tr>
        <tr>
          <td><code>suggestion-pbt</code></td>
          <td><code>pbt</code></td>
          <td><a href="https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1beta1/pbt">Katib
            PBT</a> implementation</td>
        </tr>
        <tr>
          <td><code>suggestion-enas</code></td>
          <td><code>enas</code></td>
          <td><a href="https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1beta1/nas/enas">Katib
            ENAS</a> implementation</td>
        </tr>
        <tr>
          <td><code>suggestion-darts</code></td>
          <td><code>darts</code></td>
          <td><a href="https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1beta1/nas/darts">Katib
            DARTS</a> implementation</td>
        </tr>
      </tbody>
    </table>
  </div>

The following settings are **optional**:

- `<ContainerV1>` - you can specify all
  [container parameters](https://github.com/kubernetes/api/blob/669e693933c77e91648f8602dc2555d96e6279ad/core/v1/types.go#L2608)
  inline for your Suggestion Deployment. For example, `resources` for container resources or
  `env` for container environment variables.

  Configuration for `resources` works the same as for Katib metrics collector's container `resources`.

- `serviceAccountName` - a [ServiceAccount](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/)
  for the Suggestion Deployment.

  By default, the Suggestion Pod doesn't have any specific ServiceAccount,
  in which case, the Pod uses the
  [default](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server)
  service account.

  **Note:** If you want to run your Experiments with
  [early stopping](/docs/components/katib/user-guides/early-stopping/),
  the Suggestion's Deployment must have permission to update the Experiment's
  Trial status. If you don't specify a ServiceAccount in the Katib config,
  Katib controller creates required
  [Kubernetes Role-based access control](https://kubernetes.io/docs/reference/access-authn-authz/rbac)
  for the Suggestion.

  If you need your own ServiceAccount for the Experiment's
  Suggestion with early stopping, you have to follow the rules:

  - The ServiceAccount name can't be equal to
    `<experiment-name>-<experiment-algorithm>`

  - The ServiceAccount must have sufficient permissions to update
    the Experiment's Trial status.

#### Suggestion Volume Parameters

When you create an Experiment with
[`FromVolume` resume policy](/docs/components/katib/user-guides/resume-experiment#resume-policy-fromvolume),
you are able to specify
[PersistentVolume (PV)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes)
and
[PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
settings for the Experiment's Suggestion to restore stage of the AutoML algorithm.

If PV settings are empty, Katib controller creates only PVC.
If you want to use the default volume specification, you can omit these parameters.

For example, Suggestion volume config for `random` algorithm:

```yaml
suggestions:
  - algorithmName: random
    image: docker.io/kubeflowkatib/suggestion-hyperopt:latest
    volumeMountPath: /opt/suggestion/data
    persistentVolumeClaimSpec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 3Gi
      storageClassName: katib-suggestion
    persistentVolumeSpec:
      accessModes:
        - ReadWriteMany
      capacity:
        storage: 3Gi
      hostPath:
        path: /tmp/suggestion/unique/path
      storageClassName: katib-suggestion
    persistentVolumeLabels:
      type: local
```

- `volumeMountPath` - a [mount path](https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/#configure-a-volume-for-a-pod)
  for the Suggestion Deployment's container.

  The default value is `/opt/katib/data`

- `persistentVolumeClaimSpec` - a [PVC specification](https://github.com/kubernetes/api/blob/669e693933c77e91648f8602dc2555d96e6279ad/core/v1/types.go#L487)
  for the Suggestion Deployment's PVC.

  The default value is:

  ```yaml
  persistentVolumeClaimSpec:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 1Gi
  ```

- `persistentVolumeSpec` - a [PV specification](https://github.com/kubernetes/api/blob/669e693933c77e91648f8602dc2555d96e6279ad/core/v1/types.go#L324)
  for the Suggestion Deployment's PV.

  Suggestion Deployment's PV always has **`persistentVolumeReclaimPolicy: Delete`** to properly
  remove all resources once Katib Experiment is deleted. To know more about PV reclaim policies
  check the
  [Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming).

- `persistentVolumeLabels` - [PV labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
  for the Suggestion Deployment's PV.

### Early Stoppings Parameters

Parameters set in `runtime.earlyStoppings` configure container for
[the Katib Early Stopping algorithms](/docs/components/katib/user-guides/early-stopping/#early-stopping-algorithms).
The following settings are **required** for each early stopping algorithm that you want
to use in your Katib Experiments:

- `algorithmName` - one of the early stopping algorithm names (e.g. `medianstop`).

- `image` - a Docker image for the early stopping container.

The following settings are **optional**:

- `imagePullPolicy` - an [image pull policy](https://kubernetes.io/docs/concepts/configuration/overview/#container-images)
  for the early stopping's container.

  The default value is `IfNotPresent`

- `resources` - [resources](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-requests-and-limits-of-pod-and-container)
  for the early stopping's container.

  Configuration for `resources` works the same as for Katib metrics collector's container `resources`.

## Next steps

- How to [set up environment variables](/docs/components/katib/user-guides/env-variables/) for
  various Katib component.



================================================
File: content/en/docs/components/katib/user-guides/katib-ui.md
================================================
+++
title = "How to use Katib UI"
description = "How to access and use Katib UI"
weight = 30
+++

This page describes how to access and use Katib UI. Follow
[the installation page](/docs/components/katib/installation/#installing-katib) to install Katib
control plane before accessing Katib UI.

You can use the Katib user interface (UI) to submit Katib Experiments and to monitor your
Experiments results.

## Accessing Katib UI from Kubeflow Central Dashboard

If you install Katib as part of Kubeflow platform, you can access Katib UI via
[Kubeflow Central Dashboard](/docs/components/central-dash/access/#how-to-access-the-kubeflow-central-dashboard).
Click **Experiments (AutoML)** in the left-hand menu:

<img src="/docs/components/katib/images/home-page-kubeflow-ui.png"
  alt="The Katib UI within the Kubeflow Central Dashboard"
  class="mt-3 mb-3 border border-info rounded">

## Accessing Katib UI Standalone

You can access Katib UI standalone without Kubeflow Central Dashboard. For that, port-forward the
Katib UI service:

```shell
kubectl port-forward svc/katib-ui -n kubeflow 8080:80
```

Use this URL to access Katib UI:

```shell
http://localhost:8080/katib/
```

You need to select namespace to view Katib Experiments:

<img src="/docs/components/katib/images/home-page-standalone.png"
  alt="The Katib UI Standalone"
  class="mt-3 mb-3 border border-info rounded">

## Running Hyperparameter Tuning Experiment from Katib UI

You can submit an hyperparameter tuning Experiment from the Katib UI.

### Create Katib Experiment

1. Click **New Experiment** on the Katib home page.

1. You should be able to view tabs offering you the following options:

   - **Metadata:** Type name of your Experiment.

   - **Trial Thresholds:** Choose how many Trials you want to run.

   - **Objective:** Add metrics that you want to optimize and type of optimization.

   - **Search Algorithm:** Select hyperparameter tuning algorithm and configure algorithm settings.

   - **Early Stopping:** Add early stopping algorithm if that is required.

   - **Hyper Parameters:** Add hyperparameters and search space that you want to optimize.

   - **Metrics Collector:** Modify metrics collector type if that is required.

   - **Trial Template:** Configure parameters for your Trial template. Every hyperparameter must have
     reference to the `trialParameters` values.

   <img src="/docs/components/katib/images/deploy-parameters.png"
        alt="Deploy Katib Experiment using parameters"
        class="mt-3 mb-3 border border-info rounded">

1. (Optional) If you want to modify Experiment YAML, you can click edit and submit YAML at the bottom.

   <img src="/docs/components/katib/images/deploy-yaml.png"
       alt="Deploy Katib Experiment using YAML"
       class="mt-3 mb-3 border border-info rounded">

1. Create Katib Experiment.

### Get Katib Experiment Results

Follow these steps to get Katib Experiment results:

1. You should be able to view the list of Experiments on Katib UI home page:

   <img src="/docs/components/katib/images/home-page-kubeflow-ui.png"
     alt="List of Katib Experiments"
     class="mt-3 mb-3 border border-info rounded">

1. Click the name of your Experiment. For example, click **random-example**.

1. There should be a graph showing the level of validation and train accuracy
   for various combinations of the hyperparameter values (learning rate, number
   of layers, and optimizer):

   <img src="/docs/components/katib/images/random-example-graph.png"
     alt="Graph produced by the random example"
     class="mt-3 mb-3 border border-info rounded">

1. If you click to the Trials tab, you will see list of Trials that ran withing the Experiment.

   <img src="/docs/components/katib/images/random-example-trials.png"
     alt="Trials that ran during the Experiment"
     class="mt-3 mb-3 border border-info rounded">

1. You can click on Trial name to get metrics for the particular Trial:

   <img src="/docs/components/katib/images/random-example-trial-info.png"
     alt="Trial metrics graph"
     class="mt-3 mb-3 border border-info rounded">

### Create Katib Experiment with Early Stopping

Follow [this guide](/docs/components/katib/user-guides/early-stopping) to learn how early stopping
works in Katib.

1. Select early stopping algorithm while creating Katib Experiment:

   <img src="/docs/components/katib/images/early-stopping-parameter.png"
       alt="Katib Experiment with Early Stopping"
       class="mt-3 mb-3 border border-info rounded">

1. After your Experiment is complete, you can check your results in the Katib UI. The Trial statuses
   on the Experiment monitor page should look as follows:

   <img src="/docs/components/katib/images/early-stopping-trials.png"
       alt="Trials view with early stopping"
       class="mt-3 mb-3 border border-info rounded">

1. You can click on the early stopped Trial name to get reported metrics before this Trial was early stopped:

   <img src="/docs/components/katib/images/early-stopping-trial-info.png"
     alt="Early stopped Trial metrics"
     class="mt-3 mb-3 border border-info rounded">

## Next Steps

- Understand how [Katib metrics collector works](/docs/components/katib/user-guides/metrics-collector).

- Learn how to use [early stopping within Katib Experiments](/docs/components/katib/user-guides/early-stopping)



================================================
File: content/en/docs/components/katib/user-guides/metrics-collector.md
================================================
+++
title = "How to Configure Metrics Collector"
description = "Overview of Katib metrics collector and how to configure it"
weight = 40
+++

This guide describes how Katib metrics collector works.

## Prerequisites

Before running your hyperparameter tuning Katib Experiment with Python SDK,
ensure the namespace label `katib.kubeflow.org/metrics-collector-injection: enabled`
is present. This label enables the sidecar container injection for pull-based metrics collectors to collect metrics during the experiment.

You can configure the namespace by adding the following label `katib.kubeflow.org/metrics-collector-injection: enabled`
as is shown in the sample code:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: <your-namespace>
  labels:
    katib.kubeflow.org/metrics-collector-injection: enabled
```

Or you can add the label to an existing namespace using the following command:

```bash
kubectl label namespace <your-namespace> katib.kubeflow.org/metrics-collector-injection=enabled
```

## Overview

There are two ways to collect metrics:

1. Pull-based: collects the metrics using a _sidecar_ container. A sidecar is a utility container that supports
the main container in the Kubernetes Pod.

2. Push-based: users push the metrics directly to Katib DB in the training scripts.

In the `metricsCollectorSpec` section of the Experiment YAML configuration file, you can
define how Katib should collect the metrics from each Trial, such as the accuracy and loss metrics.

## Pull-based Metrics Collector

Your training code can record the metrics into `StdOut` or into arbitrary output files.

To define the pull-based metrics collector for your Experiment:

1. Specify the collector type in the `.collector.kind` field.
   Katib's metrics collector supports the following collector types:

   - `StdOut`: Katib collects the metrics from the operating system's default
     output location (_standard output_). This is the default metrics collector.

   - `File`: Katib collects the metrics from an arbitrary file, which
     you specify in the `.source.fileSystemPath.path` field. Training container
     should log metrics to this file in `TEXT` or `JSON` format. If you select `JSON` format,
     metrics must be line-separated by `epoch` or `step` as follows, and the key for timestamp must
     be `timestamp`:

     ```json
     {"epoch": 0, "foo": "bar", "fizz": "buzz", "timestamp": "2021-12-02T14:27:51"}
     {"epoch": 1, "foo": "bar", "fizz": "buzz", "timestamp": "2021-12-02T14:27:52"}
     {"epoch": 2, "foo": "bar", "fizz": "buzz", "timestamp": "2021-12-02T14:27:53"}
     {"epoch": 3, "foo": "bar", "fizz": "buzz", "timestamp": "2021-12-02T14:27:54"}
     ```

     Check the file metrics collector example for [`TEXT`](https://github.com/kubeflow/katib/blob/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/examples/v1beta1/metrics-collector/file-metrics-collector.yaml#L14-L24)
     and [`JSON`](https://github.com/kubeflow/katib/blob/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/examples/v1beta1/metrics-collector/file-metrics-collector-with-json-format.yaml#L14-L22)
     format. Also, the default file path is `/var/log/katib/metrics.log`, and the default file format is `TEXT`.

   - `TensorFlowEvent`: Katib collects the metrics from a directory path
     containing a [tf.Event](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Event).
     These are typically written by [tensorflow.summary](https://www.tensorflow.org/api_docs/python/tf/summary).
     As of Katib 0.18, [torch.utils.tensorboard](https://pytorch.org/docs/stable/tensorboard.html) or
     [tensorboardX](https://tensorboardx.readthedocs.io/en/latest/index.html) may also be used to write metrics.
     You should specify the path in the `.source.fileSystemPath.path` field. Check the
     [TFJob example](https://github.com/kubeflow/katib/blob/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/examples/v1beta1/kubeflow-training-operator/tfjob-mnist-with-summaries.yaml#L17-L23).
     The default directory path is `/var/log/katib/tfevent/`.

   - `Custom`: Specify this value if you need to use a custom way to collect
     metrics. You must define your custom metrics collector container
     in the `.collector.customCollector` field. Check the
     [custom metrics collector example](https://github.com/kubeflow/katib/blob/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/examples/v1beta1/metrics-collector/custom-metrics-collector.yaml#L14-L36).

2. Write code in your training container to print or save to the file metrics in the format
   specified in the `.source.filter.metricsFormat` field. The default metrics format value is:

   ```
   ([\w|-]+)\s*=\s*([+-]?\d*(\.\d+)?([Ee][+-]?\d+)?)
   ```

   Each element is a regular expression with two sub-expressions. The first matched expression is
   taken as the metric name. The second matched expression is taken as the metric value.

   For example, using the default metrics format and `StdOut` metrics collector,
   if the name of your objective metric is `loss` and the additional metrics are
   `recall` and `precision`, your training code should print the following output:

   ```shell
   epoch 1:
   loss=3.0e-02
   recall=0.5
   precision=.4

   epoch 2:
   loss=1.3e-02
   recall=0.55
   precision=.5
   ```

## Push-based Metrics Collector

Your training code needs to call [`report_metrics()`](https://github.com/kubeflow/katib/blob/e251a07cb9491e2d892db306d925dddf51cb0930/sdk/python/v1beta1/kubeflow/katib/api/report_metrics.py#L26) function in Python SDK to record metrics.
The `report_metrics()` function works by parsing the metrics in `metrics` field into a gRPC request, automatically adding the current timestamp for users, and sending the request to Katib DB Manager.

But before that, `kubeflow-katib` package should be installed in your training container.

To define the push-based metrics collector for your Experiment, you have two options:

- YAML File

    1. Specify the collector type `Push` in the `.collector.kind` field.

    2. Write code in your training container to call `report_metrics()` to report metrics.

- [`tune`](https://github.com/kubeflow/katib/blob/master/sdk/python/v1beta1/kubeflow/katib/api/katib_client.py#L166) function

    Use tune function and specify the `metrics_collector_config` field. You can reference to the following example:

    ```
    import kubeflow.katib as katib

    def objective(parameters):
      import time
      import kubeflow.katib as katib
      time.sleep(5)
      result = 4 * int(parameters["a"])
      # Push metrics to Katib DB.
      katib.report_metrics({"result": result})

    katib.KatibClient(namespace="kubeflow").tune(
      name="push-metrics-exp",
      objective=objective,
      parameters= {"a": katib.search.int(min=10, max=20)}
      objective_metric_name="result",
      max_trial_count=2,
      metrics_collector_config={"kind": "Push"},
      # When SDK is released, replace it with packages_to_install=["kubeflow-katib==0.18.0"].
      # Currently, the training container should have `git` package to install this SDK.
      packages_to_install=["git+https://github.com/kubeflow/katib.git@master#subdirectory=sdk/python/v1beta1"],
    )
    ```



================================================
File: content/en/docs/components/katib/user-guides/resume-experiment.md
================================================
+++
title = "How to Resume Experiments"
description = "How to modify running Experiments and resume completed Experiments"
weight = 70
+++

This guide describes how to modify running Experiments and restart completed Experiments.
You will learn about changing the Experiment execution process and use various
resume policies for the Katib Experiment.

## Modify Running Experiment

While the Experiment is running you are able to change Trial count parameters. For example, you
can decrease the maximum number of hyperparameter sets that are trained in parallel.

You can change only **`parallelTrialCount`**, **`maxTrialCount`** and **`maxFailedTrialCount`**
Experiment parameters.

Use Kubernetes API or `kubectl`
[in-place update of resources](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#in-place-updates-of-resources)
to make Experiment changes. For example, run:

```shell
kubectl edit experiment <experiment-name> -n <experiment-namespace>
```

Make appropriate changes and save it. Controller automatically processes
the new parameters and makes necessary changes.

- If you want to increase or decrease parallel Trial execution, modify `parallelTrialCount`.
  Controller accordingly creates or deletes Trials in line with the `parallelTrialCount` value.

- If you want to increase or decrease maximum Trial count, modify `maxTrialCount`. `maxTrialCount`
  should be greater than current count of `Succeeded` Trials.
  You can remove the `maxTrialCount` parameter, if your Experiment should run endless
  with `parallelTrialCount` of parallel Trials until the Experiment reaches `Goal` or `maxFailedTrialCount`

- If you want to increase or decrease maximum failed Trial count, modify `maxFailedTrialCount`.
  You can remove the `maxFailedTrialCount` parameter, if the Experiment should not reach `Failed` status.

## Resume Succeeded Experiment

Katib Experiment is restartable only if it is in **`Succeeded`** status because `maxTrialCount`
has been reached. To check current Experiment status run:
`kubectl get experiment <experiment-name> -n <experiment-namespace>`.

To restart an Experiment, you are able to change only **`parallelTrialCount`**,
**`maxTrialCount`** and **`maxFailedTrialCount`** as described [above](#modify-running-experiment)

To control various resume policies, you can specify `.spec.resumePolicy` for the Experiment.

### Resume policy: Never

Use this policy if your Experiment should not be resumed at any time. After the Experiment has finished,
the Suggestion's [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
and [Service](https://kubernetes.io/docs/concepts/services-networking/service/)
are deleted and you can't restart the Experiment.

This is the default policy for all Katib Experiments. You can omit `.spec.resumePolicy` parameter
for that functionality.

### Resume policy: LongRunning

Use this policy if you intend to restart the Experiment. After the Experiment has finished,
the Suggestion's Deployment and Service stay running until you delete your Experiment.
Modify Experiment's Trial count parameters to restart the Experiment.

Check the
[`long-running-resume.yaml`](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/resume-experiment/long-running-resume.yaml#L17)
example for more details.

### Resume policy: FromVolume

Use this policy if you intend to restart the Experiment. In that case, [volume](https://kubernetes.io/docs/concepts/storage/volumes/)
is attached to the Suggestion's Deployment.

Katib controller creates [PersistentVolumeClaim (PVC)](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
in addition to the Suggestion's Deployment and Service.

- PVC is deployed with the name: `<suggestion-name>-<suggestion-algorithm>`
  in the Suggestion namespace.

- PV is deployed with the name:
  `<suggestion-name>-<suggestion-algorithm>-<suggestion-namespace>`

After the Experiment has finished, the Suggestion's Deployment and Service are deleted.
Suggestion data can be retained in the volume. When you restart the Experiment, the Suggestion's
Deployment and Service are created and Suggestion statistics can be recovered from the volume.

When you delete the Experiment, the Suggestion's Deployment, Service, PVC and PV are deleted automatically.

Check the
[`from-volume-resume.yaml`](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/resume-experiment/from-volume-resume.yaml#L17)
example for more details.

## Next steps

- Learn how to [configure and run your Katib Experiments](/docs/components/katib/user-guides/hp-tuning/configure-experiment).



================================================
File: content/en/docs/components/katib/user-guides/trial-template.md
================================================
+++
title = "How to use Trial Templates"
description = "Trial template parameters overview and how use CRDs with Katib Trials"
weight = 35
+++

This guide describes how to configure Trial template parameters and use custom
[Kubernetes CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
in Katib Trials. You will learn about changing Trial template specification, how to use
[Kubernetes ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/)
to store templates and how to modify Katib controller to support your
Kubernetes CRD in Katib Experiments.

Katib dynamically supports any kind of Kubernetes CRD as Trial's Worker.
In Katib examples, you can find the following examples for Trial's Workers:

- [Kubernetes `Job`](https://kubernetes.io/docs/concepts/workloads/controllers/job/)

- [Kubeflow `TFJob`](/docs/components/trainer/legacy-v1/user-guides/tensorflow)

- [Kubeflow `PyTorchJob`](/docs/components/trainer/legacy-v1/user-guides/pytorch/)

- [Kubeflow `XGBoostJob`](/docs/components/trainer/legacy-v1/user-guides/xgboost)

- [Kubeflow `MPIJob`](/docs/components/trainer/legacy-v1/user-guides/mpi)

- [Tekton `Pipelines`](https://github.com/kubeflow/katib/tree/master/examples/v1beta1/tekton)

- [Argo `Workflows`](https://github.com/kubeflow/katib/tree/master/examples/v1beta1/argo)

To use your own Kubernetes resource follow the steps [below](#use-crds-with-trial-template).

## How to use Trial Template

To run the Katib Experiment you have to specify a Trial template for your
[Worker job](/docs/components/katib/reference/architecture/#worker) where actual
model training is running.

### Configure Trial Template Specification

Trial template specification is located under `.spec.trialTemplate` of your Experiment.
To define Trial, you should specify these parameters in `.spec.trialTemplate`:

- `trialParameters` - list of the parameters which are used in the Trial template
  during Experiment execution.

  **Note:** Your Trial template must contain each parameter from the `trialParameters`. You can
  set these parameters in any field of your template, except `.metadata.name` and
  `.metadata.namespace`. For example, your training container can receive
  hyperparameters as command-line or arguments or as environment variables.

  Your Experiment's Suggestion produces `trialParameters` before running the Trial.
  Each `trialParameter` has these structure:

  - `name` - the parameter name that is replaced in your template.

  - `description` (optional) - the description of the parameter.

  - `reference` - the parameter name that Experiment's Suggestion returns. Usually, for the
    hyperparameter tuning parameter references are equal to the Experiment search space. For example,
    in grid example search space has [three parameters](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/hp-tuning/grid.yaml#L17-L29) (`lr`, `momentum`) and `trialParameters` contains each of these parameters in
    [`reference`](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/hp-tuning/grid.yaml#L32-L39).

- You have to define your Trial template in **one** of the `trialSpec` or `configMap` sources.

  **Note:** Your template must omit `.metadata.name` and `.metadata.namespace`.

  To set the parameters from the `trialParameters`, you need to use this expression:
  `${trialParameters.<parameter-name>}` in your template. Katib automatically replaces it with
  the appropriate values from the Suggestion.

  For example, `--lr=${trialParameters.learningRate}` is the `learningRate` parameter.

  - `trialSpec` - the Trial template in
    [unstructured](https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1/unstructured) format.
    The template should be a valid YAML.

  - `configMap` - Kubernetes ConfigMap specification where the Trial template is located.
    This ConfigMap must have the label `katib.kubeflow.org/component: trial-templates` and contains
    key-value pairs, where `key: <template-name>, value: <template-yaml>`. Check the example of the
    [ConfigMap with Trial templates](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/trial-template/trial-configmap-source.yaml).

    The `configMap` specification should have:

    1. `configMapName` - the ConfigMap name with the Trial templates.

    1. `configMapNamespace` - the ConfigMap namespace with the Trial templates.

    1. `templatePath` - the ConfigMap's data path to the template.

`.spec.trialTemplate` parameters below are used to control Trial behavior. If parameter has the
default value, it can be **omitted** in the Experiment YAML.

- `retain` - indicates that Trials's resources are not clean-up after the Trial
  is complete. Check the example with
  [`retain: true`](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/tekton/pipeline-run.yaml#L31) parameter.

  The default value is `false`

- `primaryPodLabels` - the Trial Worker's Pod or Pods labels. These Pods are injected by Katib
  metrics collector.

  **Note:** If `primaryPodLabels` are **omitted**, the Katib metrics collector wraps all worker's Pods.
  Check the example with
  [`primaryPodLabels`](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/kubeflow-training-operator/mpijob-horovod.yaml#L30-L31).

  The default value for Kubeflow `TFJob`, `PyTorchJob`, `MXJob`, and `XGBoostJob` is `job-role: master`

  The `primaryPodLabels` default value works only if you specify your template in
  `.spec.trialTemplate.trialSpec`. For the `configMap` template source you have to manually set
  `primaryPodLabels`.

- `primaryContainerName` - the training container name where actual model training is running.
  Katib metrics collector wraps this container to collect required metrics for the single
  Experiment optimization step.

- `successCondition` - The Trial Worker's object
  [status](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status)
  in which Trial's job has succeeded. This condition must be in
  [GJSON format](https://github.com/tidwall/gjson). Check the example with
  [`successCondition`](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/tekton/pipeline-run.yaml#L35).

  The default value for Kubernetes `Job` is:

  ```
  status.conditions.#(type=="Complete")#|#(status=="True")#
  ```

  The default value for Kubeflow `TFJob`, `PyTorchJob`, `MXJob`, and `XGBoostJob` is:

  ```
  status.conditions.#(type=="Succeeded")#|#(status=="True")#
  ```

  The `successCondition` default value works only if you specify your template
  in `.spec.trialTemplate.trialSpec`. For the `configMap` template source
  you have to manually set `successCondition`.

- `failureCondition` - The Trial Worker's object
  [status](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status)
  in which Trial's job has failed. This condition must be in
  [GJSON format](https://github.com/tidwall/gjson). Check the example with
  [`failureCondition`](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/tekton/pipeline-run.yaml#L36).

  The default value for Kubernetes `Job` and Kubeflow `TFJob`, `PyTorchJob`, `MXJob`, and `XGBoostJob` is:

  ```
  status.conditions.#(type=="Failed")#|#(status=="True")#
  ```

  The `failureCondition` default value works only if you specify your template in
  `.spec.trialTemplate.trialSpec`. For the `configMap` template source you
  have to manually set `failureCondition`.

### Use Metadata in Trial Template

You can't specify `.metadata.name` and `.metadata.namespace` in your Trial template, but you can
get this data during the Experiment run. For example, if you want to append the Trial's name to your
model storage.

To do this, point `.trialParameters[x].reference` to the appropriate metadata parameter and
use `.trialParameters[x].name` in your Trial template.

The table below shows the connection between
`.trialParameters[x].reference` value and Trial metadata.

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Reference</th>
        <th>Trial metadata</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>${trialSpec.Name}</code></td>
        <td>Trial name</td>
      </tr>
      <tr>
        <td><code>${trialSpec.Namespace}</code></td>
        <td>Trial namespace</td>
      </tr>
      <tr>
        <td><code>${trialSpec.Kind}</code></td>
        <td>Kubernetes resource kind for the Trial's worker</td>
      </tr>
      <tr>
        <td><code>${trialSpec.APIVersion}</code></td>
        <td>Kubernetes resource APIVersion for the Trial's worker</td>
      </tr>
      <tr>
        <td><code>${trialSpec.Labels[custom-key]}</code></td>
        <td>Trial's worker label with <code>custom-key</code> key </td>
      </tr>
      <tr>
        <td><code>${trialSpec.Annotations[custom-key]}</code></td>
        <td>Trial's worker annotation with <code>custom-key</code> key </td>
      </tr>
    </tbody>
  </table>
</div>

Check the example of
[using Trial metadata](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/trial-template/trial-metadata-substitution.yaml).

## Use CRDs with Trial Template

It is possible to use your own Kubernetes CRD or other Kubernetes resource
(e.g. [Kubernetes `CronJob`](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/))
as a Trial Worker without modifying Katib controller source code and building the new image.
As long as your CRD creates Kubernetes Pods, allows to inject
the [sidecar container](https://kubernetes.io/docs/concepts/workloads/pods/) on these Pods and has
succeeded and failed status, you can use it in Katib.

To do that, you need to modify Katib components before installing it on your Kubernetes cluster.
Accordingly, you have to know your CRD API group and version, the CRD object's kind.
Also, you need to know which resources your custom object is created. Check the
[Kubernetes guide](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)
to know more about CRDs.

Follow these two simple steps to integrate your custom CRD in Katib:

1. Modify Katib controller
   [ClusterRole's rules](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/manifests/v1beta1/components/controller/rbac.yaml#L5)
   with the new rule to give Katib access to all resources that are created by the Trial.
   To know more about ClusterRole, check the
   [Kubernetes guide](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole).

   In case of Tekton `Pipelines`, Trials creates Tekton `PipelineRun`, then Tekton `PipelineRun`
   creates Tekton `TaskRun`. Therefore, Katib controller ClusterRole should have access to the
   `pipelineruns` and `taskruns`:

   ```yaml
   - apiGroups:
       - tekton.dev
     resources:
       - pipelineruns
       - taskruns
     verbs:
       - "get"
       - "list"
       - "watch"
       - "create"
       - "delete"
   ```

1. Modify Katib Config
   [controller parameters](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/manifests/v1beta1/installs/katib-standalone/katib-config.yaml#L9-L15)
   with the new entity:

   ```
   trialResources:
    - <object-kind>.<object-API-version>.<object-API-group>
   ```

   For example, to support Tekton `Pipelines`:

   ```yaml
   trialResources:
     - PipelineRun.v1beta1.tekton.dev
   ```

After these changes, deploy Katib as described in the [installation guide](/docs/components/katib/installation/)
and wait until the `katib-controller` Pod is created. You can check logs from the Katib controller
to verify your resource integration:

```shell
$ kubectl logs $(kubectl get pods -n kubeflow -o name | grep katib-controller) -n kubeflow | grep '"CRD Kind":"PipelineRun"'

{"level":"info","ts":1628032648.6285546,"logger":"trial-controller","msg":"Job watch added successfully","CRD Group":"tekton.dev","CRD Version":"v1beta1","CRD Kind":"PipelineRun"}
```

If you ran the above steps successfully, you should be able to use your custom
object YAML in the Experiment's Trial template source spec.

We appreciate your feedback on using various CRDs in Katib. It would be great, if you could let us
know about your Experiments. The
[developer guide](https://github.com/kubeflow/katib/blob/master/docs/developer-guide.md)
is a good starting point to know how to contribute to the project.

## Next steps

- Understand the [Katib metrics collector capabilities](/docs/components/katib/user-guides/metrics-collector).

- Learn about [Katib Configuration](/docs/components/katib/user-guides/katib-config/).



================================================
File: content/en/docs/components/katib/user-guides/hp-tuning/_index.md
================================================
+++
title = "Hyperparameter Tuning"
description = "User guides to run Hyperparameter Tuning Experiments"
weight = 10
+++



================================================
File: content/en/docs/components/katib/user-guides/hp-tuning/configure-algorithm.md
================================================
+++
title = "How to Configure Algorithms"
description = "List of supported algorithms for hyperparameter tuning"
weight = 20
+++

This page describes hyperparameter (HP) tuning algorithms that Katib supports and how to configure
them.

## HP Tuning Algorithms

Katib currently supports several search algorithms for NAS:

- [Grid Search](#grid-search)
- [Random Search](#random-search)
- [Bayesian Optimization](#bayesian-optimization)
- [Hyperband](#hyperband)
- [Tree of Parzen Estimators](#tree-of-parzen-estimators-tpe)
- [Multivariate TPE](#multivariate-tpe)
- [Covariance Matrix Adaptation Evolution Strategy (CMA-ES)](#covariance-matrix-adaptation-evolution-strategy-cma-es)
- [Sobol Quasirandom Sequence](#sobol-quasirandom-sequence)
- [Population Based Training](#population-based-training)

### Grid Search

The algorithm name in Katib is `grid`.

Grid sampling is useful when all variables are discrete (as opposed to
continuous) and the number of possibilities is low. A grid search
performs an exhaustive combinatorial search over all possibilities,
making the search process extremely long even for medium sized problems.

Katib uses the [Optuna](https://github.com/optuna/optuna) optimization
framework for its grid search.

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting name</th>
        <th>Description</th>
        <th>Example</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>random_state</td>
        <td>[int]: Set <code>random_state</code> to something other than None
          for reproducible results.</td>
        <td>10</td>
      </tr>
    </tbody>
  </table>
</div>

### Random search

The algorithm name in Katib is `random`.

Random sampling is an alternative to grid search and is used when the number of
discrete variables to optimize is large and the time required for each
evaluation is long. When all parameters are discrete, random search performs
sampling without replacement. Random search is therefore the best algorithm to
use when combinatorial exploration is not possible. If the number of continuous
variables is high, you should use quasi random sampling instead.

Katib uses the [Hyperopt](https://hyperopt.github.io/hyperopt/),
[Goptuna](https://github.com/c-bata/goptuna) or
[Optuna](https://github.com/optuna/optuna) optimization
framework for its random search.

Katib supports the following algorithm settings:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting name</th>
        <th>Description</th>
        <th>Example</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>random_state</td>
        <td>[int]: Set <code>random_state</code> to something other than None
          for reproducible results.</td>
        <td>10</td>
      </tr>
    </tbody>
  </table>
</div>

### Bayesian optimization

The algorithm name in Katib is `bayesianoptimization`.

The [Bayesian optimization](https://arxiv.org/pdf/1012.2599.pdf) method uses
Gaussian process regression to model the search space. This technique calculates
an estimate of the loss function and the uncertainty of that estimate at every
point in the search space. The method is suitable when the number of
dimensions in the search space is low. Since the method models both
the expected loss and the uncertainty, the search algorithm converges in a few
steps, making it a good choice when the time to
complete the evaluation of a parameter configuration is long.

Katib uses the
[Scikit-Optimize](https://github.com/scikit-optimize/scikit-optimize) optimization framework
for its Bayesian search. Scikit-Optimize is also known as `skopt`.

Katib supports the following algorithm settings:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting Name</th>
        <th>Description</th>
        <th>Example</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>base_estimator</td>
        <td>[“GP”, “RF”, “ET”, “GBRT” or sklearn regressor, default=“GP”]:
          Should inherit from <code>sklearn.base.RegressorMixin</code>.
          The <code>predict</code> method should have an optional
          <code>return_std</code> argument, which returns
          <code>std(Y | x)</code> along with <code>E[Y | x]</code>. If
          <code>base_estimator</code> is one of
          [“GP”, “RF”, “ET”, “GBRT”], the system uses a default surrogate model
          of the corresponding type. Learn more information in the
          <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html#skopt.Optimizer">skopt
          documentation</a>.</td>
        <td>GP</td>
      </tr>
      <tr>
        <td>n_initial_points</td>
        <td>[int, default=10]: Number of evaluations of <code>func</code> with
          initialization points before approximating it with
          <code>base_estimator</code>. Points provided as <code>x0</code> count
          as initialization points.
          If <code>len(x0) &lt; n_initial_points</code>, the
          system samples additional points at random. Learn more information in the
          <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html#skopt.Optimizer">skopt
          documentation</a>.</td>
        <td>10</td>
      </tr>
      <tr>
        <td>acq_func</td>
        <td>[string, default=<code>&quot;gp_hedge&quot;</code>]: The function to
          minimize over the posterior distribution. Learn more information in the
          <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html#skopt.Optimizer">skopt
          documentation</a>.</td>
        <td>gp_hedge</td>
      </tr>
      <tr>
        <td>acq_optimizer</td>
        <td>[string, “sampling” or “lbfgs”, default=“auto”]: The method to
          minimize the acquisition function. The system updates the fit model
          with the optimal value obtained by optimizing <code>acq_func</code>
          with <code>acq_optimizer</code>. Learn more information in the
          <a href="https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html#skopt.Optimizer">skopt
          documentation</a>.</td>
        <td>auto</td>
      </tr>
      <tr>
        <td>random_state</td>
        <td>[int]: Set <code>random_state</code> to something other than None
          for reproducible results.</td>
        <td>10</td>
      </tr>
    </tbody>
  </table>
</div>

### Hyperband

The algorithm name in Katib is `hyperband`.

Katib supports the [Hyperband](https://arxiv.org/pdf/1603.06560.pdf)
optimization framework.
Instead of using Bayesian optimization to select configurations, Hyperband
focuses on early stopping as a strategy for optimizing resource allocation and
thus for maximizing the number of configurations that it can evaluate.
Hyperband also focuses on the speed of the search.

### Tree of Parzen Estimators (TPE)

The algorithm name in Katib is `tpe`.

Katib uses the [Hyperopt](https://hyperopt.github.io/hyperopt/),
[Goptuna](https://github.com/c-bata/goptuna) or
[Optuna](https://github.com/optuna/optuna) optimization
framework for its TPE search.

This method provides a [forward and reverse gradient-based](https://arxiv.org/pdf/1703.01785.pdf)
search.

Katib supports the following algorithm settings:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting name</th>
        <th>Description</th>
        <th>Example</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>n_EI_candidates</td>
        <td>[int]: Number of candidate samples used to calculate the expected improvement.</td>
        <td>25</td>
      </tr>
      <tr>
        <td>random_state</td>
        <td>[int]: Set <code>random_state</code> to something other than None
          for reproducible results.</td>
        <td>10</td>
      </tr>
      <tr>
        <td>gamma</td>
        <td>[float]: The threshold to split between l(x) and g(x), check equation 2 in
        <a href="https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf">
        this Paper</a>. Value must be in (0, 1) range.</td>
        <td>0.25</td>
      </tr>
      <tr>
        <td>prior_weight</td>
        <td>[float]: Smoothing factor for counts, to avoid having 0 probability.
        Value must be > 0.</td>
        <td>1.1</td>
      </tr>
    </tbody>
  </table>
</div>

### Multivariate TPE

The algorithm name in Katib is `multivariate-tpe`.

Katib uses the [Optuna](https://hyperopt.github.io/hyperopt/) optimization
framework for its Multivariate TPE search.

[Multivariate TPE](https://tech.preferred.jp/en/blog/multivariate-tpe-makes-optuna-even-more-powerful/)
is improved version of independent (default) TPE. This method finds
dependencies among hyperparameters in search space.

Katib supports the following algorithm settings:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting name</th>
        <th>Description</th>
        <th>Example</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>n_ei_candidates</td>
        <td>[int]: Number of Trials used to calculate the expected improvement.</td>
        <td>25</td>
      </tr>
      <tr>
        <td>random_state</td>
        <td>[int]: Set <code>random_state</code> to something other than None
          for reproducible results.</td>
        <td>10</td>
      </tr>
      <tr>
        <td>n_startup_trials</td>
        <td>[int]: Number of initial Trials for which the random search algorithm generates
        hyperparameters.</td>
        <td>5</td>
      </tr>
    </tbody>
  </table>
</div>

### Covariance Matrix Adaptation Evolution Strategy (CMA-ES)

The algorithm name in Katib is `cmaes`.

Katib uses the [Goptuna](https://github.com/c-bata/goptuna) or
[Optuna](https://github.com/optuna/optuna) optimization
framework for its CMA-ES search.

The [Covariance Matrix Adaptation Evolution Strategy](https://en.wikipedia.org/wiki/CMA-ES)
is a stochastic derivative-free numerical optimization algorithm for optimization
problems in continuous search spaces.
You can also use [IPOP-CMA-ES](https://sci2s.ugr.es/sites/default/files/files/TematicWebSites/EAMHCO/contributionsCEC05/auger05ARCMA.pdf) and [BIPOP-CMA-ES](https://hal.inria.fr/inria-00382093/document), variant algorithms for restarting optimization when converges to local minimum.

Katib supports the following algorithm settings:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting name</th>
        <th>Description</th>
        <th>Example</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>random_state</td>
        <td>[int]: Set <code>random_state</code> to something other than None
          for reproducible results.</td>
        <td>10</td>
      </tr>
      <tr>
        <td>sigma</td>
        <td>[float]: Initial standard deviation of CMA-ES.</td>
        <td>0.001</td>
      </tr>
      <tr>
        <td>restart_strategy</td>
        <td>[string, "none", "ipop", or "bipop", default="none"]: Strategy for restarting CMA-ES optimization when converges to a local minimum.</td>
        <td>"ipop"</td>
      </tr>
    </tbody>
  </table>
</div>

### Sobol Quasirandom Sequence

The algorithm name in Katib is `sobol`.

Katib uses the [Goptuna](https://github.com/c-bata/goptuna) optimization
framework for its Sobol's quasirandom search.

The [Sobol's quasirandom sequence](https://dl.acm.org/doi/10.1145/641876.641879)
is a low-discrepancy sequence. And it is known that Sobol's quasirandom sequence can
provide better uniformity properties.

### Population Based Training

The algorithm name in Katib is `pbt`.

Review the population based training [paper](https://arxiv.org/abs/1711.09846) for more details about the algorithm.

The PBT service requires a Persistent Volume Claim with [RWX access mode](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) to share resources between Suggestion and Trials. Currently, Katib Experiments should have <code>resumePolicy: FromVolume</code> to run the PBT algorithm. Learn more about resume policies in [this guide](/docs/components/katib/user-guides/resume-experiment/).

Katib supports the following algorithm settings:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting name</th>
        <th>Description</th>
        <th>Example</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>suggestion_trial_dir</td>
        <td>The location within the Trial container where checkpoints are saved</td>
        <td><code>/var/log/katib/checkpoints/</code></td>
      </tr>
      <tr>
        <td>n_population</td>
        <td>Number of Trial seeds per generation</td>
        <td>40</td>
      </tr>
      <tr>
        <td>resample_probability</td>
        <td>null (default): perturbs the hyperparameter by 0.8 or 1.2. 0-1: resamples the original distribution by the specified probability</td>
        <td>0.3</td>
      </tr>
      <tr>
        <td>truncation_threshold</td>
        <td>Exploit threshold for pruning low performing seeds</td>
        <td>0.4</td>
      </tr>
    </tbody>
  </table>
</div>

## Use Custom Algorithm in Katib

You can add an HP tuning algorithm to Katib yourself. The design of Katib follows the `ask-and-tell` pattern:

1. Ask for a new set of parameters
1. Walk to the Experiment and program in the new parameters
1. Observe the outcome of running the Experiment
1. Walk back to your laptop and tell the optimizer about the outcome 1. go to step 1

When an Experiment is created, one algorithm service as Kubernetes Deployment will be created.
Then Katib asks for new sets of parameters via `GetSuggestions` gRPC call. After that, Katib
creates new Trials according to the sets and observe the outcome. When the Trials are finished,
Katib tells the metrics of the finished Trials to the algorithm, and ask another new sets.

### Create a new Algorithm Service

The new algorithm needs to implement Suggestion service defined in [api.proto](https://github.com/kubeflow/katib/blob/4a2db414d85f29f17bc8ec6ff3462beef29585da/pkg/apis/manager/v1beta1/api.proto).

One sample algorithm looks like:

```python
from pkg.apis.manager.v1beta1.python import api_pb2
from pkg.apis.manager.v1beta1.python import api_pb2_grpc
from pkg.suggestion.v1beta1.internal.search_space import HyperParameter, HyperParameterSearchSpace
from pkg.suggestion.v1beta1.internal.trial import Trial, Assignment
from pkg.suggestion.v1beta1.hyperopt.base_service import BaseHyperoptService
from pkg.suggestion.v1beta1.internal.base_health_service import HealthServicer


# Inherit SuggestionServicer and implement GetSuggestions.
class HyperoptService(
        api_pb2_grpc.SuggestionServicer, HealthServicer):
    def ValidateAlgorithmSettings(self, request, context):
        # Optional, it is used to validate algorithm settings defined by users.
        pass
    def GetSuggestions(self, request, context):
        # Convert the Experiment in GRPC request to the search space.
        # search_space example:
        #   HyperParameterSearchSpace(
        #       goal: MAXIMIZE,
        #       params: [HyperParameter(name: param-1, type: INTEGER, min: 1, max: 5, step: 0),
        #                HyperParameter(name: param-2, type: CATEGORICAL, list: cat1, cat2, cat3),
        #                HyperParameter(name: param-3, type: DISCRETE, list: 3, 2, 6),
        #                HyperParameter(name: param-4, type: DOUBLE, min: 1, max: 5, step: )]
        #   )
        search_space = HyperParameterSearchSpace.convert(request.experiment)
        # Convert the Trials in GRPC request to the Trials in algorithm side.
        # Trials example:
        #   [Trial(
        #       assignment: [Assignment(name=param-1, value=2),
        #                    Assignment(name=param-2, value=cat1),
        #                    Assignment(name=param-3, value=2),
        #                    Assignment(name=param-4, value=3.44)],
        #       target_metric: Metric(name="metric-2" value="5643"),
        #       additional_metrics: [Metric(name=metric-1, value=435),
        #                            Metric(name=metric-3, value=5643)],
        #   Trial(
        #       assignment: [Assignment(name=param-1, value=3),
        #                    Assignment(name=param-2, value=cat2),
        #                    Assignment(name=param-3, value=6),
        #                    Assignment(name=param-4, value=4.44)],
        #       target_metric: Metric(name="metric-2" value="3242"),
        #       additional_metrics: [Metric(name=metric=1, value=123),
        #                            Metric(name=metric-3, value=543)],
        trials = Trial.convert(request.trials)
        #--------------------------------------------------------------
        # Your code here
        # Implement the logic to generate new assignments for the given current request number.
        # For example, if request.current_request_number is 2, you should return:
        # [
        #   [Assignment(name=param-1, value=3),
        #    Assignment(name=param-2, value=cat2),
        #    Assignment(name=param-3, value=3),
        #    Assignment(name=param-4, value=3.22)
        #   ],
        #   [Assignment(name=param-1, value=4),
        #    Assignment(name=param-2, value=cat4),
        #    Assignment(name=param-3, value=2),
        #    Assignment(name=param-4, value=4.32)
        #   ],
        # ]
        list_of_assignments = your_logic(search_space, trials, request.current_request_number)
        #--------------------------------------------------------------
        # Convert list_of_assignments to
        return api_pb2.GetSuggestionsReply(
            trials=Assignment.generate(list_of_assignments)
        )
```

### Build Docker Image for Algorithm Service

You should build Docker image for your Algorithm service, for that add a new Docker image under
`cmd/suggestion`, for example: [cmd/suggestion/hyperopt](https://github.com/kubeflow/katib/tree/6f372f68089c0a01d2c06e98489557a88e5a7183/cmd/suggestion/hyperopt/v1beta1).
The new gRPC server should serve in port **6789**.

After that you can build Docker image from your algorithm:

```shell
docker build . -f cmd/suggestion/<PATH_TO_DOCKER> -t <DOCKER_IMAGE>
```

### Update the Katib Config with

Update the [Katib config](/docs/components/katib/user-guides/katib-config/#suggestions-parameters)
with the new algorithm entity:

```diff
  runtime:
    suggestions:
      - algorithmName: random
        image: docker.io/kubeflowkatib/suggestion-hyperopt:$(KATIB_VERSION)
      - algorithmName: tpe
        image: docker.io/kubeflowkatib/suggestion-hyperopt:$(KATIB_VERSION)
+     - algorithmName: <new-algorithm-name>
+       image: <DOCKER_IMAGE>
```

### Contribute the Algorithm to Katib

If you want to contribute the algorithm to Katib, you could add unit test and/or e2e test for it
in the CI and submit a PR.

#### Add Unit Tests for the Algorithm

Here is an example [test_hyperopt_service.py](https://github.com/kubeflow/katib/blob/6f372f68089c0a01d2c06e98489557a88e5a7183/test/unit/v1beta1/suggestion/test_hyperopt_service.py):

```python
import grpc
import grpc_testing
import unittest

from pkg.apis.manager.v1beta1.python import api_pb2_grpc
from pkg.apis.manager.v1beta1.python import api_pb2

from pkg.suggestion.v1beta1.hyperopt.service import HyperoptService

class TestHyperopt(unittest.TestCase):
    def setUp(self):
        servicers = {
            api_pb2.DESCRIPTOR.services_by_name['Suggestion']: HyperoptService()
        }

        self.test_server = grpc_testing.server_from_dictionary(
            servicers, grpc_testing.strict_real_time())


if __name__ == '__main__':
    unittest.main()
```

You can setup the gRPC server using `grpc_testing`, then define your own test cases.



================================================
File: content/en/docs/components/katib/user-guides/hp-tuning/configure-experiment.md
================================================
+++
title = "How to Configure Experiment"
description = "Katib Experiment specification for hyperparameter tuning"
weight = 10
+++

This guide describes how to configure Katib Experiment for hyperparameter (HP) tuning.

## Create Image for Training Code

If you don't use `tune` API from Katib Python SDK, you must package your training code in a Docker
container image and make the image available in a registry. Check the
[Docker documentation](https://docs.docker.com/develop/develop-images/baseimages/) and the
[Kubernetes documentation](https://kubernetes.io/docs/concepts/containers/images/) to learn about it.

## Configuring the Experiment

You can configure your HP tuning job in Katib Experiment YAML file. The YAML file defines the range of
potential values (the search space) for the HPs that you want to optimize, the objective metric
to use when determining optimal values, the search algorithm to use during optimization,
and other configurations.

As a reference, you can use the YAML file of the
[random search algorithm example](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/hp-tuning/random.yaml).

The list below describes the fields in the YAML file for an Experiment.

- **objective**: The metric that you want to optimize in your hyperparameter tuning job. You should
  specify whether you want Katib to maximize or minimize the metric.

  Katib uses the `objectiveMetricName` and `additionalMetricNames` to monitor how the
  hyperparameters perform with the model. Katib records the value of the best `objectiveMetricName`
  metric (maximized or minimized based on `type`) and the corresponding hyperparameter set
  in the Experiment's `.status.currentOptimalTrial.parameterAssignments`. If the `objectiveMetricName`
  metric for a set of hyperparameters reaches the `goal`, Katib stops trying more hyperparameter combinations.

  You can run the Experiment without specifying the `goal`. In that case, Katib
  runs the Experiment until the corresponding successful Trials reach `maxTrialCount`.
  `maxTrialCount` parameter is described below.

  The default way to calculate the Experiment's objective is:

  - When the objective `type` is `maximize`, Katib compares all maximum metric values.

  - When the objective `type` is `minimize`, Katib compares all minimum metric values.

  To change this default setting, define `metricStrategies` with various rules
  (`min`, `max` or `latest`) to extract values for each metric from the Experiment's
  `objectiveMetricName` and `additionalMetricNames`. The Experiment's objective value is calculated in
  accordance with the selected strategy.

  For example, you can set the parameters in your Experiment as follows:

  ```yaml
  . . .
  objectiveMetricName: accuracy
  type: maximize
  metricStrategies:
    - name: accuracy
      value: latest
  . . .
  ```

  In that case, Katib controller searches for the best maximum from the all latest reported
  `accuracy` metrics for each Trial. Check the
  [metrics strategies example](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/metrics-collector/metrics-collection-strategy.yaml).

  The default strategy type for each metric is equal to the objective `type`.

- **algorithm**: The search algorithm that you want Katib to use to find the best HPs.
  Examples include random search, grid search, Bayesian optimization, and more.
  Check the [HP tuning algorithms](/docs/components/katib/user-guides/hp-tuning/configure-algorithm/)
  to learn how to configure them.

- **parallelTrialCount**: The maximum number of HP sets that Katib
  should train in parallel. The default value is 3.

- **maxTrialCount**: The maximum number of Trials to run. This is equivalent to the number o
  HP sets that Katib should generate to test the model. If the `maxTrialCount` value is
  **omitted**, your Experiment will be running until the objective goal is reached or the Experiment
  reaches a maximum number of failed Trials.

- **maxFailedTrialCount**: The maximum number of Trials allowed to fail. This is equivalent to the
  number of failed HP sets that Katib should test. Katib recognizes Trials with a status of
  `Failed` or `MetricsUnavailable` as `Failed` Trials, and if the number of failed Trials reaches
  `maxFailedTrialCount`, Katib stops the Experiment with a status of `Failed`.

- **parameters**: The range of the HPs that you want to tune for your machine learning (ML) model.
  The parameters define the _search space_, also known as the _feasible set_ or the _solution space_.
  In this section of the spec, you define the name, distribution, and type of HP: `int`, `double`, or
  `categorical`. Katib generates HP combinations in the range based on the HP tuning algorithm that
  you specify.

- **trialTemplate**: The template that defines the Trial. You have to package your ML training code
  into a Docker image, as described
  [above](#create-image-for-training-code). `trialTemplate.trialSpec` is your
  [unstructured](https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1/unstructured)
  template with model parameters, which are substituted from `trialTemplate.trialParameters`.
  For example, your training container can receive HPs as command-line arguments or as environment
  variables. You have to set the name of your training container in `trialTemplate.primaryContainerName`.

  Follow the [Trial template guide](/docs/components/katib/user-guides/trial-template/) to learn how
  to use any Kubernetes resource as Katib Trial and how to use ConfigMap for Trial templates.

### Running Katib Experiment with Istio

Katib Experiment from [this directory](https://github.com/kubeflow/katib/tree/ea46a7f2b73b2d316b6b7619f99eb440ede1909b/examples/v1beta1)
doesn't work with [Istio sidecar injection](https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection)
since Trials require access to the internet to download datasets. If you deploy Katib with
Kubeflow platform, you can disable Istio sidecar injection. Specify this annotation: `sidecar.istio.io/inject: "false"`
in your Experiment Trial's template to disable Istio sidecar injection:

```yaml
trialSpec:
  apiVersion: batch/v1
  kind: Job
  spec:
    template:
      metadata:
        annotations:
          "sidecar.istio.io/inject": "false"
```

If you use `PyTorchJob` or other Training Operator jobs in your Trial template, check
[here](/docs/components/trainer/legacy-v1/user-guides/tensorflow/#what-is-tfjob) how to set the annotation.

## Running the Experiment

You can create hyperparameter tuning Experiment using the
[YAML file for the random search example](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/hp-tuning/random.yaml).

The Experiment's Trials use PyTorch model to train an image classification model for the
FashionMNIST dataset. You can check [the training container source code](https://github.com/kubeflow/katib/tree/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/trial-images/pytorch-mnist). **Note:** Since this training container downloads FashionMNIST
dataset, you [need to disable Istio sidecar injection](#running-katib-experiment-with-istio)
if you deploy Katib with Kubeflow Platform.

Deploy the Experiment:

```shell
kubectl create -f https://raw.githubusercontent.com/kubeflow/katib/master/examples/v1beta1/hp-tuning/random.yaml
```

This example randomly generates the following hyperparameters:

- `--lr`: Learning rate. Type: double.
- `--momentum`: Momentum for PyTorch optimizer. Type: double.

You can check the results of your Experiment in the `status` specification.

```yaml
$ kubectl -n kubeflow get experiment random -o yaml

apiVersion: kubeflow.org/v1beta1
kind: Experiment
metadata:
  ...
  name: random
  namespace: kubeflow
  ...
spec:
  ...
status:
  currentOptimalTrial:
    bestTrialName: random-hpsrsdqp
    observation:
      metrics:
        - latest: "0.11513"
          max: "0.53415"
          min: "0.01235"
          name: loss
    parameterAssignments:
      - name: lr
        value: "0.024736875661534784"
      - name: momentum
        value: "0.6612351235123"
  runningTrialList:
    - random-2dwxbwcg
    - random-6jd8hmnd
    - random-7gks8bmf
  startTime: "2021-10-07T21:12:06Z"
  succeededTrialList:
    - random-xhpcrt2p
    - random-hpsrsdqp
    - random-kddxqqg9
    - random-4lkr5cjp
  trials: 7
  trialsRunning: 3
  trialsSucceeded: 4
```

Check information about the best Trial in `status.currentOptimalTrial` parameter. In addition,
`status` shows the Experiment's Trials with their current status. For example, run this command
to get information for the most optimal Trial:

```json
$ kubectl get experiment random -n kubeflow -o=jsonpath='{.status.currentOptimalTrial}'

{
  "bestTrialName": "random-hpsrsdqp",
  "observation": {
    "metrics": [
      {
        "latest": "0.11513",
        "max": "0.53415",
        "min": "0.01235",
        "name": "loss"
      }
    ]
  },
  "parameterAssignments": [
    {
      "name": "lr",
      "value": "0.024736875661534784",
    },
    {
      "name": "momentum",
      "value": "0.6612351235123"
    }
  ]
}
```

## Next steps

- Learn about [HP tuning algorithms](/docs/components/katib/user-guides/hp-tuning/configure-algorithm).

- How to configure [Katib Trial template](/docs/components/katib/user-guides/trial-template).

- Boost your hyperparameter tuning Experiment with
  the [early stopping guide](/docs/components/katib/user-guides/early-stopping/).



================================================
File: content/en/docs/components/katib/user-guides/nas/_index.md
================================================
+++
title = "Neural Architecture Search"
description = "User guides to run Neural Architecture Search Experiments"
weight = 20
+++

{{% alert title="Alpha version" color="warning" %}}
Neural architecture search is currently in <b>alpha</b> with limited support. The Kubeflow team is
interested in any feedback you may have, in particular with regards to usability
of the feature. You can log issues and comments in
the [Katib issue tracker](https://github.com/kubeflow/katib/issues).
{{% /alert %}}



================================================
File: content/en/docs/components/katib/user-guides/nas/configure-algorithm.md
================================================
+++
title = "How to Configure Algorithms"
description = "List of supported algorithms for neural architecture search"
weight = 20
+++

This page describes neural architecture search (NAS) algorithms that Katib supports and how to configure
them.

## NAS Algorithms

Katib currently supports several search algorithms for NAS:

- [Efficient NAS](/docs/components/katib/user-guides/nas/configure-algorithm/#efficient-neural-architecture-search-enas)

- [Differentiable Architecture Search](/docs/components/katib/user-guides/nas/configure-algorithm/#differentiable-architecture-search-darts)

### Efficient Neural Architecture Search (ENAS)

The algorithm name in Katib is `enas`.

The ENAS example —
[`enas-gpu.yaml`](https://github.com/kubeflow/katib/blob/master/examples/v1beta1/nas/enas-gpu.yaml) —
which attempts to show all possible operations. Due to the large search
space, the example is not likely to generate a good result.

Katib supports the following algorithm settings for ENAS:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting Name</th>
        <th>Type</th>
        <th>Default value</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>controller_hidden_size</td>
        <td>int</td>
        <td>64</td>
        <td>RL controller lstm hidden size. Value must be >= 1.</td>
      </tr>
      <tr>
        <td>controller_temperature</td>
        <td>float</td>
        <td>5.0</td>
        <td>RL controller temperature for the sampling logits. Value must be > 0.
          Set value to "None" to disable it in the controller.</td>
      </tr>
      <tr>
        <td>controller_tanh_const</td>
        <td>float</td>
        <td>2.25</td>
        <td>RL controller tanh constant to prevent premature convergence.
          Value must be > 0. Set value to "None" to disable it in the controller.</td>
      </tr>
      <tr>
        <td>controller_entropy_weight</td>
        <td>float</td>
        <td>1e-5</td>
        <td>RL controller weight for entropy applying to reward. Value must be > 0.
          Set value to "None" to disable it in the controller.</td>
      </tr>
      <tr>
        <td>controller_baseline_decay</td>
        <td>float</td>
        <td>0.999</td>
        <td>RL controller baseline factor. Value must be > 0 and <= 1.</td>
      </tr>
      <tr>
        <td>controller_learning_rate</td>
        <td>float</td>
        <td>5e-5</td>
        <td>RL controller learning rate for Adam optimizer. Value must be > 0 and <= 1.</td>
      </tr>
      <tr>
        <td>controller_skip_target</td>
        <td>float</td>
        <td>0.4</td>
        <td>RL controller probability, which represents the prior belief of a
          skip connection being formed. Value must be > 0 and <= 1.</td>
      </tr>
      <tr>
        <td>controller_skip_weight</td>
        <td>float</td>
        <td>0.8</td>
        <td>RL controller weight of skip penalty loss. Value must be > 0.
          Set value to "None" to disable it in the controller.</td>
      </tr>
      <tr>
        <td>controller_train_steps</td>
        <td>int</td>
        <td>50</td>
        <td>Number of RL controller training steps after each candidate runs.
          Value must be >= 1.</td>
      </tr>
      <tr>
        <td>controller_log_every_steps</td>
        <td>int</td>
        <td>10</td>
        <td>Number of RL controller training steps before logging it. Value must be >= 1.</td>
      </tr>
    </tbody>
  </table>
</div>

### Differentiable Architecture Search (DARTS)

The algorithm name in Katib is `darts`.

The DARTS example —
[`darts-gpu.yaml`](https://github.com/kubeflow/katib/blob/master/examples/v1beta1/nas/darts-gpu.yaml).

Katib supports the following algorithm settings for DARTS:

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Setting Name</th>
        <th>Type</th>
        <th>Default value</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>num_epochs</td>
        <td>int</td>
        <td>50</td>
        <td>Number of epochs to train model</td>
      </tr>
      <tr>
        <td>w_lr</td>
        <td>float</td>
        <td>0.025</td>
        <td>Initial learning rate for training model weights.
          This learning rate annealed down to <code>w_lr_min</code>
          following a cosine schedule without restart.</td>
      </tr>
      <tr>
        <td>w_lr_min</td>
        <td>float</td>
        <td>0.001</td>
        <td>Minimum learning rate for training model weights.</td>
      </tr>
      <tr>
        <td>w_momentum</td>
        <td>float</td>
        <td>0.9</td>
        <td>Momentum for training training model weights.</td>
      </tr>
      <tr>
        <td>w_weight_decay</td>
        <td>float</td>
        <td>3e-4</td>
        <td>Training model weight decay.</td>
      </tr>
      <tr>
        <td>w_grad_clip</td>
        <td>float</td>
        <td>5.0</td>
        <td>Max norm value for clipping gradient norm of training model weights.</td>
      </tr>
      <tr>
        <td>alpha_lr</td>
        <td>float</td>
        <td>3e-4</td>
        <td>Initial learning rate for alphas weights.</td>
      </tr>
      <tr>
        <td>alpha_weight_decay</td>
        <td>float</td>
        <td>1e-3</td>
        <td>Alphas weight decay.</td>
      </tr>
      <tr>
        <td>batch_size</td>
        <td>int</td>
        <td>128</td>
        <td>Batch size for dataset.</td>
      </tr>
      <tr>
        <td>num_workers</td>
        <td>int</td>
        <td>4</td>
        <td>Number of subprocesses to download the dataset.</td>
      </tr>
      <tr>
        <td>init_channels</td>
        <td>int</td>
        <td>16</td>
        <td>Initial number of channels.</td>
      </tr>
      <tr>
        <td>print_step</td>
        <td>int</td>
        <td>50</td>
        <td>Number of training or validation steps before logging it.</td>
      </tr>
      <tr>
        <td>num_nodes</td>
        <td>int</td>
        <td>4</td>
        <td>Number of DARTS nodes.</td>
      </tr>
      <tr>
        <td>stem_multiplier</td>
        <td>int</td>
        <td>3</td>
        <td>Multiplier for initial channels. It is used in the first stem cell.</td>
      </tr>
    </tbody>
  </table>
</div>

## Next steps

- Learn how [NAS algorithms work in Katib](/docs/components/katib/reference/nas-algorithms).



================================================
File: content/en/docs/components/katib/user-guides/nas/configure-experiment.md
================================================
+++
title = "How to Configure Experiment"
description = "Katib Experiment specification for neural architecture search"
weight = 10
+++

{{% alert title="Alpha version" color="warning" %}}
Neural architecture search is currently in <b>alpha</b> with limited support. The Kubeflow team is
interested in any feedback you may have, in particular with regards to usability
of the feature. You can log issues and comments in
the [Katib issue tracker](https://github.com/kubeflow/katib/issues).
{{% /alert %}}

This guide describes how to configure Katib Experiment for neural architecture search (NAS).

Before reading this guide, please follow
[the guide to configure Experiment](/docs/components/katib/user-guides/hp-tuning/configure-experiment)
for hyperparameter (HP) tuning to understand the common Experiment parameters for NAS.

## Configuring the Experiment

You can configure your NAS in Katib Experiment YAML file.

The YAML file defines the range of potential network architectures, configuration for neural network graph,
the objective metric to use when determining optimal values, the search algorithm to use during architecture search.

As a reference, you can use the YAML file of the
[efficient neural architecture search (ENAS)](https://github.com/kubeflow/katib/blob/fc858d15dd41ff69166a2020efa200199063f9ba/examples/v1beta1/nas/enas-cpu.yaml).

The list below describes the NAS-specific parameters in the YAML file for an Experiment.

- **nasConfig**: The configuration for NAS. You can specify the
  configurations of the neural network design that you want to optimize, including the number of
  layers in the network, the types of operations, and more.

  - **graphConfig**: The graph config that defines structure for a directed acyclic graph of the
    neural network. You can specify the number of layers, `input_sizes` for the input layer and
    `output_sizes` for the output layer.

  - **operations**: The range of operations that you want to tune for your ML model. For each neural
    network layer the NAS algorithm selects one of the operations to build a neural network.
    Each operation contains sets of **parameters** similar to HP tuning Experiment.

You can find all NAS examples [here](https://github.com/kubeflow/katib/tree/master/examples/v1beta1/nas).

## Next steps

- Learn about [NAS algorithms](/docs/components/katib/user-guides/nas/configure-algorithm).



================================================
File: content/en/docs/components/model-registry/OWNERS
================================================
approvers:
  - ederign
  - rareddy
  - tarilabs
  - Tomcli



================================================
File: content/en/docs/components/model-registry/_index.md
================================================
+++
title = "Model Registry"
description = "Documentation for Kubeflow Model Registry"
weight = 70
+++



================================================
File: content/en/docs/components/model-registry/getting-started.md
================================================
+++
title = "Getting started"
description = "Getting started with Model Registry using examples"
weight = 20
+++

This guide shows how to get started with Model Registry and run a few examples using the
command line or Python clients.

For an overview of the logical model of model registry, check the [Model Registry logical model](https://github.com/kubeflow/model-registry/blob/main/docs/logical_model.md).
The logical model is exposed via the Model Registry [REST API](reference/rest-api).

## Prerequisites

To follow along the examples in this guide, you will need a Kubeflow installation and the Model Registry installed:

- [Kubeflow](/docs/started/installing-kubeflow/)
- [Model Registry](/docs/components/model-registry/installation/)
- Python >= 3.9

<!-- TODO: list python client as a requirement -->

## Setup

To use Model Registry on a notebook you should first install the Python client:

```raw
!pip install model-registry=="{{% model-registry/latest-version %}}"
!pip install kserve=="0.13"
```

Note that depending on your environment there might be conflicting dependency versions for packages that depend on
`pydantic`.

You can get a client pointing to your deployed Model Registry from the previous steps:

```python
from model_registry import ModelRegistry

registry = ModelRegistry(
    server_address="http://model-registry-service.kubeflow.svc.cluster.local",
    port=8080,
    author="your name",
    is_secure=False
)
```

<!-- TODO: missing link -->

For more information on client setup and capabilities, refer to the Model Registry Python client documentation.

## Register metadata

You can use the `register_model` method to index a model's artifacts and its metadata, for instance:

```python
rm = registry.register_model(
    "iris",
    "gs://kfserving-examples/models/sklearn/1.0/model",
    model_format_name="sklearn",
    model_format_version="1",
    version="v1",
    description="Iris scikit-learn model",
    metadata={
        "accuracy": 3.14,
        "license": "BSD 3-Clause License",
    }
)
```

## Retrieving metadata

Continuing on the previous example, you can use the following methods to retrieve the metadata associated with a given Model Artifact:

```python
model = registry.get_registered_model("iris")
print("Registered Model:", model, "with ID", model.id)

version = registry.get_model_version("iris", "v1")
print("Model Version:", version, "with ID", version.id)

art = registry.get_model_artifact("iris", "v1")
print("Model Artifact:", art, "with ID", art.id)
```

These can be used to create a KServe inference endpoint.

## Deploy an inference endpoint

Normally you would need to provide your deployment metadata manually resulting in an error-prone process, especially
when such data has to be gathered from several sources.
Using Model Registry ensures simplified access to accurate metadata, and enables you to automate deployment based on the Model Registry values, as also shown in the examples below.

Note: the provided examples uses the Model Registry Python client and KServe Python SDK. You can analogously make use of the Model Registry REST APIs, and your own Add-on SDK as needed.

### Using Model Registry metadata

You can use the retrieved metadata from the previous step with the KServe Python SDK to create an inference endpoint, for example:

```python
from kubernetes import client
import kserve

isvc = kserve.V1beta1InferenceService(
    api_version=kserve.constants.KSERVE_GROUP + "/v1beta1",
    kind=kserve.constants.KSERVE_KIND,
    metadata=client.V1ObjectMeta(
        name="iris-model",
        namespace=kserve.utils.get_default_target_namespace(),
        labels={
            "modelregistry/registered-model-id": model.id,
            "modelregistry/model-version-id": version.id,
        },
    ),
    spec=kserve.V1beta1InferenceServiceSpec(
        predictor=kserve.V1beta1PredictorSpec(
            model=kserve.V1beta1ModelSpec(
                storage_uri=art.uri,
                model_format=kserve.V1beta1ModelFormat(
                    name=art.model_format_name, version=art.model_format_version
                ),
            )
        )
    ),
)
ks_client = kserve.KServeClient()
ks_client.create(isvc)
```

An inference endpoint is now created, using the artifact metadata retrieved from the Model Registry (previous step),
specifying the serving runtime to be used to serve the model, and references to the original entities in Model Registry.

### Using Model Registry Custom Storage Initializer

The Model Registry Custom Storage Initializer (CSI) is a custom implementation of the KServe Storage Initializer that allows you to use Model Registry metadata to download and deploy models (see [Installation instructions](installation.md)). You can create an InferenceService that references the model and version in the Model Registry:

```python
from kubernetes import client
import kserve

isvc = kserve.V1beta1InferenceService(
    api_version=kserve.constants.KSERVE_GROUP + "/v1beta1",
    kind=kserve.constants.KSERVE_KIND,
    metadata=client.V1ObjectMeta(
        name="iris-model",
        namespace=kserve.utils.get_default_target_namespace(),
        labels={
            "modelregistry/registered-model-id": model.id,
            "modelregistry/model-version-id": version.id,
        },
    ),
    spec=kserve.V1beta1InferenceServiceSpec(
        predictor=kserve.V1beta1PredictorSpec(
            model=kserve.V1beta1ModelSpec(
                storage_uri="model-registry://iris/v1", # The protocol is model-registry://{modelName}/{modelVersion}
                model_format=kserve.V1beta1ModelFormat(
                    name=art.model_format_name, version=art.model_format_version
                ),
            )
        )
    ),
)
ks_client = kserve.KServeClient()
ks_client.create(isvc)
```

The InferenceService is now created, the CSI retrieves the latest artifact data associated with the model version from the Model Registry, and then downloads the model from its URI.

## Using the Model Registry UI

In addition to the command line and Python clients, you can also use the Model Registry UI to manage your models. The UI provides an intuitive interface for registering, updating, and querying models and their metadata.

   <img src="/docs/components/model-registry/images/model-registry-ui-main.png"
   alt="Model Registry Overview"
   class="mt-3 mb-3">

To access the Model Registry UI, navigate to the Kubeflow central dashboard and select the Model Registry component. From there, you can perform various actions such as:

- Registering new models
- Viewing registered models and their versions
- Updating model metadata
- Deleting models

For detailed instructions on using the Model Registry UI, refer to the [Model Registry UI documentation](https://github.com/kubeflow/model-registry/blob/main/clients/ui/README.md).

## Next steps

- Get involved:
  - [Model Registry working group](https://www.kubeflow.org/docs/about/community/#kubeflow-community-meetings)
  - [GitHub repository](https://github.com/kubeflow/model-registry)
- Share your feedback:
  - [File an issue](https://github.com/kubeflow/model-registry/issues)



================================================
File: content/en/docs/components/model-registry/installation.md
================================================
+++
title = "Installation"
description = "How to set up Model Registry"
weight = 30
+++

This section details how to set up and configure Model Registry on your Kubernetes cluster with Kubeflow.

## Prerequisites

These are the minimal requirements to install Model Registry:

- Kubernetes >= 1.27
- Kustomize >= 5.0.3 ([see more](https://github.com/kubeflow/manifests/issues/2388))

<a id="model-registry-install"></a>

## Installing Model Registry

Kubeflow Model registry may be installed as part of a Kubeflow Platform, or as a standalone component.
The best option for you will depend on your specific requirements.

### Installing on Kubeflow Platform

Kubeflow Model Registry is available as an opt-in alpha component in Kubeflow Platform 1.9+, see [Installing Kubeflow](/docs/started/installing-kubeflow/) to learn more about deploying the Kubeflow Platform.

> **Note:** If you are planning to use the Kubeflow UI, please see the section below on installing Model Registry on a Kubeflow Profile. This will ensure proper integration with the Kubeflow Platform and Dashboard.

If you have deployed the Kubeflow manifests, you may follow [these instructions](https://github.com/kubeflow/manifests/tree/master/apps/model-registry/upstream#readme) to deploy Model Registry; please raise any feedback on [`kubeflow/model-registry`](https://github.com/kubeflow/model-registry/issues).

To deploy Model Registry UI, you just need to follow the [UI section](https://github.com/kubeflow/manifests/tree/master/apps/model-registry/upstream#ui-installation) under the Model Registry installation guide targeting a integrated install.

#### Installing on a Kubeflow Profile

Kubeflow Central Dashboard uses [Profiles](/docs/components/central-dash/profiles/) to handle user namespaces and permissions. By default, the manifests deploy the Model Registry in the `kubeflow` namespace, to install a compatible version of Model Registry for Kubeflow, you should first head into the [istio overlay](https://github.com/kubeflow/manifests/tree/master/apps/model-registry/upstream/options/istio) and run the following commands:

```shell
kustomize set namespace <your-profile>
kubectl apply -k .
```

Then head into the [db overlay](https://github.com/kubeflow/manifests/tree/master/apps/model-registry/upstream/overlays/db) and run the following commands:

```shell
kustomize set namespace <your-profile>
kubectl apply -k .
```

### Standalone installation

If you want to install Model Registry separately from Kubeflow, or to get a later version
of Model Registry, you can install the Model Registry manifests directly from the [Model Registry repository](https://github.com/kubeflow/model-registry).

By default, the manifests deploy the Model Registry in the `kubeflow` namespace;
you must ensure the `kubeflow` namespace is available (for example: `kubectl create namespace kubeflow`)
or modify [the kustomization file](https://github.com/kubeflow/model-registry/blob/v{{% model-registry/latest-version %}}/manifests/kustomize/overlays/db/kustomization.yaml#L3) to your desired namespace.

See the list of available versions on the [GitHub releases](https://github.com/kubeflow/model-registry/releases) of the `kubeflow/model-registry` repository. To install a specific release of the Model Registry, modify the following commands with the desired `ref=<GIT_TAG>`.

Run the following command to install the `v{{% model-registry/latest-version %}}` release of Model Registry:

```shell
MODEL_REGISTRY_VERSION={{% model-registry/latest-version %}}
kubectl apply -k "https://github.com/kubeflow/model-registry/manifests/kustomize/overlays/db?ref=v${MODEL_REGISTRY_VERSION}"
```

If your Kubernetes cluster uses Istio, you MUST apply the Istio-compatibility manifests (e.g. when using a full Kubeflow Platform). However, these are NOT required for non-Istio clusters.

```shell
MODEL_REGISTRY_VERSION={{% model-registry/latest-version %}}
kubectl apply -k "https://github.com/kubeflow/model-registry/manifests/kustomize/options/istio?ref=v${MODEL_REGISTRY_VERSION}"
```

If you want Kserve to be able to support `model-registry://` URI formats, you must apply the cluster-scoped `CustomStorageContainer` CR.

```shell
MODEL_REGISTRY_VERSION={{% model-registry/latest-version %}}
kubectl apply -k "https://github.com/kubeflow/model-registry/manifests/kustomize/options/csi?ref=v${MODEL_REGISTRY_VERSION}"
```

## Check Model Registry setup

You can check the status of the Model Registry deployment with your Kubernetes tooling, or for example with:

```shell
kubectl wait --for=condition=available -n kubeflow deployment/model-registry-deployment --timeout=1m
kubectl logs -n kubeflow deployment/model-registry-deployment
```

Optionally, you can also manually forward the REST API container port of Model Registry and interact with the [REST API](reference/rest-api),
for example with:
```shell
kubectl port-forward svc/model-registry-service -n kubeflow 8081:8080
# in another terminal:
curl -X 'GET' \
  'http://localhost:8081/api/model_registry/v1alpha3/registered_models?pageSize=100&orderBy=ID&sortOrder=DESC' \
  -H 'accept: application/json' | jq
```

If you are not receiving a `2xx` response, it might be the case you are trying to consume a different version (`v1alphaX`) of the REST API than intended.

### Perform the check from within a Notebook

To check the connection to the Model Registry from a Notebook instead, start a Terminal from the Notebook environment, then you can dry-run the connection with the following command:

```
curl model-registry-service.kubeflow.svc.cluster.local:8080/api/model_registry/v1alpha3/registered_models
```

or, alternatively, with the following command:

```
wget -nv -O- model-registry-service.kubeflow.svc.cluster.local:8080/api/model_registry/v1alpha3/registered_models
```

If the command executes without any error, you will get a JSON response from Model Registry, indicating the connection and request was successful.

You can use the same commands in a Jupyter Notebook cell by prefixing the command with `!` (e.g.: `! curl ...`).

## Next steps

- Follow the [Getting Started](getting-started.md) guide to learn how to use Model Registry.



================================================
File: content/en/docs/components/model-registry/overview.md
================================================
+++
title = "Overview"
description = "An overview for Kubeflow Model Registry"
weight = 10
+++

{{% alpha-status feedbacklink="https://github.com/kubeflow/model-registry" %}}

## What is Model Registry?

A model registry is an important component in the lifecycle of AI/ML models, an integral component for any MLOps platform and for ML workflows.

<p style="text-align: center;">
  <img src="/docs/components/model-registry/images/MLloopinnerouter.png"
    alt="Model Registry MLOps loop"
    class="mt-3 mb-3">
</p>

A model registry provides a central index for ML model developers to index and manage models, versions, and ML artifacts metadata.
It fills a gap between model experimentation and production activities.
It provides a central interface for all stakeholders in the ML lifecycle to collaborate on ML models.

<img src="/docs/components/model-registry/images/ml-lifecycle-kubeflow-modelregistry.drawio.svg"
  alt="Kubeflow Components in ML Lifecycle"
  class="mt-3 mb-3">

- **Create**: during the creation phase, the Model Registry facilitates collaboration between different teams in order to track changes, experiment with different model architectures, and maintain a history of model iterations.
- **Verify**: in the verification stage, the Model Registry supports rigorous testing and validation before progressing further, maintaining a record of performance metrics and test results for each version.
- **Package**: the Model Registry assists in organizing model artifacts and dependencies, enabling seamless integration with deployment pipelines and ensuring reproducibility across environments.
- **Release**: when releasing a model, the Model Registry manages the transition of validated versions to production-ready status, helping organization to maintain versioning conventions and facilitating approval workflows.
- **Deploy**: during deployment, the Model Registry provides information of the approved model versions and associated artifacts, ensuring consistency and traceability across deployment environments.
- **Monitor**: in the monitoring phase, the Model Registry supports ongoing performance monitoring and model drift detection by maintaining a comprehensive record of deployed models and linking to their performance metrics, facilitating proactive maintenance and retraining as needed.

DevOps, Data Scientists, and developers need to collaborate with other users in the ML workflow to get models into production.
Data scientists need an efficient way to share model versions, artifacts and metadata with other users that need access to those models as part of the MLOps workflow.

## Use Cases

This section describes Model Registry use-cases in the context of a MLOps Platform with Model Training, Experimentation, and Deployment.

A company, ACME Inc., is developing a machine-learning model for predicting customer churn. They require a centralized model registry for their MLOps platform (based on Kubeflow) for managing their ML model development lifecycle, including training, experimentation, and deployment. They want to ensure model governance, reproducibility, and efficient collaboration across data scientists and engineers.

### Personas

* **Data Scientist**: develops and evaluates different models for customer churn prediction. Tracks the performance of various model versions to compare them easily.
* **MLOps Engineer**: deploys the chosen model into production. Uses the latest model version and its metadata to configure the deployment environment.
* **Business Analyst**: Monitors the deployed model's performance and makes decisions based on its predictions. Uses model lineage and metadata to drive business outcomes.

### Use Case 1: Tracking the Training of Models

The _Data Scientist_ uses Kubeflow Notebooks to perform exploratory research and trains several types of models, with different hyperparameters and metrics. The Kubeflow Model Registry is used to track those models, in order to make comparisons and identify the best-performing model. Once the champion model is selected, the _Data Scientist_ shares the model with the team by maintaining the appropriate status flag on the registry. The _Data Scientist_ also tracks the lineage of training data sources and notebook code.

* Track models available on storage: once the model is stored, it can then be tracked in the Kubeflow Model Registry for managing its lifecycle. The Model Registry can catalog, list, index, share, record, organize this information. This allows the _Data Scientist_ to compare different versions and revert to previous versions if needed.
* Track and compare performance: View key metrics like accuracy, recall, and precision for each model version. This helps identify the best-performing model for deployment.
* Create lineage: Capture the relationships between data, code, and models. This enables the _Data Scientist_ to understand the origin of each model and reproduce specific experiments.
* Collaborate: Share models and experiment details with the _MLOps Engineer_ for deployment preparation. This ensures a seamless transition from training to production.

### Use Case 2: Experimenting with Different Model Weights to Optimize Model Accuracy

The _Data Scientist_ after identifying a base model, uses Kubeflow Pipelines, Katib, and other components to experiment model training with alternative weights, hyperparameters, and other variations to improve the model’s performance metrics; Kubeflow Model Registry can be used to track data related to experiments and runs for comparison, reproducibility and collaboration.

* Register the Base Model: Track the Base Model storage location along with hyperparameters in the Model Registry. 
* Track Experiments/Runs: With Kubeflow pipelines or using the Kubeflow Notebooks, track every variation of the hyper-parameters along with any configuration in that specific Experiment. With each run the different parameters can be tracked in the Model Registry.
* Track and compare performance: with each run view key metrics like accuracy, recall, and precision. This helps the Data Scientist identify the best-performing run/experiment for deployment.
* Reproducibility: if needed, the data tracked in Model Registry can be replayed to perform again the experiment/run to reproduce the models.
* Collaborate: Share models and experiment details with the _MLOps Engineer_ for deployment preparation. This ensures a seamless transition from training to production.

### Use Case 3: Model Deployment

The _MLOps Engineer_ uses Kubeflow Model Registry to locate the most recent version for a given model, verify it is approved for deployment, understand model format, architecture, hyperparameters, and performance metrics to configure the serving environment; once deployed, Model Registry is used to continue monitoring and track deployed models for performance and mitigate drift.

* Retrieve the latest model version: Easily access the model version approved for deployment.
* Access model metadata: Understand the model's architecture, hyperparameters, and performance metrics. This helps the MLOps engineer to configure the deployment environment and monitor performance after deployment.
* Manage serving configurations: Define how the model will be served to production applications and set up necessary resources.
* Track model deployments: Monitor the deployed model's performance and track its health over time. This allows the MLOps Engineer to identify potential issues and take corrective actions.

### Use Case 4: Monitoring and Governance

The _Business Analyst_ uses Kubeflow Model Registry to audit deployed models, monitor model performance by integrating with observability tools to track key metrics and identify when model is drifting or needs re-training; capabilities of model lineage enable identifying all related artifacts such as training which was used or the original training data.

* View model performance metrics: Links to observability tools tracking key metrics in real-time to understand how the model is performing in production.
* Identify model drift: Can be used as a reference and baseline, by integrating with other tools, to detect if the model's predictions are deviating from expected behavior.
* Access model lineage: Understand the model's origin and training details to diagnose and address performance issues.
* Audit model usage: Track who uses the model, ensuring compliance with data privacy and security regulations. Together with lineage, they provide very important capabilities in heavily regulated industries (e.g.: FSI, Healthcare, etc.) and with respect to country regulations (e.g.: GDPR, EU AI Act, etc.).

### Benefits of Model Registry:

* Improved collaboration: Facilitate communication and collaboration between Data Scientists and MLOps engineers.
* Improved experiment management: Organize and track Experiments in a centralized location for better organization and accessibility.
* Version control: Track different versions of the model with different weight configurations, allowing comparisons and revert to previous versions if needed.
* Increased efficiency: Streamline model development and deployment processes.
* Enhanced governance: Ensure model compliance with regulations and organizational policies.
* Reproducibility: Enable recreating specific experiments and model versions.
* Better decision-making: Provide data-driven insights for improving model performance and business outcomes.

### Conclusion:

By implementing a model registry, ACME Inc. can significantly enhance their MLOps platform's functionality, enabling efficient model training, experimentation, and deployment. The Model Registry empowers Data Scientists, MLOps engineers, and Business analysts to collaborate effectively and make informed decisions based on reliable data and insights.

## Next steps

- Follow the [installation guide](/docs/components/model-registry/installation/) to set up Model Registry
- Run some examples following the [getting started guide](/docs/components/model-registry/getting-started/)




================================================
File: content/en/docs/components/model-registry/reference/_index.md
================================================
+++
title = "Reference"
description = "Reference docs for Kubeflow Model Registry"
weight = 100
+++



================================================
File: content/en/docs/components/model-registry/reference/architecture.md
================================================
+++
title = "Architecture"
description = "Reference documentation for the Kubeflow Model Registry"
weight = 100
+++

![Model Registry High Level Architecture](/docs/components/model-registry/reference/images/model-registry-overview.jpg)

{{% alert title="Note" color="warning" %}}
The Model Registry is a passive repository for metadata and is not meant to be a Control Plane. It does not perform any active orchestration or expose APIs to perform actions on underlying Kubernetes components.
{{% /alert %}}


Kubeflow Model Registry makes use of the Google community project [ML-Metadata](https://github.com/google/ml-metadata) as one of its core component. ML-Metadata provides a very extensible schema that is generic, similar to a key-value store, but also allows for the creation of logical schemas that can be queried as if they were physical schemas. Those can be manipulated using their bindings in the Python library. This model is extended to provide the metadata management service capabilities for Model Registry.

The Model Registry uses the ml-metadata project’s C++ server as-is to handle the storing of the metadata, while domain-specific Model Registry features are added as extensions (microservices). As part of these extensions, Model Registry provides:
- Python/Go extensions to support the Model Registry interaction
- an OpenAPI interface to expose the Model Registry API to the clients

## Components
- *[MLMD C++ Server](https://github.com/google/ml-metadata)*
  
  This is the metadata server from Google's ml-metadata project.  This component is hosted to communicate with a backend relational database that stores the actual metadata about the models. This server exposes a “gRPC” interface for its clients to communicate with. This server provides a very flexible schema model, where using this model one can define logical data models to fit the needs of different MLOps operations, for example, metadata during the training and experimentation, metadata about metrics or model versioning, etc. 

- *[OpenAPI/REST Server](https://github.com/kubeflow/model-registry)*
  
  This component exposes a higher-level REST API of the Model Registry. In contrast, the MLMD server exposes a lower level generic API over gRPC, whereas this REST server exposes a higher level API that is much closer to the domain model of Model Registry, like:
    - Register a Model
    - Version a Model
    - Get a catalog of models
    - Manage the deployment statutes of a model
      
  The REST API server converts its requests into one or more underlying gRPC requests on the MLMD Server.

- *[CLI (Python client, SDK)](https://github.com/kubeflow/model-registry/tree/main/clients/python)*
  
  CLI is also called MR Python client/SDK, a command line tool for interacting with Model Registry. This tool can be used by a user to execute operations such as retrieving the registered models, get model’s deployment status, model’s version etc. 

The model registry provides logical mappings from the high level [logical model](https://github.com/kubeflow/model-registry/blob/main/docs/logical_model.md) available through the OpenAPI/REST Server, to the underlying ml-metadata entities.

## See also

- Model Registry [project documentation](https://github.com/kubeflow/model-registry?tab=readme-ov-file#pre-requisites).


================================================
File: content/en/docs/components/model-registry/reference/python-client.md
================================================
+++
title = "Python Client"
description = "Reference documentation for the Kubeflow Model Registry Python Client"
weight = 200
+++

Please see the Model Registry Python Client [reference documentation](https://model-registry.readthedocs.io/en/latest/) for more information.


================================================
File: content/en/docs/components/model-registry/reference/rest-api.md
================================================
+++
title = "REST API (v1alpha3)"
description = "API Reference for Kubeflow Model Registry API - v1alpha3"
weight = 300
+++

This document describes the API specification for the `v1alpha3` Kubeflow Model Registry REST API.

## About the REST API

In most deployments of the [Kubeflow Platform](/docs/started/installing-kubeflow/#kubeflow-platform), the Kubeflow Model Registry REST API is available under the `/api/model_registry/` HTTP path.

For example, if you host Kubeflow at `https://kubeflow.example.com`, the API will be available at `https://kubeflow.example.com/api/model_registry/`.

{{% alert title="Tip" color="info" %}}
We recommend using the [Model Registry Python Client](python-client.md) as it provides a more user-friendly interface.
{{% /alert %}}

### Authentication

How requests are authenticated and authorized will depend on the distribution you are using.
Typically, you will need to provide a token or cookie in the request headers.

Please refer to the documentation of your [Kubeflow distribution](/docs/started/installing-kubeflow/#kubeflow-platform) for more information.

### Example Usage

To use the API, you will need to send HTTP requests to the appropriate endpoints.

For example, to list all Artifact entities, send a `GET` request to the following URL:

```
https://kubeflow.example.com/api/model_registry/v1alpha3/artifacts?pageSize=100&orderBy=ID
```

## Swagger UI

The following [Swagger UI](https://github.com/swagger-api/swagger-ui) is automatically generated from the [`{{% model-registry/latest-version %}}`](https://github.com/kubeflow/model-registry/releases/tag/v{{% model-registry/latest-version %}}) version of Kubeflow Model Registry for the [`v1alpha3` REST API](https://github.com/kubeflow/model-registry/blob/v{{% model-registry/latest-version %}}/api/openapi/model-registry.yaml).

{{% alert title="Note" color="info" %}}
The _try it out_ feature of Swagger UI does not work due to authentication and CORS, but it can help you construct the correct API calls.
{{% /alert %}}

{{< swaggerui-inline component_name="Kubeflow Model Registry" default_input_url="https://kubeflow.example.com/" >}}
https://raw.githubusercontent.com/kubeflow/model-registry/refs/tags/v{{% model-registry/latest-version %}}/api/openapi/model-registry.yaml
{{< /swaggerui-inline >}}




================================================
File: content/en/docs/components/notebooks/OWNERS
================================================
approvers:
  - StefanoFioravanzo
  - elikatsis
  - kimwnasptd
  - thesuperzapper



================================================
File: content/en/docs/components/notebooks/_index.md
================================================
+++
title = "Kubeflow Notebooks"
description = "Documentation for Kubeflow Notebooks"
weight = 10
+++



================================================
File: content/en/docs/components/notebooks/container-images.md
================================================
+++
title = "Container Images"
description = "About Container Images for Kubeflow Notebooks"
weight = 30
+++

Kubeflow Notebooks natively supports three types of notebooks, [JupyterLab](https://github.com/jupyterlab/jupyterlab), [RStudio](https://github.com/rstudio/rstudio), and [Visual Studio Code (code-server)](https://github.com/cdr/code-server), but any web-based IDE should work.
Notebook servers run as containers inside a Kubernetes Pod, which means the type of IDE (and which packages are installed) is determined by the Docker image you pick for your server.

## Official Images

Kubeflow provides a number of [example container images](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers) to get you started with Kubeflow Notebooks.

This chart shows how the images are related to each other (note, the nodes are clickable links to the Dockerfiles):

```mermaid
%%{init: {'theme':'forest'}}%%
graph TD
  Base[Base] --> Jupyter[Jupyter]
  Base --> Code-Server[code-server]
  Base --> RStudio[RStudio]
  
  Jupyter --> PyTorch[PyTorch]
  Jupyter --> SciPy[SciPy]
  Jupyter --> TensorFlow[TensorFlow]
  
  Code-Server --> Code-Server-Conda-Python[Conda Python]
  RStudio --> Tidyverse[Tidyverse]

  PyTorch --> PyTorchFull[PyTorch Full]
  TensorFlow --> TensorFlowFull[TensorFlow Full]

  Jupyter --> PyTorchCuda[PyTorch CUDA]
  Jupyter --> TensorFlowCuda[TensorFlow CUDA]
  Jupyter --> PyTorchGaudi[PyTorch Gaudi]

  PyTorchCuda --> PyTorchCudaFull[PyTorch CUDA Full]
  TensorFlowCuda --> TensorFlowCudaFull[TensorFlow CUDA Full]
  PyTorchGaudi --> PyTorchGaudiFull[PyTorch Gaudi Full]

  click Base "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/base"
  click Jupyter "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter"
  click Code-Server "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/codeserver"
  click RStudio "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio"
  click PyTorch "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch"
  click SciPy "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-scipy"
  click TensorFlow "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow"
  click Code-Server-Conda-Python "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/codeserver-python"
  click Tidyverse "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio-tidyverse"
  click PyTorchFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-full"
  click TensorFlowFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-full"
  click PyTorchCuda "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-cuda"
  click TensorFlowCuda "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-cuda"
  click PyTorchCudaFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-cuda-full"
  click TensorFlowCudaFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-cuda-full"
  click PyTorchGaudi "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-gaudi"
  click PyTorchGaudiFull "https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-gaudi-full"
```

### Base Images

These images provide a common starting point for Kubeflow Notebook containers.

Dockerfile | Container Registry | Notes
--- | --- | ---
[`./base`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/base) | [`kubeflownotebookswg/base`](https://hub.docker.com/r/kubeflownotebookswg/base) | Common Base Image
[`./codeserver`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/codeserver) | [`kubeflownotebookswg/codeserver`](https://hub.docker.com/r/kubeflownotebookswg/codeserver) | [code-server](https://github.com/coder/code-server) (Visual Studio Code)
[`./jupyter`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter) | [`kubeflownotebookswg/jupyter`](https://hub.docker.com/r/kubeflownotebookswg/jupyter) | [JupyterLab](https://github.com/jupyterlab/jupyterlab)
[`./rstudio`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio) | [`kubeflownotebookswg/rstudio`](https://hub.docker.com/r/kubeflownotebookswg/rstudio) | [RStudio](https://github.com/rstudio/rstudio)

### Kubeflow Images

These images extend the [base images](#base-images) with common packages used in the real world.

Dockerfile | Container Registry | Notes
--- | --- | ---
[`./codeserver-python`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/codeserver-python) | [`kubeflownotebookswg/codeserver-python`](https://hub.docker.com/r/kubeflownotebookswg/codeserver-python) | code-server + Conda Python
[`./rstudio-tidyverse`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio-tidyverse) | [`kubeflownotebookswg/rstudio-tidyverse`](https://hub.docker.com/r/kubeflownotebookswg/rstudio-tidyverse) | RStudio + [Tidyverse](https://www.tidyverse.org/)
[`./jupyter-pytorch`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch) | [`kubeflownotebookswg/jupyter-pytorch`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-pytorch) | JupyterLab + PyTorch
[`./jupyter-pytorch-full`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-full) | [`kubeflownotebookswg/jupyter-pytorch-full`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-pytorch-full) | JupyterLab + PyTorch + Common Packages
[`./jupyter-pytorch-cuda`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-cuda) | [`kubeflownotebookswg/jupyter-pytorch-cuda`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-pytorch-cuda) | JupyterLab + PyTorch + CUDA
[`./jupyter-pytorch-cuda-full`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-cuda-full) | [`kubeflownotebookswg/jupyter-pytorch-cuda-full`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-pytorch-cuda-full) | JupyterLab + PyTorch + CUDA + Common Packages
[`./jupyter-scipy`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-scipy) | [`kubeflownotebookswg/jupyter-scipy`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-scipy) | JupyterLab + Common Packages
[`./jupyter-tensorflow`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow) | [`kubeflownotebookswg/jupyter-tensorflow`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-tensorflow) | JupyterLab + TensorFlow
[`./jupyter-tensorflow-full`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-full) | [`kubeflownotebookswg/jupyter-tensorflow-full`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-tensorflow-full) | JupyterLab + TensorFlow + Common Packages
[`./jupyter-tensorflow-cuda`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-cuda) | [`kubeflownotebookswg/jupyter-tensorflow-cuda`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-tensorflow-cuda) | JupyterLab + TensorFlow + CUDA
[`./jupyter-tensorflow-cuda-full`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-cuda-full) | [`kubeflownotebookswg/jupyter-tensorflow-cuda-full`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-tensorflow-cuda-full) | JupyterLab + TensorFlow + CUDA + Common Packages
[`./jupyter-pytorch-gaudi`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-gaudi) | [`kubeflownotebookswg/jupyter-pytorch-gaudi`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-pytorch-gaudi) | JupyterLab + PyTorch + Gaudi
[`./jupyter-pytorch-gaudi-full`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-gaudi-full) | [`kubeflownotebookswg/jupyter-pytorch-gaudi-full`](https://hub.docker.com/r/kubeflownotebookswg/jupyter-pytorch-gaudi-full) | JupyterLab + PyTorch + Gaudi + Common Packages

## Package Installation

Packages installed by users __after spawning__ a Kubeflow Notebook will only last the lifetime of the pod (unless installed into a PVC-backed directory).

To ensure packages are preserved throughout Pod restarts users will need to either:

1. Build [custom images](#custom-images) that include them, or
2. Ensure they are installed in a PVC-backed directory

## Custom Images

You can build your own custom images to use with Kubeflow Notebooks.

The easiest way to ensure your custom image meets the [requirements](#image-requirements) is to extend one of our [base images](#base-images).

### Image Requirements

For a container image to work with Kubeflow Notebooks, it must:

- expose an HTTP interface on port `8888`:
  - kubeflow sets an environment variable `NB_PREFIX` at runtime with the URL path we expect the container be listening under
  - kubeflow uses IFrames, so ensure your application sets `Access-Control-Allow-Origin: *` in HTTP response headers
- run as a user called `jovyan`:
  - the home directory of `jovyan` should be `/home/jovyan`
  - the UID of `jovyan` should be `1000`
- start successfully with an empty PVC mounted at `/home/jovyan`:
  - kubeflow mounts a PVC at `/home/jovyan` to keep state across Pod restarts

### Install Python Packages

You may extend one of the images and install any `pip` or `conda` packages your Kubeflow Notebook users are likely to need.
As a guide, look at [`./jupyter-pytorch-full/Dockerfile`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-full/Dockerfile) for a `pip install ...` example, and the [`./rstudio-tidyverse/Dockerfile`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio-tidyverse/Dockerfile) for `conda install ...`.

A common cause of errors is users running `pip install --user ...`, causing the home-directory (which is backed by a PVC) to contain a different or incompatible version of a package contained in  `/opt/conda/...`

### Install Linux Packages

You may extend one of the images and install any `apt-get` packages your Kubeflow Notebook users are likely to need.
Ensure you swap to `root` in the Dockerfile before running `apt-get`, and swap back to `$NB_USER` after.

### Configure S6 Overlay

Some use-cases might require custom scripts to run during the startup of the Notebook Server container, or advanced users might want to add additional services that run inside the container (for example, an Apache or NGINX web server).
To make this easy, we use the [s6-overlay](https://github.com/just-containers/s6-overlay).

The [s6-overlay](https://github.com/just-containers/s6-overlay) differs from other init systems like [tini](https://github.com/krallin/tini).
While `tini` was created to handle a single process running in a container as PID 1, the `s6-overlay` is built to manage multiple processes and allows the creator of the image to determine which process failures should silently restart, and which should cause the container to exit.

#### Create Scripts

Scripts that need to run during the startup of the container can be placed in `/etc/cont-init.d/`, and are executed in ascending alphanumeric order.

An example of a startup script can be found in [`./rstudio/s6/cont-init.d/02-rstudio-env-fix`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio/s6/cont-init.d/02-rstudio-env-fix).
This script uses the [with-contenv](https://github.com/just-containers/s6-overlay#container-environment) helper so that environment variables (passed to container) are available in the script.
The purpose of this script is to snapshot any `KUBERNETES_*` environment variables into the `Renviron.site` at pod startup, as without these variables `kubectl` does not work.

#### Create Services

Extra services to be monitored by `s6-overlay` should be placed in their own folder under `/etc/services.d/` containing a script called `run` and optionally a finishing script `finish`.

An example of a service can be found in the `run` script of [`.jupyter/s6/services.d/jupyterlab`](https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter/s6/services.d/jupyterlab) which is used to start JupyterLab itself.
For more information about the `run` and `finish` scripts, please see the [s6-overlay documentation](https://github.com/just-containers/s6-overlay#writing-a-service-script).

#### Run Services As Root

There may be cases when you need to run a service as root, to do this, you can change the Dockerfile to have `USER root` at the end, and then use `s6-setuidgid` to run the user-facing services as `$NB_USER`.

Our example images run `s6-overlay` as `$NB_USER` (not `root`), meaning any files or scripts related to `s6-overlay` must be owned by the `$NB_USER` user to successfully run.

## Next steps

- Use your container image by specifying it when spawning your notebook server.
  (See the [quickstart guide](/docs/components/notebooks/quickstart-guide/).)



================================================
File: content/en/docs/components/notebooks/jupyter-tensorflow-examples.md
================================================
+++
title = "Jupyter TensorFlow Examples"
description = "Examples using Jupyter and TensorFlow in Kubeflow Notebooks"
weight = 40

+++

## Mnist Example

(adapted from [tensorflow/tensorflow - mnist_softmax.py](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_softmax.py))

1. When creating your notebook server choose a [container image](/docs/components/notebooks/container-images/) which has Jupyter and TensorFlow installed.

2. Use Jupyter's interface to create a new **Python 3** notebook.

3. Copy the following code and paste it into your notebook:

    ```python
    from tensorflow.examples.tutorials.mnist import input_data
    mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

    import tensorflow as tf

    x = tf.placeholder(tf.float32, [None, 784])

    W = tf.Variable(tf.zeros([784, 10]))
    b = tf.Variable(tf.zeros([10]))

    y = tf.nn.softmax(tf.matmul(x, W) + b)

    y_ = tf.placeholder(tf.float32, [None, 10])
    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

    train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)

    sess = tf.InteractiveSession()
    tf.global_variables_initializer().run()

    for _ in range(1000):
      batch_xs, batch_ys = mnist.train.next_batch(100)
      sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    print("Accuracy: ", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
    ```

4. Run the code. You should see a number of `WARNING` messages from TensorFlow, followed by a line showing a training accuracy something like this:

    ```
    Accuracy:  0.9012
    ```

## Next steps

- See a [simple example](https://github.com/kubeflow/examples/tree/master/pipelines/simple-notebook-pipeline) of creating Kubeflow pipelines in a Jupyter notebook.
- Build machine-learning pipelines with the [Kubeflow Pipelines SDK](/docs/components/pipelines/legacy-v1/sdk/sdk-overview/).
- Learn the advanced features available from a Kubeflow notebook, such as [submitting Kubernetes resources](/docs/components/notebooks/submit-kubernetes/) or [building Docker images](/docs/components/notebooks/container-images/). 



================================================
File: content/en/docs/components/notebooks/overview.md
================================================
+++
title = "Overview"
description = "An overview of Kubeflow Notebooks"
weight = 5
                    
+++
{{% stable-status %}}

## What is Kubeflow Notebooks?

Kubeflow Notebooks provides a way to run web-based development environments inside your Kubernetes cluster by running them inside Pods.

Some key features include:
- Native support for [JupyterLab](https://github.com/jupyterlab/jupyterlab), [RStudio](https://github.com/rstudio/rstudio), and [Visual Studio Code (code-server)](https://github.com/cdr/code-server).
- Users can create notebook containers directly in the cluster, rather than locally on their workstations.
- Admins can provide standard notebook images for their organization with required packages pre-installed.
- Access control is managed by Kubeflow's RBAC, enabling easier notebook sharing across the organization.

## Next steps

- Get started with Kubeflow Notebooks using the [quickstart guide](/docs/components/notebooks/quickstart-guide/).
- Learn how to create your own [container images](/docs/components/notebooks/container-images/).



================================================
File: content/en/docs/components/notebooks/quickstart-guide.md
================================================
+++
title = "Quickstart Guide"
description = "Getting started with Kubeflow Notebooks"
weight = 10
                    
+++

## Summary

1. Install Kubeflow by following [Getting Started - Installing Kubeflow](/docs/started/installing-kubeflow/).
2. Open the Kubeflow [Central Dashboard](/docs/components/central-dash/) in your browser.
3. Click __"Notebooks"__ in the left-hand panel.
4. Click __"New Server"__ to create a new notebook server.
5. Specify the configs for your notebook server.
6. Click  __"CONNECT"__ once the notebook has been provisioned

## Detailed Steps

1. Open the Kubeflow [Central Dashboard](/docs/components/central-dash/) in your browser.

2. Select a Namespace:
    - Click the namespace dropdown to see the list of available namespaces.
    - Choose the namespace that corresponds to your Kubeflow Profile.
      (See the page on [profiles and namespaces](/docs/components/central-dash/profiles/) for more information)

   <img src="/docs/images/notebooks-namespace.png"
   alt="Selecting a Kubeflow namespace"
   class="mt-3 mb-3 border border-info rounded">

3. Click __"Notebook Servers"__ in the left-hand panel:

   <img src="/docs/images/jupyterlink.png"
   alt="Opening notebooks from the Kubeflow UI"
   class="mt-3 mb-3 border border-info rounded">

4. Click __"New Server"__ on the __"Notebook Servers"__ page:

   <img src="/docs/images/add-notebook-server.png"
   alt="The Kubeflow notebook servers page"
   class="mt-3 mb-3 border border-info rounded">

5. Enter a __"Name"__ for your notebook server.
    - The name can include letters and numbers, but no spaces.
    - For example, `my-first-notebook`.

   <img src="/docs/images/new-notebook-server.png"
   alt="Form for adding a Kubeflow notebook server"
   class="mt-3 mb-3 border border-info rounded">

6. Select a Docker __"Image"__ for your notebook server
    - __Custom image__: If you select the custom option, you must specify a Docker image in  the form `registry/image:tag`.
      (See the guide on [container images](/docs/components/notebooks/container-images/).)
    - __Standard image__: Click the __"Image"__ dropdown menu to see the list of available images.
      (You can choose from the list configured by your Kubeflow administrator)

7. Specify the amount of __"CPU"__ that your notebook server will request.

8. Specify the amount of __"RAM"__ that your notebook server will request.

9. Specify a __"workspace volume"__ to be mounted as a PVC Volume on your home folder.

10. *(Optional)* Specify one or more __"data volumes"__ to be mounted as a PVC Volumes.

11. *(Optional)* Specify one or more additional __"configurations"__
    - These correspond to [PodDefault resources](https://github.com/kubeflow/kubeflow/blob/master/components/admission-webhook/README.md) which exist in your profile namespace.
    - Kubeflow matches the labels in the __"configurations"__ field against the properties specified in the PodDefault manifest.
    - For example, select the label `add-gcp-secret` in the __"configurations"__ field to match to a PodDefault manifest containing the following configuration:
    ```yaml
    apiVersion: kubeflow.org/v1alpha1
    kind: PodDefault
    metadata:
      name: add-gcp-secret
      namespace: MY_PROFILE_NAMESPACE
    spec:
     selector:
      matchLabels:
        add-gcp-secret: "true"
     desc: "add gcp credential"
     volumeMounts:
     - name: secret-volume
       mountPath: /secret/gcp
     volumes:
     - name: secret-volume
       secret:
        secretName: gcp-secret
    ```

12. *(Optional)* Specify any __"GPUs"__ that your notebook server will request.
    - Kubeflow uses "limits" in Pod requests to provision GPUs onto the notebook Pods
      (Details about scheduling GPUs can be found in the [Kubernetes Documentation](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/).)

13. *(Optional)* Specify the setting for __"enable shared memory"__.
    - Some libraries like PyTorch use shared memory for multiprocessing.
    - Currently, there is no implementation in Kubernetes to activate shared memory.
    - As a workaround, Kubeflow mounts an empty directory volume at `/dev/shm`.

14. Click __"LAUNCH"__ to create a new Notebook CRD with your specified settings.
    - You should see an entry for your new notebook server on the __"Notebook Servers"__ page
    - There should be a spinning indicator in the __"Status"__ column.
    - It can take a few minutes for kubernetes to provision the notebook server pod.
    - You can check the status of your Pod by hovering your mouse cursor over the icon in the __"Status"__ column.

15. Click __"CONNECT"__ to view the web interface exposed by your notebook server.

    <img src="/docs/images/notebook-servers.png"
    alt="Opening notebooks from the Kubeflow UI"
    class="mt-3 mb-3 border border-info rounded">

## Next steps

- Learn how to create your own [container images](/docs/components/notebooks/container-images/).
- Review examples of using [jupyter and tensorflow](/docs/components/notebooks/jupyter-tensorflow-examples/).
- Visit the [troubleshooting guide](/docs/components/notebooks/troubleshooting) to fix common errors.



================================================
File: content/en/docs/components/notebooks/submit-kubernetes.md
================================================
+++
title = "Submit Kubernetes Resources"
description = "Submitting Kubernetes resources from a Notebook"
weight = 40
                    
+++

## Notebook Pod ServiceAccount

Kubeflow assigns the `default-editor` Kubernetes ServiceAccount to the Notebook Pods.
The Kubernetes `default-editor` ServiceAccount is bound to the `kubeflow-edit` ClusterRole, which has namespace-scoped permissions to many Kubernetes resources.

You can get the full list of RBAC for `ClusterRole/kubeflow-edit` using:
```
kubectl describe clusterrole kubeflow-edit
```

## Kubectl in Notebook Pod

Because every Notebook Pod has the highly-privileged `default-editor` Kubernetes ServiceAccount bound to it, you can run `kubectl` inside it without providing additional authentication.

For example, the following command will create the resources defined in `test.yaml`:

```shell
kubectl create -f "test.yaml" --namespace "MY_PROFILE_NAMESPACE"
```

## Next steps

- See the Kubeflow Notebook [quickstart guide](/docs/components/notebooks/quickstart-guide/).
- Explore the other [components of Kubeflow](/docs/components/).



================================================
File: content/en/docs/components/notebooks/troubleshooting.md
================================================
+++
title = "Troubleshooting"
description = "Problems and solutions for common problems with Kubeflow Notebooks"
weight = 100
                    
+++

## ISSUE: notebook not starting

### SOLUTION: check events of Notebook

Run the following command then check the `events` section to make sure that there are no errors:

```shell
kubectl describe notebooks "${MY_NOTEBOOK_NAME}" --namespace "${MY_PROFILE_NAMESPACE}"
```

### SOLUTION: check events of Pod

Run the following command then check the `events` section to make sure that there are no errors:

```shell
kubectl describe pod "${MY_NOTEBOOK_NAME}-0" --namespace "${MY_PROFILE_NAMESPACE}"
```

### SOLUTION: check YAML of Pod

Run the following command and check the Pod YAML looks as expected:

```shell
kubectl get pod "${MY_NOTEBOOK_NAME}-0" --namespace "${MY_PROFILE_NAMESPACE}" -o yaml
```

### SOLUTION: check logs of Pod

Run the following command to get the logs from the Pod:

```shell
kubectl logs "${MY_NOTEBOOK_NAME}-0" --namespace "${MY_PROFILE_NAMESPACE}"
```

## ISSUE: manually delete notebook

### SOLUTION: use kubectl to delete Notebook resource

Run the following command to delete a Notebook resource manually:

```shell
kubectl delete notebook "${MY_NOTEBOOK_NAME}" --namespace "${MY_PROFILE_NAMESPACE}"
```


================================================
File: content/en/docs/components/notebooks/api-reference/_index.md
================================================
+++
title = "API Reference"
description = "Reference documentation for Kubeflow Notebooks"
weight = 900
+++


================================================
File: content/en/docs/components/notebooks/api-reference/notebook-v1.md
================================================
+++
title = "Notebook (v1)"
description = "Reference documentation for the `v1` version of the `Notebook` resource"
weight = 10
+++

<p>Packages:</p>
<ul>
<li>
<a href="#kubeflow.org/v1">kubeflow.org/v1</a>
</li>
</ul>
<h2 id="kubeflow.org/v1">kubeflow.org/v1</h2>
<p>
<p>Package v1 contains API Schema definitions for the kubeflow.org v1 API group</p>
</p>
Resource Types:
<ul></ul>
<h3 id="kubeflow.org/v1.Notebook">Notebook
</h3>
<p>
<p>Notebook is the Schema for the notebooks API</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>metadata</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
</a>
</em>
</td>
<td>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.
</td>
</tr>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="#kubeflow.org/v1.NotebookSpec">
NotebookSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>template</code></br>
<em>
<a href="#kubeflow.org/v1.NotebookTemplateSpec">
NotebookTemplateSpec
</a>
</em>
</td>
<td>
<p>INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
Important: Run &ldquo;make&rdquo; to regenerate code after modifying this file</p>
</td>
</tr>
</table>
</td>
</tr>
<tr>
<td>
<code>status</code></br>
<em>
<a href="#kubeflow.org/v1.NotebookStatus">
NotebookStatus
</a>
</em>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<h3 id="kubeflow.org/v1.NotebookCondition">NotebookCondition
</h3>
<p>
(<em>Appears on:</em>
<a href="#kubeflow.org/v1.NotebookStatus">NotebookStatus</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>type</code></br>
<em>
string
</em>
</td>
<td>
<p>Type is the type of the condition. Possible values are Running|Waiting|Terminated</p>
</td>
</tr>
<tr>
<td>
<code>lastProbeTime</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta">
Kubernetes meta/v1.Time
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Last time we probed the condition.</p>
</td>
</tr>
<tr>
<td>
<code>reason</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>(brief) reason the container is in the current state</p>
</td>
</tr>
<tr>
<td>
<code>message</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Message regarding why the container is in the current state.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="kubeflow.org/v1.NotebookSpec">NotebookSpec
</h3>
<p>
(<em>Appears on:</em>
<a href="#kubeflow.org/v1.Notebook">Notebook</a>)
</p>
<p>
<p>NotebookSpec defines the desired state of Notebook</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>template</code></br>
<em>
<a href="#kubeflow.org/v1.NotebookTemplateSpec">
NotebookTemplateSpec
</a>
</em>
</td>
<td>
<p>INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
Important: Run &ldquo;make&rdquo; to regenerate code after modifying this file</p>
</td>
</tr>
</tbody>
</table>
<h3 id="kubeflow.org/v1.NotebookStatus">NotebookStatus
</h3>
<p>
(<em>Appears on:</em>
<a href="#kubeflow.org/v1.Notebook">Notebook</a>)
</p>
<p>
<p>NotebookStatus defines the observed state of Notebook</p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>conditions</code></br>
<em>
<a href="#kubeflow.org/v1.NotebookCondition">
[]NotebookCondition
</a>
</em>
</td>
<td>
<p>Conditions is an array of current conditions</p>
</td>
</tr>
<tr>
<td>
<code>readyReplicas</code></br>
<em>
int32
</em>
</td>
<td>
<p>ReadyReplicas is the number of Pods created by the StatefulSet controller that have a Ready Condition.</p>
</td>
</tr>
<tr>
<td>
<code>containerState</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#containerstate-v1-core">
Kubernetes core/v1.ContainerState
</a>
</em>
</td>
<td>
<p>ContainerState is the state of underlying container.</p>
</td>
</tr>
</tbody>
</table>
<h3 id="kubeflow.org/v1.NotebookTemplateSpec">NotebookTemplateSpec
</h3>
<p>
(<em>Appears on:</em>
<a href="#kubeflow.org/v1.NotebookSpec">NotebookSpec</a>)
</p>
<p>
</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>spec</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#podspec-v1-core">
Kubernetes core/v1.PodSpec
</a>
</em>
</td>
<td>
<br/>
<br/>
<table>
<tr>
<td>
<code>volumes</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#volume-v1-core">
[]Kubernetes core/v1.Volume
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>List of volumes that can be mounted by containers belonging to the pod.
More info: <a href="https://kubernetes.io/docs/concepts/storage/volumes">https://kubernetes.io/docs/concepts/storage/volumes</a></p>
</td>
</tr>
<tr>
<td>
<code>initContainers</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#container-v1-core">
[]Kubernetes core/v1.Container
</a>
</em>
</td>
<td>
<p>List of initialization containers belonging to the pod.
Init containers are executed in order prior to containers being started. If any
init container fails, the pod is considered to have failed and is handled according
to its restartPolicy. The name for an init container or normal container must be
unique among all containers.
Init containers may not have Lifecycle actions, Readiness probes, Liveness probes, or Startup probes.
The resourceRequirements of an init container are taken into account during scheduling
by finding the highest request/limit for each resource type, and then using the max of
of that value or the sum of the normal containers. Limits are applied to init containers
in a similar fashion.
Init containers cannot currently be added or removed.
Cannot be updated.
More info: <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</a></p>
</td>
</tr>
<tr>
<td>
<code>containers</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#container-v1-core">
[]Kubernetes core/v1.Container
</a>
</em>
</td>
<td>
<p>List of containers belonging to the pod.
Containers cannot currently be added or removed.
There must be at least one container in a Pod.
Cannot be updated.</p>
</td>
</tr>
<tr>
<td>
<code>ephemeralContainers</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#ephemeralcontainer-v1-core">
[]Kubernetes core/v1.EphemeralContainer
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>List of ephemeral containers run in this pod. Ephemeral containers may be run in an existing
pod to perform user-initiated actions such as debugging. This list cannot be specified when
creating a pod, and it cannot be modified by updating the pod spec. In order to add an
ephemeral container to an existing pod, use the pod&rsquo;s ephemeralcontainers subresource.
This field is alpha-level and is only honored by servers that enable the EphemeralContainers feature.</p>
</td>
</tr>
<tr>
<td>
<code>restartPolicy</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Restart policy for all containers within the pod.
One of Always, OnFailure, Never.
Default to Always.
More info: <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy</a></p>
</td>
</tr>
<tr>
<td>
<code>terminationGracePeriodSeconds</code></br>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>Optional duration in seconds the pod needs to terminate gracefully. May be decreased in delete request.
Value must be non-negative integer. The value zero indicates delete immediately.
If this value is nil, the default grace period will be used instead.
The grace period is the duration in seconds after the processes running in the pod are sent
a termination signal and the time when the processes are forcibly halted with a kill signal.
Set this value longer than the expected cleanup time for your process.
Defaults to 30 seconds.</p>
</td>
</tr>
<tr>
<td>
<code>activeDeadlineSeconds</code></br>
<em>
int64
</em>
</td>
<td>
<em>(Optional)</em>
<p>Optional duration in seconds the pod may be active on the node relative to
StartTime before the system will actively try to mark it failed and kill associated containers.
Value must be a positive integer.</p>
</td>
</tr>
<tr>
<td>
<code>dnsPolicy</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Set DNS policy for the pod.
Defaults to &ldquo;ClusterFirst&rdquo;.
Valid values are &lsquo;ClusterFirstWithHostNet&rsquo;, &lsquo;ClusterFirst&rsquo;, &lsquo;Default&rsquo; or &lsquo;None&rsquo;.
DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy.
To have DNS options set along with hostNetwork, you have to specify DNS policy
explicitly to &lsquo;ClusterFirstWithHostNet&rsquo;.</p>
</td>
</tr>
<tr>
<td>
<code>nodeSelector</code></br>
<em>
map[string]string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeSelector is a selector which must be true for the pod to fit on a node.
Selector which must match a node&rsquo;s labels for the pod to be scheduled on that node.
More info: <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</a></p>
</td>
</tr>
<tr>
<td>
<code>serviceAccountName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>ServiceAccountName is the name of the ServiceAccount to use to run this pod.
More info: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</a></p>
</td>
</tr>
<tr>
<td>
<code>serviceAccount</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>DeprecatedServiceAccount is a depreciated alias for ServiceAccountName.
Deprecated: Use serviceAccountName instead.</p>
</td>
</tr>
<tr>
<td>
<code>automountServiceAccountToken</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>AutomountServiceAccountToken indicates whether a service account token should be automatically mounted.</p>
</td>
</tr>
<tr>
<td>
<code>nodeName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>NodeName is a request to schedule this pod onto a specific node. If it is non-empty,
the scheduler simply schedules this pod onto that node, assuming that it fits resource
requirements.</p>
</td>
</tr>
<tr>
<td>
<code>hostNetwork</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Host networking requested for this pod. Use the host&rsquo;s network namespace.
If this option is set, the ports that will be used must be specified.
Default to false.</p>
</td>
</tr>
<tr>
<td>
<code>hostPID</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Use the host&rsquo;s pid namespace.
Optional: Default to false.</p>
</td>
</tr>
<tr>
<td>
<code>hostIPC</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Use the host&rsquo;s ipc namespace.
Optional: Default to false.</p>
</td>
</tr>
<tr>
<td>
<code>shareProcessNamespace</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>Share a single process namespace between all of the containers in a pod.
When this is set containers will be able to view and signal processes from other containers
in the same pod, and the first process in each container will not be assigned PID 1.
HostPID and ShareProcessNamespace cannot both be set.
Optional: Default to false.</p>
</td>
</tr>
<tr>
<td>
<code>securityContext</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#podsecuritycontext-v1-core">
Kubernetes core/v1.PodSecurityContext
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>SecurityContext holds pod-level security attributes and common container settings.
Optional: Defaults to empty.  See type description for default values of each field.</p>
</td>
</tr>
<tr>
<td>
<code>imagePullSecrets</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#localobjectreference-v1-core">
[]Kubernetes core/v1.LocalObjectReference
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec.
If specified, these secrets will be passed to individual puller implementations for them to use. For example,
in the case of docker, only DockerConfig type secrets are honored.
More info: <a href="https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod">https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod</a></p>
</td>
</tr>
<tr>
<td>
<code>hostname</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>Specifies the hostname of the Pod
If not specified, the pod&rsquo;s hostname will be set to a system-defined value.</p>
</td>
</tr>
<tr>
<td>
<code>subdomain</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>If specified, the fully qualified Pod hostname will be &ldquo;<hostname>.<subdomain>.<pod namespace>.svc.<cluster domain>&rdquo;.
If not specified, the pod will not have a domainname at all.</p>
</td>
</tr>
<tr>
<td>
<code>affinity</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#affinity-v1-core">
Kubernetes core/v1.Affinity
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>If specified, the pod&rsquo;s scheduling constraints</p>
</td>
</tr>
<tr>
<td>
<code>schedulerName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>If specified, the pod will be dispatched by specified scheduler.
If not specified, the pod will be dispatched by default scheduler.</p>
</td>
</tr>
<tr>
<td>
<code>tolerations</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#toleration-v1-core">
[]Kubernetes core/v1.Toleration
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>If specified, the pod&rsquo;s tolerations.</p>
</td>
</tr>
<tr>
<td>
<code>hostAliases</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#hostalias-v1-core">
[]Kubernetes core/v1.HostAlias
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>HostAliases is an optional list of hosts and IPs that will be injected into the pod&rsquo;s hosts
file if specified. This is only valid for non-hostNetwork pods.</p>
</td>
</tr>
<tr>
<td>
<code>priorityClassName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>If specified, indicates the pod&rsquo;s priority. &ldquo;system-node-critical&rdquo; and
&ldquo;system-cluster-critical&rdquo; are two special keywords which indicate the
highest priorities with the former being the highest priority. Any other
name must be defined by creating a PriorityClass object with that name.
If not specified, the pod priority will be default or zero if there is no
default.</p>
</td>
</tr>
<tr>
<td>
<code>priority</code></br>
<em>
int32
</em>
</td>
<td>
<em>(Optional)</em>
<p>The priority value. Various system components use this field to find the
priority of the pod. When Priority Admission Controller is enabled, it
prevents users from setting this field. The admission controller populates
this field from PriorityClassName.
The higher the value, the higher the priority.</p>
</td>
</tr>
<tr>
<td>
<code>dnsConfig</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#poddnsconfig-v1-core">
Kubernetes core/v1.PodDNSConfig
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>Specifies the DNS parameters of a pod.
Parameters specified here will be merged to the generated DNS
configuration based on DNSPolicy.</p>
</td>
</tr>
<tr>
<td>
<code>readinessGates</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#podreadinessgate-v1-core">
[]Kubernetes core/v1.PodReadinessGate
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>If specified, all readiness gates will be evaluated for pod readiness.
A pod is ready when all its containers are ready AND
all conditions specified in the readiness gates have status equal to &ldquo;True&rdquo;
More info: <a href="https://git.k8s.io/enhancements/keps/sig-network/0007-pod-ready%2B%2B.md">https://git.k8s.io/enhancements/keps/sig-network/0007-pod-ready%2B%2B.md</a></p>
</td>
</tr>
<tr>
<td>
<code>runtimeClassName</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>RuntimeClassName refers to a RuntimeClass object in the node.k8s.io group, which should be used
to run this pod.  If no RuntimeClass resource matches the named class, the pod will not be run.
If unset or empty, the &ldquo;legacy&rdquo; RuntimeClass will be used, which is an implicit class with an
empty definition that uses the default runtime handler.
More info: <a href="https://git.k8s.io/enhancements/keps/sig-node/runtime-class.md">https://git.k8s.io/enhancements/keps/sig-node/runtime-class.md</a>
This is a beta feature as of Kubernetes v1.14.</p>
</td>
</tr>
<tr>
<td>
<code>enableServiceLinks</code></br>
<em>
bool
</em>
</td>
<td>
<em>(Optional)</em>
<p>EnableServiceLinks indicates whether information about services should be injected into pod&rsquo;s
environment variables, matching the syntax of Docker links.
Optional: Defaults to true.</p>
</td>
</tr>
<tr>
<td>
<code>preemptionPolicy</code></br>
<em>
string
</em>
</td>
<td>
<em>(Optional)</em>
<p>PreemptionPolicy is the Policy for preempting pods with lower priority.
One of Never, PreemptLowerPriority.
Defaults to PreemptLowerPriority if unset.
This field is alpha-level and is only honored by servers that enable the NonPreemptingPriority feature.</p>
</td>
</tr>
<tr>
<td>
<code>overhead</code></br>
<em>
object
</em>
</td>
<td>
<em>(Optional)</em>
<p>Overhead represents the resource overhead associated with running a pod for a given RuntimeClass.
This field will be autopopulated at admission time by the RuntimeClass admission controller. If
the RuntimeClass admission controller is enabled, overhead must not be set in Pod create requests.
The RuntimeClass admission controller will reject Pod create requests which have the overhead already
set. If RuntimeClass is configured and selected in the PodSpec, Overhead will be set to the value
defined in the corresponding RuntimeClass, otherwise it will remain unset and treated as zero.
More info: <a href="https://git.k8s.io/enhancements/keps/sig-node/20190226-pod-overhead.md">https://git.k8s.io/enhancements/keps/sig-node/20190226-pod-overhead.md</a>
This field is alpha-level as of Kubernetes v1.16, and is only honored by servers that enable the PodOverhead feature.</p>
</td>
</tr>
<tr>
<td>
<code>topologySpreadConstraints</code></br>
<em>
<a href="https://v1-29.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#topologyspreadconstraint-v1-core">
[]Kubernetes core/v1.TopologySpreadConstraint
</a>
</em>
</td>
<td>
<em>(Optional)</em>
<p>TopologySpreadConstraints describes how a group of pods ought to spread across topology
domains. Scheduler will schedule pods in a way which abides by the constraints.
This field is alpha-level and is only honored by clusters that enables the EvenPodsSpread
feature.
All topologySpreadConstraints are ANDed.</p>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<hr/>
<p><em>
Generated with <code>gen-crd-api-reference-docs</code>
on git commit <code>3b35937</code>.
</em></p>



================================================
File: content/en/docs/components/pipelines/OWNERS
================================================
approvers:
  - chensun
  - connor-mccarthy
  - rimolive
  - hbelmiro



================================================
File: content/en/docs/components/pipelines/_index.md
================================================
+++
title = "Kubeflow Pipelines"
description = "Documentation for Kubeflow Pipelines."
weight = 15
+++



================================================
File: content/en/docs/components/pipelines/getting-started.md
================================================
+++
title = "Getting started"
description = "Create your first pipeline"
weight = 2
+++

{{% kfp-v2-keywords %}}

To get started with the tutorials, pip install `kfp` v2:

```sh
pip install kfp
```

Here is a simple pipeline that prints a greeting:

```python
from kfp import dsl

@dsl.component
def say_hello(name: str) -> str:
    hello_text = f'Hello, {name}!'
    print(hello_text)
    return hello_text

@dsl.pipeline
def hello_pipeline(recipient: str) -> str:
    hello_task = say_hello(name=recipient)
    return hello_task.output
```

You can [compile the pipeline][compile-a-pipeline] to YAML with the KFP SDK DSL [`Compiler`][compiler]:

```python
from kfp import compiler

compiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')
```

The [`dsl.component`][dsl-component] and [`dsl.pipeline`][dsl-pipeline] decorators turn your type-annotated Python functions into components and pipelines, respectively. The KFP SDK compiler compiles the domain-specific language (DSL) objects to a self-contained pipeline [YAML file][ir-yaml].

You can submit the YAML file to a KFP-conformant backend for execution. If you have already deployed a [KFP open source backend instance][installation] and obtained the endpoint for your deployment, you can submit the pipeline for execution using the KFP SDK [`Client`][client]. The following submits the pipeline for execution with the argument `recipient='World'`:

```python
from kfp.client import Client

client = Client(host='<MY-KFP-ENDPOINT>')
run = client.create_run_from_pipeline_package(
    'pipeline.yaml',
    arguments={
        'recipient': 'World',
    },
)
```

The client will print a link to view the pipeline execution graph and logs in the UI. In this case, the pipeline has one task that prints and returns `'Hello, World!'`.

## Next steps

In the next few sections, you'll learn more about the core concepts of authoring pipelines and how to create more expressive, useful pipelines.

* Learn more about [Connecting the Pipelines SDK to Kubeflow Pipelines](/docs/components/pipelines/user-guides/core-functions/connect-api/).

[installation]: /docs/components/pipelines/operator-guides/installation/
[client]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html#kfp.client.Client
[compiler]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/compiler.html#kfp.compiler.Compiler
[ir-yaml]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline#ir-yaml
[compile-a-pipeline]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline/
[dsl-pipeline]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.pipeline
[dsl-component]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.component



================================================
File: content/en/docs/components/pipelines/interfaces.md
================================================
+++
title = "Interfaces"
description = "The ways you can interact with the Kubeflow Pipelines system"
weight = 3
                    
+++

This page introduces the interfaces that you can use to build and run
machine learning (ML) workflows with Kubeflow Pipelines.

## User interface (UI)

You can access the Kubeflow Pipelines UI by clicking **Pipeline Dashboard** on 
the Kubeflow UI. The Kubeflow Pipelines UI looks like this:
  <img src="/docs/images/pipelines/v1/pipelines-ui.png" 
    alt="Pipelines UI"
    class="mt-3 mb-3 border border-info rounded">

From the Kubeflow Pipelines UI you can perform the following tasks:

* Run one or more of the preloaded samples to try out pipelines quickly.
* Upload a pipeline as a compressed file. The pipeline can be one that you
  have built (see how to [build a 
  pipeline](/docs/components/pipelines/legacy-v1/sdk/build-pipeline/)) or one 
  that someone has shared with you.
* Create an *experiment* to group one or more of your pipeline runs.
  See the [definition of an
  experiment](/docs/components/pipelines/concepts/experiment/).
* Create and start a *run* within the experiment. A run is a single execution
  of a pipeline. See the [definition of a
  run](/docs/components/pipelines/concepts/run/).
* Explore the configuration, graph, and output of your pipeline run.
* Compare the results of one or more runs within an experiment.
* Schedule runs by creating a recurring run.

See the [quickstart guide](/docs/components/pipelines/legacy-v1/overview/quickstart/) for more
information about accessing the Kubeflow Pipelines UI and running the samples.

When building a pipeline component, you can write out information for display
in the UI. See the guides to [exporting 
metrics](/docs/components/pipelines/legacy-v1/sdk/output-viewer/#v2-sdk-use-sdk-visualization-apis) and [visualizing results in 
the UI](/docs/components/pipelines/legacy-v1/sdk/output-viewer/).

## Python SDK

The Kubeflow Pipelines SDK provides a set of Python packages that you can use to 
specify and run your ML workflows.

See the [introduction to the Kubeflow Pipelines 
SDK](/docs/components/pipelines/legacy-v1/sdk/sdk-overview/) for an overview of the ways you can
use the SDK to build pipeline components and pipelines.

## REST API

The Kubeflow Pipelines API is useful for continuous integration/deployment
systems, for example, where you want to incorporate your pipeline executions
into shell scripts or other systems. 
For example, you may want to trigger a pipeline run when new data comes in.

See the [Kubeflow Pipelines API reference 
documentation](/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec/).



================================================
File: content/en/docs/components/pipelines/overview.md
================================================
+++
title = "Overview"
description = "What is Kubeflow Pipelines?"
weight = 1
+++

{{% kfp-v2-keywords %}}

Kubeflow Pipelines (KFP) is a platform for building and deploying portable and scalable machine learning (ML) workflows using Docker containers.

With KFP you can author [components][components] and [pipelines][pipelines] using the [KFP Python SDK][pypi], compile pipelines to an [intermediate representation YAML][ir-yaml], and submit the pipeline to run on a KFP-conformant backend such as the [open source KFP backend][installation] or [Google Cloud Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction).

The [open source KFP backend][installation] is available as a core component of Kubeflow or as a standalone installation. Follow the [installation][installation] instructions and [Hello World Pipeline][hello-world-pipeline] example to quickly get started with KFP.

<!-- TODO: Include these links once the topic is available -->
<!-- [Learn more about installing Kubeflow][Installation]
[Learn more about installing Kubeflow Pipelines standalone][Installation] -->

## Why Kubeflow Pipelines?

KFP enables data scientists and machine learning engineers to:

* Author end-to-end ML workflows natively in Python
* Create fully custom ML components or leverage an ecosystem of existing components
* Easily manage, track, and visualize pipeline definitions, runs, experiments, and ML artifacts
* Efficiently use compute resources through parallel task execution and through caching to eliminating redundant executions
* Maintain cross-platform pipeline portability through a platform-neutral [IR YAML pipeline definition][ir-yaml]

## What is a pipeline?

A [pipeline][pipelines] is a definition of a workflow that composes one or more [components][components] together to form a computational directed acyclic graph (DAG). At runtime, each component execution corresponds to a single container execution, which may create ML artifacts. Pipelines may also feature [control flow][control-flow].

<!-- TODO: Uncomment these links once the topic is created -->
## Next steps

* [Hello World Pipeline][hello-world-pipeline]
* Learn more about [authoring components][components]
* Learn more about [authoring pipelines][pipelines]

[components]: /docs/components/pipelines/user-guides/components
[pipelines]: /docs/components/pipelines/user-guides
[installation]: /docs/components/pipelines/operator-guides/installation
[ir-yaml]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline#ir-yaml
[pypi]: https://pypi.org/project/kfp/
[hello-world-pipeline]: /docs/components/pipelines/getting-started
[control-flow]: /docs/components/pipelines/user-guides/core-functions/control-flow



================================================
File: content/en/docs/components/pipelines/concepts/_index.md
================================================
+++
title = "Concepts"
description = "Concepts used in Kubeflow Pipelines"
weight = 4
+++


================================================
File: content/en/docs/components/pipelines/concepts/component.md
================================================
+++
title = "Component"
description = "Conceptual overview of components in Kubeflow Pipelines"
weight = 20
                    
+++

A *pipeline component* is self-contained set of code that performs one step in
the ML workflow (pipeline), such as data preprocessing, data transformation,
model training, and so on. A component is analogous to a function, in that it
has a name, parameters, return values, and a body.

## Component code

The code for each component includes the following:

* **Client code:** The code that talks to endpoints to submit jobs. For example, 
  code to talk to the Google Dataproc API to submit a Spark job.

* **Runtime code:** The code that does the actual job and usually runs in the 
  cluster. For example, Spark code that transforms raw data into preprocessed 
  data.

Note the naming convention for client code and runtime code&mdash;for a task 
named "mytask":

* The `mytask.py` program contains the client code.
* The `mytask` directory contains all the runtime code.

## Component definition

A component specification in YAML format describes the component for the
Kubeflow Pipelines system. A component definition has the following parts:

* **Metadata:** name, description, etc.
* **Interface:** input/output specifications (name, type, description, default 
  value, etc).
* **Implementation:** A specification of how to run the component given a 
  set of argument values for the component's inputs. The implementation section 
  also describes how to get the output values from the component once the
  component has finished running.

For the complete definition of a component, see the
[component specification](/docs/components/pipelines/reference/component-spec/).

## Containerizing components

You must package your component as a 
[Docker image](https://docs.docker.com/get-started/). Components represent a 
specific program or entry point inside a container.

Each component in a pipeline executes independently. The components do not run
in the same process and cannot directly share in-memory data. You must serialize
(to strings or files) all the data pieces that you pass between the components
so that the data can travel over the distributed network. You must then
deserialize the data for use in the downstream component.

## Next steps

* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.
* Build your own 
  [component and pipeline](/docs/components/pipelines/legacy-v1/sdk/component-development/).
* Build a [reusable component](/docs/components/pipelines/legacy-v1/sdk/component-development/) for
  sharing in multiple pipelines.


================================================
File: content/en/docs/components/pipelines/concepts/experiment.md
================================================
+++
title = "Experiment"
description = "Conceptual overview of experiments in Kubeflow Pipelines"
weight = 40
                    
+++

An *experiment* is a workspace where you can try different configurations of
your pipelines. You can use experiments to organize your runs into logical
groups. Experiments can contain arbitrary runs, including 
[recurring runs](/docs/components/pipelines/concepts/run/).

## Next steps

* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.



================================================
File: content/en/docs/components/pipelines/concepts/graph.md
================================================
+++
title = "Graph"
description = "Conceptual overview of graphs in Kubeflow Pipelines"
weight = 30
                    
+++

A *graph* is a pictorial representation in the Kubeflow Pipelines UI of the
runtime execution of a pipeline. The graph shows the steps that a pipeline run
has executed or is executing, with arrows indicating the parent/child
relationships between the pipeline components represented by each step. The
graph is viewable as soon as the run begins. Each node within the graph
corresponds to a step within the pipeline and is labeled accordingly.

The screenshot below shows an example of a pipeline graph:

<img src="/docs/images/pipelines-xgboost-graph.png" 
  alt="XGBoost results on the pipelines UI"
  class="mt-3 mb-3 border border-info rounded">

At the top right of each node is an icon indicating its status: running,
succeeded, failed, or skipped. (A node can be skipped when its 
parent contains a conditional clause.)

## Next steps

* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.


================================================
File: content/en/docs/components/pipelines/concepts/metadata.md
================================================
+++
title = "ML Metadata"
description = "Conceptual overview about Metadata in Kubeflow Pipelines"
weight = 90
                    
+++

**Note:** Kubeflow Pipelines has moved from using [kubeflow/metadata](https://github.com/kubeflow/metadata)
to using [google/ml-metadata](https://github.com/google/ml-metadata) for Metadata dependency.

Kubeflow Pipelines backend stores runtime information of a pipeline run in Metadata store.
Runtime information includes the status of a task, availability of artifacts, custom properties associated
with Execution or Artifact, etc. Learn more at [ML Metadata Get Started](https://github.com/google/ml-metadata/tree/master).

You can view the connection between Artifacts and Executions across Pipeline Runs, if 
one Artifact is being used by multiple Executions in different Runs. This connection visualization
is called a *Lineage Graph*.

## Next steps

* Learn about [output Artifact](/docs/components/pipelines/concepts/output-artifact).



================================================
File: content/en/docs/components/pipelines/concepts/output-artifact.md
================================================
+++
title = "Output Artifact"
description = "Conceptual overview of output artifacts in Kubeflow Pipelines"
weight = 80
                    
+++

An *output artifact* is an output emitted by a pipeline component, which the
Kubeflow Pipelines UI understands and can render as rich visualizations. It’s
useful for pipeline components to include artifacts so that you can provide for
performance evaluation, quick decision making for the run, or comparison across
different runs. Artifacts also make it possible to understand how the pipeline’s
various components work. An artifact can range from a plain textual view of the
data to rich interactive visualizations.

## Next steps

* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.
* Read more about the available 
  [output viewers](/docs/components/pipelines/legacy-v1/sdk/output-viewer/) 
  and how to provide the metadata to make use of the visualizations
  that the output viewers provide.



================================================
File: content/en/docs/components/pipelines/concepts/pipeline-root.md
================================================
+++
title = "Pipeline Root"
description = "Getting started with Kubeflow Pipelines pipeline root"
weight = 15

+++

Starting from [Kubeflow Pipelines SDK v2](https://kubeflow-pipelines.readthedocs.io/en/stable/) and Kubeflow Pipelines v2, Kubeflow Pipelines supports a new intermediate artifact repository feature: pipeline root in both [standalone deployment](/docs/components/pipelines/operator-guides/installation/) and [AI Platform Pipelines](https://cloud.google.com/ai-platform/pipelines/docs).

## Before you start
This guide tells you the basic concepts of Kubeflow Pipelines pipeline root and how to use it.
This guide assumes that you already have Kubeflow Pipelines installed, or want to use standalone or AI Platform Pipelines options in the [Kubeflow Pipelines deployment
guide](/docs/components/pipelines/operator-guides/installation/) to deploy Kubeflow Pipelines.

## What is pipeline root?

Pipeline root represents the path within an object store bucket where Kubeflow Pipelines stores a pipeline's artifacts.
This feature supports MinIO, S3, GCS natively using [Go CDK](https://github.com/google/go-cloud). 

Artifacts can be more accessible in S3 and GCS when integrating Kubeflow Pipelines with other systems.

## How to configure pipeline root authentication 
#### MinIO
You don't need to pass the authentication for MinIO.
Kubeflow Pipelines is configured with the authentication of the MinIO instance deployed with itself.

#### GCS
If you want to specify the `pipeline root` to GCS :

check [authentication-pipelines](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/authentication-pipelines/)

#### S3
If you want to specify the `pipeline root` to S3, please choose one of the following options:

* Via [AWS IRSA](https://aws.amazon.com/blogs/containers/cross-account-iam-roles-for-kubernetes-service-accounts/):

* Via kfp sdk:
`dsl.get_pipeline_conf().add_op_transformer(aws.use_aws_secret('xxx', ‘xxx’, ‘xxx’))`
  
**references**:
* [add-op-transformer](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.PipelineConf.add_op_transformer)
* [use-aws-secret](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.extensions.html#kfp.aws.use_aws_secret)

## How to configure pipeline root

#### Via ConfigMaps
The default Pipeline root at the Kubeflow pipeline deployment level can be changed by configuring the KFP Launcher configmap.

Instructions can be found [here](/docs/components/pipelines/operator-guides/configure-object-store.md#kfp-launcher-object-store-configuration).

####  Via Building Pipelines
You can configure a pipeline root through the `kfp.dsl.pipeline` annotation when [building pipelines](/docs/components/pipelines/legacy-v1/sdk/build-pipeline/#build-your-pipeline)

####  Via Submitting a Pipeline through SDK
You can configure pipeline root via `pipeline_root` argument when you submit a Pipeline using one of the following:
* [create_run_from_pipeline_func](https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html#kfp.Client.create_run_from_pipeline_func)
* [create_run_from_pipeline_package](https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html#kfp.Client.create_run_from_pipeline_package) 
* [run_pipeline](https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html#kfp.Client.run_pipeline).

####  Via Submitting a Pipeline Run through UI
You can configure a pipeline root via the `pipeline_root` run parameters when you submit a pipeline run in the UI
<img src="/docs/images/pipelines/v1/v2-compatible/pipelines-ui-pipelineroot.png"
alt="Configure pipeline root on the pipelines UI"
class="mt-3 mb-3 border border-info rounded">



================================================
File: content/en/docs/components/pipelines/concepts/pipeline.md
================================================
+++
title = "Pipeline"
description = "Conceptual overview of pipelines in Kubeflow Pipelines"
weight = 10
                    
+++

A *pipeline* is a description of a machine learning (ML) workflow, including all
of the [components](/docs/components/pipelines/concepts/component/) in the workflow and how the components relate to each other in
the form of a [graph](/docs/components/pipelines/concepts/graph/). The pipeline
configuration includes the definition of the inputs (parameters) required to run
the pipeline and the inputs and outputs of each component.

When you run a pipeline, the system launches one or more Kubernetes Pods
corresponding to the [steps](/docs/components/pipelines/concepts/step/) (components) in your workflow (pipeline). The Pods
start Docker containers, and the containers in turn start your programs.

After developing your pipeline, you can upload your pipeline using the Kubeflow Pipelines UI or the Kubeflow Pipelines SDK.

## Next steps
* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.



================================================
File: content/en/docs/components/pipelines/concepts/run-trigger.md
================================================
+++
title = "Run Trigger"
description = "Conceptual overview of run triggers in Kubeflow Pipelines"
weight = 60
                    
+++

A *run trigger* is a flag that tells the system when a recurring run
configuration spawns a new run. The following types of run trigger are
available:

* Periodic: for an interval-based scheduling of runs (for example: every 2 hours 
  or every 45 minutes).
* [Cron](https://en.wikipedia.org/wiki/Cron): for specifying `cron` semantics for scheduling runs.

## Next steps

* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.



================================================
File: content/en/docs/components/pipelines/concepts/run.md
================================================
+++
title = "Run and Recurring Run"
description = "Conceptual overview of runs in Kubeflow Pipelines"
weight = 50
                    
+++

A *run* is a single execution of a pipeline. Runs comprise an immutable log of
all experiments that you attempt, and are designed to be self-contained to allow
for reproducibility. You can track the progress of a run by looking at its
details page on the Kubeflow Pipelines UI, where you can see the runtime graph,
output artifacts, and logs for each step in the run.

<a id=recurring-run></a>
A *recurring run*, or job in the Kubeflow Pipelines [backend APIs](https://github.com/kubeflow/pipelines/tree/06e4dc660498ce10793d566ca50b8d0425b39981/backend/api/go_http_client/job_client), is a repeatable run of
a pipeline. The configuration for a recurring run includes a copy of a pipeline
with all parameter values specified and a 
[run trigger](/docs/components/pipelines/concepts/run-trigger/).
You can start a recurring run inside any experiment, and it will periodically
start a new copy of the run configuration. You can enable/disable the recurring
run from the Kubeflow Pipelines UI. You can also specify the maximum number of
concurrent runs, to limit the number of runs launched in parallel. This can be
helpful if the pipeline is expected to run for a long period of time and is
triggered to run frequently.

## Next steps

* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.



================================================
File: content/en/docs/components/pipelines/concepts/step.md
================================================
+++
title = "Step"
description = "Conceptual overview of steps in Kubeflow Pipelines"
weight = 70
                    
+++

A *step* is an execution of one of the components in the pipeline. The
relationship between a step and its component is one of instantiation, much like
the relationship between a run and its pipeline. In a complex pipeline,
components can execute multiple times in loops, or conditionally after resolving
an if/else like clause in the pipeline code.

## Next steps

* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.


================================================
File: content/en/docs/components/pipelines/legacy-v1/_index.md
================================================
+++
title = "Legacy (v1)"
description = "Kubeflow Pipelines v1 Documentation"
weight = 999
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}



================================================
File: content/en/docs/components/pipelines/legacy-v1/introduction.md
================================================
+++
title = "Introduction"
description = "An introduction to the goals and main concepts of Kubeflow Pipelines"
weight = 10
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

Kubeflow Pipelines is a platform for building and deploying portable, 
scalable machine learning (ML) workflows based on Docker containers.

## Quickstart

Run your first pipeline by following the 
[pipelines quickstart guide](/docs/components/pipelines/legacy-v1/overview/quickstart).

## What is Kubeflow Pipelines?

The Kubeflow Pipelines platform consists of:

* A user interface (UI) for managing and tracking experiments, jobs, and runs.
* An engine for scheduling multi-step ML workflows.
* An SDK for defining and manipulating pipelines and components.
* Notebooks for interacting with the system using the SDK.

The following are the goals of Kubeflow Pipelines:

* End-to-end orchestration: enabling and simplifying the orchestration of
  machine learning pipelines.
* Easy experimentation: making it easy for you to try numerous ideas and 
  techniques and manage your various trials/experiments.
* Easy re-use: enabling you to re-use components and pipelines to quickly 
  create end-to-end solutions without having to rebuild each time.

Kubeflow Pipelines is available as a core component of Kubeflow or as a standalone installation.

* [Learn more about installing Kubeflow](/docs/started/).
* [Learn more about installing Kubeflow Pipelines standalone](/docs/components/pipelines/legacy-v1/overview/).

{{% pipelines-compatibility %}}

## What is a pipeline?

A _pipeline_ is a description of an ML workflow, including all of the components 
in the workflow and how they combine in the form of a graph. (See the
screenshot below showing an example of a pipeline graph.) The pipeline
includes the definition of the inputs (parameters) required to run the pipeline 
and the inputs and outputs of each component.

After developing your pipeline, you can upload and share it on the 
Kubeflow Pipelines UI.

A _pipeline component_ is a self-contained set of user code, packaged as a 
[Docker image](https://docs.docker.com/get-started/), that 
performs one step in the pipeline. For example, a component can be responsible
for data preprocessing, data transformation, model training, and so on.

See the conceptual guides to [pipelines](/docs/components/pipelines/concepts/pipeline/)
and [components](/docs/components/pipelines/concepts/component/).

## Example of a pipeline

The screenshots and code below show the `xgboost-training-cm.py` pipeline, which
creates an XGBoost model using structured data in CSV format. You can see the
source code and other information about the pipeline on
[GitHub](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core/xgboost_training_cm).

### The runtime execution graph of the pipeline

The screenshot below shows the example pipeline's runtime execution graph in the
Kubeflow Pipelines UI:

<img src="/docs/images/pipelines-xgboost-graph.png" 
  alt="XGBoost results on the pipelines UI"
  class="mt-3 mb-3 border border-info rounded">

### The Python code that represents the pipeline

Below is an extract from the Python code that defines the 
`xgboost-training-cm.py` pipeline. You can see the full code on 
[GitHub](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core/xgboost_training_cm).

```python
@dsl.pipeline(
    name='XGBoost Trainer',
    description='A trainer that does end-to-end distributed training for XGBoost models.'
)
def xgb_train_pipeline(
    output='gs://your-gcs-bucket',
    project='your-gcp-project',
    cluster_name='xgb-%s' % dsl.RUN_ID_PLACEHOLDER,
    region='us-central1',
    train_data='gs://ml-pipeline-playground/sfpd/train.csv',
    eval_data='gs://ml-pipeline-playground/sfpd/eval.csv',
    schema='gs://ml-pipeline-playground/sfpd/schema.json',
    target='resolution',
    rounds=200,
    workers=2,
    true_label='ACTION',
):
    output_template = str(output) + '/' + dsl.RUN_ID_PLACEHOLDER + '/data'

    # Current GCP pyspark/spark op do not provide outputs as return values, instead,
    # we need to use strings to pass the uri around.
    analyze_output = output_template
    transform_output_train = os.path.join(output_template, 'train', 'part-*')
    transform_output_eval = os.path.join(output_template, 'eval', 'part-*')
    train_output = os.path.join(output_template, 'train_output')
    predict_output = os.path.join(output_template, 'predict_output')

    with dsl.ExitHandler(exit_op=dataproc_delete_cluster_op(
        project_id=project,
        region=region,
        name=cluster_name
    )):
        _create_cluster_op = dataproc_create_cluster_op(
            project_id=project,
            region=region,
            name=cluster_name,
            initialization_actions=[
              os.path.join(_PYSRC_PREFIX,
                           'initialization_actions.sh'),
            ],
            image_version='1.2'
        )

        _analyze_op = dataproc_analyze_op(
            project=project,
            region=region,
            cluster_name=cluster_name,
            schema=schema,
            train_data=train_data,
            output=output_template
        ).after(_create_cluster_op).set_display_name('Analyzer')

        _transform_op = dataproc_transform_op(
            project=project,
            region=region,
            cluster_name=cluster_name,
            train_data=train_data,
            eval_data=eval_data,
            target=target,
            analysis=analyze_output,
            output=output_template
        ).after(_analyze_op).set_display_name('Transformer')

        _train_op = dataproc_train_op(
            project=project,
            region=region,
            cluster_name=cluster_name,
            train_data=transform_output_train,
            eval_data=transform_output_eval,
            target=target,
            analysis=analyze_output,
            workers=workers,
            rounds=rounds,
            output=train_output
        ).after(_transform_op).set_display_name('Trainer')

        _predict_op = dataproc_predict_op(
            project=project,
            region=region,
            cluster_name=cluster_name,
            data=transform_output_eval,
            model=train_output,
            target=target,
            analysis=analyze_output,
            output=predict_output
        ).after(_train_op).set_display_name('Predictor')

        _cm_op = confusion_matrix_op(
            predictions=os.path.join(predict_output, 'part-*.csv'),
            output_dir=output_template
        ).after(_predict_op)

        _roc_op = roc_op(
            predictions_dir=os.path.join(predict_output, 'part-*.csv'),
            true_class=true_label,
            true_score_column=true_label,
            output_dir=output_template
        ).after(_predict_op)

    dsl.get_pipeline_conf().add_op_transformer(
        gcp.use_gcp_secret('user-gcp-sa'))
```

### Pipeline input data on the Kubeflow Pipelines UI

The partial screenshot below shows the Kubeflow Pipelines UI for kicking off a 
run of the pipeline. The pipeline definition in your code determines which 
parameters appear in the UI form. The pipeline definition can also set default 
values for the parameters: 

<img src="/docs/images/pipelines-start-xgboost-run.png" 
  alt="Starting the XGBoost run on the pipelines UI"
  class="mt-3 mb-3 border border-info rounded">

### Outputs from the pipeline

The following screenshots show examples of the pipeline output visible on
the Kubeflow Pipelines UI.

Prediction results:

<img src="/docs/images/predict.png" 
  alt="Prediction output"
  class="mt-3 mb-3 p-3 border border-info rounded">

Confusion matrix:

<img src="/docs/images/cm.png" 
  alt="Confusion matrix"
  class="mt-3 mb-3 p-3 border border-info rounded">

Receiver operating characteristics (ROC) curve:

<img src="/docs/images/roc.png" 
  alt="ROC"
  class="mt-3 mb-3 p-3 border border-info rounded">

## Architectural overview

<img src="/docs/images/pipelines-architecture.png" 
  alt="Pipelines architectural diagram"
  class="mt-3 mb-3 p-3 border border-info rounded">

At a high level, the execution of a pipeline proceeds as follows: 

* **Python SDK**: You create components or specify a pipeline using the Kubeflow
  Pipelines domain-specific language 
  ([DSL](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/sdk/python/kfp/dsl)).
* **DSL compiler**: The
  [DSL compiler](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/sdk/python/kfp/compiler)
  transforms your pipeline's Python code into a static configuration (YAML).
* **Pipeline Service**: You call the Pipeline Service to create a
  pipeline run from the static configuration. 
* **Kubernetes resources**: The Pipeline Service calls the Kubernetes API server to create
  the necessary Kubernetes resources
  ([CRDs](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/))
  to run the pipeline.
* **Orchestration controllers**: A set of orchestration controllers
  execute the containers needed to complete the pipeline.
  The containers execute within Kubernetes Pods on virtual machines.
  An example controller is the
  **[Argo Workflow](https://github.com/argoproj/argo-workflows)** controller,
  which orchestrates task-driven workflows.
* **Artifact storage**: The Pods store two kinds of data: 

  * **Metadata:** Experiments, jobs, pipeline runs, and single scalar metrics.
    Metric data is aggregated for the purpose of sorting and filtering.
    Kubeflow Pipelines stores the metadata in a MySQL database.
  * **Artifacts:** Pipeline packages, views, and large-scale metrics (time series).
    Use large-scale metrics to debug a pipeline run or investigate an individual run’s performance.
    Kubeflow Pipelines stores the artifacts in an artifact store like
    [Minio server](https://docs.minio.io/) or
    [Cloud Storage](https://cloud.google.com/storage/docs/).

    The MySQL database and the Minio server are both backed by the Kubernetes
    [PersistentVolume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes)
    subsystem.

* **Persistence agent and ML metadata**: The Pipeline Persistence Agent
  watches the Kubernetes resources created by the Pipeline Service and
  persists the state of these resources in the ML Metadata Service. The
  Pipeline Persistence Agent records the set of containers that executed as
  well as their inputs and outputs. The input/output consists of either
  container parameters or data artifact URIs. 
* **Pipeline web server**: The Pipeline web server gathers data from various
  services to display relevant views: the list of pipelines currently running,
  the history of pipeline execution, the list of data artifacts, debugging
  information about individual pipeline runs, execution status about individual
  pipeline runs.

## Next steps

* Follow the 
  [pipelines quickstart guide](/docs/components/pipelines/legacy-v1/overview/quickstart) to 
  deploy Kubeflow and run a sample pipeline directly from the 
  Kubeflow Pipelines UI.
* Build machine-learning pipelines with the [Kubeflow Pipelines 
  SDK](/docs/components/pipelines/legacy-v1/sdk/sdk-overview/).
* Follow the full guide to experimenting with
  [the Kubeflow Pipelines samples](/docs/components/pipelines/legacy-v1/tutorials/build-pipeline/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/troubleshooting.md
================================================
+++
title = "Troubleshooting"
description = "Troubleshooting guide for Kubeflow Pipelines"
weight = 90
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This page presents some hints for troubleshooting specific problems that you
may encounter.

## Diagnosing problems in your Kubeflow Pipelines environment

For help diagnosing environment issues that affect Kubeflow Pipelines, run
the [`kfp diagnose_me` command-line tool](/docs/components/pipelines/legacy-v1/sdk/sdk-overview/#kubeflow-pipelines-cli-tool).

The `kfp diagnose_me` CLI reports on the configuration of your local
development environment, Kubernetes cluster, or Google Cloud environment.
Use this command to help resolve issues like the following:

*  Python library dependencies
*  Trouble accessing resources or APIs using Kubernetes secrets
*  Trouble accessing Persistent Volume Claims

To use the `kfp diagnose_me` CLI, follow these steps:

1.  Install the [Kubeflow Pipelines SDK](/docs/components/pipelines/legacy-v1/sdk/install-sdk/).
1.  Follow the [guide to configuring access to Kubernetes clusters][kubeconfig],
    to update your kubeconfig file with appropriate credentials and endpoint
    information to access your Kubeflow cluster.
    If your Kubeflow Pipelines cluster is hosted on a cloud provider like
    Google Cloud, use your cloud provider's instructions for configuring
    access to your Kubernetes cluster. 
1.  Run the `kfp diagnose_me` command.
1.  Analyze the results to troubleshoot your environment.

[kubeconfig]: https://kubernetes.io/docs/reference/access-authn-authz/authentication/

## Troubleshooting the Kubeflow Pipelines SDK

The following sections describe how to resolve issues that can occur when
installing or using the Kubeflow Pipelines SDK.

### Error: Could not find a version that satisfies the requirement kfp

This error indicates that you have not installed the `kfp` package in your
Python3 environment. Follow the instructions in the [Kubeflow Pipelines SDK
installation guide](/docs/components/pipelines/legacy-v1/sdk/install-sdk/), if you have not already
installed the SDK.

If you have already installed the Kubeflow Pipelines SDK, check that you have
Python 3.5 or higher:

```
python3 -V
```

The response should be something like the following:

```
Python 3.7.3
```

If you do not have Python 3.5 or higher, you can
[download Python](https://www.python.org/downloads/) from the Python
Software Foundation.

### kfp or dsl-compile command not found

If your install the Kubeflow Pipelines SDK with the `--user` flag, you may
get the following error when using the `kfp` or `dsl-compile` command-line
tools.

```
bash: kfp: command not found
```

This error occurs because installing the Kubeflow Pipelines SDK with
`--user` stores `kfp` and `dsl-compile` in your `~/.local/bin` directory.
In some Linux distributions, the `~/.local/bin` directory is not part of the
$PATH environment variable.

You can resolve this issue by using one of the following options:

*  Add `export $PATH=$PATH:~/.local/bin` to the end of your `~/.bashrc` file.
   Then restart your terminal session or run `source ~/.bashrc`.
*  Run the `kfp` and `dsl-compile` commands as `~/.local/bin/kfp` and
   `~/.local/bin/dsl-compile`.

## TFX visualizations do not show up or throw an error

Confirm your Kubeflow Pipelines backend version is compatible with your TFX version, refer to [Kubeflow Pipelines Compatibility Matrix](/docs/components/pipelines/legacy-v1/installation/compatibility-matrix/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/installation/_index.md
================================================
+++
title = "Installation"
description = "Options for installing Kubeflow Pipelines"
weight = 35
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}


================================================
File: content/en/docs/components/pipelines/legacy-v1/installation/choose-executor.md
================================================
+++
title = "Choosing an Argo Workflows Executor"
description = "How to choose an Argo Workflows Executor"
weight = 80
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

An Argo workflow executor is a process that conforms to a specific interface that allows Argo to perform certain actions like monitoring pod logs, collecting artifacts, managing container lifecycles, etc.

Kubeflow Pipelines runs on [Argo Workflows](https://argoproj.github.io/workflows/) as the workflow engine, so Kubeflow Pipelines users need to choose a workflow executor.

## Choosing the Workflow Executor

1. [Emissary executor](#emissary-executor) has been Kubeflow Pipelines' default executor since Feburay 2022 when KFP 1.8 went GA. 
   We recommend Emissary executor unless you have known compatibility issues with Emissary, in which case please submit your
   feedback in [the Emissary Executor feedback Github issue](https://github.com/kubeflow/pipelines/issues/6249).

1. [Docker executor](#docker-executor) is available as a legacy choice. In case you do have compatibilty issues with Emissary executor,
   and your cluster is running on an older version of Kubernetes (<1.20), you can configure to use Docker executor.

Note that Argo Workflows support other workflow executors, but the Kubeflow Pipelines
team only recommend choosing between emissary executor and docker executor.

### Emissary Executor

Emissary executor is the **default** workflow executor for Kubeflow Pipelines v1.8+. It was first released in Argo Workflows v3.1 (June 2021).
The Kubeflow Pipelines team believe that its architectural and portability
improvements can make it the default executor that most people should use going forward.

* Container Runtime: any
* Reliability: not yet well-tested and not yet popular, but the Kubeflow Pipelines
  team supports it.
* Security: more secure
  * No `privileged` access.
  * Cannot escape the privileges of the pod's service account.
* Migration: `command` must be specified in [Kubeflow Pipelines component specification](/docs/components/pipelines/reference/component-spec/).

  Note, the same migration requirement is required by [Kubeflow Pipelines v2 compatible mode](/docs/components/pipelines/user-guides/migration/), refer to
  [known caveats & breaking changes](https://github.com/kubeflow/pipelines/issues/6133).

#### Migrate to Emissary Executor

Prerequisite: emissary executor is only available in Kubeflow Pipelines backend version 1.7+.
To upgrade, refer to [upgrading Kubeflow Pipelines](/docs/components/pipelines/legacy-v1/installation/upgrade//).

##### Configure an existing Kubeflow Pipelines cluster to use emissary executor

1. Install [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl).
1. Connect to your cluster via kubectl.
1. Switch to the namespace you installed Kubeflow Pipelines:

    ```bash
    kubectl config set-context --current --namespace <your-kfp-namespace>
    ```

    Note, usually it's `kubeflow` or `default`.

1. Confirm current workflow executor:

    ```bash
    kubectl describe configmap workflow-controller-configmap | grep -A 2 containerRuntimeExecutor
    ```

    You'll see output like the following when using docker executor:

    ```text
    containerRuntimeExecutor:
    ----
    docker
    ```

1. Configure workflow executor to emissary:

    ```bash
    kubectl patch configmap workflow-controller-configmap --patch '{"data":{"containerRuntimeExecutor":"emissary"}}'
    ```

1. Confirm workflow executor is changed successfully:

    ```bash
    kubectl describe configmap workflow-controller-configmap | grep -A 2 containerRuntimeExecutor
    ```

    You'll see output like the following:

    ```text
    containerRuntimeExecutor:
    ----
    emissary
    ```

##### Deploy a new Kubeflow Pipelines cluster with emissary executor

For [AI Platform Pipelines](https://cloud.google.com/ai-platform/pipelines/docs), check the "Use emissary executor" checkbox during installation.

For [Kubeflow Pipelines Standalone](/docs/components/pipelines/legacy-v1/installation/standalone-deployment/), install `env/platform-agnostic-emissary`:

```bash
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-emissary?ref=$PIPELINE_VERSION"
```

When in doubt, you can always deploy your Kubeflow Pipelines cluster first and
configure workflow executor after installation using the instructions for
existing clusters.

##### Migrate pipeline components to run on emissary executor

Some pipeline components require manual updates to run on emissary executor.
For [Kubeflow Pipelines component specification](/docs/components/pipelines/reference/component-spec/) YAML,
the `command` field must be specified.

Step by step component migration tutorial:

1. There is a hello world component:

    ```yaml
    name: hello-world
    implementation:
      container:
        image: hello-world
    ```

1. We can run the container without command/args:

    ```bash
    $ docker run hello-world
    Hello from Docker!
    ...
    ```

1. Find out what the default ENTRYPOINT and CMD is in the image:

    ```bash
    $ docker image inspect -f '{{.Config.Entrypoint}} {{.Config.Cmd}}' hello-world
    [] [/hello]
    ```

    So ENTRYPOINT is not specified, and CMD is ["/hello"].
    Note, ENTRYPOINT roughly means `command` and CMD roughly
    means `arguments`. `command` and `arguments` are concatenated as the user
    command.

1. Update the component YAML:

    ```yaml
    name: hello-world
    implementation:
      container:
        image: hello-world
        command: ["/hello"]
    ```

1. The updated component can run on emissary executor now.

Note: Kubeflow Pipelines SDK compiler always specifies a command for
[python function based components](/docs/components/pipelines/legacy-v1/sdk/python-function-components/).
Therefore, these components will continue to work on emissary executor without
modifications.

### Docker Executor

Docker executor used to be the default workflow executor before Kubeflow Pipelines v1.8.

{{% alert title="Warning" color="warning" %}}
Docker executor depends on docker container runtime, which is deprecated on Kubernetes 1.20+.
{{% /alert %}}

* Container Runtime: docker only. However, [Kubernetes is deprecating Docker as a container runtime after v1.20](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/).
  On Google Kubernetes Engine (GKE) 1.19+, container runtime already defaults to containerd.
* Reliability: most well-tested and most popular argo workflows executor
* Security: least secure
  * It requires `privileged` access to `docker.sock` of the host to be mounted which.
  Often rejected by Open Policy Agent (OPA) or your Pod Security Policy (PSP).
  GKE autopilot mode also rejects it, because [No privileged Pods](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview#no_privileged_pods).
  * It can escape the privileges of the pod's service account.

#### Prepare a GKE cluster for Docker Executor

For GKE, the node image decides which container runtime is used. To use docker
container runtime, you need to [specify a node image](https://cloud.google.com/kubernetes-engine/docs/how-to/node-images) with Docker.

You must use one of the following node images:

* Container-Optimized OS with Docker (cos)
* Ubuntu with Docker (ubuntu)

If your nodes are not using docker as container runtime, when you run pipelines
you will always find error messages like:

> This step is in Error state with this message: failed to save outputs: Error response from daemon: No such container: XXXXXX

## References

* [Argo Workflow Executors documentation](https://argoproj.github.io/argo-workflows/workflow-executors/)
* KFP docker executor doesn't support Kubernetes 1.19 or above [kubeflow/pipelines#5714](https://github.com/kubeflow/pipelines/issues/5714)
* Feature request - default to emissary executor [kubeflow/pipelines#5718](https://github.com/kubeflow/pipelines/issues/5718)



================================================
File: content/en/docs/components/pipelines/legacy-v1/installation/compatibility-matrix.md
================================================
+++
title = "Compatibility Matrix"
description = "Kubeflow Pipelines compatibility matrix with TensorFlow Extended (TFX)"
weight = 100
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

## Kubeflow Pipelines Backend and TFX compatibility

Pipelines written in any version of [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx) will execute on any version of Kubeflow Pipelines (KFP) backend. However, some UI features may not be functioning properly if the TFX and Kubeflow Pipelines Backend versions are not compatible.

The following table shows UI feature compatibility for TFX and Kubeflow Pipelines Backend versions:

| [TFX] \ [KFP Backend] | [KFP Backend] <= 1.5                              | [KFP Backend] >= 1.7                           |
| --------------------- | ------------------------------------------------- | ---------------------------------------------- |
| [TFX] <= 0.28.0       | Fully Compatible  ✅                              | Metadata UI not compatible<sup>[2](#fn2)</sup> |
| [TFX] 0.29.0, 0.30.0  | Visualizations not compatible<sup>[1](#fn1)</sup> | Metadata UI not compatible<sup>[2](#fn2)</sup> |
| [TFX] 1.0.0           | Metadata UI not compatible<sup>[2](#fn2)</sup>    | Metadata UI not compatible<sup>[2](#fn2)</sup> |
| [TFX] >= 1.2.0        | Metadata UI not compatible<sup>[2](#fn2)</sup>    | Fully Compatible  ✅                           |

Detailed explanations:

<a name="fn1">1.</a> **Visualizations not compatible**: Kubeflow Pipelines UI and TFDV, TFMA visualizations is not compatible. Visualizations throw an error in Kubeflow Pipelines UI.

<a name="fn2">2.</a> **Metadata UI not compatible**: Kubeflow Pipelines UI and TFX recorded ML Metadata is not compatible. ML Metadata tab in run details page shows error message "Corresponding ML Metadata not found". As a result, visualizations based on ML Metadata do not show up in visualizations tab either.

<!--
Issues that caused the incompatibilities:
* TFX 1.0.0+
	* https://github.com/kubeflow/pipelines/issues/6138#issuecomment-898190223
	* https://github.com/kubeflow/pipelines/issues/6138#issuecomment-899917056
* TFX 0.29.0 https://github.com/tensorflow/tfx/issues/3933
-->

[TFX]: https://github.com/tensorflow/tfx/releases
[KFP Backend]: https://github.com/kubeflow/pipelines/releases



================================================
File: content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md
================================================
+++
title = "Local Deployment"
description = "Information about local Deployment of Kubeflow Pipelines (kind, K3s, K3ai)"
weight = 20
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This guide shows how to deploy Kubeflow Pipelines standalone on a local
Kubernetes cluster using:

- kind
- K3s
- K3s on Windows Subsystem for Linux (WSL)
- K3ai [*alpha*]
- Docker-Desktop

Such deployment methods can be part of your local environment using the supplied
kustomize manifests for test purposes. This guide is an alternative to

[Deploying Kubeflow Pipelines
(KFP)](/docs/started/installing-kubeflow).

## Before you get started

- You should be familiar with [Kubernetes](https://kubernetes.io/docs/home/),
  [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/), and
  [kustomize](https://kustomize.io/).

- For native support of kustomize, you will need kubectl v1.14 or higher. You
  can download and install kubectl by following the [kubectl installation
  guide](https://kubernetes.io/docs/tasks/tools/install-kubectl/).

## kind

### 1. Installing kind

[kind](https://kind.sigs.k8s.io) is a tool for running local Kubernetes clusters
using Docker container nodes. `kind` was primarily designed for testing
Kubernetes itself. It can also be used for local development or CI.

You can install and configure kind by following the
[official quick start](https://kind.sigs.k8s.io/docs/user/quick-start/).

To get started with kind:

**On Linux:**

Download and move the `kind` executable to your directory in your PATH by
running the following commands:

```shell
curl -Lo ./kind https://kind.sigs.k8s.io/dl/{KIND_VERSION}/kind-linux-amd64 && \
chmod +x ./kind && \
mv ./kind /{YOUR_KIND_DIRECTORY}/kind
```

Replace the following:

* `{KIND_VERSION}`: the kind version; for example, `v0.8.1` as of the date this
  guide was written
* `{YOUR_KIND_DIRECTORY}`: your directory in PATH

**On macOS:**

You can use [Homebrew](https://brew.sh) to install kind:

```shell
brew install kind
```

**On Windows:**

- You can use the administrative PowerShell console to run the following
  commands to download and move the `kind` executable to a directory in your
  PATH:

- **PowerShell:** Run these commands to download and move the `kind` executable
  to a directory in your PATH:

  ```powershell
  curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/{KIND_VERSION}/kind-windows-amd64
  Move-Item .\kind-windows-amd64.exe c:\{YOUR_KIND_DIRECTORY}\kind.exe
  ```

  Replace the following:

  - `{KIND_VERSION}`: the kind version - for example, `v0.9` (check the latest
  stable binary versions on the [kind releases
  pages](https://github.com/kubernetes-sigs/kind/releases))
  - `{YOUR_KIND_DIRECTORY}`: your directory for kind in PATH

* `{KIND_VERSION}`: the kind version; for example, `v0.8.1` as of the date this
  guide was written
* `{YOUR_KIND_DIRECTORY}`: your directory in PATH

- Alternatively, you can use Chocolatey [https://chocolatey.org/packages/kind](https://chocolatey.org/packages/kind):

  ```SHELL
  choco install kind
  ```

**Note:** kind uses containerd as a default container-runtime hence you cannot
use the standard kubeflow pipeline manifests.

**References**:

**References:**

- [kind: Quick Start Guide](https://kind.sigs.k8s.io/docs/user/quick-start/)
- [kind: Known Issues](https://kind.sigs.k8s.io/docs/user/known-issues/)
- [kind: Working Offline](https://kind.sigs.k8s.io/docs/user/working-offline/)

### 2. Creating a cluster on kind

Having installed kind, you can create a Kubernetes cluster on kind with this
command:

```shell
kind create cluster
```

This will bootstrap a Kubernetes cluster using a pre-built node image. You can
find that image on the Docker Hub `kindest/node`
[here](https://hub.docker.com/r/kindest/node). If you wish to build the node
image yourself, you can use the `kind build node-image` command—see the official
[building
image](https://kind.sigs.k8s.io/docs/user/quick-start/#building-images) section
for more details. And, to specify another image, use the `--image` flag.

By default, the cluster will be given the name kind. Use the `--name` flag to
assign the cluster a different context name.

## K3s

### 1. Setting up a cluster on K3s

K3s is a fully compliant Kubernetes distribution with the following
enhancements:

* Packaged as a single binary.
* Lightweight storage backend based on sqlite3 as the default storage mechanism.
  etcd3, MySQL, Postgres also still available.
* Wrapped in simple launcher that handles a lot of the complexity of TLS and
  options.
* Secure by default with reasonable defaults for lightweight environments.
* Simple but powerful “batteries-included” features have been added, such as: a
  local storage provider, a service load balancer, a Helm controller, and the
  Traefik ingress controller.
* Operation of all Kubernetes control plane components is encapsulated in a
  single binary and process. This allows K3s to automate and manage complex
  cluster operations like distributing certificates.
* External dependencies have been minimized (just a modern kernel and cgroup
  mounts needed). K3s packages required dependencies, including:

  * containerd
  * Flannel
  * CoreDNS
  * CNI
  * Host utilities (iptables, socat, etc)
  * Ingress controller (traefik)
  * Embedded service loadbalancer
  * Embedded network policy controller

You can find the the official K3s installation script to install it as a service
on systemd- or openrc-based systems on the official
[K3s website](https://get.k3s.io).

To install K3s using that method, run the following command:

```SHELL
curl -sfL https://get.k3s.io | sh -
```

**References**:

* [K3s: Quick Start Guide](https://rancher.com/docs/k3s/latest/en/quick-start/)

* [K3s: Known Issues](https://rancher.com/docs/k3s/latest/en/known-issues/)

* [K3s: FAQ](https://rancher.com/docs/k3s/latest/en/faq/)

### 2. Creating a cluster on K3s

1. To create a Kubernetes cluster on K3s, use the following command:

    ```shell
    sudo k3s server &
    ```

    This will bootstrap a Kubernetes cluster kubeconfig is written to
    `/etc/rancher/k3s/k3s.yaml`.

2. (Optional) Check your cluster:

    ```shell
    sudo k3s kubectl get node
    ```

    K3s embeds the popular kubectl command directly in the binaries, so you may
   immediately interact with the cluster through it.

3. (Optional) Run the below command on a different node. `NODE_TOKEN` comes from
   `/var/lib/rancher/k3s/server/node-token` on your server:

    ```shell
    sudo k3s agent --server https://myserver:6443 --token {YOUR_NODE_TOKEN}
    ```

## K3s on Windows Subsystem for Linux (WSL)

### 1. Setting up a cluster on K3s on Windows Subsystem for Linux (WSL)

The Windows Subsystem for Linux (WSL) lets developers run a GNU/Linux
environment—including most command-line tools, utilities, and applications—
directly on Windows, unmodified, without the overhead of a traditional virtual
machine or dualboot setup.

The full instructions for installing WSL can be found on the
[official Windows site](https://docs.microsoft.com/en-us/windows/wsl/install-win10).

The following steps summarize what you'll need to set up WSL and then K3s on
WSL.

1. Install [WSL] by following the official [docs](https://docs.microsoft.com/en-us/windows/wsl/install-win10).

2. As per the official instructions, update WSL and download your preferred
   distibution:

- [SUSE Linux Enterprise Server 15
  SP1](https://www.microsoft.com/store/apps/9PN498VPMF3Z)
- [openSUSE Leap 15.2](https://www.microsoft.com/store/apps/9MZD0N9Z4M4H)
- [Ubuntu 18.04 LTS](https://www.microsoft.com/store/apps/9N9TNGVNDL3Q)
- [Debian GNU/Linux](https://www.microsoft.com/store/apps/9MSVKQC78PK6)

**References**:

* [K3s on WSL: Quick Start Guide](https://gist.github.com/ibuildthecloud/1b7d6940552ada6d37f54c71a89f7d00)

### 2. Creating a cluster on K3s on WSL

Below are the steps to create a cluster on K3s in WSL

1. To create a Kubernetes cluster on K3s on WSL, run the following command:

    ```shell
    sudo ./k3s server
    ```

    This will bootstrap a Kubernetes cluster but you will cannot yet access from
    your Windows machine to the cluster itself.

    **Note:** You can't install K3s using the curl script because there is no
    supervisor (systemd or openrc) in WSL.

2. Download the K3s binary from https://github.com/rancher/k3s/releases/latest.
   Then, inside the directory where you download the K3s binary to, run this
   command to add execute permission to the K3s binary:

    ```shell
    chmod +x k3s
    ```

3. Start K3s:

    ```shell
    sudo ./k3s server
    ```

### 3. Setting up access to WSL instance

To set up access to your WSL instance:

1. Copy `/etc/rancher/k3s/k3s.yaml` from WSL to `$HOME/.kube/config`.

2. Edit the copied file by changing the server URL from `https://localhost:6443`
   to the IP of the your WSL instance (`ip addr show dev eth0`) (For example,
   `https://192.168.170.170:6443`.)

3. Run kubectl in a Windows terminal. If you don't kubectl installed, follow the
   official [Kubernetes on Windows instructions](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-windows).

## K3ai [*alpha*]

K3ai is a lightweight "infrastructure in a box" designed specifically to install
and configure AI tools and platforms on portable hardware, such as laptops and
edge devices. This enables users to perform quick experimentations with Kubeflow
on a local cluster.

K3ai's main goal is to provide a quick way to install Kubernetes (K3s-based) and
Kubeflow Pipelines with NVIDIA GPU support and TensorFlow Serving with just one
line. (For Kubeflow and other component support, check [K3ai's
website](https://kf5ai.gitbook.io/k3ai/#components-of-k-3-ai) for updates.) To
install Kubeflow Pipelines using K3ai, run the following commands:

- With CPU-only support:

```SHELL
curl -sfL https://get.k3ai.in | bash -s -- --cpu --plugin_kfpipelines
```

- With GPU support:

```SHELL
curl -sfL https://get.k3ai.in | bash -s -- --gpu --plugin_kfpipelines
```

For more information about K3ai, refer to the
[official documentation](https://k3ai.github.io/docs/intro).

## Docker-Desktop

Docker Desktop is secure, out-of-the-box containerization software offering developers and teams a robust, hybrid toolkit to build, share, and run applications anywhere.

### 1. Installing Docker-Desktop

You can install and configure Docker-Desktop by following the
[official quick start](https://www.docker.com/products/docker-desktop/).

**on Windows** - 
download and run the Docker Desktop Installer.exe file, and follow the instructions inside the installer.

### 2. Creating a cluster on Docker-Desktop

Having installed Docker-Desktop, you can create a Kubernetes cluster on Docker-Desktop by following those steps:
1. Open the Docker-Desktop dashboard.
2. Open the settings by clicking the settings button.
3. Navigate to the Kubernetes tab on the left side.
4. Check the 'Enable Kubernetes' checkbox, click 'Apply and restart' and wait for the app to restart.
5. In order to check if the cluster is up and running, open the command prompt and run the following command:
    ```SHELL
    kubectl cluster-info   
    ```
   You should see the following output:
    ```SHELL
    Kubernetes control plane is running at https://kubernetes.docker.internal:<some-port>
    CoreDNS is running at https://kubernetes.docker.internal:<some-port>/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    ```

To further debug and diagnose cluster problems, use `kubectl cluster-info dump`.

## Deploying Kubeflow Pipelines

The installation process for Kubeflow Pipelines is the same for all three
environments covered in this guide: kind, K3s, Docker-desktop, and K3ai.

1. To deploy the Kubeflow Pipelines, run the following commands:

    ```shell
    export PIPELINE_VERSION={{% pipelines/latest-version %}}
    kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
    kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
    kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic?ref=$PIPELINE_VERSION"
    ```

    The Kubeflow Pipelines deployment may take several minutes to complete.

2. Verify that the Kubeflow Pipelines UI is accessible by port-forwarding:

    ```shell
    kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80
    ```

    Then, open the Kubeflow Pipelines UI at `http://localhost:8080/` or - if you are
    using kind or K3s within a virtual machine - `http://{YOUR_VM_IP_ADDRESS}:8080/`
    
    Note that K3ai will automatically print the URL for the web UI at the end of
    the installation process.


    **Note**: `kubectl apply -k` accepts local paths and paths that are
    formatted as
    [hashicorp/go-getter URLs](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/remoteBuild.md#url-format).
    While the paths in the preceding commands look like URLs, they are not valid
    URLs.

## Uninstalling Kubeflow Pipelines

Below are the steps to remove Kubeflow Pipelines on kind, K3s, or K3ai:

- To uninstall Kubeflow Pipelines using your manifest file, run the following command,
  replacing `{YOUR_MANIFEST_FILE}` with the name of your manifest file:

  ```shell
  kubectl delete -k {YOUR_MANIFEST_FILE}`
  ```

- To uninstall Kubeflow Pipelines using manifests from Kubeflow Pipelines's
  GitHub repository, run these commands:

  ```shell
  export PIPELINE_VERSION={{% pipelines/latest-version %}}
  kubectl delete -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic?ref=$PIPELINE_VERSION"
  kubectl delete -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
  ```

- To uninstall Kubeflow Pipelines using manifests from your local repository or
  file system, run the following commands:

  ```shell
  kubectl delete -k manifests/kustomize/env/platform-agnostic
  kubectl delete -k manifests/kustomize/cluster-scoped-resources
  ```



================================================
File: content/en/docs/components/pipelines/legacy-v1/installation/overview.md
================================================
+++
title = "Installation Options"
description = "Overview of the ways to deploy Kubeflow Pipelines"
weight = 10
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

Kubeflow Pipelines offers a few installation options.
This page describes the options and the features available
with each option:

* [Kubeflow Pipelines Standalone](#kubeflow-pipelines-standalone) is the minimal
portable installation that only includes Kubeflow Pipelines.
* Kubeflow Pipelines as [part of a full Kubeflow deployment](/docs/started/installing-kubeflow/#kubeflow-platform) provides
all Kubeflow components and more integration with each platform.
* **Beta**: [Google Cloud AI Platform Pipelines](#google-cloud-ai-platform-pipelines) makes it easier to install and use Kubeflow Pipelines on Google Cloud by providing a management UI on [Google Cloud Console](https://console.cloud.google.com/ai-platform/pipelines/clusters).
* A [local](/docs/components/pipelines/legacy-v1/installation/localcluster-deployment) Kubeflow Pipelines deployment for testing purposes.

## Choosing an installation option

1. Do you want to use other Kubeflow components in addition to Pipelines?

    If yes, choose the [full Kubeflow deployment](/docs/started/installing-kubeflow/#kubeflow-platform).
1. Can you use a cloud/on-prem Kubernetes cluster?

    If you can't, you should try using Kubeflow Pipelines on a local Kubernetes cluster for learning and testing purposes by following the steps in [Deploying Kubeflow Pipelines on a local cluster](/docs/components/pipelines/legacy-v1/installation/localcluster-deployment).
1. Do you want to use Kubeflow Pipelines with [multi-user support](https://github.com/kubeflow/pipelines/issues/1223)?

    If yes, choose the [full Kubeflow deployment](/docs/started/installing-kubeflow/#kubeflow-platform) with version >= v1.1.
1. Do you deploy on Google Cloud?

    If yes, deploy [Kubeflow Pipelines Standalone](#kubeflow-pipelines-standalone). You can also
    use [Google Cloud AI Platform Pipelines](#google-cloud-ai-platform-pipelines) to deploy Kubeflow Pipelines
    using a user interface, but there are limitations in
    customizability and upgradability. For details, please read corresponding
    sections.
1. You deploy on other platforms.

    Please compare your platform specific [full Kubeflow](/docs/started/installing-kubeflow/#kubeflow-platform) with the
    [Kubeflow Pipelines Standalone](#kubeflow-pipelines-standalone) before making your decision.

**Warning:** Choose your installation option with caution, there's no current
supported path to migrate data between different installation options. Please
create [a GitHub issue](https://github.com/kubeflow/pipelines/issues/new/choose)
if that's important for you.


<a id="standalone"></a>
## Kubeflow Pipelines Standalone

Use this option to deploy Kubeflow Pipelines to an on-premises, cloud
or even local Kubernetes cluster, without the other components of Kubeflow.
To deploy Kubeflow Pipelines Standalone, you use kustomize manifests only.
This process makes it simpler to customize your deployment and to integrate
Kubeflow Pipelines into an existing Kubernetes cluster.

Installation guide
: [Kubeflow Pipelines Standalone deployment
  guide](/docs/components/pipelines/legacy-v1/installation/standalone-deployment/)

Interfaces
:
  * Kubeflow Pipelines UI
  * Kubeflow Pipelines SDK
  * Kubeflow Pipelines API
  * Kubeflow Pipelines endpoint is **only auto-configured** for Google Cloud.

  If you wish to deploy Kubeflow Pipelines on other platforms, you can either access it through
  `kubectl port-forward` or configure your own platform specific auth-enabled
  endpoint by yourself.

Release Schedule
: Kubeflow Pipelines Standalone is available for every Kubeflow Pipelines release.
You will have access to the latest features.

Upgrade Support (**Beta**)
: [Upgrading Kubeflow Pipelines Standalone](/docs/components/pipelines/legacy-v1/installation/standalone-deployment/#upgrading-kubeflow-pipelines) introduces how to upgrade
in place.

Google Cloud Integrations
:
  * A Kubeflow Pipelines public endpoint with auth support is **auto-configured** for you.
  * Open the Kubeflow Pipelines UI via the **Open Pipelines Dashboard** link in [the AI Platform Pipelines dashboard of Cloud Console](https://console.cloud.google.com/ai-platform/pipelines/clusters).
  * (Optional) You can choose to persist your data in Google Cloud managed storage (Cloud SQL and Cloud Storage).
  * [All options to authenticate to Google Cloud](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/authentication-pipelines/) are supported.

Notes on specific features
:
  * After deployment, your Kubernetes cluster contains Kubeflow Pipelines only.
  It does not include the other Kubeflow components.
  For example, to use a Jupyter Notebook, you must use a local notebook or a
  hosted notebook in a cloud service such as the [AI Platform
  Notebooks](https://cloud.google.com/ai-platform/notebooks/docs/).
  * Kubeflow Pipelines multi-user support is **not available** in standalone, because
  multi-user support depends on other Kubeflow components.

<a id="full-kubeflow"></a>
## Full Kubeflow deployment

Use this option to deploy Kubeflow Pipelines to your local machine, on-premises,
or to a cloud, as part of a full Kubeflow installation.

Installation guide
: [Kubeflow installation guide](/docs/started/)

Interfaces
:
  * Kubeflow UI
  * Kubeflow Pipelines UI within or outside the Kubeflow UI
  * Kubeflow Pipelines SDK
  * Kubeflow Pipelines API
  * Other Kubeflow APIs
  * Kubeflow Pipelines endpoint is auto-configured with auth support for each platform

Release Schedule
: The full Kubeflow is released quarterly. It has significant delay in receiving
Kubeflow Pipelines updates.

| Kubeflow Version       | Kubeflow Pipelines Version |
|------------------------|----------------------------|
| 0.7.0                  | 0.1.31                     |
| 1.0.0                  | 0.2.0                      |
| 1.0.2                  | 0.2.5                      |
| 1.1.0                  | 1.0.0                      |
| 1.2.0                  | 1.0.4                      |
| 1.3.0                  | 1.5.0                      |
| 1.4.0                  | 1.7.0                      |

Note: Google Cloud, AWS, and IBM Cloud have supported Kubeflow Pipelines 1.0.0 with multi-user separation. Other platforms might not be up-to-date for now, refer to [this GitHub issue](https://github.com/kubeflow/manifests/issues/1364#issuecomment-668415871) for status.

Upgrade Support
:
Refer to [the full Kubeflow section of upgrading Kubeflow Pipelines on Google Cloud](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/upgrade/#full-kubeflow) guide.

Google Cloud Integrations
:
  * A Kubeflow Pipelines public endpoint with auth support is **auto-configured** for you using [Cloud Identity-Aware Proxy](https://cloud.google.com/iap).
  * There's no current support for persisting your data in Google Cloud managed storage (Cloud SQL and Cloud Storage). Refer to [this GitHub issue](https://github.com/kubeflow/pipelines/issues/4356) for the latest status.
  * You can [authenticate to Google Cloud with Workload Identity](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/authentication-pipelines/#workload-identity).

Notes on specific features
:
  * After deployment, your Kubernetes cluster includes all the
  [Kubeflow components](/docs/components/).
  For example, you can use the Jupyter notebook services
  [deployed with Kubeflow](/docs/components/notebooks/) to create one or more notebook
  servers in your Kubeflow cluster.
  * Kubeflow Pipelines multi-user support is **only available** in full Kubeflow. It supports
  using a single Kubeflow Pipelines control plane to orchestrate user pipeline
  runs in multiple user namespaces with authorization.
  * Latest features and bug fixes may not be available soon because release
  cadence is long.

<a id="marketplace"></a>
## Google Cloud AI Platform Pipelines

{{% alert title="Beta release" color="warning" %}}
<p>Google Cloud AI Platform Pipelines is currently in <b>Beta</b> with
  limited support. The Kubeflow Pipelines team is interested in any feedback you may have,
  in particular on the usability of the feature.

  You can raise any issues or discussion items in the
  <a href="https://github.com/kubeflow/pipelines/issues">Kubeflow Pipelines
  issue tracker</a>.</p>
{{% /alert %}}

Use this option to deploy Kubeflow Pipelines to Google Kubernetes Engine (GKE)
from Google Cloud Marketplace. You can deploy Kubeflow Pipelines to an existing or new
GKE cluster and manage your cluster within Google Cloud.

Installation guide
: [Google Cloud AI Platform Pipelines documentation](https://cloud.google.com/ai-platform/pipelines/docs)

Interfaces
:
  * Google Cloud Console for managing the Kubeflow Pipelines cluster and other Google Cloud
    services
  * Kubeflow Pipelines UI via the **Open Pipelines Dashboard** link in the
    Google Cloud Console
  * Kubeflow Pipelines SDK in Cloud Notebooks
  * Kubeflow Pipelines endpoint of your instance is auto-configured for you

Release Schedule
: AI Platform Pipelines is available for a chosen set of stable Kubeflow
Pipelines releases. You will receive updates slightly slower than Kubeflow
Pipelines Standalone.

Upgrade Support (**Alpha**)
: An in-place upgrade is not supported.

To upgrade AI Platform Pipelines by reinstalling it (with existing data), refer to the [Upgrading AI Platform Pipelines](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/upgrade/#ai-platform-pipelines) guide.

Google Cloud Integrations
:
  * You can deploy AI Platform Pipelines on [Cloud Console UI](https://console.cloud.google.com/marketplace/details/google-cloud-ai-platform/kubeflow-pipelines).
  * A Kubeflow Pipelines public endpoint with auth support is **auto-configured** for you.
  * (Optional) You can choose to persist your data in Google Cloud managed storage services (Cloud SQL and Cloud Storage).
  * You can [authenticate to Google Cloud with the Compute Engine default service account](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/authentication-pipelines/#compute-engine-default-service-account). However, this method may not be suitable if you need workload permission separation.
  * You can deploy AI Platform Pipelines on both public and private GKE clusters as long as the cluster [has sufficient resources for AI Platform Pipelines](https://cloud.google.com/ai-platform/pipelines/docs/configure-gke-cluster#ensure).

Notes on specific features
:
  * After deployment, your Kubernetes cluster contains Kubeflow Pipelines only.
  It does not include the other Kubeflow components.
  For example, to use a Jupyter Notebook, you can use [AI Platform
  Notebooks](https://cloud.google.com/ai-platform/notebooks/docs/).
  * Kubeflow Pipelines multi-user support is **not available** in AI Platform Pipelines, because
  multi-user support depends on other Kubeflow components.



================================================
File: content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md
================================================
+++
title = "Standalone Deployment"
description = "Information about Standalone Deployment of Kubeflow Pipelines"
weight = 30
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

As an alternative to deploying Kubeflow Pipelines (KFP) as part of the
[Kubeflow deployment](/docs/started/installing-kubeflow), you also have a choice
to deploy only Kubeflow Pipelines. Follow the instructions below to deploy
Kubeflow Pipelines standalone using the supplied kustomize manifests.

You should be familiar with [Kubernetes](https://kubernetes.io/docs/home/),
[kubectl](https://kubernetes.io/docs/reference/kubectl/overview/), and [kustomize](https://kustomize.io/).

{{% alert title="Installation options for Kubeflow Pipelines standalone" color="info" %}}
This guide currently describes how to install Kubeflow Pipelines standalone
on Google Cloud Platform (GCP). You can also install Kubeflow Pipelines standalone on other
platforms. This guide needs updating. See [Issue 1253](https://github.com/kubeflow/website/issues/1253).
{{% /alert %}}

## Before you get started

Working with Kubeflow Pipelines Standalone requires a Kubernetes cluster as well as an installation of kubectl.

### Download and install kubectl

Download and install kubectl by following the [kubectl installation guide](https://kubernetes.io/docs/tasks/tools/install-kubectl/).

You need kubectl version 1.14 or higher for native support of kustomize.

### Set up your cluster

If you have an existing Kubernetes cluster, continue with the instructions for [configuring kubectl to talk to your cluster](#configure-kubectl).

See the GKE guide to [creating a cluster](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster) for Google Cloud Platform (GCP).

Use the [gcloud container clusters create command](https://cloud.google.com/sdk/gcloud/reference/container/clusters/create) to create a cluster that can run all Kubeflow Pipelines samples:
```
# The following parameters can be customized based on your needs.

CLUSTER_NAME="kubeflow-pipelines-standalone"
ZONE="us-central1-a"
MACHINE_TYPE="e2-standard-2" # A machine with 2 CPUs and 8GB memory.
SCOPES="cloud-platform" # This scope is needed for running some pipeline samples. Read the warning below for its security implication

gcloud container clusters create $CLUSTER_NAME \
     --zone $ZONE \
     --machine-type $MACHINE_TYPE \
     --scopes $SCOPES
```

**Note**: `e2-standard-2` doesn't support GPU. You can choose machine types that meet your need by referring to guidance in [Cloud Machine families](https://cloud.google.com/compute/docs/machine-resource).

**Warning**: Using `SCOPES="cloud-platform"` grants all GCP permissions to the cluster. For a more secure cluster setup, refer to [Authenticating Pipelines to GCP](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/authentication/#authentication-from-kubeflow-pipelines).

Note, some legacy pipeline examples may need minor code change to run on clusters with `SCOPES="cloud-platform"`, refer to [Authoring Pipelines to use default service account](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/#authoring-pipelines-to-use-default-service-account).

**References**:

  * [GCP regions and zones documentation](https://cloud.google.com/compute/docs/regions-zones/#available)

  * [gcloud command-line tool guide](https://cloud.google.com/sdk/gcloud/)

  * [gcloud command reference](https://cloud.google.com/sdk/gcloud/reference/container/clusters/create)

### Configure kubectl to talk to your cluster {#configure-kubectl}

See the Google Kubernetes Engine (GKE) guide to
[configuring cluster access for kubectl](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl).

## Deploying Kubeflow Pipelines

1. Deploy the Kubeflow Pipelines:

     ```
     export PIPELINE_VERSION={{% pipelines/latest-version %}}
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
     kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"
     ```

     The Kubeflow Pipelines deployment requires approximately 3 minutes to complete.

     **Note**: The above commands apply to Kubeflow Pipelines version 0.4.0 and higher.

     For Kubeflow Pipelines version 0.2.0 ~ 0.3.0, use:
     ```
     export PIPELINE_VERSION=<kfp-version-between-0.2.0-and-0.3.0>
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/base/crds?ref=$PIPELINE_VERSION"
     kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"
     ```

     For Kubeflow Pipelines version < 0.2.0, use:
     ```
     export PIPELINE_VERSION=<kfp-version-0.1.x>
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"
     ```

     **Note**: `kubectl apply -k` accepts local paths and paths that are formatted as [hashicorp/go-getter URLs](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/remoteBuild.md#url-format). While the paths in the preceding commands look like URLs, the paths are not valid URLs.

     {{% alert title="Deprecation Notice" color="warning" %}}
Kubeflow Pipelines will change default executor from Docker to Emissary starting KFP backend v1.8, docker executor has been
deprecated on Kubernetes 1.20+. 

For Kubeflow Pipelines before v1.8, configure to use Emissary executor by
referring to [Argo Workflow Executors](/docs/components/pipelines/legacy-v1/installation/choose-executor).
     {{% /alert %}}

1. Get the public URL for the Kubeflow Pipelines UI and use it to access the Kubeflow Pipelines UI:

     ```
     kubectl describe configmap inverse-proxy-config -n kubeflow | grep googleusercontent.com
     ```

## Upgrading Kubeflow Pipelines

1. For release notices and breaking changes, refer to [Upgrading Kubeflow Pipelines](/docs/components/pipelines/legacy-v1/installation/upgrade/).

1. Check the [Kubeflow Pipelines GitHub repository](https://github.com/kubeflow/pipelines/releases) for available releases.

1. To upgrade to Kubeflow Pipelines 0.4.0 and higher, use the following commands:
     ```
     export PIPELINE_VERSION=<version-you-want-to-upgrade-to>
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
     kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"
     ```

     To upgrade to Kubeflow Pipelines 0.3.0 and lower, use the [deployment instructions](#deploying-kubeflow-pipelines) to upgrade your Kubeflow Pipelines cluster.

1. Delete obsolete resources manually.

     Depending on the version you are upgrading from and the version you are upgrading to,
     some Kubeflow Pipelines resources may have become obsolete.

     If you are upgrading from Kubeflow Pipelines < 0.4.0 to 0.4.0 or above, you can remove the
     following obsolete resources after the upgrade:
     `metadata-deployment`, `metadata-service`.

     Run the following command to check if these resources exist on your cluster:
     ```
     kubectl -n <KFP_NAMESPACE> get deployments | grep metadata-deployment
     kubectl -n <KFP_NAMESPACE> get service | grep metadata-service
     ```

     If these resources exist on your cluster, run the following commands to delete them:
     ```
     kubectl -n <KFP_NAMESPACE> delete deployment metadata-deployment
     kubectl -n <KFP_NAMESPACE> delete service metadata-service
     ```

     For other versions, you don't need to do anything.

## Customizing Kubeflow Pipelines

Kubeflow Pipelines can be configured through kustomize [overlays](https://kubectl.docs.kubernetes.io/references/kustomize/glossary/#overlay).

To begin, first clone the [Kubeflow Pipelines GitHub repository](https://github.com/kubeflow/pipelines),
and use it as your working directory.

### Deploy on GCP with Cloud SQL and Google Cloud Storage

**Note**: This is recommended for production environments. For more details about customizing your environment
for GCP, see the [Kubeflow Pipelines GCP manifests](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/manifests/kustomize/env/gcp).

### Change deployment namespace

To deploy Kubeflow Pipelines standalone in namespace `<my-namespace>`:

1. Set the `namespace` field to `<my-namespace>` in
   [dev/kustomization.yaml](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/env/dev/kustomization.yaml) or
   [gcp/kustomization.yaml](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/env/gcp/kustomization.yaml).

1. Set the `namespace` field to `<my-namespace>` in [cluster-scoped-resources/kustomization.yaml](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/cluster-scoped-resources/kustomization.yaml)

1. Apply the changes to update the Kubeflow Pipelines deployment:

     ```
     kubectl apply -k manifests/kustomize/cluster-scoped-resources
     kubectl apply -k manifests/kustomize/env/dev
     ```

     **Note**: If using GCP Cloud SQL and Google Cloud Storage, set the proper values in [manifests/kustomize/env/gcp/params.env](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/env/gcp/params.env), then apply with this command:

     ```
     kubectl apply -k manifests/kustomize/cluster-scoped-resources
     kubectl apply -k manifests/kustomize/env/gcp
     ```

### Disable the public endpoint

By default, the KFP standalone deployment installs an [inverting proxy agent](https://github.com/google/inverting-proxy) that exposes a public URL. If you want to skip the installation of the inverting proxy agent, complete the following:

1. Comment out the proxy components in the base `kustomization.yaml`. For example in [manifests/kustomize/env/dev/kustomization.yaml](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/env/dev/kustomization.yaml) comment out `inverse-proxy`.

1. Apply the changes to update the Kubeflow Pipelines deployment:

     ```
     kubectl apply -k manifests/kustomize/env/dev
     ```

     **Note**: If using GCP Cloud SQL and Google Cloud Storage, set the proper values in [manifests/kustomize/env/gcp/params.env](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/env/gcp/params.env), then apply with this command:

     ```
     kubectl apply -k manifests/kustomize/env/gcp
     ```

1. Verify that the Kubeflow Pipelines UI is accessible by port-forwarding:

     ```
     kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80
     ```

1. Open the Kubeflow Pipelines UI at `http://localhost:8080/`.

## Uninstalling Kubeflow Pipelines

To uninstall Kubeflow Pipelines, run `kubectl delete -k <manifest-file>`.

For example, to uninstall KFP using manifests from a GitHub repository, run:

```
export PIPELINE_VERSION={{% pipelines/latest-version %}}
kubectl delete -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"
kubectl delete -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
```

To uninstall KFP using manifests from your local repository or file system, run:

```
kubectl delete -k manifests/kustomize/env/dev
kubectl delete -k manifests/kustomize/cluster-scoped-resources
```

**Note**: If you are using GCP Cloud SQL and Google Cloud Storage, run:

```
kubectl delete -k manifests/kustomize/env/gcp
kubectl delete -k manifests/kustomize/cluster-scoped-resources
```

## Best practices for maintaining manifests

Similar to source code, configuration files belong in source control.
A repository manages the changes to your
manifest files and ensures that you can repeatedly deploy, upgrade,
and uninstall your components.

### Maintain your manifests in source control

After creating or customizing your deployment manifests, save your manifests
to a local or remote source control repository.
For example, save the following `kustomization.yaml`:

```
# kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
# Edit the following to change the deployment to your custom namespace.
namespace: kubeflow
# You can add other customizations here using kustomize.
# Edit ref in the following link to deploy a different version of Kubeflow Pipelines.
bases:
- github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref={{% pipelines/latest-version %}}
```

### Further reading

* To learn about kustomize workflows with off-the-shelf configurations, see the
[kustomize configuration guide](https://kubectl.docs.kubernetes.io/guides/config_management/offtheshelf/).


## Troubleshooting

* If your pipelines are stuck in ContainerCreating state and it has pod events like
```
MountVolume.SetUp failed for volume "gcp-credentials-user-gcp-sa" : secret "user-gcp-sa" not found
```

You should remove `use_gcp_secret` usages as documented in [Authenticating Pipelines to GCP](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/authentication-pipelines/#authoring-pipelines-to-use-workload-identity).


## What's next

* [Connecting to Kubeflow Pipelines standalone on Google Cloud using the SDK](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/authentication-sdk/#connecting-to-kubeflow-pipelines-standalone-or-ai-platform-pipelines)
* [Authenticating Pipelines to GCP](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/authentication-pipelines/#authoring-pipelines-to-use-workload-identity) if you want to use GCP services in Kubeflow Pipelines.



================================================
File: content/en/docs/components/pipelines/legacy-v1/installation/upgrade.md
================================================
+++
title = "Upgrade Notes"
description = "Notices and breaking changes when you upgrade Kubeflow Pipelines Backend"
weight = 90
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This page introduces notices and breaking changes you need to know when upgrading Kubeflow Pipelines Backend.

For upgrade instructions, refer to distribution specific documentations:

* [Upgrading Kubeflow Pipelines on Google Cloud](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/upgrade/)

## Upgrading to v2.0

* **Notice**: In v2.0 frontend, run metrics columns are deprecated in the run list page, but users can still get the same information by using [KFPv2 Scalar metrics](/docs/components/pipelines/legacy-v1/sdk/output-viewer/#scalar-metrics)

## Upgrading to [v1.7]

[v1.7]: https://github.com/kubeflow/pipelines/releases/tag/1.7.0

* **Breaking Change**: Metadata UI and visualizations are not compatible with TensorFlow Extended (TFX) <= v1.0.0. Upgrade to v1.2.0 or above, refer to [Kubeflow Pipelines Backend and TensorFlow Extended (TFX) compatibility matrix](/docs/components/pipelines/legacy-v1/installation/compatibility-matrix/).

* **Notice**: Emissary executor (Alpha), a new argo workflow executor is available as an option. Due to [Kubernetes deprecating Docker as a container runtime after v1.20](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/), emissary may become the default workflow executor for Kubeflow Pipelines in the near future.

    For example, the current default docker executor does not work on Google Kubernetes Engine (GKE) 1.19+ out of the box. To use docker executor, your cluster node image must be configured to use docker (deprecated) as container runtime.

    Alternatively, using emissary executor (Alpha) removes the restriction on container runtime, but note some of your pipelines may require manual migrations. The Kubeflow Pipelines team welcomes your feedback in [the Emissary Executor feedback github issue](https://github.com/kubeflow/pipelines/issues/6249).

    For detailed configuration and migration instructions for both options, refer to [Argo Workflow Executors](/docs/components/pipelines/legacy-v1/installation/choose-executor/).

* **Notice**: Kubeflow Pipelines SDK v2 compatibility mode (Beta) was recently released. The new mode adds support for tracking pipeline runs and artifacts using ML Metadata. In v1.7 backend, complete UI support and caching capabilities for v2 compatibility mode are newly added. We welcome any [feedback](https://github.com/kubeflow/pipelines/issues/6451) on positive experiences or issues you encounter.



================================================
File: content/en/docs/components/pipelines/legacy-v1/overview/_index.md
================================================
+++
title = "Overview"
description = "Overview of Kubeflow Pipelines"
weight = 20
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}



================================================
File: content/en/docs/components/pipelines/legacy-v1/overview/caching.md
================================================
+++
title = "Caching"
description = "Getting started with Kubeflow Pipelines step caching"
weight = 40
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

## Before you start

This guide tells you the basic concepts of Kubeflow Pipelines step caching and how to use it. 
This guide assumes that you already have Kubeflow Pipelines installed or want to use options in the [Kubeflow Pipelines deployment guide](/docs/components/pipelines/operator-guides/installation/) to deploy Kubeflow Pipelines.

## What is step caching?

Kubeflow Pipelines caching provides step-level output caching. 
And caching is enabled by default for all pipelines submitted through the KFP backend and UI. 
The exception is pipelines authored using TFX SDK which has its own caching mechanism. 
The cache key calculation is based on the component (base image, command-line, code), arguments passed to the component (values or artifacts) and any additional customizations. 
If the component is exactly the same and the arguments are exactly the same as in some previous execution, then the task can be skipped and the outputs of the old step can be used. 
The cache reuse behavior can be controlled and the pipeline author can specify the maximum staleness of the cached data considered for reuse. 
With caching enabled, the system can skip a step that has already been executed which saves time and money. 

## Disabling/enabling caching

Cache is enabled by default after Kubeflow Pipelines 0.4. 
These are instructions on disabling and enabling cache service:

### Configure access to your Kubeflow cluster

Use the following instructions to configure `kubectl` with access to your
Kubeflow cluster. 

1.  To check if you have `kubectl` installed, run the following command:

    ```bash
    which kubectl
    ```

    The response should be something like this:

    ```bash
    /usr/bin/kubectl
    ```

    If you do not have `kubectl` installed, follow the instructions in the
    guide to [installing and setting up kubectl][kubectl-install].

2.  Follow the [guide to configuring access to Kubernetes
    clusters][kubectl-access]. 

### Disabling caching in your Kubeflow Pipelines deployment:

1. Make sure `mutatingwebhookconfiguration` exists in your cluster:

    ```
    export NAMESPACE=<Namespace where KFP is installed>
    kubectl get mutatingwebhookconfiguration cache-webhook-${NAMESPACE}
    ```
2. Change `mutatingwebhookconfiguration` rules:

    ```
    kubectl patch mutatingwebhookconfiguration cache-webhook-${NAMESPACE} --type='json' -p='[{"op":"replace", "path": "/webhooks/0/rules/0/operations/0", "value": "DELETE"}]'
    ```

### Enabling caching

1. Make sure `mutatingwebhookconfiguration` exists in your cluster:

    ```
    export NAMESPACE=<Namespace where KFP is installed>
    kubectl get mutatingwebhookconfiguration cache-webhook-${NAMESPACE}
    ```
2. Change back `mutatingwebhookconfiguration` rules:

    ```
    kubectl patch mutatingwebhookconfiguration cache-webhook-${NAMESPACE} --type='json' -p='[{"op":"replace", "path": "/webhooks/0/rules/0/operations/0", "value": "CREATE"}]'
    ```

## Managing caching staleness

The cache is enabled by default and if you ever executed same component with the same arguments, any new execution of the component will be skipped and the outputs will be taken from the cache.
For some scenarios, the cached output data of some components might become too stale for use after some time.
To control the maximum staleness of the reused cached data, you can set the step's `max_cache_staleness` parameter.
The `max_cache_staleness` is in [RFC3339 Duration](https://www.ietf.org/rfc/rfc3339.txt) format (so 30 days = "P30D"). 
By default the `max_cache_staleness` is set to infinity so any old cached data will be reused.

Set `max_cache_staleness` to 30 days for a step:

```
def some_pipeline():
      # task is a target step in a pipeline
      task = some_op()
      task.execution_options.caching_strategy.max_cache_staleness = "P30D"
```

Ideally, the component code should be pure and deterministic in the sense that it produces same outputs given same inputs.
If your component is not deterministic (for example, it returns a different random number on every invocation) you might want to disable caching for the tasks created from this component by setting `max_cache_staleness` to 0:

```
def some_pipeline():
      # task is a target step in a pipeline
      task_never_use_cache = some_op()
      task_never_use_cache.execution_options.caching_strategy.max_cache_staleness = "P0D"
```
A better solution would be to make the component deterministic. If the component uses random number generation, you can expose the RNG seed as a component input. If the component fetches some changing data you can add a timestamp or date input.

[kubectl-access]: https://kubernetes.io/docs/reference/access-authn-authz/authentication/
[kubectl-install]: https://kubernetes.io/docs/tasks/tools/install-kubectl/


================================================
File: content/en/docs/components/pipelines/legacy-v1/overview/quickstart.md
================================================
+++
title = "Quickstart"
description = "Getting started with Kubeflow Pipelines"
weight = 10

+++ 
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

Use this guide if you want to get an introduction to the Kubeflow Pipelines user interface (UI) and get a simple pipeline running quickly. 

The goal with this quickstart guide is to shows how to use two of the samples that come with 
the Kubeflow Pipelines installation and are visible on the Kubeflow Pipelines
UI. You can use this guide as an introduction to the 
Kubeflow Pipelines UI.

## Deploy Kubeflow and open the Kubeflow Pipelines UI

There are several options to [deploy Kubeflow Pipelines](/docs/components/pipelines/legacy-v1/overview/), follow the option that best suits your needs. If you are uncertain and just want to try out kubeflow pipelines it is recommended to start with the [standalone deployment](/docs/components/pipelines/legacy-v1/installation/standalone-deployment/).

Once you have deployed Kubeflow Pipelines, make sure you can access the UI. The steps to access the UI vary based on the method you used to deploy Kubeflow Pipelines.

## Run a basic pipeline

Kubeflow Pipelines offers a few samples that you can use to try out
Kubeflow Pipelines quickly. The steps below show you how to run a basic sample that
includes some Python operations, but doesn't include a machine learning (ML) 
workload:

1. Click the name of the sample, **[Tutorial] Data passing in python components**, on the pipelines UI:
  <img src="/docs/images/click-pipeline-sample.png" 
    alt="Pipelines UI"
    class="mt-3 mb-3 border border-info rounded">

1. Click **Create experiment**:
  <img src="/docs/images/pipelines-start-experiment.png" 
    alt="Creating an experiment on the pipelines UI"
    class="mt-3 mb-3 border border-info rounded">

1. Follow the prompts to create an **experiment** and then create a **run**. 
  The sample supplies default values for all the parameters you need. The 
  following screenshot assumes you've already created an experiment named
  _My experiment_ and are now creating a run named _My first run_:
  <img src="/docs/images/pipelines-start-run.png" 
    alt="Creating a run on the pipelines UI"
    class="mt-3 mb-3 border border-info rounded">

1. Click **Start** to run the pipeline.
1. Click the name of the run on the experiments dashboard:
  <img src="/docs/images/pipelines-experiments-dashboard.png" 
    alt="Experiments dashboard on the pipelines UI"
    class="mt-3 mb-3 border border-info rounded">

1. Explore the graph and other aspects of your run by clicking on the 
  components of the graph and the other UI elements:
  <img src="/docs/images/pipelines-basic-run.png" 
    alt="Run results on the pipelines UI"
    class="mt-3 mb-3 border border-info rounded">

You can find the [source code for the **Data passing in python components** tutorial](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/tutorials/Data%20passing%20in%20python%20components) in the Kubeflow Pipelines repo.

## Run an ML pipeline

This section shows you how to run the XGBoost sample available
from the pipelines UI. Unlike the basic sample described above, the
XGBoost sample does include ML components. 

Follow these steps to run the sample:

1. Click the name of the sample, 
  **[Demo] XGBoost - Iterative model training**, on the pipelines UI:
  <img src="/docs/images/click-xgboost-sample.png" 
    alt="XGBoost sample on the pipelines UI"
    class="mt-3 mb-3 border border-info rounded">

1. Click **Create experiment**.
1. Follow the prompts to create an **experiment** and then create a **run**.

    The following screenshot shows the run details:
    <img src="/docs/images/pipelines-start-xgboost-run.png" 
      alt="Starting the XGBoost run on the pipelines UI"
      class="mt-3 mb-3 border border-info rounded">

1. Click **Start** to create the run.
1. Click the name of the run on the experiments dashboard.
1. Explore the graph and other aspects of your run by clicking on the 
  components of the graph and the other UI elements. The following screenshot
  shows part of the graph when the pipeline has finished running:
    <img src="/docs/images/pipelines-xgboost-graph.png" 
      alt="XGBoost results on the pipelines UI"
      class="mt-3 mb-3 border border-info rounded">

You can find the [source code for the **XGBoost - Iterative model training** demo](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core/xgboost_training_cm) in the Kubeflow Pipelines repo.

## Next steps

* Learn more about the 
  [important concepts](/docs/components/pipelines/concepts/) in Kubeflow
  Pipelines.
* This page showed you how to run some of the examples supplied in the Kubeflow
  Pipelines UI. Next, you may want to run a pipeline from a notebook, or compile 
  and run a sample from the code. See the guide to experimenting with
  [the Kubeflow Pipelines samples](/docs/components/pipelines/legacy-v1/tutorials/build-pipeline/).
* Build your own machine-learning pipelines with the [Kubeflow Pipelines 
  SDK](/docs/components/pipelines/legacy-v1/sdk/sdk-overview/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/reference/_index.md
================================================
+++
title = "Reference"
description = "Reference docs for Kubeflow Pipelines Version 1"
weight = 100
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}



================================================
File: content/en/docs/components/pipelines/legacy-v1/reference/sdk.md
================================================
+++
title = " Pipelines SDK Reference"
description = "Reference documentation for the Kubeflow Pipelines SDK Version 1"
weight = 20
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

See the [generated reference docs for the Python 
SDK](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/) (hosted on 
*Read the Docs*).



================================================
File: content/en/docs/components/pipelines/legacy-v1/reference/api/kubeflow-pipeline-api-spec.md
================================================
+++
title = "Pipelines API Reference (v1beta1)"
description = "API Reference for Kubeflow Pipelines API - v1beta1"
weight = 3
+++

{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This document describes the API specification for the `v1beta1` Kubeflow Pipelines REST API.

## About the REST API

In most deployments of the [Kubeflow Platform](/docs/started/installing-kubeflow/#kubeflow-platform), the Kubeflow Pipelines REST API is available under the `/pipeline/` HTTP path.
For example, if you host Kubeflow at `https://kubeflow.example.com`, the API will be available at `https://kubeflow.example.com/pipeline/`.

{{% alert title="Tip" color="dark" %}}
We recommend using the [Kubeflow Pipelines Python SDK](/docs/components/pipelines/legacy-v1/reference/sdk/) as it provides a more user-friendly interface.
See the [Connect SDK to the API](/docs/components/pipelines/user-guides/core-functions/connect-api/) guide for more information.
{{% /alert %}}

### Authentication

How requests are authenticated and authorized will depend on the distribution you are using.
Typically, you will need to provide a token or cookie in the request headers.

Please refer to the documentation of your [Kubeflow distribution](/docs/started/installing-kubeflow/#kubeflow-platform) for more information.

### Example Usage

To use the API, you will need to send HTTP requests to the appropriate endpoints.

For example, to list pipeline runs in the `team-1` namespace, send a `GET` request to the following URL:

```
https://kubeflow.example.com/pipeline/apis/v1beta1/runs?resource_reference_key.type=NAMESPACE&resource_reference_key.id=team-1
```

## Swagger UI

The following [Swagger UI](https://github.com/swagger-api/swagger-ui) is automatically generated from the [`{{% pipelines/latest-version %}}`](https://github.com/kubeflow/pipelines/releases/tag/{{% pipelines/latest-version %}}) version of Kubeflow Pipelines for the [`v1beta1` REST API](https://github.com/kubeflow/pipelines/blob/{{% pipelines/latest-version %}}/backend/api/v1beta1/swagger/kfp_api_single_file.swagger.json).

{{% alert title="Note" color="info" %}}
The _try it out_ feature of Swagger UI does not work due to authentication and CORS, but it can help you construct the correct API calls.
{{% /alert %}}

{{< swaggerui-inline component_name="Kubeflow Pipelines" default_input_url="https://kubeflow.example.com/pipeline/" >}}
https://raw.githubusercontent.com/kubeflow/pipelines/{{% pipelines/latest-version %}}/backend/api/v1beta1/swagger/kfp_api_single_file.swagger.json
{{< /swaggerui-inline >}}



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/_index.md
================================================
+++
title = "Pipelines SDK"
description = "Information about the Kubeflow Pipelines SDK"
weight = 40
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}


================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/best-practices.md
================================================
+++
title = "Best Practices for Designing Components"
description = "Designing and writing components for Kubeflow Pipelines"
weight = 60
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This page describes some recommended practices for designing
components. For an application of these best practices, see the
[component development guide](/docs/components/pipelines/legacy-v1/sdk/component-development). If 
you're new to pipelines, see the conceptual guides to 
[pipelines](/docs/components/pipelines/concepts/pipeline/)
and [components](/docs/components/pipelines/concepts/component/).

<a id="general"></a>
### General component design rules

*   Design your components with composability in mind. Think about
    upstream and downstream components. What formats to consume as inputs from
    the upstream components. What formats to use for output data so that
    downstream components can consume it.
*   Component code must use local files for input/output data (unless impossible
    - for example, Cloud ML Engine and BigQuery require Cloud Storage staging
    paths).
*   Components must be *pure* - they must not use any outside data except data
    that comes through inputs (unless impossible). Everything should either be
    inside the container or come from inputs. Network access is strongly
    discouraged unless that's the explicit purpose of a component (for example,
    upload/download).

## Writing component code

*   The program must be runnable both locally and inside the Docker
    container.
*   Programming languages:

    *   Generally, use the language that makes the most sense. If the
        component wraps a Java library, then it may make sense to use Java to
        expose that library.
    *   For most new components when the performance is not a concern
        the Python language is preferred (use version 3 wherever possible).
    *   If a component wraps an existing program, it's preferred to
        directly expose the program in the component command line.
    *   If there needs to be some wrapper around the program (small
        pre-processing or post-processing like file renaming), it can be done
        with a shell script.
    *   Follow the best practices for the chosen language.

*   Each output data piece should be written to a separate file (see next line).
*   The input and output file paths must be passed in the command line and
    not hard coded:

    *   Typical command line:

        ```
        program.py --input-data <input path> --output-data <output path> --param 42
        ```

    *   Do NOT hardcode paths in the program:
    
        ```
        open("/output.txt", "w")
        ```

*   For temporary data you should use library functions that create
    temporary files. For example, for Python use
    [https://docs.python.org/3/library/tempfile.html](https://docs.python.org/3/library/tempfile.html).
    Do not just write to the root, or testing will be hard.
*   Design the code to be testable.

## Writing tests

*   Follow the [general rules](#general) section so that writing the tests is
    easier.
*   Use the unit testing libraries that are standard for the language you're
    using.
*   Try to design the component code so that it can be tested using unit tests.
    Do not use network unless necessary.

*   Prepare small input data files so that the component code can be tested in
    isolation. For example, for an ML prediction component prepare a small model
    and evaluation dataset.

*   Use testing best practices.

    *   Test the expected behavior of the code. Don't just verify that
        "nothing has changed":

        *   For training you can look at loss at final iteration.
        *   For prediction you can look at the result metrics.
        *   For data augmenting you can check for some desired post-invariants.

*   If the component cannot be tested locally or in isolation, then create a
    small proof-of-concept pipeline that tests the component. You can use
    conditionals to verify the output values of a particular task and only
    enable the "success" task if the results are expected.

## Writing a Dockerfile

*   Follow the 
    [Docker best practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)

*   Structure the Dockerfile so that the required packages are installed
    first and the main component scripts/binaries are added last. Ideally, split
    the Dockerfile in two parts (base image and component code) so that the
    main component image build is fast and more reliable (not require network
    access).

## Writing a component specification YAML file

For the complete definition of a Kubeflow Pipelines component, see the
[component specification](/docs/components/pipelines/reference/component-spec/).
When creating your `component.yaml` file, you can look at the definitions for 
some
[existing components](https://github.com/kubeflow/pipelines/search?q=filename%3Acomponent.yaml&unscoped_q=filename%3Acomponent.yaml).

*   Use the `{inputValue: Input name}` command-line placeholder for small
    values that should be directly inserted into the command-line.
*   Use the `{inputPath: Input name}` command-line placeholder for input file
    locations.
*   Use the `{outputPath: Output name}` command-line placeholder for output file
    locations.
*   Specify the full command line in ‘command:' instead of just arguments to the
    entry point.


================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Build a Pipeline
> A tutorial on building pipelines to orchestrate your ML workflow


A Kubeflow pipeline is a portable and scalable definition of a machine learning
(ML) workflow. Each step in your ML workflow, such as preparing data or
training a model, is an instance of a pipeline component. This document
provides an overview of pipeline concepts and best practices, and instructions
describing how to build an ML pipeline.

## Before you begin

1.  Run the following command to install the Kubeflow Pipelines SDK. If you run this command in a Jupyter
    notebook, restart the kernel after installing the SDK. 
"""

!pip install kfp --upgrade

"""
2.  Import the `kfp` and `kfp.components` packages.
"""

import kfp
import kfp.components as comp

"""
## Understanding pipelines

A Kubeflow pipeline is a portable and scalable definition of an ML workflow,
based on containers. A pipeline is composed of a set of input parameters and a
list of the steps in this workflow. Each step in a pipeline is an instance of a
component, which is represented as an instance of 
[`ContainerOp`][container-op].

You can use pipelines to:

*   Orchestrate repeatable ML workflows.
*   Accelerate experimentation by running a workflow with different sets of
    hyperparameters.

### Understanding pipeline components

A pipeline component is a containerized application that performs one step in a
pipeline's workflow. Pipeline components are defined in
[component specifications][component-spec], which define the following:

*   The component's interface, its inputs and outputs.
*   The component's implementation, the container image and the command to
    execute.
*   The component's metadata, such as the name and description of the
    component.

You can build components by [defining a component specification for a
containerized application][component-dev], or you can [use the Kubeflow
Pipelines SDK to generate a component specification for a Python
function][python-function-component]. You can also [reuse prebuilt components
in your pipeline][prebuilt-components]. 

### Understanding the pipeline graph

Each step in your pipeline's workflow is an instance of a component. When
you define your pipeline, you specify the source of each step's inputs. Step
inputs can be set from the pipeline's input arguments, constants, or step
inputs can depend on the outputs of other steps in this pipeline. Kubeflow
Pipelines uses these dependencies to define your pipeline's workflow as
a graph.

For example, consider a pipeline with the following steps: ingest data,
generate statistics, preprocess data, and train a model. The following
describes the data dependencies between each step.

*   **Ingest data**: This step loads data from an external source which is
    specified using a pipeline argument, and it outputs a dataset. Since
    this step does not depend on the output of any other steps, this step
    can run first.
*   **Generate statistics**: This step uses the ingested dataset to generate
    and output a set of statistics. Since this step depends on the dataset
    produced by the ingest data step, it must run after the ingest data step.
*   **Preprocess data**: This step preprocesses the ingested dataset and
    transforms the data into a preprocessed dataset. Since this step depends
    on the dataset produced by the ingest data step, it must run after the
    ingest data step.
*   **Train a model**: This step trains a model using the preprocessed dataset,
    the generated statistics, and pipeline parameters, such as the learning
    rate. Since this step depends on the preprocessed data and the generated
    statistics, it must run after both the preprocess data and generate
    statistics steps are complete.

Since the generate statistics and preprocess data steps both depend on the
ingested data, the generate statistics and preprocess data steps can run in
parallel. All other steps are executed once their data dependencies are
available.

## Designing your pipeline

When designing your pipeline, think about how to split your ML workflow into
pipeline components. The process of splitting an ML workflow into pipeline
components is similar to the process of splitting a monolithic script into
testable functions. The following rules can help you define the components
that you need to build your pipeline.

*   Components should have a single responsibility. Having a single
    responsibility makes it easier to test and reuse a component. For example,
    if you have a component that loads data you can reuse that for similar
    tasks that load data. If you have a component that loads and transforms
    a dataset, the component can be less useful since you can use it only when
    you need to load and transform that dataset. 

*   Reuse components when possible. Kubeflow Pipelines provides [components for
    common pipeline tasks and for access to cloud
    services][prebuilt-components].

*   Consider what you need to know to debug your pipeline and research the
    lineage of the models that your pipeline produces. Kubeflow Pipelines
    stores the inputs and outputs of each pipeline step. By interrogating the
    artifacts produced by a pipeline run, you can better understand the
    variations in model quality between runs or track down bugs in your
    workflow.

In general, you should design your components with composability in mind. 

Pipelines are composed of component instances, also called steps. Steps can
define their inputs as depending on the output of another step. The
dependencies between steps define the pipeline workflow graph.

### Building pipeline components

Kubeflow pipeline components are containerized applications that perform a
step in your ML workflow. Here are the ways that you can define pipeline
components:

*   If you have a containerized application that you want to use as a
    pipeline component, create a component specification to define this
    container image as a pipeline component.
    
    This option provides the flexibility to include code written in any
    language in your pipeline, so long as you can package the application
    as a container image. Learn more about [building pipeline
    components][component-dev].

*   If your component code can be expressed as a Python function, [evaluate if
    your component can be built as a Python function-based
    component][python-function-component]. The Kubeflow Pipelines SDK makes it
    easier to build lightweight Python function-based components by saving you
    the effort of creating a component specification.

Whenever possible, [reuse prebuilt components][prebuilt-components] to save
yourself the effort of building custom components.

The example in this guide demonstrates how to build a pipeline that uses a
Python function-based component and reuses a prebuilt component.

### Understanding how data is passed between components

When Kubeflow Pipelines runs a component, a container image is started in a
Kubernetes Pod and your component’s inputs are passed in as command-line
arguments. When your component has finished, the component's outputs are
returned as files.

In your component's specification, you define the components inputs and outputs
and how the inputs and output paths are passed to your program as command-line
arguments. You can pass small inputs, such as short strings or numbers, to your
component by value. Large inputs, such as datasets, must be passed to your
component as file paths. Outputs are written to the paths that Kubeflow
Pipelines provides.

Python function-based components make it easier to build pipeline components
by building the component specification for you. Python function-based
components also handle the complexity of passing inputs into your component
and passing your function’s outputs back to your pipeline.

Learn more about how [Python function-based components handle inputs and
outputs][python-function-component-data-passing]. 

## Getting started building a pipeline

The following sections demonstrate how to get started building a Kubeflow
pipeline by walking through the process of converting a Python script into
a pipeline.

### Design your pipeline

The following steps walk through some of the design decisions you may face
when designing a pipeline.

1.  Evaluate the process. In the following example, a Python function downloads
    a zipped tar file (`.tar.gz`) that contains several CSV files, from a
    public website. The function extracts the CSV files and then merges them
    into a single file.

[container-op]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.ContainerOp
[component-spec]: https://www.kubeflow.org/docs/components/pipelines/reference/component-spec/
[python-function-component]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/python-function-components/
[component-dev]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/component-development/
[python-function-component-data-passing]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/python-function-components/#understanding-how-data-is-passed-between-components
[prebuilt-components]: https://www.kubeflow.org/docs/examples/shared-resources/
"""

import glob
import pandas as pd
import tarfile
import urllib.request
    
def download_and_merge_csv(url: str, output_csv: str):
  with urllib.request.urlopen(url) as res:
    tarfile.open(fileobj=res, mode="r|gz").extractall('data')
  df = pd.concat(
      [pd.read_csv(csv_file, header=None) 
       for csv_file in glob.glob('data/*.csv')])
  df.to_csv(output_csv, index=False, header=False)

"""
2.  Run the following Python command to test the function. 
"""

download_and_merge_csv(
    url='https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz', 
    output_csv='merged_data.csv')

"""
3.  Run the following to print the first few rows of the
    merged CSV file.
"""

!head merged_data.csv

"""
4.  Design your pipeline. For example, consider the following pipeline designs.

    *   Implement the pipeline using a single step. In this case, the pipeline
        contains one component that works similarly to the example function.
        This is a straightforward function, and implementing a single-step
        pipeline is a reasonable approach in this case.
        
        The down side of this approach is that the zipped tar file would not be
        an artifact of your pipeline runs. Not having this artifact available 
        could make it harder to debug this component in production.
        
    *   Implement this as a two-step pipeline. The first step downloads a file
        from a website. The second step extracts the CSV files from a zipped
        tar file and merges them into a single file. 
        
        This approach has a few benefits:
        
        *   You can reuse the [Web Download component][web-download-component]
            to implement the first step.
        *   Each step has a single responsibility, which makes the components
            easier to reuse.
        *   The zipped tar file is an artifact of the first pipeline step.
            This means that you can examine this artifact when debugging
            pipelines that use this component.
    
    This example implements a two-step pipeline.

### Build your pipeline components

        
1.  Build your pipeline components. This example modifies the initial script to
    extract the contents of a zipped tar file, merge the CSV files that were
    contained in the zipped tar file, and return the merged CSV file.
    
    This example builds a Python function-based component. You can also package
    your component's code as a Docker container image and define the component
    using a ComponentSpec.
    
    In this case, the following modifications were required to the original
    function.

    *   The file download logic was removed. The path to the zipped tar file
        is passed as an argument to this function.
    *   The import statements were moved inside of the function. Python
        function-based components require standalone Python functions. This
        means that any required import statements must be defined within the
        function, and any helper functions must be defined within the function.
        Learn more about [building Python function-based
        components][python-function-components].
    *   The function's arguments are decorated with the
        [`kfp.components.InputPath`][input-path] and the
        [`kfp.components.OutputPath`][output-path] annotations. These
        annotations let Kubeflow Pipelines know to provide the path to the
        zipped tar file and to create a path where your function stores the
        merged CSV file. 
        
    The following example shows the updated `merge_csv` function.

[web-download-component]: https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/components/web/Download/component.yaml
[python-function-components]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/python-function-components/
[input-path]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html?highlight=inputpath#kfp.components.InputPath
[output-path]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html?highlight=outputpath#kfp.components.OutputPath
"""

def merge_csv(file_path: comp.InputPath('Tarball'),
              output_csv: comp.OutputPath('CSV')):
  import glob
  import pandas as pd
  import tarfile

  tarfile.open(name=file_path, mode="r|gz").extractall('data')
  df = pd.concat(
      [pd.read_csv(csv_file, header=None) 
       for csv_file in glob.glob('data/*.csv')])
  df.to_csv(output_csv, index=False, header=False)

"""
2.  Use [`kfp.components.create_component_from_func`][create_component_from_func]
    to return a factory function that you can use to create pipeline steps.
    This example also specifies the base container image to run this function
    in, the path to save the component specification to, and a list of PyPI
    packages that need to be installed in the container at runtime.

[create_component_from_func]: (https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.create_component_from_func
[container-op]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp
"""

create_step_merge_csv = kfp.components.create_component_from_func(
    func=merge_csv,
    output_component_file='component.yaml', # This is optional. It saves the component spec for future use.
    base_image='python:3.7',
    packages_to_install=['pandas==1.1.4'])

"""
### Build your pipeline

1.  Use [`kfp.components.load_component_from_url`][load_component_from_url]
    to load the component specification YAML for any components that you are
    reusing in this pipeline.

[load_component_from_url]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html?highlight=load_component_from_url#kfp.components.load_component_from_url
"""

web_downloader_op = kfp.components.load_component_from_url(
    'https://raw.githubusercontent.com/kubeflow/pipelines/3fa2ac5f4e04111bf5758fd5c01a2f0d7ac4b866/components/contrib/web/Download/component.yaml')

"""
2.  Define your pipeline as a Python function. 

    Your pipeline function's arguments define your pipeline's parameters. Use
    pipeline parameters to experiment with different hyperparameters, such as
    the learning rate used to train a model, or pass run-level inputs, such as
    the path to an input file, into a pipeline run.
    
    Use the factory functions created by
    `kfp.components.create_component_from_func` and
    `kfp.components.load_component_from_url` to create your pipeline's tasks. 
    The inputs to the component factory functions can be pipeline parameters,
    the outputs of other tasks, or a constant value. In this case, the
    `web_downloader_task` task uses the `url` pipeline parameter, and the
    `merge_csv_task` uses the `data` output of the `web_downloader_task`.
    
"""

# Define a pipeline and create a task from a component:
def my_pipeline(url):
  web_downloader_task = web_downloader_op(url=url)
  merge_csv_task = create_step_merge_csv(file=web_downloader_task.outputs['data'])
  # The outputs of the merge_csv_task can be referenced using the
  # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']

"""
### Compile and run your pipeline

After defining the pipeline in Python as described in the preceding section, use one of the following options to compile the pipeline and submit it to the Kubeflow Pipelines service.

#### Option 1: Compile and then upload in UI

1.  Run the following to compile your pipeline and save it as `pipeline.yaml`. 

"""

kfp.compiler.Compiler().compile(
    pipeline_func=my_pipeline,
    package_path='pipeline.yaml')

"""
2.  Upload and run your `pipeline.yaml` using the Kubeflow Pipelines user interface.
See the guide to [getting started with the UI][quickstart].

[quickstart]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/overview/quickstart
"""

"""
#### Option 2: run the pipeline using Kubeflow Pipelines SDK client

1.  Create an instance of the [`kfp.Client` class][kfp-client] following steps in [connecting to Kubeflow Pipelines using the SDK client][connect-api].

[kfp-client]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/client.html#kfp.Client
[connect-api]: https://www.kubeflow.org/docs/components/pipelines/user-guides/core-functions/connect-api
"""

client = kfp.Client() # change arguments accordingly

"""
2.  Run the pipeline using the `kfp.Client` instance:
"""

client.create_run_from_pipeline_func(
    my_pipeline,
    arguments={
        'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'
    })

"""

## Next steps

*   Learn about advanced pipeline features, such as [authoring recursive
    components][recursion] and [using conditional execution in a
    pipeline][conditional].
*   Learn how to [manipulate Kubernetes resources in a
    pipeline][k8s-resources] (Experimental).

[conditional]: https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/tutorials/DSL%20-%20Control%20structures/DSL%20-%20Control%20structures.py
[recursion]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/dsl-recursion/
[k8s-resources]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/manipulate-resources/
"""



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.md
================================================
+++
title = "Build a Pipeline"
description = "A tutorial on building pipelines to orchestrate your ML workflow"
weight = 30
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

<!--
AUTOGENERATED FROM content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb
PLEASE UPDATE THE JUPYTER NOTEBOOK AND REGENERATE THIS FILE USING scripts/nb_to_md.py.-->

<style>
.notebook-links {display: flex; margin: 1em 0;}
.notebook-links a {padding: .75em; margin-right: .75em; font-weight: bold;}
a.colab-link {
padding-left: 3.25em;
background-image: url(/docs/images/logos/colab.ico);
background-repeat: no-repeat;
background-size: contain;
}
a.github-link {
padding-left: 2.75em;
background-image: url(/docs/images/logos/github.png);
background-repeat: no-repeat;
background-size: auto 75%;
background-position: left center;
}
</style>
<div class="notebook-links">
<a class="colab-link" href="https://colab.research.google.com/github/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb">Run in Google Colab</a>
<a class="github-link" href="https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb">View source on GitHub</a>
</div>



A Kubeflow pipeline is a portable and scalable definition of a machine learning
(ML) workflow. Each step in your ML workflow, such as preparing data or
training a model, is an instance of a pipeline component. This document
provides an overview of pipeline concepts and best practices, and instructions
describing how to build an ML pipeline.

## Before you begin

1.  Run the following command to install the Kubeflow Pipelines SDK. If you run this command in a Jupyter
    notebook, restart the kernel after installing the SDK. 


```python
$ pip install kfp --upgrade
```

2.  Import the `kfp` and `kfp.components` packages.


```python
import kfp
import kfp.components as comp
```

## Understanding pipelines

A Kubeflow pipeline is a portable and scalable definition of an ML workflow,
based on containers. A pipeline is composed of a set of input parameters and a
list of the steps in this workflow. Each step in a pipeline is an instance of a
component, which is represented as an instance of 
[`ContainerOp`][container-op].

You can use pipelines to:

*   Orchestrate repeatable ML workflows.
*   Accelerate experimentation by running a workflow with different sets of
    hyperparameters.

### Understanding pipeline components

A pipeline component is a containerized application that performs one step in a
pipeline's workflow. Pipeline components are defined in
[component specifications][component-spec], which define the following:

*   The component's interface, its inputs and outputs.
*   The component's implementation, the container image and the command to
    execute.
*   The component's metadata, such as the name and description of the
    component.

You can build components by [defining a component specification for a
containerized application][component-dev], or you can [use the Kubeflow
Pipelines SDK to generate a component specification for a Python
function][python-function-component]. You can also [reuse prebuilt components
in your pipeline][prebuilt-components]. 

### Understanding the pipeline graph

Each step in your pipeline's workflow is an instance of a component. When
you define your pipeline, you specify the source of each step's inputs. Step
inputs can be set from the pipeline's input arguments, constants, or step
inputs can depend on the outputs of other steps in this pipeline. Kubeflow
Pipelines uses these dependencies to define your pipeline's workflow as
a graph.

For example, consider a pipeline with the following steps: ingest data,
generate statistics, preprocess data, and train a model. The following
describes the data dependencies between each step.

*   **Ingest data**: This step loads data from an external source which is
    specified using a pipeline argument, and it outputs a dataset. Since
    this step does not depend on the output of any other steps, this step
    can run first.
*   **Generate statistics**: This step uses the ingested dataset to generate
    and output a set of statistics. Since this step depends on the dataset
    produced by the ingest data step, it must run after the ingest data step.
*   **Preprocess data**: This step preprocesses the ingested dataset and
    transforms the data into a preprocessed dataset. Since this step depends
    on the dataset produced by the ingest data step, it must run after the
    ingest data step.
*   **Train a model**: This step trains a model using the preprocessed dataset,
    the generated statistics, and pipeline parameters, such as the learning
    rate. Since this step depends on the preprocessed data and the generated
    statistics, it must run after both the preprocess data and generate
    statistics steps are complete.

Since the generate statistics and preprocess data steps both depend on the
ingested data, the generate statistics and preprocess data steps can run in
parallel. All other steps are executed once their data dependencies are
available.

## Designing your pipeline

When designing your pipeline, think about how to split your ML workflow into
pipeline components. The process of splitting an ML workflow into pipeline
components is similar to the process of splitting a monolithic script into
testable functions. The following rules can help you define the components
that you need to build your pipeline.

*   Components should have a single responsibility. Having a single
    responsibility makes it easier to test and reuse a component. For example,
    if you have a component that loads data you can reuse that for similar
    tasks that load data. If you have a component that loads and transforms
    a dataset, the component can be less useful since you can use it only when
    you need to load and transform that dataset. 

*   Reuse components when possible. Kubeflow Pipelines provides [components for
    common pipeline tasks and for access to cloud
    services][prebuilt-components].

*   Consider what you need to know to debug your pipeline and research the
    lineage of the models that your pipeline produces. Kubeflow Pipelines
    stores the inputs and outputs of each pipeline step. By interrogating the
    artifacts produced by a pipeline run, you can better understand the
    variations in model quality between runs or track down bugs in your
    workflow.

In general, you should design your components with composability in mind. 

Pipelines are composed of component instances, also called steps. Steps can
define their inputs as depending on the output of another step. The
dependencies between steps define the pipeline workflow graph.

### Building pipeline components

Kubeflow pipeline components are containerized applications that perform a
step in your ML workflow. Here are the ways that you can define pipeline
components:

*   If you have a containerized application that you want to use as a
    pipeline component, create a component specification to define this
    container image as a pipeline component.
    
    This option provides the flexibility to include code written in any
    language in your pipeline, so long as you can package the application
    as a container image. Learn more about [building pipeline
    components][component-dev].

*   If your component code can be expressed as a Python function, [evaluate if
    your component can be built as a Python function-based
    component][python-function-component]. The Kubeflow Pipelines SDK makes it
    easier to build lightweight Python function-based components by saving you
    the effort of creating a component specification.

Whenever possible, [reuse prebuilt components][prebuilt-components] to save
yourself the effort of building custom components.

The example in this guide demonstrates how to build a pipeline that uses a
Python function-based component and reuses a prebuilt component.

### Understanding how data is passed between components

When Kubeflow Pipelines runs a component, a container image is started in a
Kubernetes Pod and your component’s inputs are passed in as command-line
arguments. When your component has finished, the component's outputs are
returned as files.

In your component's specification, you define the components inputs and outputs
and how the inputs and output paths are passed to your program as command-line
arguments. You can pass small inputs, such as short strings or numbers, to your
component by value. Large inputs, such as datasets, must be passed to your
component as file paths. Outputs are written to the paths that Kubeflow
Pipelines provides.

Python function-based components make it easier to build pipeline components
by building the component specification for you. Python function-based
components also handle the complexity of passing inputs into your component
and passing your function’s outputs back to your pipeline.

Learn more about how [Python function-based components handle inputs and
outputs][python-function-component-data-passing]. 

## Getting started building a pipeline

The following sections demonstrate how to get started building a Kubeflow
pipeline by walking through the process of converting a Python script into
a pipeline.

### Design your pipeline

The following steps walk through some of the design decisions you may face
when designing a pipeline.

1.  Evaluate the process. In the following example, a Python function downloads
    a zipped tar file (`.tar.gz`) that contains several CSV files, from a
    public website. The function extracts the CSV files and then merges them
    into a single file.

[container-op]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp
[component-spec]: /docs/components/pipelines/reference/component-spec/
[python-function-component]: /docs/components/pipelines/legacy-v1/sdk/python-function-components/
[component-dev]: /docs/components/pipelines/legacy-v1/sdk/component-development/
[python-function-component-data-passing]: /docs/components/pipelines/legacy-v1/sdk/python-function-components/#understanding-how-data-is-passed-between-components
[prebuilt-components]: /docs/examples/shared-resources/


```python
import glob
import pandas as pd
import tarfile
import urllib.request
    
def download_and_merge_csv(url: str, output_csv: str):
  with urllib.request.urlopen(url) as res:
    tarfile.open(fileobj=res, mode="r|gz").extractall('data')
  df = pd.concat(
      [pd.read_csv(csv_file, header=None) 
       for csv_file in glob.glob('data/*.csv')])
  df.to_csv(output_csv, index=False, header=False)
```

2.  Run the following Python command to test the function. 


```python
download_and_merge_csv(
    url='https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz', 
    output_csv='merged_data.csv')
```

3.  Run the following to print the first few rows of the
    merged CSV file.


```python
$ head merged_data.csv
```

4.  Design your pipeline. For example, consider the following pipeline designs.

    *   Implement the pipeline using a single step. In this case, the pipeline
        contains one component that works similarly to the example function.
        This is a straightforward function, and implementing a single-step
        pipeline is a reasonable approach in this case.
        
        The down side of this approach is that the zipped tar file would not be
        an artifact of your pipeline runs. Not having this artifact available 
        could make it harder to debug this component in production.
        
    *   Implement this as a two-step pipeline. The first step downloads a file
        from a website. The second step extracts the CSV files from a zipped
        tar file and merges them into a single file. 
        
        This approach has a few benefits:
        
        *   You can reuse the [Web Download component][web-download-component]
            to implement the first step.
        *   Each step has a single responsibility, which makes the components
            easier to reuse.
        *   The zipped tar file is an artifact of the first pipeline step.
            This means that you can examine this artifact when debugging
            pipelines that use this component.
    
    This example implements a two-step pipeline.

### Build your pipeline components

        
1.  Build your pipeline components. This example modifies the initial script to
    extract the contents of a zipped tar file, merge the CSV files that were
    contained in the zipped tar file, and return the merged CSV file.
    
    This example builds a Python function-based component. You can also package
    your component's code as a Docker container image and define the component
    using a ComponentSpec.
    
    In this case, the following modifications were required to the original
    function.

    *   The file download logic was removed. The path to the zipped tar file
        is passed as an argument to this function.
    *   The import statements were moved inside of the function. Python
        function-based components require standalone Python functions. This
        means that any required import statements must be defined within the
        function, and any helper functions must be defined within the function.
        Learn more about [building Python function-based
        components][python-function-components].
    *   The function's arguments are decorated with the
        [`kfp.components.InputPath`][input-path] and the
        [`kfp.components.OutputPath`][output-path] annotations. These
        annotations let Kubeflow Pipelines know to provide the path to the
        zipped tar file and to create a path where your function stores the
        merged CSV file. 
        
    The following example shows the updated `merge_csv` function.

[web-download-component]: https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/components/web/Download/component.yaml
[python-function-components]: /docs/components/pipelines/legacy-v1/sdk/python-function-components/
[input-path]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html?highlight=inputpath#kfp.components.InputPath
[output-path]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html?highlight=outputpath#kfp.components.OutputPath


```python
def merge_csv(file_path: comp.InputPath('Tarball'),
              output_csv: comp.OutputPath('CSV')):
  import glob
  import pandas as pd
  import tarfile

  tarfile.open(name=file_path, mode="r|gz").extractall('data')
  df = pd.concat(
      [pd.read_csv(csv_file, header=None) 
       for csv_file in glob.glob('data/*.csv')])
  df.to_csv(output_csv, index=False, header=False)
```

2.  Use [`kfp.components.create_component_from_func`][create_component_from_func]
    to return a factory function that you can use to create pipeline steps.
    This example also specifies the base container image to run this function
    in, the path to save the component specification to, and a list of PyPI
    packages that need to be installed in the container at runtime.

[create_component_from_func]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.create_component_from_func
[container-op]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp


```python
create_step_merge_csv = kfp.components.create_component_from_func(
    func=merge_csv,
    output_component_file='component.yaml', # This is optional. It saves the component spec for future use.
    base_image='python:3.7',
    packages_to_install=['pandas==1.1.4'])
```

### Build your pipeline

1.  Use [`kfp.components.load_component_from_url`][load_component_from_url]
    to load the component specification YAML for any components that you are
    reusing in this pipeline.

[load_component_from_url]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html?highlight=load_component_from_url#kfp.components.load_component_from_url


```python
web_downloader_op = kfp.components.load_component_from_url(
    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/contrib/web/Download/component.yaml')
```

2.  Define your pipeline as a Python function. 

    Your pipeline function's arguments define your pipeline's parameters. Use
    pipeline parameters to experiment with different hyperparameters, such as
    the learning rate used to train a model, or pass run-level inputs, such as
    the path to an input file, into a pipeline run.
    
    Use the factory functions created by
    `kfp.components.create_component_from_func` and
    `kfp.components.load_component_from_url` to create your pipeline's tasks. 
    The inputs to the component factory functions can be pipeline parameters,
    the outputs of other tasks, or a constant value. In this case, the
    `web_downloader_task` task uses the `url` pipeline parameter, and the
    `merge_csv_task` uses the `data` output of the `web_downloader_task`.
    


```python
# Define a pipeline and create a task from a component:
def my_pipeline(url):
  web_downloader_task = web_downloader_op(url=url)
  merge_csv_task = create_step_merge_csv(file=web_downloader_task.outputs['data'])
  # The outputs of the merge_csv_task can be referenced using the
  # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']
```

### Compile and run your pipeline

After defining the pipeline in Python as described in the preceding section, use one of the following options to compile the pipeline and submit it to the Kubeflow Pipelines service.

#### Option 1: Compile and then upload in UI

1.  Run the following to compile your pipeline and save it as `pipeline.yaml`. 



```python
kfp.compiler.Compiler().compile(
    pipeline_func=my_pipeline,
    package_path='pipeline.yaml')
```

2.  Upload and run your `pipeline.yaml` using the Kubeflow Pipelines user interface.
See the guide to [getting started with the UI][quickstart].

[quickstart]: /docs/components/pipelines/legacy-v1/overview/quickstart

#### Option 2: run the pipeline using Kubeflow Pipelines SDK client

1.  Create an instance of the [`kfp.Client` class][kfp-client] following steps in [connecting to Kubeflow Pipelines using the SDK client][connect-api].

[kfp-client]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html#kfp.Client
[connect-api]: /docs/components/pipelines/user-guides/core-functions/connect-api


```python
client = kfp.Client() # change arguments accordingly
```

2.  Run the pipeline using the `kfp.Client` instance:


```python
client.create_run_from_pipeline_func(
    my_pipeline,
    arguments={
        'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'
    })
```


## Next steps

*   Learn about advanced pipeline features, such as [authoring recursive
    components][recursion] and [using conditional execution in a
    pipeline][conditional].
*   Learn how to [manipulate Kubernetes resources in a
    pipeline][k8s-resources] (Experimental).

[conditional]: https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/tutorials/DSL%20-%20Control%20structures/DSL%20-%20Control%20structures.py
[recursion]: /docs/components/pipelines/legacy-v1/sdk/dsl-recursion/
[k8s-resources]: /docs/components/pipelines/legacy-v1/sdk/manipulate-resources/


<div class="notebook-links">
<a class="colab-link" href="https://colab.research.google.com/github/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb">Run in Google Colab</a>
<a class="github-link" href="https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb">View source on GitHub</a>
</div>



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md
================================================
+++
title = "Building Components"
description = "A tutorial on how to create components and use them in a pipeline"
weight = 40
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

A pipeline component is a self-contained set of code that performs one step in
your ML workflow. This document describes the concepts required to build
components, and demonstrates how to get started building components.

## Before you begin

Run the following command to install the Kubeflow Pipelines SDK.

```python
$ pip install kfp==1.8
```

For more information about the Kubeflow Pipelines SDK, see the [SDK reference guide][sdk-ref].

[sdk-ref]: https://kubeflow-pipelines.readthedocs.io/en/stable/index.html

## Understanding pipeline components

Pipeline components are self-contained sets of code that perform one step in
your ML workflow, such as preprocessing data or training a model. To create a
component, you must _build the component's implementation_ and _define the
component specification_.

Your component's implementation includes the component's executable code and
the Docker container image that the code runs in.  [Learn more about designing
a pipeline component](#design).

Once you have built your component's implementation, you can define your
component's interface as a component specification. A component specification
defines:

*   The component's inputs and outputs.
*   The container image that your component's code runs in, the command to use
    to run your component's code, and the command-line arguments to pass to
    your component's code.
*   The component's metadata, such as the name and description.

[Learn more about creating a component specification](#component-spec).

If your component's code is implemented as a Python function, use the
Kubeflow Pipelines SDK to package your function as a component. [Learn more
about building Python function-based components][python-function-components].

[python-function-components]: https://www.kubeflow.org/docs/components/pipelines/user-guides/components/

<a name="design"></a>
## Designing a pipeline component

When Kubeflow Pipelines executes a component, a container image is started in a
Kubernetes Pod and your component's inputs are passed in as command-line
arguments. You can pass small inputs, such as strings and numbers, by value.
Larger inputs, such as CSV data, must be passed as paths to files. When your
component has finished, the component's outputs are returned as files.

When you design your component's code, consider the following:

*   Which inputs can be passed to your component by value? Examples of inputs that you
    can pass by value include numbers, booleans, and short strings. Any value that you
    could reasonably pass as a command-line argument can be passed to your component by
    value.  All other inputs are passed to your component by a reference to the input's path.
*   To return an output from your component, the output's data must be stored as a file.
    When you define your component, you let Kubeflow Pipelines know what outputs your
    component produces. When your pipeline runs, Kubeflow Pipelines passes the
    paths that you use to store your component's outputs as inputs to your component.
*   Outputs are typically written to a single file. In some cases, you may need to
    return a directory of files as an output. In this case, create a directory at the
    output path and write the output files to that location. In both cases, it may be
    necessary to create parent directories if they do not exist.
*   Your component's goal may be to create a dataset in an external service,
    such as a BigQuery table. In this case, it may make sense for the component
    to output an identifier for the produced data, such as a table name,
    instead of the data itself. We recommend that you limit this pattern to
    cases where the data must be put into an external system instead of keeping it
    inside the Kubeflow Pipelines system.
*   Since your inputs and output paths are passed in as command-line
    arguments, your component's code must be able to read inputs from the
    command line. If your component is built with Python, libraries such as
    [argparse](https://docs.python.org/3/library/argparse.html) and
    [absl.flags](https://abseil.io/docs/python/guides/flags) make it easier to
    read your component's inputs.
*   Your component's code can be implemented in any language, so long as it can
    run in a container image.

The following is an example program written using Python3. This program reads a
given number of lines from an input file and writes those lines to an output
file. This means that this function accepts three command-line parameters: 

*   The path to the input file.
*   The number of lines to read.
*   The path to the output file. 

```python
#!/usr/bin/env python3
import argparse
from pathlib import Path

# Function doing the actual work (Outputs first N lines from a text file)
def do_work(input1_file, output1_file, param1):
  for x, line in enumerate(input1_file):
    if x >= param1:
      break
    _ = output1_file.write(line)
  
# Defining and parsing the command-line arguments
parser = argparse.ArgumentParser(description='My program description')
# Paths must be passed in, not hardcoded
parser.add_argument('--input1-path', type=str,
  help='Path of the local file containing the Input 1 data.')
parser.add_argument('--output1-path', type=str,
  help='Path of the local file where the Output 1 data should be written.')
parser.add_argument('--param1', type=int, default=100,
  help='The number of lines to read from the input and write to the output.')
args = parser.parse_args()

# Creating the directory where the output file is created (the directory
# may or may not exist).
Path(args.output1_path).parent.mkdir(parents=True, exist_ok=True)

with open(args.input1_path, 'r') as input1_file:
    with open(args.output1_path, 'w') as output1_file:
        do_work(input1_file, output1_file, args.param1)
```

If this program is saved as `program.py`, the command-line invocation of this
program is:

```bash
python3 program.py --input1-path <path-to-the-input-file> \
  --param1 <number-of-lines-to-read> \
  --output1-path <path-to-write-the-output-to> 
```

## Containerize your component's code

For Kubeflow Pipelines to run your component, your component must be packaged
as a [Docker][docker] container image and published to a container registry
that your Kubernetes cluster can access. The steps to create a container image
are not specific to Kubeflow Pipelines. To make things easier for you, this
section provides some guidelines on standard container creation.

1.  Create a [Dockerfile][dockerfile] for your container. A Dockerfile
    specifies:

    *   The base container image. For example, the operating system that your
        code runs on.
    *   Any dependencies that need to be installed for your code to run. 
    *   Files to copy into the container, such as the runnable code for this
        component.

    The following is an example Dockerfile.

    ```
    FROM python:3.7
    RUN python3 -m pip install keras
    COPY ./src /pipelines/component/src
    ```
    
    In this example:

    *   The base container image is [`python:3.7`][python37].
    *   The `keras` Python package is installed in the container image.
    *   Files in your `./src` directory are copied into
        `/pipelines/component/src` in the container image.
  
1.  Create a script named `build_image.sh` that uses Docker to build your
    container image and push your container image to a container registry.
    Your Kubernetes cluster must be able to access your container registry
    to run your component. Examples of container registries include [Google
    Container Registry][google-container-registry] and 
    [Docker Hub][docker-hub].

    The following example builds a container image, pushes it to a container
    registry, and outputs the strict image name. It is a best practice to use
    the strict image name in your component specification to ensure that you
    are using the expected version of a container image in each component
    execution.

    ```bash
    #!/bin/bash -e
    image_name=gcr.io/my-org/my-image
    image_tag=latest
    full_image_name=${image_name}:${image_tag}

    cd "$(dirname "$0")" 
    docker build -t "${full_image_name}" .
    docker push "$full_image_name"

    # Output the strict image name, which contains the sha256 image digest
    docker inspect --format="{{index .RepoDigests 0}}" "${full_image_name}"
    ``` 

    In the preceding example: 

    *   The `image_name` specifies the full name of your container image in the
        container registry.
    *   The `image_tag` specifies that this image should be tagged as
        **latest**.

    Save this file and run the following to make this script executable.

    ```bash
    chmod +x build_image.sh
    ```

1.  Run your `build_image.sh` script to build your container image and push it
    to a container registry.

1.  [Use `docker run` to test your container image locally][docker-run]. If
    necessary, revise your application and Dockerfile until your application
    works as expected in the container.

[python37]: https://hub.docker.com/_/python
[docker]: https://docs.docker.com/get-started/
[dockerfile]: https://docs.docker.com/engine/reference/builder/
[google-container-registry]: https://cloud.google.com/container-registry/docs/
[docker-hub]: https://hub.docker.com/
[docker-run]: https://docs.docker.com/engine/reference/commandline/run/

<a name="component-spec"></a>
## Creating a component specification

To create a component from your containerized program, you must create a
component specification that defines the component's interface and
implementation. The following sections provide an overview of how to create a
component specification by demonstrating how to define the component's
implementation, interface, and metadata.

To learn more about defining a component specification, see the 
[component specification reference guide][component-spec].

[component-spec]: /docs/components/pipelines/reference/component-spec/

### Define your component's implementation

The following example creates a component specification YAML and defines the
component's implementation. 

1.  Create a file named `component.yaml` and open it in a text editor.
1.  Create your component's implementation section and specify the strict name
    of your container image. The strict image name is provided when you run
    your `build_image.sh` script.

    ```yaml
    implementation:
      container:
        # The strict name of a container image that you've pushed to a container registry.
        image: gcr.io/my-org/my-image@sha256:a172..752f
    ```

1.  Define a `command` for your component's implementation. This field
    specifies the command-line arguments that are used to run your program in
    the container. 

    ```yaml
    implementation:
      container:
        image: gcr.io/my-org/my-image@sha256:a172..752f
        # command is a list of strings (command-line arguments). 
        # The YAML language has two syntaxes for lists and you can use either of them. 
        # Here we use the "flow syntax" - comma-separated strings inside square brackets.
        command: [
          python3, 
          # Path of the program inside the container
          /pipelines/component/src/program.py,
          --input1-path,
          {inputPath: Input 1},
          --param1, 
          {inputValue: Parameter 1},
          --output1-path, 
          {outputPath: Output 1},
        ]
    ```

    The `command` is formatted as a list of strings. Each string in the
    `command` is a command-line argument or a placeholder. At runtime,
    placeholders are replaced with an input or output. In the preceding
    example, two inputs and one output path are passed into a Python script at
    `/pipelines/component/src/program.py`.

    There are three types of input/output placeholders:

    *   `{inputValue: <input-name>}`:
        This placeholder is replaced with the value of the specified input.
        This is useful for small pieces of input data, such as numbers or small
        strings.

    *   `{inputPath: <input-name>}`:
        This placeholder is replaced with the path to this input as a file.
        Your component can read the contents of that input at that path during
        the pipeline run.

    *   `{outputPath: <output-name>}`:
        This placeholder is replaced with the path where your program writes
        this output's data. This lets the Kubeflow Pipelines system read the
        contents of the file and store it as the value of the specified output.
 
    The `<input-name>` name must match the name of an input in the `inputs`
    section of your component specification. The `<output-name>` name must
    match the name of an output in the `outputs` section of your component
    specification.  

### Define your component's interface

The following examples demonstrate how to specify your component's interface. 

1.  To define an input in your `component.yaml`, add an item to the
    `inputs` list with the following attributes:

    *   `name`: Human-readable name of this input. Each input's name must be
        unique.
    *   `description`: (Optional.) Human-readable description of the input.
    *   `default`: (Optional.) Specifies the default value for this input.
    *   `type`: (Optional.) Specifies the input’s type. Learn more about the
        [types defined in the Kubeflow Pipelines SDK][dsl-types] and [how type
        checking works in pipelines and components][dsl-type-checking].
    *   `optional`: Specifies if this input is optional. The value of this
        attribute is of type `Bool`, and defaults to **False**.

    In this example, the Python program has two inputs: 

    *   `Input 1` contains `String` data.
    *   `Parameter 1` contains an `Integer`.

    ```yaml
    inputs:
    - {name: Input 1, type: String, description: 'Data for input 1'}
    - {name: Parameter 1, type: Integer, default: '100', description: 'Number of lines to copy'}
    ```

    Note: `Input 1` and `Parameter 1` do not specify any details about how they
    are stored or how much data they contain. Consider using naming conventions
    to indicate if inputs are expected to be small enough to pass by value. 

1.  After your component finishes its task, the component's outputs are passed
    to your pipeline as paths. At runtime, Kubeflow Pipelines creates a
    path for each of your component's outputs. These paths are passed as inputs
    to your component's implementation.

    To define an output in your component specification YAML, add an item to
    the `outputs` list with the following attributes:

    *   `name`: Human-readable name of this output. Each output's name must be
        unique.
    *   `description`: (Optional.) Human-readable description of the output.
    *   `type`: (Optional.) Specifies the output's type. Learn more about the
        [types defined in the Kubeflow Pipelines SDK][dsl-types] and [how type
        checking works in pipelines and components][dsl-type-checking].

    In this example, the Python program returns one output. The output is named
    `Output 1` and it contains `String` data.

    ```yaml
    outputs:
    - {name: Output 1, type: String, description: 'Output 1 data.'}
    ```

    Note: Consider using naming conventions to indicate if this output is
    expected to be small enough to pass by value. You should limit the amount
    of data that is passed by value to 200 KB per pipeline run.

1.  After you define your component's interface, the `component.yaml` should be
    something like the following:

    ```yaml
    inputs:
    - {name: Input 1, type: String, description: 'Data for input 1'}
    - {name: Parameter 1, type: Integer, default: '100', description: 'Number of lines to copy'}
    
    outputs:
    - {name: Output 1, type: String, description: 'Output 1 data.'}

    implementation:
      container:
        image: gcr.io/my-org/my-image@sha256:a172..752f
        # command is a list of strings (command-line arguments). 
        # The YAML language has two syntaxes for lists and you can use either of them. 
        # Here we use the "flow syntax" - comma-separated strings inside square brackets.
        command: [
          python3, 
          # Path of the program inside the container
          /pipelines/component/src/program.py,
          --input1-path,
          {inputPath: Input 1},
          --param1, 
          {inputValue: Parameter 1},
          --output1-path, 
          {outputPath: Output 1},
        ]
    ```

[dsl-types]: https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/types.py
[dsl-type-checking]: /docs/components/pipelines/legacy-v1/sdk/static-type-checking/

### Specify your component's metadata

To define your component's metadata, add the `name` and `description`
fields to your `component.yaml`

```yaml
name: Get Lines
description: Gets the specified number of lines from the input file.

inputs:
- {name: Input 1, type: String, description: 'Data for input 1'}
- {name: Parameter 1, type: Integer, default: '100', description: 'Number of lines to copy'}

outputs:
- {name: Output 1, type: String, description: 'Output 1 data.'}

implementation:
  container:
    image: gcr.io/my-org/my-image@sha256:a172..752f
    # command is a list of strings (command-line arguments). 
    # The YAML language has two syntaxes for lists and you can use either of them. 
    # Here we use the "flow syntax" - comma-separated strings inside square brackets.
    command: [
      python3, 
      # Path of the program inside the container
      /pipelines/component/src/program.py,
      --input1-path,
      {inputPath: Input 1},
      --param1, 
      {inputValue: Parameter 1},
      --output1-path, 
      {outputPath: Output 1},
    ]
```

## Using your component in a pipeline

You can use the Kubeflow Pipelines SDK to load your component using methods
such as the following:

*   [`kfp.components.load_component_from_file`][kfp-load-comp-file]: 
    Use this method to load your component from a `component.yaml` path.
*   [`kfp.components.load_component_from_url`][kfp-load-comp-url]:
    Use this method to load a `component.yaml` from a URL.
*   [`kfp.components.load_component_from_text`][kfp-load-comp-text]:
    Use this method to load your component specification YAML from a string.
    This method is useful for rapidly iterating on your component
    specification.

These functions create a factory function that you can use to create
[`ContainerOp`][kfp-containerop] instances to use as steps in your pipeline.
This factory function's input arguments include your component's inputs and
the paths to your component's outputs. The function signature may be modified
in the following ways to ensure that it is valid and Pythonic.

*   Inputs with default values will come after the inputs without default
    values and outputs.
*   Input and output names are converted to Pythonic names (spaces and symbols
    are replaced with underscores and letters are converted to lowercase). For
    example, an input named `Input 1` is converted to `input_1`. 

The following example demonstrates how to load the text of your component
specification and run it in a two-step pipeline. Before you run this
example, update the component specification to use the component
specification you defined in the previous sections. 

To demonstrate data passing between components, we create another component
that simply uses bash commands to write some text value to an output file. 
And the output file can be passed to our previous component as an input.

```python
import kfp
import kfp.components as comp

create_step_get_lines = comp.load_component_from_text("""
name: Get Lines
description: Gets the specified number of lines from the input file.

inputs:
- {name: Input 1, type: Data, description: 'Data for input 1'}
- {name: Parameter 1, type: Integer, default: '100', description: 'Number of lines to copy'}

outputs:
- {name: Output 1, type: Data, description: 'Output 1 data.'}

implementation:
  container:
    image: gcr.io/my-org/my-image@sha256:a172..752f
    # command is a list of strings (command-line arguments). 
    # The YAML language has two syntaxes for lists and you can use either of them. 
    # Here we use the "flow syntax" - comma-separated strings inside square brackets.
    command: [
      python3, 
      # Path of the program inside the container
      /pipelines/component/src/program.py,
      --input1-path,
      {inputPath: Input 1},
      --param1, 
      {inputValue: Parameter 1},
      --output1-path, 
      {outputPath: Output 1},
    ]""")

# create_step_get_lines is a "factory function" that accepts the arguments
# for the component's inputs and output paths and returns a pipeline step
# (ContainerOp instance).
#
# To inspect the get_lines_op function in Jupyter Notebook, enter 
# "get_lines_op(" in a cell and press Shift+Tab.
# You can also get help by entering `help(get_lines_op)`, `get_lines_op?`,
# or `get_lines_op??`.

# Create a simple component using only bash commands. The output of this component
# can be passed to a downstream component that accepts an input with the same type.
create_step_write_lines = comp.load_component_from_text("""
name: Write Lines
description: Writes text to a file.

inputs:
- {name: text, type: String}

outputs:
- {name: data, type: Data}

implementation:
  container:
    image: busybox
    command:
    - sh
    - -c
    - |
      mkdir -p "$(dirname "$1")"
      echo "$0" > "$1"
    args:
    - {inputValue: text}
    - {outputPath: data}
""")

# Define your pipeline 
def my_pipeline():
    write_lines_step = create_step_write_lines(
        text='one\ntwo\nthree\nfour\nfive\nsix\nseven\neight\nnine\nten')

    get_lines_step = create_step_get_lines(
        # Input name "Input 1" is converted to pythonic parameter name "input_1"
        input_1=write_lines_step.outputs['data'],
        parameter_1='5',
    )

# If you run this command on a Jupyter notebook running on Kubeflow,
# you can exclude the host parameter.
# client = kfp.Client()
client = kfp.Client(host='<your-kubeflow-pipelines-host-name>')

# Compile, upload, and submit this pipeline for execution.
client.create_run_from_pipeline_func(my_pipeline, arguments={})
```

[kfp-load-comp-file]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.load_component_from_file
[kfp-load-comp-url]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.load_component_from_url
[kfp-load-comp-text]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.load_component_from_text
[kfp-containerop]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp

## Organizing the component files

This section provides a recommended way to organize a component's files. There
is no requirement that you must organize the files in this way. However, using 
the standard organization makes it possible to reuse the same scripts for
testing, image building, and component versioning.

```
components/<component group>/<component name>/

    src/*            # Component source code files
    tests/*          # Unit tests
    run_tests.sh     # Small script that runs the tests
    README.md        # Documentation. If multiple files are needed, move to docs/.

    Dockerfile       # Dockerfile to build the component container image
    build_image.sh   # Small script that runs docker build and docker push

    component.yaml   # Component definition in YAML format
```

See this [sample component][org-sample] for a real-life component example.

[org-sample]: https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/components/sample/keras/train_classifier

## Next steps

* Consolidate what you've learned by reading the
  [best practices](/docs/components/pipelines/legacy-v1/sdk/best-practices/) for designing and
  writing components.
* For quick iteration,
  [build lightweight Python function-based components](/docs/components/pipelines/legacy-v1/sdk/python-function-components/)
  directly from Python functions.
* See how to [export metrics from your
  pipeline](/docs/components/pipelines/legacy-v1/sdk/output-viewer/#v2-sdk-use-sdk-visualization-apis).
* Visualize the output of your component by
  [adding metadata for an output
  viewer](/docs/components/pipelines/legacy-v1/sdk/output-viewer/).
* Explore the [reusable components and other shared
  resources](/docs/examples/shared-resources/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/dsl-recursion.md
================================================
+++
title = "DSL Recursion"
description = "Author a recursive function in DSL"
weight = 110
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This page describes how to write recursive functions in the domain specific language (DSL) provided by the Kubeflow Pipelines SDK.

## Motivation
Recursion is a feature that is supported by almost all languages to express complex semantics in a succinct way. 
In machine learning workflows, recursions are especially important to enable features such as multiple rounds of training, 
iterative model analysis, and hypertuning. The recursion support also covers the loop feature since it enables the same code 
block to be executed and exited based on dynamic conditions.

## How to write a recursive function

### Decorator

Decorate the recursive function with [kfp.dsl.graph_component](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/_component.py)
as illustrated below. The decorator does not require any arguments.
```python
import kfp.dsl as dsl
@dsl.graph_component
def graph_component_a(input_x):
  with dsl.Condition(input_x == 'value_x'):
    op_a = task_factory_a(input_x)
    op_b = task_factory_b().after(op_a)
    graph_component_a(op_b.output)
    
@dsl.pipeline(
  name='pipeline',
  description='shows how to use the recursion.'
)
def pipeline():
  op_a = task_factory_a()
  op_b = task_factory_b()
  graph_op_a = graph_component_a(op_a.output)
  graph_op_a.after(op_b)
  task_factory_c(op_a.output).after(graph_op_a)
```

### Function signature
Define the function signature as a standard Python function. The input parameters are [PipelineParams](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/_pipeline_param.py).

### Function body
Similar to the pipeline function body, you can instantiate components, create [conditions](https://github.com/kubeflow/pipelines/blob/f8b0f5bf0cc0b5aceb8aedfd21e93156e363ea48/sdk/python/kfp/dsl/_ops_group.py#L110),
use the input parameters from the function signature, and specify dependencies explicitly among components. 
In the example above, one condition is created inside the recursive function and 
two components *op_a* and *op_b* are created inside the condition.   

### Call the recursive function in the pipeline function
You can pass pipeline/component output to the recursive function and specify the dependencies explicitly with *after()* function, similar to
the [ContainerOp](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/_container_op.py). In the example above, the output of *op_a* 
defined in the pipeline is passed to the recursive function and the *task_factory_c* component is specified to depend on the *graph_op_a*. 
The recursive function can also be explicitly specified to depend on the ContainerOps. For example, *graph_op_a* depends on *op_b* in the pipeline.

### More examples
Here is another example where the recursive function call is at the end of the function body, similar to [do-while](https://en.wikipedia.org/wiki/Do_while_loop) loops.
```python
import kfp.dsl as dsl
@dsl.graph_component
def graph_component_a(input_x):
  op_a = task_factory_a(input_x)
  op_b = task_factory_b().after(op_a)
  with dsl.Condition(op_b.output == 'value_x'):
    graph_component_a(op_b.output)
 
@dsl.pipeline(
  name='pipeline',
  description='shows how to use the recursion.'
)
def pipeline():
  op_a = task_factory_a()
  op_b = task_factory_b()
  graph_op_a = graph_component_a(op_a.output)
  graph_op_a.after(op_b)
  task_factory_c(op_a.output).after(graph_op_a)
```

## Limitations

* [Type checking](/docs/components/pipelines/legacy-v1/sdk/static-type-checking) does not work for the recursive functions. In other words, The type information that is annotated to the recursive 
function signature will not be checked.
* Since the output of the recursive functions cannot be dynamically resolved, the downstream ContainerOps cannot
access the output from the recursive functions.
* A known [issue](https://github.com/kubeflow/pipelines/issues/1065) is that the recursion fails to work when there are 
multiple recursive function calls in the function body.

## Next steps

* See [recursion sample](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/recursion/recursion.py)



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/enviroment_variables.md
================================================
+++
title = "Using environment variables in pipelines"
description = "How to set and use environment variables in Kubeflow pipelines"
weight = 115
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This page describes how to pass environment variables to Kubeflow pipeline 
components.

## Before you start

Set up your environment: 

- [Install Kubeflow](/docs/started/)
- [Install the Kubeflow Pipelines SDK](/docs/components/pipelines/legacy-v1/sdk/install-sdk/)



## Using environment variables 

In this example, you pass an environment variable to a lightweight Python 
component, which writes the variable's value to the log.

[Learn more about lightweight Python components](/docs/components/pipelines/user-guides/components/lightweight-python-components/)

To build a component, define a stand-alone Python function and then call 
[kfp.components.func_to_container_op(func)](https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.func_to_container_op) to convert the 
function to a component that can be used in a pipeline. The following function gets an 
environment variable and writes it to the log.

```python
def logg_env_function():
  import os
  import logging
  logging.basicConfig(level=logging.INFO)
  env_variable = os.getenv('example_env')
  logging.info('The environment variable is: {}'.format(env_variable))
```

Transform the function into a component using 
[kfp.components.func_to_container_op(func)](https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.func_to_container_op).  
```python
image_name = 'tensorflow/tensorflow:1.11.0-py3'
logg_env_function_op = comp.func_to_container_op(logg_env_function,
                                                 base_image=image_name)
```

Add this component to a pipeline. Use [add_env_variable](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp.container) to pass an 
environment variable into the component. This code is the same no matter if your
using python lightweight components or a [container operation](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp). 


```python
import kfp.dsl as dsl
from kubernetes.client.models import V1EnvVar

@dsl.pipeline(
  name='Env example',
  description='A pipeline showing how to use environment variables'
)
def environment_pipeline():
  env_var = V1EnvVar(name='example_env', value='env_variable')
  #Returns a dsl.ContainerOp class instance. 
  container_op = logg_env_function_op().add_env_variable(env_var) 
```

To pass more environment variables into a component, add more instances of 
[add_env_variable()](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp.container). Use the following command to run this pipeline using the 
Kubeflow Pipelines SDK.

```python
#Specify pipeline argument values
arguments = {}

#Submit a pipeline run
kfp.Client().create_run_from_pipeline_func(environment_pipeline,
                                           arguments=arguments)
```



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/gcp.md
================================================
+++
title = "GCP-specific Uses of the SDK"
description = "SDK features that are available on Google Cloud Platform (GCP) only"
weight = 130
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

For pipeline features that are specific to GCP, including SDK features, see the 
[GCP section of the docs](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/pipelines/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md
================================================
+++
title = "Install the Kubeflow Pipelines SDK"
description = "Setting up your Kubeflow Pipelines development environment"
weight = 20
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This guide tells you how to install the 
[Kubeflow Pipelines SDK](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/sdk)
which you can use to build machine learning pipelines. You can use the SDK
to execute your pipeline, or alternatively you can upload the pipeline to
the Kubeflow Pipelines UI for execution.

All of the SDK's classes and methods are described in the auto-generated [SDK reference docs](https://kubeflow-pipelines.readthedocs.io/en/stable/).

**Note:** If you are running [Kubeflow Pipelines with Tekton](https://github.com/kubeflow/kfp-tekton),
instead of the default [Kubeflow Pipelines with Argo](https://github.com/kubeflow/pipelines), you should use the
[Kubeflow Pipelines SDK for Tekton](/docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton).

## Set up Python

You need **Python 3.5** or later to use the Kubeflow Pipelines SDK. This
guide uses Python 3.7.

If you haven't yet set up a Python 3 environment, do so now. This guide
recommends [Miniconda](https://conda.io/miniconda.html), but you can use
a virtual environment manager of your choice, such as `virtualenv`.

Follow the steps below to set 
up Python using [Miniconda](https://conda.io/miniconda.html):

1. Choose one of the following methods to install Miniconda, depending on your
  environment:

    * Debian/Ubuntu/[Cloud Shell](https://console.cloud.google.com/cloudshell):   

        ```bash
        apt-get update; apt-get install -y wget bzip2
        wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
        bash Miniconda3-latest-Linux-x86_64.sh
        ```

    * Windows: Download the 
    [installer](https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe)
    and make sure you select the option to
    **Add Miniconda to my PATH environment variable** during the installation.

    * MacOS: Download the 
    [installer](https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh)
    and run the following command:

        ```bash
        bash Miniconda3-latest-MacOSX-x86_64.sh
        ```

1. Check that the `conda` command is available:

    ```bash
    which conda
    ```

    If the `conda` command is not found, add Miniconda to your path:
 
    ```bash
    export PATH=<YOUR_MINICONDA_PATH>/bin:$PATH
    ```

1. Create a clean Python 3 environment with a name of your choosing. This
  example uses Python 3.7 and an environment name of `mlpipeline`.:
 
    ```bash
    conda create --name mlpipeline python=3.7
    conda activate mlpipeline
    ```
 
## Install the Kubeflow Pipelines SDK

Run the following command to install the Kubeflow Pipelines SDK:

```bash
pip install kfp==1.8
```

**Note:** If you are not using a virtual environment, such as `conda`, when installing the Kubeflow Pipelines SDK, you may receive the following error:

```bash
ERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/local/lib/python3.5/dist-packages/kfp-<version>.dist-info'
Consider using the `--user` option or check the permissions.
```

If you get this error, install `kfp` with the `--user` option:

```bash
pip install kfp==1.8
```

This command installs the `dsl-compile` and `kfp` binaries under `~/.local/bin`, which is not part of the PATH in some Linux distributions, such as Ubuntu. You can add `~/.local/bin` to your PATH by appending the following to a new line at the end of your `.bashrc` file:

```bash
export PATH=$PATH:~/.local/bin
```

After successful installation, the command `dsl-compile` should be available.
You can use this command to verify it:

```bash
which dsl-compile
```

The response should be something like this:

```
/<PATH_TO_YOUR_USER_BIN>/miniconda3/envs/mlpipeline/bin/dsl-compile
```

## Next steps

* [See how to use the SDK](/docs/components/pipelines/legacy-v1/sdk/sdk-overview/).
* [Build a component and a pipeline](/docs/components/pipelines/legacy-v1/sdk/component-development/).
* [Get started with the UI](/docs/components/pipelines/legacy-v1/overview/quickstart).
* [Understand pipeline concepts](/docs/components/pipelines/concepts/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/manipulate-resources.md
================================================
+++
title = "Manipulate Kubernetes Resources as Part of a Pipeline"
description = "Overview of using the SDK to manipulate Kubernetes resources dynamically as steps of the pipeline"
weight = 1350
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This page describes how to manipulate Kubernetes resources through individual
Kubeflow Pipelines components during a pipeline.
Users may handle any Kubernetes resource, while creating
[Persistent Volume Claims](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
and
[Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/)
is rendered easy in the common case.

## Kubernetes Resources

### ResourceOp

This class represents a step of the pipeline which manipulates Kubernetes resources.
It implements
[Argo's resource template](https://github.com/argoproj/argo-workflows/tree/master/examples#kubernetes-resources).

This feature allows users to perform some action (`get`, `create`, `apply`,
`delete`, `replace`, `patch`) on Kubernetes resources.
Users are able to set conditions that denote the success or failure of the
step undertaking that action.

[Link](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ResourceOp)
to the corresponding Python library.

#### Arguments

Only most significant arguments are presented in this section.
For more information, please refer to the aforementioned link to the library.

* `k8s_resource`: Definition of the Kubernetes resource.
  (_required_)
* `action`: Action to be performed (defaults to `create`).
* `merge_strategy`: Merge strategy when action is `patch`.
  (_optional_)
* `success_condition`: Condition to denote success of the step once it is true.
  (_optional_)
* `failure_condition`: Condition to denote failure of the step once it is true.
  (_optional_)
* `attribute_outputs`: Similar to `file_outputs` of
  [`kfp.dsl.ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp).
  Maps output parameter names to JSON paths in the Kubernetes object.
  More on that in the following section.
  (_optional_)

#### Outputs

ResourceOps can produce output parameters.
They can output field values of the resource which is being manipulated.
For example:

```python
job = kubernetes_client.V1Job(...)

rop = kfp.dsl.ResourceOp(
    name="create-job",
    k8s_resource=job,
    action="create",
    attribute_outputs={"name": "{.metadata.name}"}
)
```

By default, ResourceOps output the resource's name as well as the whole resource
specification.

### Samples

For better understanding, please refer to the following samples:
[1](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/resource_ops/resource_ops.py)

---

## Persistent Volume Claims (PVCs)

Request the creation of PVC instances simple and fast.

### VolumeOp

A ResourceOp specialized in PVC creation.

[Link](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.VolumeOp)
to the corresponding Python library.

#### Arguments

The following arguments are an extension to `ResourceOp` arguments.
If a `k8s_resource` is passed, then none of the following should be provided.

* `resource_name`: The name of the resource which will be created.
  This string will be prepended with the workflow name.
  This may contain `PipelineParam`s.
  (_required_)
* `size`: The requested size for the PVC.
  This may contain `PipelineParam`s.
  (_required_)
* `storage_class`: The storage class to be used.
  This may contain `PipelineParam`s.
  (_optional_)
* `modes`: The `accessModes` of the PVC (defaults to `RWM`).
  Check
  [this documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)
  for further information.
  The user may find the following modes built-in:
    * `VOLUME_MODE_RWO`: `["ReadWriteOnce"]`
    * `VOLUME_MODE_RWM`: `["ReadWriteMany"]`
    * `VOLUME_MODE_ROM`: `["ReadOnlyMany"]`  
* `annotations`: Annotations to be patched in the PVC.
  These may contain `PipelineParam`s.
  (_optional_)
* `data_source`: It is used to create a PVC from a `VolumeSnapshot`.
  It can be either a `string` or a `V1TypedLocalObjectReference`, and may contain
  `PipelineParam`s. (_Alpha feature_, _optional_)

#### Outputs

Additionally to the whole specification of the resource and its name
(`ResourceOp` defaults), a `VolumeOp` also outputs the storage size of the
bounded Persistent Volume (as `step.outputs["size"]`).
However, this may be empty if the storage provisioner has a
`WaitForFirstConsumer` binding mode.
This value, if not empty, is always greater than or equal to the requested size.

#### Useful information

1. `VolumeOp` steps have a `.volume` attribute which is a `PipelineVolume`
   referencing the created PVC.
   More information on Pipeline Volumes in the following section.
2. A `ContainerOp` has a `pvolumes` argument in its constructor.
   This is a dictionary with mount paths as keys and volumes as values and
   functions similarly to `file_outputs` (which can then be used as
   `op.outputs["key"]` or `op.output`).
   For example:

```python
vop = dsl.VolumeOp(
    name="volume_creation",
    resource_name="mypvc",
    size="1Gi"
)
step1 = dsl.ContainerOp(
    name="step1",
    ...
    pvolumes={"/mnt": vop.volume}  # Implies execution after vop
)
step2 = dsl.ContainerOp(
    name="step2",
    ...
    pvolumes={"/data": step1.pvolume,  # Implies execution after step1
              "/mnt": dsl.PipelineVolume(pvc="existing-pvc")}
)
step3 = dsl.ContainerOp(
    name="step3",
    ...
    pvolumes={"/common": step2.pvolumes["/mnt"]}  # Implies execution after step2
)
```

### PipelineVolume

Reference Kubernetes volumes easily, mount them and express dependencies
through them.

A `PipelineVolume` is essentially a Kubernetes `Volume`(\*) carrying
dependencies, supplemented with an `.after()` method extending them.
Those dependencies can then be parsed properly by a `ContainerOp`, when consumed
in `pvolumes` argument or `add_pvolumes()` method, to extend the dependencies
of that step.

[Link](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.PipelineVolume)
to the corresponding Python library.

_(*) Inherits from V1Volume class of Kubernetes Python client._

#### Arguments

`PipelineVolume` constructor accepts all arguments `V1Volume` constructor does.
However, `name` can be omitted and a pseudo-random name for that volume is
generated instead.

Extra arguments:

* `pvc`: Name of an existing PVC to be referenced by this `PipelineVolume`.
  This value can be a `PipelineParam`.
* `volume`: Initialize a new `PipelineVolume` instance from an existing
  `V1Volume`, or its inherited types (e.g. `PipelineVolume`).

### Samples

For better understanding, please refer to the following samples:
[1](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/volume_ops/volume_ops.py),
[2](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/contrib/volume_ops/volumeop_dag.py),
[3](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/contrib/volume_ops/volumeop_parallel.py),
[4](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/contrib/volume_ops/volumeop_sequential.py)

---

## Volume Snapshots

Request the creation of Volume Snapshot instances simple and fast.

### VolumeSnapshotOp

A ResourceOp specialized in Volume Snapshot creation.

[Link](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.VolumeSnapshotOp)
to the corresponding Python library.

**NOTE:** You should check if your Kubernetes cluster admin has Volume Snapshots
enabled in your cluster.

#### Arguments

The following arguments are an extension to the `ResourceOp` arguments.
If a `k8s_resource` is passed, then none of the following may be provided.

* `resource_name`: The name of the resource which will be created.
  This string will be prepended with the workflow name.
  This may contain `PipelineParam`s.
  (_required_)
* `pvc`: The name of the PVC to be snapshotted.
  This may contain `PipelineParam`s.
  (_optional_)
* `snapshot_class`: The snapshot storage class to be used.
  This may contain `PipelineParam`s.
  (_optional_)
* `volume`: An instance of a `V1Volume`, or its inherited type (e.g.
  `PipelineVolume`).
  This may contain `PipelineParam`s.
  (_optional_)
* `annotations`: Annotations to be patched in the `VolumeSnapshot`.
  These may contain `PipelineParam`s.
  (_optional_)

**NOTE:** One of the `pvc` or `volume` needs to be provided.

#### Outputs

Additionally to the whole specification of the resource and its name
(`ResourceOp` defaults), a `VolumeSnapshotOp` also outputs the `restoreSize` of
the bounded `VolumeSnapshot` (as `step.outputs["size"]`).
This is the minimum size for a PVC clone of that snapshot.

#### Useful information

`VolumeSnapshotOp` steps have a `.snapshot` attribute which is a
`V1TypedLocalObjectReference`.
This can be passed as a `data_source` to create a PVC out of that
`VolumeSnapshot`.
The user may otherwise use the `step.outputs["name"]` as `data_source`.

### Samples

For better understanding, please refer to the following samples:
[1](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/volume_snapshot_ops/volume_snapshot_ops.py),
[2](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/contrib/volume_snapshot_ops/volume_snapshotop_rokurl.py)

## Next steps

* See samples in Kubeflow Pipelines 
  [repository](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples).
  For instance, check these samples of 
  [ResourceOps](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core/resource_ops), 
  [VolumeOps](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core/volume_ops)
  and 
  [VolumeSnapshotOps](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core/volume_snapshot_ops).
* Learn more about the 
  [Kubeflow Pipelines domain-specific language (DSL)](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html),
  a set of Python libraries that you can use to specify ML pipelines.
* For quick iteration, 
  [build components and pipelines](/docs/components/pipelines/legacy-v1/sdk/component-development/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md
================================================
+++
title = "Visualize Results in the Pipelines UI"
description = "Visualizing the results of your pipelines component"
weight = 80
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}


This page shows you how to use the Kubeflow Pipelines UI to visualize output 
from a Kubeflow Pipelines [component](/docs/components/pipelines/concepts/component/). 

## Introduction

The Kubeflow Pipelines UI offers built-in support for several types of 
visualizations, which you can use to provide rich performance evaluation and 
comparison data. Follow the instructions below to write visualization output
data to the file system. You can do this at any point during the pipeline execution.

You can view the output visualizations in the following places on the Kubeflow
Pipelines UI:

* The **Run output** tab shows the visualizations for all pipeline steps in the
  selected run. To open the tab in the Kubeflow Pipelines UI:

  1. Click **Experiments** to see your current pipeline experiments.
  1. Click the *experiment name* of the experiment that you want to view.
  1. Click the *run name* of the run that you want to view.
  1. Click the **Run output** tab.

    <img src="/docs/images/taxi-tip-run-output.png" 
      alt="Output visualization from a pipeline run"
      class="mt-3 mb-3 border border-info rounded">

* The **Visualizations** tab shows the visualization for the selected pipeline step.
  To open the tab in the Kubeflow Pipelines UI:

  1. Click **Experiments** to see your current pipeline experiments.
  1. Click the *experiment name* of the experiment that you want to view.
  1. Click the *run name* of the run that you want to view.
  1. On the **Graph** tab, click the step representing the pipeline component 
    that you want to view. The step details slide into view, showing the
    **Visualizations** tab.

    <img src="/docs/images/pipelines/v1/confusion-matrix-task.png" 
      alt="Table-based visualization from a pipeline component"
      class="mt-3 mb-3 border border-info rounded">

All screenshots and code snippets on this page come from a 
sample pipeline that you can run directly from the Kubeflow Pipelines UI.
See the [sample description and links below](#example-source).

<a id="v2-visualization"></a>
## v2 SDK: Use SDK visualization APIs

For KFP [SDK v2 and v2 compatible mode](https://kubeflow-pipelines.readthedocs.io/en/stable/), you can use 
convenient SDK APIs and system artifact types for metrics visualization. Currently KFP
supports ROC Curve, Confusion Matrix and Scalar Metrics formats. Full pipeline example
of all metrics visualizations can be found in [metrics_visualization_v2.py](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/test/metrics_visualization_v2.py). 

### Requirements

* Use Kubeflow Pipelines v1.7.0 or above: [upgrade Kubeflow Pipelines](/docs/components/pipelines/legacy-v1/installation/standalone-deployment/#upgrading-kubeflow-pipelines).
* Use `kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE` mode when you [compile and run your pipelines](/docs/components/pipelines/user-guides/core-functions/run-a-pipeline/).
* Make sure to use the latest environment kustomize manifest [pipelines/manifests/kustomize/env/dev/kustomization.yaml](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/env/dev/kustomization.yaml).


For a usage guide of each metric visualization output, refer to sections below:

### Confusion Matrix

Define `Output[ClassificationMetrics]` argument in your component function, then
output Confusion Matrix data using API 
`log_confusion_matrix(self, categories: List[str], matrix: List[List[int]])`. `categories`
provides a list of names for each label, `matrix` provides prediction performance for corresponding
label. There are multiple APIs you can use for logging Confusion Matrix. Refer to 
[artifact_types.py](https://github.com/kubeflow/pipelines/blob/55a2fb5c20011b01945c9867ddff0d39e9db1964/sdk/python/kfp/v2/components/types/artifact_types.py#L255-L256) for detail.

Refer to example below for logging Confusion Matrix:

```
@component(
    packages_to_install=['sklearn'],
    base_image='python:3.9'
)
def iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):
    from sklearn import datasets, model_selection
    from sklearn.linear_model import SGDClassifier
    from sklearn.metrics import confusion_matrix

    iris_dataset = datasets.load_iris()
    train_x, test_x, train_y, test_y = model_selection.train_test_split(
        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)


    classifier = SGDClassifier()
    classifier.fit(train_x, train_y)
    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)
    metrics.log_confusion_matrix(
        ['Setosa', 'Versicolour', 'Virginica'],
        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.
    )

@dsl.pipeline(
    name='metrics-visualization-pipeline')
def metrics_visualization_pipeline():
    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)
```

Visualization of Confusion Matrix is as below:

<img src="/docs/images/pipelines/v1/v2-compatible/confusion-matrix.png" 
  alt="V2 Confusion matrix visualization"
  class="mt-3 mb-3 border border-info rounded">

### ROC Curve 

Define `Output[ClassificationMetrics]` argument in your component function, then
output ROC Curve data using API 
`log_roc_curve(self, fpr: List[float], tpr: List[float], threshold: List[float])`. 
`fpr` defines a list of False Positive Rate values, `tpr` defines a list of 
True Positive Rate values, `threshold` indicates the level of sensitivity and specificity 
of this probability curve. There are multiple APIs you can use for logging ROC Curve. Refer to 
[artifact_types.py](https://github.com/kubeflow/pipelines/blob/55a2fb5c20011b01945c9867ddff0d39e9db1964/sdk/python/kfp/v2/components/types/artifact_types.py#L163-L164) for detail.

```
@component(
    packages_to_install=['sklearn'],
    base_image='python:3.9',
)
def wine_classification(metrics: Output[ClassificationMetrics]):
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import roc_curve
    from sklearn.datasets import load_wine
    from sklearn.model_selection import train_test_split, cross_val_predict

    X, y = load_wine(return_X_y=True)
    # Binary classification problem for label 1.
    y = y == 1

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
    rfc = RandomForestClassifier(n_estimators=10, random_state=42)
    rfc.fit(X_train, y_train)
    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')
    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')
    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)
    metrics.log_roc_curve(fpr, tpr, thresholds)

@dsl.pipeline(
    name='metrics-visualization-pipeline')
def metrics_visualization_pipeline():
    wine_classification_op = wine_classification()
```

Visualization of ROC Curve is as below:

<img src="/docs/images/pipelines/v1/v2-compatible/roc-curve.png" 
  alt="V2 ROC Curve visualization"
  class="mt-3 mb-3 border border-info rounded">

### Scalar Metrics

Define `Output[Metrics]` argument in your component function, then
output Scalar data using API `log_metric(self, metric: str, value: float)`. 
You can define any amount of metric by calling this API multiple times.
`metric` defines the name of metric, `value` is the value of this metric. Refer to 
[artifacts_types.py](https://github.com/kubeflow/pipelines/blob/55a2fb5c20011b01945c9867ddff0d39e9db1964/sdk/python/kfp/v2/components/types/artifact_types.py#L124) 
for detail.

```
@component(
    packages_to_install=['sklearn'],
    base_image='python:3.9',
)
def digit_classification(metrics: Output[Metrics]):
    from sklearn import model_selection
    from sklearn.linear_model import LogisticRegression
    from sklearn import datasets
    from sklearn.metrics import accuracy_score

    # Load digits dataset
    iris = datasets.load_iris()

    # # Create feature matrix
    X = iris.data

    # Create target vector
    y = iris.target

    #test size
    test_size = 0.33

    seed = 7
    #cross-validation settings
    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)

    #Model instance
    model = LogisticRegression()
    scoring = 'accuracy'
    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)

    #split data
    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)
    #fit model
    model.fit(X_train, y_train)

    #accuracy on test set
    result = model.score(X_test, y_test)
    metrics.log_metric('accuracy', (result*100.0))

@dsl.pipeline(
    name='metrics-visualization-pipeline')
def metrics_visualization_pipeline():
    digit_classification_op = digit_classification()
```

Visualization of Scalar Metrics is as below:

<img src="/docs/images/pipelines/v1/v2-compatible/scalar-metrics.png" 
  alt="V2 Scalar Metrics visualization"
  class="mt-3 mb-3 border border-info rounded">


### Markdown

Define `Output[Markdown]` argument in your component function, then
write Markdown file to path `<artifact_argument_name>.path`. 
Refer to
[artifact_types.py](https://github.com/kubeflow/pipelines/blob/55a2fb5c20011b01945c9867ddff0d39e9db1964/sdk/python/kfp/v2/components/types/artifact_types.py#L420-L428) 
for detail.

```
@component
def markdown_visualization(markdown_artifact: Output[Markdown]):
    markdown_content = '## Hello world \n\n Markdown content'
    with open(markdown_artifact.path, 'w') as f:
        f.write(markdown_content)
```

<img src="/docs/images/pipelines/v1/v2-compatible/markdown-visualization.png" 
  alt="Markdown visualization in v2 compatible mode"
  class="mt-3 mb-3 border border-info rounded">



### Single HTML file

You can specify an HTML file that your component creates, and the Kubeflow Pipelines UI renders that HTML in the output page. The HTML file must be self-contained, with no references to other files in the filesystem. The HTML file can contain absolute references to files on the web. Content running inside the HTML file is sandboxed in an iframe and cannot communicate with the Kubeflow Pipelines UI.

Define `Output[HTML]` argument in your component function, then
write HTML file to path `<artifact_argument_name>.path`. 
Refer to
[artifact_types.py](https://github.com/kubeflow/pipelines/blob/55a2fb5c20011b01945c9867ddff0d39e9db1964/sdk/python/kfp/v2/components/types/artifact_types.py#L409-L417) 
for detail.


```
@component
def html_visualization(html_artifact: Output[HTML]):
    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'
    with open(html_artifact.path, 'w') as f:
        f.write(html_content)
```

<img src="/docs/images/taxi-tip-analysis-step-output-webapp-popped-out.png" 
  alt="Web app output from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">


## Source of v2 examples

The metric visualization in V2 or V2 compatible mode depends on SDK visualization APIs,
refer to [metrics_visualization_v2.py](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/test/metrics_visualization_v2.py)
for a complete pipeline example. Follow instruction
[Compile and run your pipeline](/docs/components/pipelines/user-guides/core-functions/run-a-pipeline/)
to compile in V2 compatible mode.

## v1 SDK: Writing out metadata for the output viewers

For KFP v1, the pipeline component must write a JSON file specifying metadata
for the output viewer(s) that you want to use for visualizing the results. The
component must also export a file output artifact with an artifact name of
`mlpipeline-ui-metadata`, or else the Kubeflow Pipelines UI will not render
the visualization. In other words, the `.outputs.artifacts` setting for the
generated pipeline component should show:
`- {name: mlpipeline-ui-metadata, path: /mlpipeline-ui-metadata.json}`.
The JSON filepath does not matter, although `/mlpipeline-ui-metadata.json`
is used for consistency in the examples below.

The JSON specifies an array of `outputs`. Each `outputs` entry describes the
metadata for an output viewer. The JSON structure looks like this:

```
{
  "version": 1,
  "outputs": [
    {
      "type": "confusion_matrix",
      "format": "csv",
      "source": "my-dir/my-matrix.csv",
      "schema": [
        {"name": "target", "type": "CATEGORY"},
        {"name": "predicted", "type": "CATEGORY"},
        {"name": "count", "type": "NUMBER"},
      ],
      "labels": "vocab"
    },
    {
      ...
    }
  ]
}
```

If the component writes such a file to its container filesystem, the Kubeflow
Pipelines system extracts the file, and the Kubeflow Pipelines UI uses the file
to generate the specified viewer(s). The metadata specifies where to load the
artifact data from. The Kubeflow Pipelines UI loads the data **into memory**
and renders it. *Note:* You should keep this data at a volume that's manageable
by the UI, for example by running a sampling step before exporting the file as
an artifact.

The table below shows the available metadata fields that you can specify in the 
`outputs` array. Each `outputs` entry must have a `type`. Depending on value of 
`type`, other fields may also be required as described in the list of output 
viewers later on the page.

<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Field name</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>format</code></td>
        <td>The format of the artifact data. The default is <code>csv</code>. 
          <em>Note:</em> The only format currently available is 
          <code>csv</code>.
        </td>
      </tr>
      <tr>
        <td><code>header</code></td>
        <td>A list of strings to be used as headers for the artifact data. For 
          example, in a table these strings are used in the first row.</td>
      </tr>
      <tr>
        <td><code>labels</code></td>
        <td>A list of strings to be used as labels for artifact columns or 
          rows.</td>
      </tr>
      <tr>
        <td><code>predicted_col</code></td>
        <td>Name of the predicted column.</td>
      </tr>
      <tr>
        <td><code>schema</code></td>
        <td>A list of <code>{type, name}</code> objects that specify the schema 
          of the artifact data.</td>
      </tr>
      <tr>
        <td><code>source</code></td>
        <td><p>The full path to the data. The available locations
          include <code>http</code>, <code>https</code>, 
          <a href="https://aws.amazon.com/s3/">Amazon S3</a>, 
          <a href="https://docs.minio.io/">Minio</a>, and 
          <a href="https://cloud.google.com/storage/docs/">Google Cloud 
          Storage</a>.</p>
          <p>The path can contain wildcards ‘*’, in 
          which case the Kubeflow Pipelines UI concatenates the data from the 
          matching source files.</p>
          <p><code>source</code> can also contain inlined string data instead of
          a path when <code>storage='inline'</code>.</p>
          </td>
      </tr>
      <tr>
        <td><code>storage</code></td>
        <td><p>(Optional) When <code>storage</code> is <code>inline</code>, the value of
        <code>source</code> is parsed as inline data instead of a path. This applies
        to all types of outputs except <code>tensorboard</code>. See 
        <a href="#markdown">Markdown</a> or <a href="#web-app">Web app</a>
        below as examples.</p>
        <p><b>Be aware</b>, support for inline visualizations, other than
        markdown, was introduced in Kubeflow Pipelines 0.2.5. Before using these
        visualizations, [upgrade  your Kubeflow Pipelines cluster](/docs/components/pipelines/legacy-v1/installation/upgrade//)
        to version 0.2.5 or higher.</p>
        </td>
      </tr>
      <tr>
        <td><code>target_col</code></td>
        <td>Name of the target column.</td>
      </tr>
      <tr>
        <td><code>type</code></td>
        <td>Name of the viewer to be used to visualize the data. The list 
          <a href="#output-types">below</a> shows the available types.</td>
      </tr>
    </tbody>
  </table>
</div>

<a id="output-types"></a>
## Available output viewers

The sections below describe the available viewer types and the **required** 
metadata fields for each type.

### Confusion matrix

**Type:** `confusion_matrix`

**Required metadata fields:**

- `format`
- `labels`
- `schema`
- `source`

**Optional metadata fields:**

- `storage`

The `confusion_matrix` viewer plots a confusion matrix visualization of the data
from the given `source` path, using the `schema` to parse the data. The `labels`
provide the names of the classes to be plotted on the x and y axes.

Specify `'storage': 'inline'` to embed raw content of the
confusion matrix CSV file as a string in `source` field directly.

**Example:**

```Python

def confusion_matrix_viz(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json
    
  metadata = {
    'outputs' : [{
      'type': 'confusion_matrix',
      'format': 'csv',
      'schema': [
        {'name': 'target', 'type': 'CATEGORY'},
        {'name': 'predicted', 'type': 'CATEGORY'},
        {'name': 'count', 'type': 'NUMBER'},
      ],
      'source': <CONFUSION_MATRIX_CSV_FILE>,
      # Convert vocab to string because for bealean values we want "True|False" to match csv data.
      'labels': list(map(str, vocab)),
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)
```

**Visualization on the Kubeflow Pipelines UI:**

<img src="/docs/images/taxi-tip-confusion-matrix-step-output.png" 
  alt="Confusion matrix visualization from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

<a id="type-markdown"></a>
### Markdown

**Type:** `markdown`

**Required metadata fields:**

- `source`

**Optional metadata fields:**

- `storage`

The `markdown` viewer renders Markdown strings on the Kubeflow Pipelines UI. 
The viewer can read the Markdown data from the following locations:

* A Markdown-formatted string embedded in the `source` field. The value of the
 `storage` field must be `inline`.
* Markdown code in a remote file, at a path specified in the `source` field.
  The `storage` field can be empty or contain any value except `inline`.

**Example:**
```Python
def markdown_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json
    
  metadata = {
    'outputs' : [
    # Markdown that is hardcoded inline
    {
      'storage': 'inline',
      'source': '# Inline Markdown\n[A link](https://www.kubeflow.org/)',
      'type': 'markdown',
    },
    # Markdown that is read from a file
    {
      'source': 'gs://your_project/your_bucket/your_markdown_file',
      'type': 'markdown',
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)
```

**Visualization on the Kubeflow Pipelines UI:**

<img src="/docs/images/markdown-output.png" 
  alt="Markdown visualization from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

### ROC curve

**Type:** `roc`

**Required metadata fields:**

- `format`
- `schema`
- `source`

The `roc` viewer plots a receiver operating characteristic 
([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic))
curve using the data from the given source path. The Kubeflow Pipelines UI
assumes that the schema includes three columns with the following names:

* `fpr` (false positive rate)
* `tpr` (true positive rate)
* `thresholds`

**Optional metadata fields:**

- `storage`

When viewing the ROC curve, you can hover your cursor over the ROC curve to see 
the threshold value used for the cursor's closest `fpr` and `tpr` values.

Specify `'storage': 'inline'` to embed raw content of the ROC
curve CSV file as a string in `source` field directly.

**Example:**

```Python
def roc_vis(roc_csv_file_path: str, mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json

  df_roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})
  roc_file = os.path.join(roc_csv_file_path, 'roc.csv')
  with file_io.FileIO(roc_file, 'w') as f:
    df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'], header=False, index=False)

  metadata = {
    'outputs': [{
      'type': 'roc',
      'format': 'csv',
      'schema': [
        {'name': 'fpr', 'type': 'NUMBER'},
        {'name': 'tpr', 'type': 'NUMBER'},
        {'name': 'thresholds', 'type': 'NUMBER'},
      ],
      'source': roc_file
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)
```

**Visualization on the Kubeflow Pipelines UI:**

<img src="/docs/images/taxi-tip-roc-step-output.png" 
  alt="ROC curve visualization from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

### Table

**Type:** `table`

**Required metadata fields:**

- `format`
- `header`
- `source`

**Optional metadata fields:**

- `storage`

The `table` viewer builds an HTML table out of the data at the given `source`
path, where the `header` field specifies the values to be shown in the first row
of the table. The table supports pagination.

Specify `'storage': 'inline'` to embed CSV table content string
in `source` field directly.

**Example:**

```Python
def table_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json

  metadata = {
    'outputs' : [{
      'type': 'table',
      'storage': 'gcs',
      'format': 'csv',
      'header': [x['name'] for x in schema],
      'source': prediction_results
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)
```

**Visualization on the Kubeflow Pipelines UI:**

<img src="/docs/images/pipelines/v1/taxi-tip-prediction-step-output-table.png" 
  alt="Table-based visualization from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

### TensorBoard

**Type:** `tensorboard`

**Required metadata Fields:**

- `source`

The `tensorboard` viewer adds a **Start Tensorboard** button to the output page. 

When viewing the output page, you can:

* Click **Start Tensorboard** to start a 
  [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) Pod
  in your Kubeflow cluster. The button text switches to **Open Tensorboard**. 
* Click **Open Tensorboard** to open the TensorBoard interface in a new tab, 
  pointing to the logdir data specified in the `source` field.
* Click **Delete Tensorboard** to shutdown the Tensorboard instance.

**Note:** The Kubeflow Pipelines UI doesn't fully manage your TensorBoard 
instances. The "Start Tensorboard" button is a convenience feature so that
you don't have to interrupt your workflow when looking at pipeline runs. You're
responsible for recycling or deleting the TensorBoard Pods using your Kubernetes
management tools.

**Example:**

```Python
def tensorboard_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json

  metadata = {
    'outputs' : [{
      'type': 'tensorboard',
      'source': args.job_dir,
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)
```

**Visualization on the Kubeflow Pipelines UI:**

<img src="/docs/images/taxi-tip-training-step-output-tensorboard.png" 
  alt="TensorBoard option output from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

### Web app

**Type:** `web-app`

**Required metadata fields:**

- `source`

**Optional metadata fields:**

- `storage`

The `web-app` viewer provides flexibility for rendering custom output. You can
specify an HTML file that your component creates, and the Kubeflow Pipelines UI
renders that HTML in the output page. The HTML file must be self-contained, with
no references to other files in the filesystem. The HTML file can contain
absolute references to files on the web. Content running inside the web app is
sandboxed in an iframe and cannot communicate with the Kubeflow Pipelines UI.

Specify `'storage': 'inline'` to embed raw html in `source` field directly.

**Example:**

```Python
def tensorboard_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json

  static_html_path = os.path.join(output_dir, _OUTPUT_HTML_FILE)
  file_io.write_string_to_file(static_html_path, rendered_template)

  metadata = {
    'outputs' : [{
      'type': 'web-app',
      'storage': 'gcs',
      'source': static_html_path,
    }, {
      'type': 'web-app',
      'storage': 'inline',
      'source': '<h1>Hello, World!</h1>',
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)
```

**Visualization on the Kubeflow Pipelines UI:**

<img src="/docs/images/taxi-tip-analysis-step-output-webapp-popped-out.png" 
  alt="Web app output from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

<a id="example-source"></a>
## Source of v1 examples

The v1 examples come from the *tax tip prediction* sample that is
pre-installed when you deploy Kubeflow. 

You can run the sample by selecting 
**[Sample] ML - TFX - Taxi Tip Prediction Model Trainer** from the 
Kubeflow Pipelines UI. For help getting started with the UI, follow the 
[Kubeflow Pipelines quickstart](/docs/components/pipelines/legacy-v1/overview/quickstart/).

<!--- TODO: Will replace the tfx cab with tfx oss when it is ready.-->
The pipeline uses a number of prebuilt, reusable components, including:

* The [Confusion Matrix 
  component](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/components/local/confusion_matrix/src/confusion_matrix.py)
  which writes out the data for the `confusion_matrix` viewer.
* The [ROC 
  component](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/components/local/roc/src/roc.py)
  which writes out the data for the `roc` viewer.
* The [dnntrainer 
  component](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/components/kubeflow/dnntrainer/src/trainer/task.py)
  which writes out the data for the `tensorboard` viewer.
* The [tfma 
  component](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/components/deprecated/dataflow/tfma/component.yaml)
  which writes out the data for the `web-app` viewer.

## Lightweight Python component Notebook example

For a complete example of lightweigh Python component, you can refer to
[the lightweight python component notebook example](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/lightweight_component/lightweight_component.ipynb) to learn more about declaring output visualizations.

## YAML component example

You can also configure visualization in a component.yaml file. Refer to `{name: MLPipeline UI Metadata}` output in [Create Tensorboard Visualization component](https://github.com/kubeflow/pipelines/blob/f61048b5d2e1fb5a6a61782d570446b0ec940ff7/components/tensorflow/tensorboard/prepare_tensorboard/component.yaml#L12).



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/parameters.md
================================================
+++
title = "Pipeline Parameters"
description = "Passing data between pipeline components"
weight = 70
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

The [`kfp.dsl.PipelineParam` 
class](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.PipelineParam)
represents a reference to future data that will be passed to the pipeline or produced by a task.

Your pipeline function should have parameters, so that they can later be configured in the Kubeflow Pipelines UI.

When your pipeline function is called, each function argument will be a `PipelineParam` object.
You can pass those objects to the components as arguments to instantiate them and create tasks.
A `PipelineParam` can also represent an intermediate value that you pass between pipeline tasks.
Each task has outputs and you can get references to them from the `task.outputs[<output_name>]` dictionary.
The task output references can again be passed to other components as arguments.

In most cases you do not need to construct `PipelineParam` objects manually.

The following code sample shows how to define a pipeline with parameters:

```python
@kfp.dsl.pipeline(
  name='My pipeline',
  description='My machine learning pipeline'
)
def my_pipeline(
    my_num: int = 1000, 
    my_name: str = 'some text', 
    my_url: str = 'http://example.com'
):
  ...
  # In the pipeline function body you can use the `my_num`, `my_name`, 
  # `my_url` arguments as PipelineParam objects.
```

For more information, you can refer to the guide on
[building components and pipelines](/docs/components/pipelines/legacy-v1/sdk/component-development/).


================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton.md
================================================
+++
title = "Kubeflow Pipelines SDK for Tekton"
description = "How to run Kubeflow Pipelines with Tekton"
weight = 140
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

You can use the [KFP-Tekton SDK](https://github.com/kubeflow/kfp-tekton/tree/master/sdk)
to compile, upload and run your Kubeflow Pipeline DSL Python scripts on a 
[Kubeflow Pipelines with Tekton backend](https://github.com/kubeflow/kfp-tekton/tree/master/guides/kfp-user-guide).

## SDK packages

The `kfp-tekton` SDK is an extension to the [Kubeflow Pipelines SDK](/docs/components/pipelines/legacy-v1/sdk/sdk-overview/)
adding the `TektonCompiler` and the `TektonClient`:

* `kfp_tekton.compiler` includes classes and methods for compiling pipeline 
  Python DSL into a Tekton PipelineRun YAML spec. The methods in this package
  include, but are not limited to, the following:

  * `kfp_tekton.compiler.TektonCompiler.compile` compiles your Python DSL code
    into a single static configuration (in YAML format) that the Kubeflow Pipelines service
    can process. The Kubeflow Pipelines service converts the static 
    configuration into a set of Kubernetes resources for execution.

* `kfp_tekton.TektonClient` contains the Python client libraries for the [Kubeflow Pipelines API](/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec/).
  Methods in this package include, but are not limited to, the following:

  * `kfp_tekton.TektonClient.upload_pipeline` uploads a local file to create a new pipeline in Kubeflow Pipelines.
  * `kfp_tekton.TektonClient.create_experiment` creates a pipeline
    [experiment](/docs/components/pipelines/concepts/experiment/) and returns an
    experiment object.
  * `kfp_tekton.TektonClient.run_pipeline` runs a pipeline and returns a run object.
  * `kfp_tekton.TektonClient.create_run_from_pipeline_func` compiles a pipeline
    function and submits it for execution on Kubeflow Pipelines.
  * `kfp_tekton.TektonClient.create_run_from_pipeline_package` runs a local 
    pipeline package on Kubeflow Pipelines.


## Installing the KFP-Tekton SDK

You need **Python 3.5** or later to use the Kubeflow Pipelines SDK for Tekton.
We recommend to create a Python virtual environment first using
[Miniconda](https://conda.io/miniconda.html) or a virtual environment
manager such as `virtualenv` or the Python 3 `venv` module:

    python3 -m venv .venv-kfp-tekton
    source .venv-kfp-tekton/bin/activate

You can install the latest release of the `kfp-tekton` compiler from
[PyPi](https://pypi.org/project/kfp-tekton/):
    
    pip3 install kfp-tekton --upgrade

## Compiling Kubeflow Pipelines DSL scripts

The `kfp-tekton` Python package comes with the `dsl-compile-tekton` command line
executable, which should be available in your terminal shell environment after
installing the `kfp-tekton` Python package.

If you cloned the `kfp-tekton` project, you can find example pipelines in the
`samples` folder or in the `sdk/python/tests/compiler/testdata` folder.

    dsl-compile-tekton \
        --py sdk/python/tests/compiler/testdata/parallel_join.py \
        --output pipeline.yaml


**Note**: If the KFP DSL script contains a `__main__` method calling the
`kfp_tekton.compiler.TektonCompiler.compile()` function:

```Python
if __name__ == "__main__":
    from kfp_tekton.compiler import TektonCompiler
    TektonCompiler().compile(pipeline_func, "pipeline.yaml")
```

The pipeline can then be compiled by running the DSL script with `python3`
directly from the command line, producing a Tekton YAML file `pipeline.yaml`
in the same directory:

    python3 pipeline.py

## Additional documentation

* [Installing Kubeflow Pipelines with Tekton Backend](https://github.com/kubeflow/kfp-tekton/blob/master/guides/kfp_tekton_install.md)
* [KFP-Tekton Compiler Features](https://github.com/kubeflow/kfp-tekton/blob/master/sdk/FEATURES.md)
* [Kubeflow Pipelines for Tekton on GitHub](https://github.com/kubeflow/kfp-tekton)



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/python-based-visualizations.md
================================================
+++
title = "Python Based Visualizations (Deprecated)"
description = "Predefined and custom visualizations of pipeline outputs"
weight = 1400
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

{{% alert title="Deprecated" color="warning" %}}
Python based visualization is deprecated. We recommend fetching data via
Kubeflow Pipelines SDK and visualizing from your own notebook instead.
{{% /alert %}}

This page describes Python based visualizations, how to create them, and how to
use them to visualize results within the Kubeflow Pipelines UI. Python based
visualizations are available in Kubeflow Pipelines version
[0.1.29](https://github.com/kubeflow/pipelines/releases/tag/0.1.29) and later, and
in Kubeflow version 0.7.0 and later.

While Python based visualizations are intended to be the main method of
visualizing data within the Kubeflow Pipelines UI, they do not replace the
previous method of visualizing data within the Kubeflow Pipelines UI. When
considering which method of visualization to use within your pipeline, check the
limitations of Python based visualizations in the section below and compare
them with the requirements of your visualizations.

## Introduction

Python based visualizations are a new method to visualize results within the
Kubeflow Pipelines UI. This new method of visualizing results is done through
the usage of [nbconvert](https://github.com/jupyter/nbconvert). Alongside the
usage of nbconvert, results of a pipeline can now be visualized without a
component being included within the pipeline itself because the process of
visualizing results is now decoupled from a pipeline.

Python based visualizations provide two categories of visualizations. The first
being **predefined visualizations**. These visualizations are provided by
default in Kubeflow Pipelines and serve as a way for you and your customers to
easily and quickly generate powerful visualizations. The second category is
**custom visualizations**. Custom visualizations allow for you and your
customers to provided Python visualization code to be used to generate
visualizations. These visualizations allow for rapid development,
experimentation, and customization when visualizing results.

<img src="/docs/images/python-based-visualizations1.png"
  alt="Confusion matrix visualization from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

## Using predefined visualizations

### Predefined matrix visualization

<img src="/docs/images/python-based-visualizations2.png"
  alt="Confusion matrix visualization from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

1. Open the details of a run.
2. Select a component.
    * The component that is selected does not matter. But, if you want to
    visualize the output of a specific component, it is easier to do that within
    that component.
3. Select the **Artifacts** tab.
4. At the top of the tab you should see a card named **Visualization Creator**.
5. Within the card, provide a visualization type, a source, and any necessary
arguments.
    * Any required or optional arguments will be shown as a placeholder.
6. Click **Generate Visualization**.
7. View generated visualization by scrolling down.

### Predefined TFX visualization

<video width="100%" max-width="100%" height="auto" max-height="100%" controls>
  <source src="/docs/videos/tfdv_example_with_taxi_pipeline.webm" type="video/webm">
</video>

1. On the Pipelines page, click **[Sample] Unified DSL - Taxi Tip Prediction Model Trainer** to open the Pipeline Details page.
2. On the Pipeline Details page, click **Create run**.
3. On the Create run page,
    * Use a run name and an experiment name of your choice.
    * In the **pipeline-root** field, specify a storage bucket that you have permission to write to. For example, enter the path to a Google Cloud Storage bucket or an Amazon S3 bucket.
    * Click **Start** to create the run.
4. After the run is complete, on the Run Details page, click any step. For example, click the first step **csvexamplegen** as shown in the video above.
5. In the side panel of the selected step,
    * Click the **Artifacts** tab.
    * In the **Visualization Creator** section, choose **TFDV** from the drop down menu.
    * In the **Source** field, use **gs://ml-pipeline-playground/tfx_taxi_simple/data/data.csv**, which is the input data used for this run.
    * Click **Generate Visualization** and wait.
6. Move to the bottom of the **Artifacts** tab to find the generated visualization.

## Using custom visualizations

<img src="/docs/images/python-based-visualizations3.png"
  alt="Confusion matrix visualization from a pipeline component"
  class="mt-3 mb-3 border border-info rounded">

1. Enable custom visualizations within Kubeflow Pipelines.
    * If you have not yet deployed Kubeflow Pipelines to your cluster,
    you can edit the [frontend deployment YAML](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/base/pipeline/ml-pipeline-ui-deployment.yaml)
    file to include the following YAML that specifies that custom visualizations
    are allowed via environment variables.

        ```
        - env:
          - name: ALLOW_CUSTOM_VISUALIZATIONS
            value: true
        ```
    * If you already have Kubeflow Pipelines deployed within a cluster, you can
    edit the frontend deployment YAML to specify that custom visualizations are
    allowed in the same way described above. Details about updating
    deployments can be found in the Kubernetes documentation about
    [updating a deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment).
2. Open the details of a run.
3. Select a component.
    * The component that is selected does not matter. But, if you want to
    visualize the output of a specific component, it is easier to do that within
    that component.
4. Select the **Artifacts** tab.
5. At the top of the tab you should see a card named **Visualization Creator**.
6. Within the card, select the **CUSTOM** visualization type then provide a
source, and any necessary arguments (the source and argument variables are
optional for custom visualizations).
7. Provide the custom visualization code.
8. Click **Generate Visualization**.
9. View generated visualization by scrolling down.

A demo of the above instructions is as follows.

<video width="100%" max-width="100%" height="auto" max-height="100%" controls>
  <source src="/docs/videos/taxi_custom_visualization.webm" type="video/webm">
</video>

1. On the Pipelines page, click **[Sample] Unified DSL - Taxi Tip Prediction Model Trainer** to open the Pipeline Details page.
2. On the Pipeline Details page, click **Create run**.
3. On the Create run page,
    * Use a run name and an experiment name of your choice or simply use the default names chosen for you.
    * In the **pipeline-root** field, specify a storage bucket that you have permission to write to. For example, enter the path to a Google Cloud Storage bucket or an Amazon S3 bucket.
    * Click **Start** to create the run.
4. After the run is complete, on the Run Details page, click the step of **statisticsgen**. This step's output is statistics data generated by Tensorflow Data Validation.
5. In the side panel of the selected step,
    * Click the **Input/Output** tab to find out the **mlpipeline-ui-metadata** item and click the minio link there. This will open a new browser tab with information on output file path. Copy the output file path as shown in the demo video.
    * Get back to the Run Details page, and click the **Artifacts** tab.
    * At the top of the tab you should see a card named **Visualization Creator**, choose **Custom** from the drop down menu.
    * In the **Custom Visualization Code** field, fill in the following code snippet and replace [output file path] with the output file path you just copied from **mlpipeline-ui-metadata**.
    <pre style="overflow-x:scroll;overflow-y:hidden;white-space:pre;">
      import tensorflow_data_validation as tfdv
      stats = tfdv.load_statistics('[output file path]/stats_tfrecord')
      tfdv.visualize_statistics(stats)
    </pre>
    * Click **Generate Visualization** and wait.
6. Move to the bottom of the **Artifacts** tab to find the generated visualization.

## Known limitations
* Multiple visualizations cannot be generated concurrently.
    * This is because a single Python kernel is used to generate visualizations.
    * If visualizations are a major part of your workflow, it is recommended to
    increase the number of replicas within the
    [visualization deployment YAML](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/manifests/kustomize/base/pipeline/ml-pipeline-visualization-deployment.yaml)
    file or within the visualization service deployment itself.
        * _Please note that this does not directly solve the issue, instead it
        decreases the likelihood of experiencing delays when generating
        visualizations._
* Visualizations that take longer than 30 seconds will fail to generate.
    * For visualizations where the 30 second timeout is reached, you can add the
    **TimeoutValue** header to the request made by the frontend, specifying a
    _positive integer as ASCII string of at most 8 digits_ for the length of
    time required to generate a visualization as specified by the
    [grpc documentation](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests).
    * For visualizations that take longer than 100 seconds, you will have to
    specify a **TimeoutValue** within the request headers **AND** change the
    default kernel timeout of the visualization service. To change the default
    kernel timeout of the visualization service, set the **KERNEL_TIMEOUT**
    environment variable of the visualization service deployment to be the new
    timeout length in seconds within the
    [visualization deployment YAML](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/manifests/kustomize/base/pipeline/ml-pipeline-visualization-deployment.yaml)
    file or within the visualization service deployment itself.

        ```
        - env:
          - name: KERNEL_TIMEOUT
            value: 100
        ```

* The HTML content of the generated visualizations cannot be larger than 4MB.
    * gRPC by default imposes a limit of 4MB as the maximum size that can be
    sent and received by a server. To allow for visualizations that are larger
    than 4MB in size to be generated, you must manually set
    **MaxCallRecvMsgSize** for gRPC. This can be done by editing the provided
    options given to the gRPC server within [main.go](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/backend/src/apiserver/main.go#L128)
    to

        ```golang
        var maxCallRecvMsgSize = 4 * 1024 * 1024
        if serviceName == "Visualization" {
              // Only change the maxCallRecvMesSize if it is for visualizations
              maxCallRecvMsgSize = 50 * 1024 * 1024
        }
        opts := []grpc.DialOption{
              grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxCallRecvMsgSize)),
              grpc.WithInsecure(),
        }
        ```



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Building Python function-based components
> Building your own lightweight pipelines components using Python

A Kubeflow Pipelines component is a self-contained set of code that performs one step in your
ML workflow. A pipeline component is composed of:

*   The component code, which implements the logic needed to perform a step in your ML workflow.
*   A component specification, which defines the following:
    
    *   The component's metadata, its name and description.
    *   The component's interface, the component's inputs and outputs.
    *   The component's implementation, the Docker container image
        to run, how to pass inputs to your component code, and how
        to get the component's outputs.

Python function-based components make it easier to iterate quickly by letting you build your
component code as a Python function and generating the [component specification][component-spec] for you.
This document describes how to build Python function-based components and use them in your pipeline.

[component-spec]: https://www.kubeflow.org/docs/components/pipelines/reference/component-spec/

## Before you begin

1. Run the following command to install the Kubeflow Pipelines SDK. If you run this command in a Jupyter
   notebook, restart the kernel after installing the SDK. 
"""

!pip3 install kfp --upgrade

"""
2. Import the `kfp` package.
"""

import kfp
from kfp.components import create_component_from_func

"""
3. Create an instance of the [`kfp.Client` class][kfp-client] following steps in [connecting to Kubeflow Pipelines using the SDK client][connect-api].

[kfp-client]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/client.html#kfp.Client
[connect-api]: https://www.kubeflow.org/docs/components/pipelines/user-guides/core-functions/connect-api
"""

client = kfp.Client() # change arguments accordingly

"""
For more information about the Kubeflow Pipelines SDK, see the [SDK reference guide][sdk-ref].

[sdk-ref]: https://kubeflow-pipelines.readthedocs.io/en/stable/index.html

## Getting started with Python function-based components

This section demonstrates how to get started building Python function-based components by walking
through the process of creating a simple component.

1.  Define your component's code as a [standalone python function](#standalone). In this example,
    the function adds two floats and returns the sum of the two arguments.
"""

def add(a: float, b: float) -> float:
  '''Calculates sum of two arguments'''
  return a + b

"""
2.  Use `kfp.components.create_component_from_func` to generate the component specification YAML and return a
    factory function that you can use to create [`kfp.dsl.ContainerOp`][container-op] class instances for your pipeline.
    The component specification YAML is a reusable and shareable definition of your component.

[container-op]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp
"""

add_op = create_component_from_func(
    add, output_component_file='add_component.yaml')

"""
3.  Create and run your pipeline. [Learn more about creating and running pipelines][build-pipelines].

[build-pipelines]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/component-development/
"""

import kfp.dsl as dsl
@dsl.pipeline(
  name='Addition pipeline',
  description='An example pipeline that performs addition calculations.'
)
def add_pipeline(
  a='1',
  b='7',
):
  # Passes a pipeline parameter and a constant value to the `add_op` factory
  # function.
  first_add_task = add_op(a, 4)
  # Passes an output reference from `first_add_task` and a pipeline parameter
  # to the `add_op` factory function. For operations with a single return
  # value, the output reference can be accessed as `task.output` or
  # `task.outputs['output_name']`.
  second_add_task = add_op(first_add_task.output, b)

# Specify argument values for your pipeline run.
arguments = {'a': '7', 'b': '8'}

# Create a pipeline run, using the client you initialized in a prior step.
client.create_run_from_pipeline_func(add_pipeline, arguments=arguments)

"""
## Building Python function-based components

Use the following instructions to build a Python function-based component:

<a name="standalone"></a>

1.  Define a standalone Python function. This function must meet the following
    requirements:

    *   It should not use any code declared outside of the function definition.
    *   Import statements must be added inside the function. [Learn more about
        using and installing Python packages in your component](#packages).
    *   Helper functions must be defined inside this function.

1.  Kubeflow Pipelines uses your function's inputs and outputs to define your
    component's interface. [Learn more about passing data between
    components](#pass-data). Your function's inputs and outputs must meet the
    following requirements:
    
    *   If the function accepts or returns large amounts of data or complex
        data types, you must pass that data as a file. [Learn more about using
        large amounts of data as inputs or outputs](#pass-by-file).
    *   If the function accepts numeric values as parameters, the parameters
        must have type hints. Supported types are `int` and `float`. Otherwise,
        parameters are passed as strings.
    *   If your component returns multiple small outputs (short strings,
        numbers, or booleans), annotate your function with the
        [`typing.NamedTuple`][named-tuple-hint] type hint and use the
        [`collections.namedtuple`][named-tuple] function return your function's
        outputs as a new subclass of tuple. For an example, read
        [Passing parameters by value](#pass-by-value).

1.  (Optional.) If your function has complex dependencies, choose or build a
    container image for your Python function to run in. [Learn more about
    selecting or building your component's container image](#containers).
    
1.  Call [`kfp.components.create_component_from_func(func)`][create-component-from-func]
    to convert your function into a pipeline component.
    
    *   **func**: The Python function to convert.
    *   **base_image**: (Optional.) Specify the Docker container image to run
        this function in. [Learn more about selecting or building a container
        image](#containers).  
    *   **output_component_file**: (Optional.) Writes your component definition
        to a file. You can use this file to share the component with colleagues
        or reuse it in different pipelines.
    *   **packages_to_install**: (Optional.) A list of versioned Python
        packages to install before running your function. 

<a name="packages"></a>
### Using and installing Python packages

When Kubeflow Pipelines runs your pipeline, each component runs within a Docker
container image on a Kubernetes Pod. To load the packages that your Python
function depends on, one of the following must be true:

*   The package must be installed on the container image.
*   The package must be defined using the `packages_to_install` parameter of the
    [`kfp.components.create_component_from_func(func)`][create-component-from-func]
    function.
*   Your function must install the package. For example, your function can use
    the [`subprocess` module][subprocess] to run a command like `pip install`
    that installs a package.

<a name="containers"></a>
### Selecting or building a container image

Currently, if you do not specify a container image, your Python-function based
component uses the [`python:3.7` container image][python37]. If your function
has complex dependencies, you may benefit from using a container image that has
your dependencies preinstalled, or building a custom container image.
Preinstalling your dependencies reduces the amount of time that your component
runs in, since your component does not need to download and install packages
each time it runs.

Many frameworks, such as [TensorFlow][tf-docker] and [PyTorch][pytorch-docker],
and cloud service providers offer prebuilt container images that have common
dependencies installed.

If a prebuilt container is not available, you can build a custom container
image with your Python function's dependencies. For more information about
building a custom container, read the [Dockerfile reference guide in the Docker
documentation][dockerfile].

If you build or select a container image, instead of using the default
container image, the container image must use Python 3.5 or later.

<a name="pass-data"></a>
### Understanding how data is passed between components

When Kubeflow Pipelines runs your component, a container image is started in a
Kubernetes Pod and your component's inputs are passed in as command-line
arguments. When your component has finished, the component’s outputs are
returned as files.

Python function-based components make it easier to build pipeline components by
building the component specification for you. Python function-based components
also handle the complexity of passing inputs into your component and passing
your function's outputs back to your pipeline.  

The following sections describe how to pass parameters by value and by file. 

*   Parameters that are passed by value include numbers, booleans, and short
    strings. Kubeflow Pipelines passes parameters to your component by value,
    by passing the values as command-line arguments.
*   Parameters that are passed by file include CSV, images, and complex types.
    These files are stored in a location that is accessible to your component
    running on Kubernetes, such as a persistent volume claim or a cloud
    storage service. Kubeflow Pipelines passes parameters to your component by
    file, by passing their paths as a command-line argument.

<a name="parameter-names"></a>
#### Input and output parameter names

When you use the Kubeflow Pipelines SDK to convert your Python function to a
pipeline component, the Kubeflow Pipelines SDK uses the function's interface
to define the interface of your component in the following ways:

*   Some arguments define input parameters.
*   Some arguments define output parameters.
*   The function's return value is used as an output parameter. If the return
    value is a [`collections.namedtuple`][named-tuple], the named tuple is used
    to return several small values. 

Since you can pass parameters between components as a value or as a path, the
Kubeflow Pipelines SDK removes common parameter suffixes that leak the
component's expected implementation. For example, a Python function-based
component that ingests data and outputs CSV data may have an output argument
that is defined as `csv_path: comp.OutputPath(str)`. In this case, the output
is the CSV data, not the path. So, the Kubeflow Pipelines SDK simplifies the
output name to `csv`.

The Kubeflow Pipelines SDK uses the following rules to define the input and
output parameter names in your component's interface:

*   If the argument name ends with `_path` and the argument is annotated as an
    [`kfp.components.InputPath`][input-path] or
    [`kfp.components.OutputPath`][output-path], the parameter name is the
    argument name with the trailing `_path` removed.
*   If the argument name ends with `_file`, the parameter name is the argument
    name with the trailing `_file` removed.
*   If you return a single small value from your component using the `return`
    statement, the output parameter is named `output`.
*   If you return several small values from your component by returning a 
    [`collections.namedtuple`][named-tuple], the Kubeflow Pipelines SDK uses
    the tuple's field names as the output parameter names. 

Otherwise, the Kubeflow Pipelines SDK uses the argument name as the parameter
name.

<a name="pass-by-value"></a>
#### Passing parameters by value

Python function-based components make it easier to pass parameters between
components by value (such as numbers, booleans, and short strings), by letting
you define your component’s interface by annotating your Python function. The
supported types are `int`, `float`, `bool`, and `str`. You can also pass 
`list` or `dict` instances by value, if they contain small values, such as
`int`, `float`, `bool`, or `str` values. If you do not annotate your function,
these input parameters are passed as strings.

If your component returns multiple outputs by value, annotate your function
with the [`typing.NamedTuple`][named-tuple-hint] type hint and use the
[`collections.namedtuple`][named-tuple] function to return your function's
outputs as a new subclass of `tuple`.

You can also return metadata and metrics from your function.

*   Metadata helps you visualize pipeline results.
    [Learn more about visualizing pipeline metadata][kfp-visualize].
*   Metrics help you compare pipeline runs.
    [Learn more about using pipeline metrics][kfp-metrics].
    
The following example demonstrates how to return multiple outputs by value,
including component metadata and metrics. 

[python37]: https://hub.docker.com/layers/python/library/python/3.7/images/sha256-7eef781ed825f3b95c99f03f4189a8e30e718726e8490651fa1b941c6c815ad1?context=explore
[create-component-from-func]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.create_component_from_func
[subprocess]: https://docs.python.org/3/library/subprocess.html
[tf-docker]: https://www.tensorflow.org/install/docker
[pytorch-docker]: https://hub.docker.com/r/pytorch/pytorch/tags
[dockerfile]: https://docs.docker.com/engine/reference/builder/
[named-tuple-hint]: https://docs.python.org/3/library/typing.html#typing.NamedTuple
[named-tuple]: https://docs.python.org/3/library/collections.html#collections.namedtuple
[kfp-visualize]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/output-viewer/
[kfp-metrics]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/output-viewer/#v2-sdk-use-sdk-visualization-apis
[input-path]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.InputPath
[output-path]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.OutputPath
"""

from typing import NamedTuple
def multiple_return_values_example(a: float, b: float) -> NamedTuple(
  'ExampleOutputs',
  [
    ('sum', float),
    ('product', float),
    ('mlpipeline_ui_metadata', 'UI_metadata'),
    ('mlpipeline_metrics', 'Metrics')
  ]):
  """Example function that demonstrates how to return multiple values."""  
  sum_value = a + b
  product_value = a * b

  # Export a sample tensorboard
  metadata = {
    'outputs' : [{
      'type': 'tensorboard',
      'source': 'gs://ml-pipeline-dataset/tensorboard-train',
    }]
  }

  # Export two metrics
  metrics = {
    'metrics': [
      {
        'name': 'sum',
        'numberValue':  float(sum_value),
      },{
        'name': 'product',
        'numberValue':  float(product_value),
      }
    ]  
  }

  from collections import namedtuple
  example_output = namedtuple(
      'ExampleOutputs',
      ['sum', 'product', 'mlpipeline_ui_metadata', 'mlpipeline_metrics'])
  return example_output(sum_value, product_value, metadata, metrics)

"""
<a name="pass-by-file"></a>
#### Passing parameters by file

Python function-based components make it easier to pass files to your
component, or to return files from your component, by letting you annotate
your Python function's parameters to specify which parameters refer to a file. 
Your Python function's parameters can refer to either input or output files.
If your parameter is an output file, Kubeflow Pipelines passes your function a
path or stream that you can use to store your output file.

The following example accepts a file as an input and returns two files as outputs.
"""

def split_text_lines(
    source_path: comp.InputPath(str),
    odd_lines_path: comp.OutputPath(str),
    even_lines_path: comp.OutputPath(str)):
    """Splits a text file into two files, with even lines going to one file
    and odd lines to the other."""

    with open(source_path, 'r') as reader:
        with open(odd_lines_path, 'w') as odd_writer:
            with open(even_lines_path, 'w') as even_writer:
                while True:
                    line = reader.readline()
                    if line == "":
                        break
                    odd_writer.write(line)
                    line = reader.readline()
                    if line == "":
                        break
                    even_writer.write(line)

"""
In this example, the inputs and outputs are defined as parameters of the
`split_text_lines` function. This lets Kubeflow Pipelines pass the path to the
source data file and the paths to the output data files into the function.

To accept a file as an input parameter, use one of the following type annotations:

*   [`kfp.components.InputBinaryFile`][input-binary]: Use this annotation to
    specify that your function expects a parameter to be an
    [`io.BytesIO`][bytesio] instance that this function can read.
*   [`kfp.components.InputPath`][input-path]: Use this annotation to specify that
    your function expects a parameter to be the path to the input file as
    a `string`.
*   [`kfp.components.InputTextFile`][input-text]: Use this annotation to specify
    that your function expects a parameter to be an
    [`io.TextIOWrapper`][textiowrapper] instance that this function can read.

To return a file as an output, use one of the following type annotations:

*   [`kfp.components.OutputBinaryFile`][output-binary]: Use this annotation to
    specify that your function expects a parameter to be an
    [`io.BytesIO`][bytesio] instance that this function can write to.
*   [`kfp.components.OutputPath`][output-path]: Use this annotation to specify that
    your function expects a parameter to be the path to store the output file at
    as a `string`.
*   [`kfp.components.OutputTextFile`][output-text]: Use this annotation to specify
    that your function expects a parameter to be an
    [`io.TextIOWrapper`][textiowrapper] that this function can write to.

[input-binary]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.InputBinaryFile
[input-path]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.InputPath
[input-text]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.InputTextFile
[output-binary]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.OutputBinaryFile
[output-path]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.OutputPath
[output-text]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.OutputTextFile
[bytesio]: https://docs.python.org/3/library/io.html#io.BytesIO
[textiowrapper]: https://docs.python.org/3/library/io.html#io.TextIOWrapper

## Example Python function-based component

This section demonstrates how to build a Python function-based component that uses imports,
helper functions, and produces multiple outputs.

1.  Define your function. This example function uses the `numpy` package to calculate the quotient
    and remainder for a given dividend and divisor in a helper function. In addition to the quotient
    and remainder, the function also returns metadata for visualization and two metrics.
"""

from typing import NamedTuple

def my_divmod(
  dividend: float,
  divisor: float) -> NamedTuple(
    'MyDivmodOutput',
    [
      ('quotient', float),
      ('remainder', float),
      ('mlpipeline_ui_metadata', 'UI_metadata'),
      ('mlpipeline_metrics', 'Metrics')
    ]):
    '''Divides two numbers and calculate  the quotient and remainder'''

    # Import the numpy package inside the component function
    import numpy as np

    # Define a helper function
    def divmod_helper(dividend, divisor):
        return np.divmod(dividend, divisor)

    (quotient, remainder) = divmod_helper(dividend, divisor)

    from tensorflow.python.lib.io import file_io
    import json

    # Export a sample tensorboard
    metadata = {
      'outputs' : [{
        'type': 'tensorboard',
        'source': 'gs://ml-pipeline-dataset/tensorboard-train',
      }]
    }

    # Export two metrics
    metrics = {
      'metrics': [{
          'name': 'quotient',
          'numberValue':  float(quotient),
        },{
          'name': 'remainder',
          'numberValue':  float(remainder),
        }]}

    from collections import namedtuple
    divmod_output = namedtuple('MyDivmodOutput',
        ['quotient', 'remainder', 'mlpipeline_ui_metadata',
         'mlpipeline_metrics'])
    return divmod_output(quotient, remainder, json.dumps(metadata),
                         json.dumps(metrics))

"""
2.  Test your function by running it directly, or with unit tests.
"""

my_divmod(100, 7)

"""
3.  This should return a result like the following:

    ```
    MyDivmodOutput(quotient=14, remainder=2, mlpipeline_ui_metadata='{"outputs": [{"type": "tensorboard", "source": "gs://ml-pipeline-dataset/tensorboard-train"}]}', mlpipeline_metrics='{"metrics": [{"name": "quotient", "numberValue": 14.0}, {"name": "remainder", "numberValue": 2.0}]}')
    ```

4.  Use `kfp.components.create_component_from_func` to return a factory function that you can use to create
    [`kfp.dsl.ContainerOp`][container-op] class instances for your pipeline. This example also specifies the base container
    image to run this function in.

[container-op]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp
"""

divmod_op = comp.create_component_from_func(
    my_divmod, base_image='tensorflow/tensorflow:1.11.0-py3')

"""
4.  Define your pipeline. This example uses the `divmod_op` factory function and the `add_op`
    factory function from an earlier example.
"""

import kfp.dsl as dsl
@dsl.pipeline(
   name='Calculation pipeline',
   description='An example pipeline that performs arithmetic calculations.'
)
def calc_pipeline(
   a='1',
   b='7',
   c='17',
):
    # Passes a pipeline parameter and a constant value as operation arguments.
    add_task = add_op(a, 4) # The add_op factory function returns
                            # a dsl.ContainerOp class instance. 

    # Passes the output of the add_task and a pipeline parameter as operation
    # arguments. For an operation with a single return value, the output
    # reference is accessed using `task.output` or
    # `task.outputs['output_name']`.
    divmod_task = divmod_op(add_task.output, b)

    # For an operation with multiple return values, output references are
    # accessed as `task.outputs['output_name']`.
    result_task = add_op(divmod_task.outputs['quotient'], c)

"""
5.  Compile and run your pipeline. [Learn more about compiling and running pipelines][build-pipelines].

[build-pipelines]: https://www.kubeflow.org/docs/components/pipelines/legacy-v1/sdk/build-pipeline/#compile-and-run-your-pipeline
"""

# Specify pipeline argument values
arguments = {'a': '7', 'b': '8'}

# Submit a pipeline run
client.create_run_from_pipeline_func(calc_pipeline, arguments=arguments)



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.md
================================================
+++
title = "Building Python function-based components"
description = "Building your own lightweight pipelines components using Python"
weight = 50
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

<!--
AUTOGENERATED FROM content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb
PLEASE UPDATE THE JUPYTER NOTEBOOK AND REGENERATE THIS FILE USING scripts/nb_to_md.py.-->

<style>
.notebook-links {display: flex; margin: 1em 0;}
.notebook-links a {padding: .75em; margin-right: .75em; font-weight: bold;}
a.colab-link {
padding-left: 3.25em;
background-image: url(/docs/images/logos/colab.ico);
background-repeat: no-repeat;
background-size: contain;
}
a.github-link {
padding-left: 2.75em;
background-image: url(/docs/images/logos/github.png);
background-repeat: no-repeat;
background-size: auto 75%;
background-position: left center;
}
</style>
<div class="notebook-links">
<a class="colab-link" href="https://colab.research.google.com/github/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb">Run in Google Colab</a>
<a class="github-link" href="https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb">View source on GitHub</a>
</div>


A Kubeflow Pipelines component is a self-contained set of code that performs one step in your
ML workflow. A pipeline component is composed of:

*   The component code, which implements the logic needed to perform a step in your ML workflow.
*   A component specification, which defines the following:
    
    *   The component's metadata, its name and description.
    *   The component's interface, the component's inputs and outputs.
    *   The component's implementation, the Docker container image
        to run, how to pass inputs to your component code, and how
        to get the component's outputs.

Python function-based components make it easier to iterate quickly by letting you build your
component code as a Python function and generating the [component specification][component-spec] for you.
This document describes how to build Python function-based components and use them in your pipeline.

[component-spec]: /docs/components/pipelines/reference/component-spec/

## Before you begin

1. Run the following command to install the Kubeflow Pipelines SDK. If you run this command in a Jupyter
   notebook, restart the kernel after installing the SDK. 


```python
$ pip install kfp==1.8
```

2. Import the `kfp` package.


```python
import kfp
from kfp.components import create_component_from_func
```

3. Create an instance of the [`kfp.Client` class][kfp-client] following steps in [connecting to Kubeflow Pipelines using the SDK client][connect-api].

[kfp-client]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html#kfp.Client
[connect-api]: /docs/components/pipelines/user-guides/core-functions/connect-api


```python
client = kfp.Client() # change arguments accordingly
```

For more information about the Kubeflow Pipelines SDK, see the [SDK reference guide][sdk-ref].

[sdk-ref]: https://kubeflow-pipelines.readthedocs.io/en/stable/index.html

## Getting started with Python function-based components

This section demonstrates how to get started building Python function-based components by walking
through the process of creating a simple component.

1.  Define your component's code as a [standalone Python function](#standalone). In this example,
    the function adds two floats and returns the sum of the two arguments.


```python
def add(a: float, b: float) -> float:
  '''Calculates sum of two arguments'''
  return a + b
```

2.  Use `kfp.components.create_component_from_func` to generate the component specification YAML and return a
    factory function that you can use to create [`kfp.dsl.ContainerOp`][container-op] class instances for your pipeline.
    The component specification YAML is a reusable and shareable definition of your component.

[container-op]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp


```python
add_op = create_component_from_func(
    add, output_component_file='add_component.yaml')
```

3.  Create and run your pipeline. [Learn more about creating and running pipelines][build-pipelines].

[build-pipelines]: /docs/components/pipelines/legacy-v1/sdk/component-development/


```python
import kfp.dsl as dsl
@dsl.pipeline(
  name='Addition pipeline',
  description='An example pipeline that performs addition calculations.'
)
def add_pipeline(
  a='1',
  b='7',
):
  # Passes a pipeline parameter and a constant value to the `add_op` factory
  # function.
  first_add_task = add_op(a, 4)
  # Passes an output reference from `first_add_task` and a pipeline parameter
  # to the `add_op` factory function. For operations with a single return
  # value, the output reference can be accessed as `task.output` or
  # `task.outputs['output_name']`.
  second_add_task = add_op(first_add_task.output, b)

# Specify argument values for your pipeline run.
arguments = {'a': '7', 'b': '8'}

# Create a pipeline run, using the client you initialized in a prior step.
client.create_run_from_pipeline_func(add_pipeline, arguments=arguments)
```

## Building Python function-based components

Use the following instructions to build a Python function-based component:

<a name="standalone"></a>

1.  Define a standalone Python function. This function must meet the following
    requirements:

    *   It should not use any code declared outside of the function definition.
    *   Import statements must be added inside the function. [Learn more about
        using and installing Python packages in your component](#packages).
    *   Helper functions must be defined inside this function.

1.  Kubeflow Pipelines uses your function's inputs and outputs to define your
    component's interface. [Learn more about passing data between
    components](#pass-data). Your function's inputs and outputs must meet the
    following requirements:
    
    *   If the function accepts or returns large amounts of data or complex
        data types, you must pass that data as a file. [Learn more about using
        large amounts of data as inputs or outputs](#pass-by-file).
    *   If the function accepts numeric values as parameters, the parameters
        must have type hints. Supported types are `int` and `float`. Otherwise,
        parameters are passed as strings.
    *   If your component returns multiple small outputs (short strings,
        numbers, or booleans), annotate your function with the
        [`typing.NamedTuple`][named-tuple-hint] type hint and use the
        [`collections.namedtuple`][named-tuple] function return your function's
        outputs as a new subclass of tuple. For an example, read
        [Passing parameters by value](#pass-by-value).

1.  (Optional.) If your function has complex dependencies, choose or build a
    container image for your Python function to run in. [Learn more about
    selecting or building your component's container image](#containers).
    
1.  Call [`kfp.components.create_component_from_func(func)`][create-component-from-func]
    to convert your function into a pipeline component.
    
    *   **func**: The Python function to convert.
    *   **base_image**: (Optional.) Specify the Docker container image to run
        this function in. [Learn more about selecting or building a container
        image](#containers).  
    *   **output_component_file**: (Optional.) Writes your component definition
        to a file. You can use this file to share the component with colleagues
        or reuse it in different pipelines.
    *   **packages_to_install**: (Optional.) A list of versioned Python
        packages to install before running your function. 

<a name="packages"></a>
### Using and installing Python packages

When Kubeflow Pipelines runs your pipeline, each component runs within a Docker
container image on a Kubernetes Pod. To load the packages that your Python
function depends on, one of the following must be true:

*   The package must be installed on the container image.
*   The package must be defined using the `packages_to_install` parameter of the
    [`kfp.components.create_component_from_func(func)`][create-component-from-func]
    function.
*   Your function must install the package. For example, your function can use
    the [`subprocess` module][subprocess] to run a command like `pip install`
    that installs a package.

<a name="containers"></a>
### Selecting or building a container image

Currently, if you do not specify a container image, your Python-function based
component uses the [`python:3.7` container image][python37]. If your function
has complex dependencies, you may benefit from using a container image that has
your dependencies preinstalled, or building a custom container image.
Preinstalling your dependencies reduces the amount of time that your component
runs in, since your component does not need to download and install packages
each time it runs.

Many frameworks, such as [TensorFlow][tf-docker] and [PyTorch][pytorch-docker],
and cloud service providers offer prebuilt container images that have common
dependencies installed.

If a prebuilt container is not available, you can build a custom container
image with your Python function's dependencies. For more information about
building a custom container, read the [Dockerfile reference guide in the Docker
documentation][dockerfile].

If you build or select a container image, instead of using the default
container image, the container image must use Python 3.5 or later.

<a name="pass-data"></a>
### Understanding how data is passed between components

When Kubeflow Pipelines runs your component, a container image is started in a
Kubernetes Pod and your component's inputs are passed in as command-line
arguments. When your component has finished, the component’s outputs are
returned as files.

Python function-based components make it easier to build pipeline components by
building the component specification for you. Python function-based components
also handle the complexity of passing inputs into your component and passing
your function's outputs back to your pipeline.  

The following sections describe how to pass parameters by value and by file. 

*   Parameters that are passed by value include numbers, booleans, and short
    strings. Kubeflow Pipelines passes parameters to your component by value,
    by passing the values as command-line arguments.
*   Parameters that are passed by file include CSV, images, and complex types.
    These files are stored in a location that is accessible to your component
    running on Kubernetes, such as a persistent volume claim or a cloud
    storage service. Kubeflow Pipelines passes parameters to your component by
    file, by passing their paths as a command-line argument.

<a name="parameter-names"></a>
#### Input and output parameter names

When you use the Kubeflow Pipelines SDK to convert your Python function to a
pipeline component, the Kubeflow Pipelines SDK uses the function's interface
to define the interface of your component in the following ways:

*   Some arguments define input parameters.
*   Some arguments define output parameters.
*   The function's return value is used as an output parameter. If the return
    value is a [`collections.namedtuple`][named-tuple], the named tuple is used
    to return several small values. 

Since you can pass parameters between components as a value or as a path, the
Kubeflow Pipelines SDK removes common parameter suffixes that leak the
component's expected implementation. For example, a Python function-based
component that ingests data and outputs CSV data may have an output argument
that is defined as `csv_path: comp.OutputPath(str)`. In this case, the output
is the CSV data, not the path. So, the Kubeflow Pipelines SDK simplifies the
output name to `csv`.

The Kubeflow Pipelines SDK uses the following rules to define the input and
output parameter names in your component's interface:

*   If the argument name ends with `_path` and the argument is annotated as an
    [`kfp.components.InputPath`][input-path] or
    [`kfp.components.OutputPath`][output-path], the parameter name is the
    argument name with the trailing `_path` removed.
*   If the argument name ends with `_file`, the parameter name is the argument
    name with the trailing `_file` removed.
*   If you return a single small value from your component using the `return`
    statement, the output parameter is named `output`.
*   If you return several small values from your component by returning a 
    [`collections.namedtuple`][named-tuple], the Kubeflow Pipelines SDK uses
    the tuple's field names as the output parameter names. 

Otherwise, the Kubeflow Pipelines SDK uses the argument name as the parameter
name.

<a name="pass-by-value"></a>
#### Passing parameters by value

Python function-based components make it easier to pass parameters between
components by value (such as numbers, booleans, and short strings), by letting
you define your component’s interface by annotating your Python function. The
supported types are `int`, `float`, `bool`, and `str`. You can also pass 
`list` or `dict` instances by value, if they contain small values, such as
`int`, `float`, `bool`, or `str` values. If you do not annotate your function,
these input parameters are passed as strings.

If your component returns multiple outputs by value, annotate your function
with the [`typing.NamedTuple`][named-tuple-hint] type hint and use the
[`collections.namedtuple`][named-tuple] function to return your function's
outputs as a new subclass of `tuple`.

You can also return metadata and metrics from your function.

*   Metadata helps you visualize pipeline results.
    [Learn more about visualizing pipeline metadata][kfp-visualize].
*   Metrics help you compare pipeline runs.
    [Learn more about using pipeline metrics][kfp-metrics].
    
The following example demonstrates how to return multiple outputs by value,
including component metadata and metrics. 

[python37]: https://hub.docker.com/layers/python/library/python/3.7/images/sha256-7eef781ed825f3b95c99f03f4189a8e30e718726e8490651fa1b941c6c815ad1?context=explore
[create-component-from-func]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.create_component_from_func
[subprocess]: https://docs.python.org/3/library/subprocess.html
[tf-docker]: https://www.tensorflow.org/install/docker
[pytorch-docker]: https://hub.docker.com/r/pytorch/pytorch/tags
[dockerfile]: https://docs.docker.com/engine/reference/builder/
[named-tuple-hint]: https://docs.python.org/3/library/typing.html#typing.NamedTuple
[named-tuple]: https://docs.python.org/3/library/collections.html#collections.namedtuple
[kfp-visualize]: /docs/components/pipelines/legacy-v1/sdk/output-viewer/
[kfp-metrics]: /docs/components/pipelines/legacy-v1/sdk/output-viewer/#v2-sdk-use-sdk-visualization-apis
[input-path]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.InputPath
[output-path]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.OutputPath


```python
from typing import NamedTuple
def multiple_return_values_example(a: float, b: float) -> NamedTuple(
  'ExampleOutputs',
  [
    ('sum', float),
    ('product', float),
    ('mlpipeline_ui_metadata', 'UI_metadata'),
    ('mlpipeline_metrics', 'Metrics')
  ]):
  """Example function that demonstrates how to return multiple values."""  
  sum_value = a + b
  product_value = a * b

  # Export a sample tensorboard
  metadata = {
    'outputs' : [{
      'type': 'tensorboard',
      'source': 'gs://ml-pipeline-dataset/tensorboard-train',
    }]
  }

  # Export two metrics
  metrics = {
    'metrics': [
      {
        'name': 'sum',
        'numberValue':  float(sum_value),
      },{
        'name': 'product',
        'numberValue':  float(product_value),
      }
    ]  
  }

  from collections import namedtuple
  example_output = namedtuple(
      'ExampleOutputs',
      ['sum', 'product', 'mlpipeline_ui_metadata', 'mlpipeline_metrics'])
  return example_output(sum_value, product_value, metadata, metrics)
```

<a name="pass-by-file"></a>
#### Passing parameters by file

Python function-based components make it easier to pass files to your
component, or to return files from your component, by letting you annotate
your Python function's parameters to specify which parameters refer to a file. 
Your Python function's parameters can refer to either input or output files.
If your parameter is an output file, Kubeflow Pipelines passes your function a
path or stream that you can use to store your output file.

The following example accepts a file as an input and returns two files as outputs.


```python
def split_text_lines(
    source_path: comp.InputPath(str),
    odd_lines_path: comp.OutputPath(str),
    even_lines_path: comp.OutputPath(str)):
    """Splits a text file into two files, with even lines going to one file
    and odd lines to the other."""

    with open(source_path, 'r') as reader:
        with open(odd_lines_path, 'w') as odd_writer:
            with open(even_lines_path, 'w') as even_writer:
                while True:
                    line = reader.readline()
                    if line == "":
                        break
                    odd_writer.write(line)
                    line = reader.readline()
                    if line == "":
                        break
                    even_writer.write(line)
```

In this example, the inputs and outputs are defined as parameters of the
`split_text_lines` function. This lets Kubeflow Pipelines pass the path to the
source data file and the paths to the output data files into the function.

To accept a file as an input parameter, use one of the following type annotations:

*   [`kfp.components.InputBinaryFile`][input-binary]: Use this annotation to
    specify that your function expects a parameter to be an
    [`io.BytesIO`][bytesio] instance that this function can read.
*   [`kfp.components.InputPath`][input-path]: Use this annotation to specify that
    your function expects a parameter to be the path to the input file as
    a `string`.
*   [`kfp.components.InputTextFile`][input-text]: Use this annotation to specify
    that your function expects a parameter to be an
    [`io.TextIOWrapper`][textiowrapper] instance that this function can read.

To return a file as an output, use one of the following type annotations:

*   [`kfp.components.OutputBinaryFile`][output-binary]: Use this annotation to
    specify that your function expects a parameter to be an
    [`io.BytesIO`][bytesio] instance that this function can write to.
*   [`kfp.components.OutputPath`][output-path]: Use this annotation to specify that
    your function expects a parameter to be the path to store the output file at
    as a `string`.
*   [`kfp.components.OutputTextFile`][output-text]: Use this annotation to specify
    that your function expects a parameter to be an
    [`io.TextIOWrapper`][textiowrapper] that this function can write to.

[input-binary]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.InputBinaryFile
[input-path]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.InputPath
[input-text]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.InputTextFile
[output-binary]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.OutputBinaryFile
[output-path]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.OutputPath
[output-text]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.OutputTextFile
[bytesio]: https://docs.python.org/3/library/io.html#io.BytesIO
[textiowrapper]: https://docs.python.org/3/library/io.html#io.TextIOWrapper

## Example Python function-based component

This section demonstrates how to build a Python function-based component that uses imports,
helper functions, and produces multiple outputs.

1.  Define your function. This example function uses the `numpy` package to calculate the quotient
    and remainder for a given dividend and divisor in a helper function. In addition to the quotient
    and remainder, the function also returns metadata for visualization and two metrics.


```python
from typing import NamedTuple

def my_divmod(
  dividend: float,
  divisor: float) -> NamedTuple(
    'MyDivmodOutput',
    [
      ('quotient', float),
      ('remainder', float),
      ('mlpipeline_ui_metadata', 'UI_metadata'),
      ('mlpipeline_metrics', 'Metrics')
    ]):
    '''Divides two numbers and calculate  the quotient and remainder'''

    # Import the numpy package inside the component function
    import numpy as np

    # Define a helper function
    def divmod_helper(dividend, divisor):
        return np.divmod(dividend, divisor)

    (quotient, remainder) = divmod_helper(dividend, divisor)

    from tensorflow.python.lib.io import file_io
    import json

    # Export a sample tensorboard
    metadata = {
      'outputs' : [{
        'type': 'tensorboard',
        'source': 'gs://ml-pipeline-dataset/tensorboard-train',
      }]
    }

    # Export two metrics
    metrics = {
      'metrics': [{
          'name': 'quotient',
          'numberValue':  float(quotient),
        },{
          'name': 'remainder',
          'numberValue':  float(remainder),
        }]}

    from collections import namedtuple
    divmod_output = namedtuple('MyDivmodOutput',
        ['quotient', 'remainder', 'mlpipeline_ui_metadata',
         'mlpipeline_metrics'])
    return divmod_output(quotient, remainder, json.dumps(metadata),
                         json.dumps(metrics))
```

2.  Test your function by running it directly, or with unit tests.


```python
my_divmod(100, 7)
```

3.  This should return a result like the following:

    ```
    MyDivmodOutput(quotient=14, remainder=2, mlpipeline_ui_metadata='{"outputs": [{"type": "tensorboard", "source": "gs://ml-pipeline-dataset/tensorboard-train"}]}', mlpipeline_metrics='{"metrics": [{"name": "quotient", "numberValue": 14.0}, {"name": "remainder", "numberValue": 2.0}]}')
    ```

4.  Use `kfp.components.create_component_from_func` to return a factory function that you can use to create
    [`kfp.dsl.ContainerOp`][container-op] class instances for your pipeline. This example also specifies the base container
    image to run this function in.

[container-op]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp


```python
divmod_op = comp.create_component_from_func(
    my_divmod, base_image='tensorflow/tensorflow:1.11.0-py3')
```

4.  Define your pipeline. This example uses the `divmod_op` factory function and the `add_op`
    factory function from an earlier example.


```python
import kfp.dsl as dsl
@dsl.pipeline(
   name='Calculation pipeline',
   description='An example pipeline that performs arithmetic calculations.'
)
def calc_pipeline(
   a='1',
   b='7',
   c='17',
):
    # Passes a pipeline parameter and a constant value as operation arguments.
    add_task = add_op(a, 4) # The add_op factory function returns
                            # a dsl.ContainerOp class instance. 

    # Passes the output of the add_task and a pipeline parameter as operation
    # arguments. For an operation with a single return value, the output
    # reference is accessed using `task.output` or
    # `task.outputs['output_name']`.
    divmod_task = divmod_op(add_task.output, b)

    # For an operation with multiple return values, output references are
    # accessed as `task.outputs['output_name']`.
    result_task = add_op(divmod_task.outputs['quotient'], c)
```

5.  Compile and run your pipeline. [Learn more about compiling and running pipelines][build-pipelines].

[build-pipelines]: /docs/components/pipelines/legacy-v1/sdk/build-pipeline/#compile-and-run-your-pipeline


```python
# Specify pipeline argument values
arguments = {'a': '7', 'b': '8'}

# Submit a pipeline run
client.create_run_from_pipeline_func(calc_pipeline, arguments=arguments)
```


<div class="notebook-links">
<a class="colab-link" href="https://colab.research.google.com/github/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb">Run in Google Colab</a>
<a class="github-link" href="https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb">View source on GitHub</a>
</div>


================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/sdk-overview.md
================================================
+++
title = "Introduction to the Pipelines SDK"
description = "Overview of using the SDK to build components and pipelines"
weight = 10
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

The [Kubeflow Pipelines 
SDK](https://kubeflow-pipelines.readthedocs.io/en/stable/source/html)
provides a set of Python packages that you can use to specify and run your 
machine learning (ML) workflows. A *pipeline* is a description of an ML 
workflow, including all of the *components* that make up the steps in the 
workflow and how the components interact with each other. 

**Note**: The SDK documentation here refers to [Kubeflow Pipelines with Argo](https://github.com/kubeflow/pipelines) which is the default.
If you are running [Kubeflow Pipelines with Tekton](https://github.com/kubeflow/kfp-tekton) instead,
please follow the [Kubeflow Pipelines SDK for Tekton](/docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton) documentation.

## SDK packages

The Kubeflow Pipelines SDK includes the following packages:

* [`kfp.compiler`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/compiler.html)
  includes classes and methods for compiling pipeline Python DSL into a workflow yaml spec
    Methods in this package include, but are not limited
  to, the following:

  * `kfp.compiler.Compiler.compile` compiles your Python DSL code into a single 
    static configuration (in YAML format) that the Kubeflow Pipelines service
    can process. The Kubeflow Pipelines service converts the static 
    configuration into a set of Kubernetes resources for execution.

* [`kfp.components`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html)
  includes classes and methods for interacting with pipeline components. 
  Methods in this package include, but are not limited to, the following:

  * `kfp.components.func_to_container_op` converts a Python function to a 
    pipeline component and returns a factory function.
    You can then call the factory function to construct an instance of a 
    pipeline task
    ([`ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp)) 
    that runs the original function in a container.

  * `kfp.components.load_component_from_file` loads a pipeline component from
    a file and returns a factory function.
    You can then call the factory function to construct an instance of a 
    pipeline task 
    ([`ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp)) 
    that runs the component container image.

  * `kfp.components.load_component_from_url` loads a pipeline component from
    a URL and returns a factory function.
    You can then call the factory function to construct an instance of a 
    pipeline task 
    ([`ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp)) 
    that runs the component container image.

* [`kfp.dsl`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html)
  contains the domain-specific language (DSL) that you can use to define and
  interact with pipelines and components. 
  Methods, classes, and modules in this package include, but are not limited to, 
  the following:

  * `kfp.dsl.PipelineParam` represents a pipeline parameter that you can pass
    from one pipeline component to another. See the guide to 
    [pipeline parameters](/docs/components/pipelines/legacy-v1/sdk/parameters/).
  * `kfp.dsl.component` is a decorator for DSL functions that returns a
    pipeline component.
    ([`ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ContainerOp)).
  * `kfp.dsl.pipeline` is a decorator for Python functions that returns a
    pipeline.
  * `kfp.dsl.python_component` is a decorator for Python functions that adds
    pipeline component metadata to the function object.
  * [`kfp.dsl.types`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.types.html) 
    contains a list of types defined by the Kubeflow Pipelines SDK. Types
    include basic types like `String`, `Integer`, `Float`, and `Bool`, as well
    as domain-specific types like `GCPProjectID` and `GCRPath`.
    See the guide to 
    [DSL static type checking](/docs/components/pipelines/legacy-v1/sdk/static-type-checking).
  * [`kfp.dsl.ResourceOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ResourceOp)
    represents a pipeline task (op) which lets you directly manipulate 
    Kubernetes resources (`create`, `get`, `apply`, ...).
  * [`kfp.dsl.VolumeOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.VolumeOp)
    represents a pipeline task (op) which creates a new `PersistentVolumeClaim` 
    (PVC). It aims to make the common case of creating a `PersistentVolumeClaim` 
    fast.
  * [`kfp.dsl.VolumeSnapshotOp`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.VolumeSnapshotOp)
    represents a pipeline task (op) which creates a new `VolumeSnapshot`. It 
    aims to make the common case of creating a `VolumeSnapshot` fast.
  * [`kfp.dsl.PipelineVolume`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.PipelineVolume)
    represents a volume used to pass data between pipeline steps. `ContainerOp`s 
    can mount a `PipelineVolume` either via the constructor's argument 
    `pvolumes` or `add_pvolumes()` method.
  * [`kfp.dsl.ParallelFor`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ParallelFor)
    represents a parallel for loop over a static or dynamic set of items in a pipeline.
    Each iteration of the for loop is executed in parallel.
  
  * [`kfp.dsl.ExitHandler`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ExitHandler)
    represents an exit handler that is invoked upon exiting a pipeline. A typical
    usage of `ExitHandler` is garbage collection.
  
  * [`kfp.dsl.Condition`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.Condition)
    represents a group of ops, that will only be executed when a certain condition is met.
    The condition specified need to be determined at runtime, by incorporating at least one task output, 
    or PipelineParam in the boolean expression.

* [`kfp.Client`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html)
  contains the Python client libraries for the [Kubeflow Pipelines 
  API](/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec/).
  Methods in this package include, but are not limited to, the following:

  * `kfp.Client.create_experiment` creates a pipeline 
    [experiment](/docs/components/pipelines/concepts/experiment/) and returns an
    experiment object.
  * `kfp.Client.run_pipeline` runs a pipeline and returns a run object.
  * `kfp.Client.create_run_from_pipeline_func` compiles a pipeline function and submits it
    for execution on Kubeflow Pipelines.
  * `kfp.Client.create_run_from_pipeline_package` runs a local pipeline package on Kubeflow Pipelines.
  * `kfp.Client.upload_pipeline` uploads a local file to create a new pipeline in Kubeflow Pipelines.
  * `kfp.Client.upload_pipeline_version` uploads a local file to create a pipeline version. [Follow an example to learn more about creating a pipeline version](/docs/components/pipelines/legacy-v1/tutorials/sdk-examples).

* [Kubeflow Pipelines extension modules](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.extensions.html)
  include classes and functions for specific platforms on which you can use
  Kubeflow Pipelines. Examples include utility functions for on premises,
  Google Cloud Platform (GCP), Amazon Web Services (AWS), and Microsoft Azure.

* [Kubeflow Pipelines diagnose_me modules](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/sdk/python/kfp/cli/diagnose_me) include classes and functions that help with environment diagnostic tasks. 
 
  * `kfp.cli.diagnose_me.dev_env` reports on diagnostic metadata from your development environment, such as your python library version.
  * `kfp.cli.diagnose_me.kubernetes_cluster` reports on diagnostic data from your Kubernetes cluster, such as Kubernetes secrets.
  * `kfp.cli.diagnose_me.gcp` reports on diagnostic data related to your GCP environment.
 
## Kubeflow Pipelines CLI tool 
The Kubeflow Pipelines CLI tool enables you to use a subset of the Kubeflow Pipelines SDK directly from the command line. The Kubeflow Pipelines CLI tool provides the following commands:

* `kfp diagnose_me` runs environment diagnostic with specified parameters.
  * `--json` - Indicates that this command must return its results as JSON. Otherwise, results are returned in human readable format.
  * `--namespace TEXT` - Specifies the Kubernetes namespace to use. all-namespaces is the default value.
  * `--project-id TEXT` - For GCP deployments, this value specifies the GCP project to use. If this value is not specified, the environment default is used.
  
* `kfp pipeline <COMMAND>` provides the following commands to help you manage pipelines.
  * `get`  - Gets detailed information about a Kubeflow pipeline from your Kubeflow Pipelines cluster.
  * `list` - Lists the pipelines that have been uploaded to your Kubeflow Pipelines cluster.
  * `upload` - Uploads a pipeline to your Kubeflow Pipelines cluster.
  
* `kfp run <COMMAND>` provides the following commands to help you manage pipeline runs.
  * `get` - Displays the details of a pipeline run.
  * `list` - Lists recent pipeline runs.
  * `submit` - Submits a pipeline run.
  
* `kfp --endpoint <ENDPOINT>` - Specifies the endpoint that the Kubeflow Pipelines CLI should connect to.

## Installing the SDK

Follow the guide to 
[installing the Kubeflow Pipelines SDK](/docs/components/pipelines/legacy-v1/sdk/install-sdk/).

## Building pipelines and components

This section summarizes the ways you can use the SDK to build pipelines and 
components.

A Kubeflow _pipeline_ is a portable and scalable definition of an ML workflow.
Each step in your ML workflow, such as preparing data or training a model,
is an instance of a pipeline component.

[Learn more about building pipelines](/docs/components/pipelines/legacy-v1/sdk/build-pipeline).

A pipeline _component_ is a self-contained set of code that performs one step
in your ML workflow. Components are defined in a component specification, which
defines the following:

*   The component’s interface, its inputs and outputs.
*   The component’s implementation, the container image and the command to
    execute.
*   The component’s metadata, such as the name and description of the
    component.

Use the following options to create or reuse pipeline components.    

*   You can build components by defining a component specification for a
    containerized application.

    [Learn more about building pipeline components](/docs/components/pipelines/legacy-v1/sdk/component-development).

*   Lightweight Python function-based components make it easier to build a
    component by using the Kubeflow Pipelines SDK to generate the component
    specification for a Python function.

    [Learn how to build a Python function-based component](/docs/components/pipelines/legacy-v1/sdk/python-function-components).

*   You can reuse prebuilt components in your pipeline.

    [Learn more about reusing prebuilt components](/docs/examples/shared-resources/).


## Next steps

* Learn how to [write recursive functions in the 
  DSL](/docs/components/pipelines/legacy-v1/sdk/dsl-recursion).
* Build a [pipeline component](/docs/components/pipelines/legacy-v1/sdk/component-development/).
* Find out how to use the DSL to [manipulate Kubernetes resources dynamically 
  as steps of your pipeline](/docs/components/pipelines/legacy-v1/sdk/manipulate-resources/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/sdk/static-type-checking.md
================================================
+++
title = "DSL Static Type Checking"
description = "Statically check the component I/O types"
weight = 100
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This page describes how to integrate the type information in the pipeline and utilize the 
static type checking for fast development iterations.

## Motivation

A pipeline is a workflow consisting of [components](/docs/components/pipelines/legacy-v1/sdk/component-development/) and each
component contains inputs and outputs. The DSL compiler supports static type checking to ensure the type consistency among the component
I/Os within the same pipeline. Static type checking helps you to identify component I/O inconsistencies without running the pipeline. 
It also shortens the development cycles by catching the errors early. 
This feature is especially useful in two cases: 

* When the pipeline is huge and manually checking the types is infeasible; 
* When some components are shared ones and the type information is not immediately available.

## Type system  

In Kubeflow pipeline, a type is defined as a type name with an [OpenAPI Schema](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md)
property, which defines the input parameter schema. **Warning**: the pipeline system 
currently does not check the input value against the schema when you submit a pipeline run. However, this feature will come in the near 
future. 

There is a set of [core types](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/types.py) defined in the 
Pipelines SDK and you can use these core types or define your custom types. 

In the component YAML, types are specified as a string or a dictionary with the OpenAPI Schema, as illustrated below.
"*component a*" expects an input with Integer type and emits three outputs with the type GCSPath, customized_type and GCRPath. 
Among these types, Integer, GCSPath, and GCRPath are core types that are predefined in the SDK while customized_type is a user-defined
type.  

```yaml
name: component a
description: component desc
inputs:
  - {name: field_l, type: Integer}
outputs:
  - {name: field_m, type: {GCSPath: {openapi_schema_validator: {type: string, pattern: "^gs://.*$" } }}}
  - {name: field_n, type: customized_type}
  - {name: field_o, type: GCRPath} 
implementation:
  container:
    image: gcr.io/ml-pipeline/component-a
    command: [python3, /pipelines/component/src/train.py]
    args: [
      --field-l, {inputValue: field_l},
    ]
    fileOutputs: 
      field_m: /schema.txt
      field_n: /feature.txt
      field_o: /output.txt
```

Similarly, when you write a component with the decorator, you can annotate I/O with types in the function signature, as shown below.

```python
from kfp.dsl import component
from kfp.dsl.types import Integer, GCRPath


@component
def task_factory_a(field_l: Integer()) -> {
    'field_m': {
        'GCSPath': {
            'openapi_schema_validator':
                '{"type": "string", "pattern": "^gs://.*$"}'
        }
    },
    'field_n': 'customized_type',
    'field_o': GCRPath()
}:
  return ContainerOp(
      name='operator a',
      image='gcr.io/ml-pipeline/component-a',
      command=['python3', '/pipelines/component/src/train.py'],
      arguments=[
          '--field-l',
          field_l,
      ],
      file_outputs={
          'field_m': '/schema.txt',
          'field_n': '/feature.txt',
          'field_o': '/output.txt'
      })
```

You can also annotate pipeline inputs with types and the input are checked against the component I/O types as well. For example,

```python
@component
def task_factory_a(
    field_m: {
        'GCSPath': {
            'openapi_schema_validator':
                '{"type": "string", "pattern": "^gs://.*$"}'
        }
    }, field_o: 'Integer'):
  return ContainerOp(
      name='operator a',
      image='gcr.io/ml-pipeline/component-a',
      arguments=[
          '--field-l',
          field_m,
          '--field-o',
          field_o,
      ],
  )


# Pipeline input types are also checked against the component I/O types.
@dsl.pipeline(name='type_check', description='')
def pipeline(
    a: {
        'GCSPath': {
            'openapi_schema_validator':
                '{"type": "string", "pattern": "^gs://.*$"}'
        }
    } = 'good',
    b: Integer() = 12):
  task_factory_a(field_m=a, field_o=b)


try:
  compiler.Compiler().compile(pipeline, 'pipeline.tar.gz', type_check=True)
except InconsistentTypeException as e:
  print(e)
```

## How does the type checking work?

The basic checking criterion is the equality checking. In other words, type checking passes only when the type name strings are equal
and the corresponding OpenAPI Schema properties are equal. Examples of type checking failure are:

* "GCSPath" vs. "GCRPath"
* "Integer" vs. "Float"
* {'GCSPath': {'openapi_schema_validator': '{"type": "string", "pattern": "^gs://.*$"}'}} vs.  
{'GCSPath': {'openapi_schema_validator': '{"type": "string", "pattern": "^gcs://.*$"}'}}

If inconsistent types are detected, it throws an [InconsistentTypeException](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/types.py).


## Type checking configuration

Type checking is enabled by default and it can be disabled in two ways:

If you compile the pipeline programmably:

```python
compiler.Compiler().compile(pipeline_a, 'pipeline_a.tar.gz', type_check=False)
```

If you compile the pipeline using the dsl-compiler tool:

```bash
dsl-compiler --py pipeline.py --output pipeline.zip --disable-type-check
```

### Fine-grained configuration

Sometimes, you might want to enable the type checking but disable certain arguments. For example, 
when the upstream component generates an output with type "*Float*" and the downstream can ingest either 
"*Float*" or "*Integer*", it might fail if you define the type as "*Float_or_Integer*". 
Disabling the type checking per-argument is also supported as shown below.

```python
@dsl.pipeline(name='type_check_a', description='')
def pipeline():
  a = task_factory_a(field_l=12)
  # For each of the arguments, you can also ignore the types by calling
  # ignore_type function.
  b = task_factory_b(
      field_x=a.outputs['field_n'],
      field_y=a.outputs['field_o'],
      field_z=a.outputs['field_m'].ignore_type())

compiler.Compiler().compile(pipeline, 'pipeline.tar.gz', type_check=True)
```

### Missing types

DSL compiler passes the type checking if either of the upstream or the downstream components lack the type information for some parameters. 
The effects are the same as that of ignoring the type information. However, 
type checking would still fail if some I/Os lack the type information and some I/O types are incompatible.

## Next steps

Learn how to define a KubeFlow pipeline with Python DSL and compile the
pipeline with type checking: a 
[Jupyter notebook demo](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/dsl_static_type_checking/dsl_static_type_checking.ipynb).



================================================
File: content/en/docs/components/pipelines/legacy-v1/tutorials/_index.md
================================================
+++
title = "Samples and Tutorials"
description = "Samples and tutorials for Kubeflow Pipelines"
weight = 90
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}


================================================
File: content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md
================================================
+++
title = "Experiment with the Kubeflow Pipelines API"
description = "Get started with the Kubeflow Pipelines API"
weight = 20
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This tutorial demonstrates how to use the Kubeflow Pipelines API to build, run, and manage pipelines. This guide is recommended for users who would like to learn how to manage Kubeflow Pipelines using the REST API.

## Before you start

This tutorial assumes that you have access to the `ml-pipeline` service. If Kubeflow is not configured to use an identity provider, use port-forwarding to directly access the service.

```
SVC_PORT=$(kubectl -n kubeflow get svc/ml-pipeline -o json | jq ".spec.ports[0].port")
kubectl port-forward -n kubeflow svc/ml-pipeline ${SVC_PORT}:8888
```

This tutorial assumes that the service is accessible on localhost.

You also need to install [jq](https://stedolan.github.io/jq/download/), and the [Kubeflow Pipelines SDK](/docs/components/pipelines/legacy-v1/sdk/install-sdk/).

## Building and running a pipeline

Follow this guide to download, compile, and run the [`sequential.py` sample pipeline](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/sequential/sequential.py). To learn how to compile and run pipelines using the Kubeflow Pipelines SDK or a Jupyter notebook, follow the [experimenting with Kubeflow Pipelines samples tutorial](/docs/components/pipelines/legacy-v1/tutorials/build-pipeline/).

```
PIPELINE_URL=https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/core/sequential/sequential.py
PIPELINE_FILE=${PIPELINE_URL##*/}
PIPELINE_NAME=${PIPELINE_FILE%.*}

wget -O ${PIPELINE_FILE} ${PIPELINE_URL}
dsl-compile --py ${PIPELINE_FILE} --output ${PIPELINE_NAME}.tar.gz
```

After running the commands above, you should get two files in your current directory: `sequential.py` and `sequential.tar.gz`. Run the following command to deploy the generated `.tar.gz` file as you would do using the [Kubeflow Pipelines UI](/docs/components/pipelines/user-guides/core-functions/run-a-pipeline/#run-pipeline---kfp-dashboard), but this time using the REST API.

```
SVC=localhost:8888
PIPELINE_ID=$(curl -F "uploadfile=@${PIPELINE_NAME}.tar.gz" ${SVC}/apis/v1beta1/pipelines/upload | jq -r .id)
```

If the operation was successful, you should see the pipeline in the central dashboard. You can also get the details using the `PIPELINE_ID` with the following API call.

```
curl ${SVC}/apis/v1beta1/pipelines/${PIPELINE_ID} | jq
```

The response should be similar to the following one:

```
{
  "id": "d30d28d7-0bfc-4f0c-8a57-6844a8ec9742",
  "created_at": "2020-02-20T16:15:02Z",
  "name": "sequential.tar.gz",
  "parameters": [
    {
      "name": "url",
      "value": "gs://ml-pipeline-playground/shakespeare1.txt"
    }
  ],
  "default_version": {
    "id": "d30d28d7-0bfc-4f0c-8a57-6844a8ec9742",
    "name": "sequential.tar.gz",
    "created_at": "2020-02-20T16:15:02Z",
    "parameters": [
      {
        "name": "url",
        "value": "gs://ml-pipeline-playground/shakespeare1.txt"
      }
    ],
    "resource_references": [
      {
        "key": {
          "type": "PIPELINE",
          "id": "d30d28d7-0bfc-4f0c-8a57-6844a8ec9742"
        },
        "relationship": "OWNER"
      }
    ]
  }
}
```

Finally, use the `PIPELINE_ID` to trigger a run of your pipeline.

```
RUN_ID=$((
curl -H "Content-Type: application/json" -X POST ${SVC}/apis/v1beta1/runs \
-d @- << EOF
{
   "name":"${PIPELINE_NAME}_run",
   "pipeline_spec":{
      "pipeline_id":"${PIPELINE_ID}"
   }
}
EOF
) | jq -r .run.id)
```

Run the following command occasionally to see how the status of your run changes. After a while, the status of your pipeline should change to **Succeeded**.

```
curl ${SVC}/apis/v1beta1/runs/${RUN_ID} | jq
```

The response should be similar to the following one:

```
{
  "run": {
    "id": "4ff0debd-d6d7-4681-8593-21ec002e6e0c",
    "name": "sequential_run",
    "pipeline_spec": {
      "pipeline_id": "d30d28d7-0bfc-4f0c-8a57-6844a8ec9742",
      "pipeline_name": "sequential.tar.gz",
      "workflow_manifest": "{...}"
    },
    "resource_references": [
      {
        "key": {
          "type": "EXPERIMENT",
          "id": "27af7eee-ce0a-44ba-a44d-07142abfc83c"
        },
        "name": "Default",
        "relationship": "OWNER"
      }
    ],
    "created_at": "2020-02-20T16:18:58Z",
    "scheduled_at": "1970-01-01T00:00:00Z",
    "finished_at": "1970-01-01T00:00:00Z",
    "status": "Succeeded"
  },
  "pipeline_runtime": {
    "workflow_manifest": "{...}"
  }
}
```

Read [Kubeflow Pipelines API Reference](/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec/) to learn more about how to use the API.



================================================
File: content/en/docs/components/pipelines/legacy-v1/tutorials/benchmark-examples.md
================================================
+++
title = "Using the Kubeflow Pipelines Benchmark Scripts"
description = "How to use the Kubeflow Pipelines Benchmark Scripts"
weight = 10
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This guide explains the Kubeflow Pipelines [benchmark scripts](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/tools/benchmarks)
and demonstrates how to use them to collect basic performance data of a given
Kubeflow Pipelines instance.

## About the Kubeflow Pipelines benchmark script

The Kubeflow Pipelines benchmark scripts simulate typical workloads and record
performance metrics, such as server latencies and pipeline run durations. To simulate a typical workload, the benchmark script uploads a pipeline
manifest file to a Kubeflow Pipelines instance as a pipeline or
a pipeline version, and creates multiple runs simultaneously.

You can specify the pipelines manifest used in the benchmark script, or you can
use one of the preloaded samples pipelines. For example, the preloaded samples
in Kubeflow pipelines can be used.
Moreover, it is also a good practice to use a pipeline manifest that is representative of your particular use case. For example, if your Kubeflow Pipelines cluster is mainly used for pipelines of image recognition tasks, then it would be desirable to use an image recognition pipeline in the benchmark scripts.

After a proper pipeline is chosen, the benchmark scripts will run it multiple
times simultaneously as mentioned before. Among all the operations that the Kubeflow
Pipelines can perform, running a pipeline is arguably the most unpredictable and
costly one. Other operations, such as creating a pipeline (version) or creating an
experiment, usually induce a predictable and moderate cost. For example, creating a
pipeline version will introduce a new row in the pipeline versions table and a new
file in minio server. The new file's size depends on the pipeline version's
manifest. If we exclude the rare case of an extremely large manifest and assume
an average sized manifest for each created pipeline version, the total cost of
creating a pipeline version grows linearly with the number of pipeline versions.
However, on the other hand, the cost of running a pipeline or a pipeline version
involves much more uncertainty and sometimes quite a high cost. A pipeline or a
pipeline version can have arbitrary components and hence running a pipeline or a
pipeline version can incur arbitrary time and space complexities. For example, a step
in a pipeline can use a customized container image which performs a super
expensive training task. In addition, the runs in a Kubeflow Pipelines instance
also consume more DB space than pipelines, pipeline versions, experiments, etc.
Therefore, in order to understand the performance and scalability pain points in
a Kubeflow Pipelines instance, it is more effective to focus on the run operation
in workloads.

## Prerequisites for running benchmark scripts

To run the provided benchmark scripts, you need the following:

*  A Jupyter notebook environment with access to the Kubeflow Pipelines API on
   your Kubeflow Pipelines cluster. For example, you must be able to call the
   `CREATE`, `GET`, `DELETE`, and `LIST` methods of the pipeline, pipeline version,
   run, job, and experiment services from your Jupyter notebook environment.
*  A Kubeflow Pipelines cluster. If you do not have a Kubeflow Pipelines
   cluster, learn more about your [options for installing Kubeflow
   Pipelines](/docs/components/pipelines/legacy-v1/overview/).
*  A pipeline manifest. For example, this guide uses the
   [taxi_updated_pool.yaml](https://storage.googleapis.com/ml-pipeline/sample-benchmark/taxi_updated_pool.yaml)
   pipeline manifest file.

One way of setting up everything and running a benchmark script is shown below
as an example.

## Running the benchmark scripts

Use the following instructions to run the benchmark script on your Kubeflow
Pipelines cluster.

1. Download the [`run_service_api.ipynb`](https://storage.googleapis.com/ml-pipeline/sample-benchmark/run_service_api.ipynb)
   benchmark script in your Jupyter notebook environment.
1. Open [`run_service_api.ipynb`](https://storage.googleapis.com/ml-pipeline/sample-benchmark/run_service_api.ipynb)
   in the local Jupyter notebook. This benchmark script:

   1. Creates a new pipeline.
   1. Uses the default pipeline version of this pipeline to create multiple runs.
   1. Records the number of successful runs.
   1. Records the duration of each of the successful runs.
   1. Records the latency of `CREATE`, `GET`, `DELETE`.
   1. Cleans up the pipeline and its default pipeline version, the experiment and the runs.

1. In the benchmark script, enter the correct values for host,
   `pipeline_file_url`, `num_runs`, `run_status_polling_interval_sec` in the
   benchmark script.

   * **host**: The URL of the API server in your Kubeflow Pipelines cluster.

   * **pipeline_file_url**: The URL of the pipeline manifest file to use in
     your benchmark.
     [`taxi_updated_pool.yaml`](https://storage.googleapis.com/ml-pipeline/sample-benchmark/taxi_updated_pool.yaml)
     is used in this example. This example pipeline makes use of
     [nodeSelector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
     to explicitly schedule the runs of this pipeline onto the node pool named `pool-1`. If you use the taxi_updated_pool.yaml pipeline manifest in your benchmarks, ensure that a node pool named `pool-1` exists in your cluster.

     **NOTE**: Do not use the value `https://storage.cloud.google.com/ml-pipeline/sample-benchmark/taxi_updated_pool.yaml`
     when running your benchmarks. Addresses that start with
     `storage.cloud.google.com` incur a redirect that doesn't work well with
     Kubeflow Pipelines.

   * **num_runs**: Specifies how many runs will be created in the benchmark script,
     and is a direct indicator of the simulated workload.

     **NOTE**: Running the benchmark script on a cloud
     service incurs charges due to the consumption of cloud resources.

   * **run_status_polling_interval_sec**: Sets the time interval between two
     adjacent pollings of the run status. When a run reaches the success status, the
     run duration is recorded. Only the durations of successful runs are recorded.

1. After the parameters are properly set, you can run the
   benchmark script in the notebook.

The following snapshot shows the results of running the benchmark script
[`run_service_api.ipynb`](https://storage.googleapis.com/ml-pipeline/sample-benchmark/run_service_api.ipynb)
using the [`taxi_updated_pool.yaml`](https://storage.googleapis.com/ml-pipeline/sample-benchmark/taxi_updated_pool.yaml)
pipeline manifest for 50 runs on a Kubernetes cluster with two node pools.
Each node pool has three nodes of machine type `n1-standard-8`.

<img src="/docs/images/benchmark-snapshot-1.png"
alt="Benchmark Sample Output Plots"
class="mt-3 mb-3 border border-info rounded">

## Interpreting the results

In the above example output, there are two types of plots. One is the
distribution plot, for latency and duration measurement; the other is the count
plot, for counting succeeded and failed runs. The reading of those plots is in
general straightforward.

In a count plot, the x-axis represents the possible run status: success or fail;
and the y-axis shows how many runs fall into certain status respectively.

In a distribution plot, both histogram plot and rug plot are shown. In addition,
it is also possible to show a KDE (Kernel Density Estimate) plot. If the KDE plot is
desirable, use `kde=True` in the `distplot()` method.

## Tuning with different configurations

The above example shows one performance report via running the benchmark script.
In fact, there are multiple ways of tuning the pipeline and/or the Kubernetes
clusters to acquire performance reports. The common ways are trying with

- Different cluster sizes/zones, different numbers of pools in a cluster,
different numbers of nodes in each pool, different node configurations (for
example, RAM and disk configuration for each node, CPU or GPU configuration for
each node)

- Different number of runs

- Different sizes/complexities of a pipeline, for example, the number of steps
in a pipeline

- Different running configurations, for example, specifying the node pools for
running components or pipelines

- Etc.

## Limitations and future work

When the benchmark script is tuned to generate a moderate workload, for example, 50
runs in the above example, the latency and run duration measurements can be
made properly. However, it is also interesting to see how a Kubeflow Pipelines
instance behaves or breaks under some extremely heavy workloads, or in
other words, to probe the Kubeflow Pipelines instance. The example benchmark
script can be used for that purpose as well. In that case, the measurement plots
of the benchmark script are no longer the expected output. Instead, the errors
and error logs provide information on the performance and scalability of the
Kubeflow Pipelines deployment. With them, bugs and pain points can be discovered and then fixed. Moreover, when probing the
Kubeflow Pipelines instance with extreme workloads, it will be really helpful
to add internal monitoring to the server code to track server performance.
For example, in the future, it would be desirable to use [Prometheus](https://prometheus.io/)
to track and visualize metrics for Kubeflow Pipelines servers.

The internal performance monitoring inside the servers is complementary to the
performance measurement from the client-side. When the example benchmark script
measures the latencies from the client-side, the resulting measurements depend
on both the Kubeflow Pipelines instance and the network transmission. On the
other hand, the internal monitoring focuses on the actual processing cost inside
the server given certain requests. Therefore, having both the client-side
measurements and server side monitoring are useful in profiling accurately the
performance and scalability of Kubeflow Pipelines.

## Contact

If you run into any issues with the benchmark script and have any suggestions in
profiling the performance and scalability of Kubeflow Pipelines, [open an
issue](https://github.com/kubeflow/pipelines/issues/new) with us.



================================================
File: content/en/docs/components/pipelines/legacy-v1/tutorials/build-pipeline.md
================================================
+++
title = "Experiment with the Pipelines Samples"
description = "Get started with the Kubeflow Pipelines notebooks and samples"
weight = 30
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

You can learn how to build and deploy pipelines by running the samples
provided in the Kubeflow Pipelines repository or by walking through a
Jupyter notebook that describes the process.

## Compiling the samples on the command line

This section shows you how to compile the 
[Kubeflow Pipelines samples](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples)
and deploy them using the Kubeflow Pipelines UI.

### Before you start

Set up your environment:

1. Clone or download the
  [Kubeflow Pipelines samples](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples).
1. Install the [Kubeflow Pipelines SDK](/docs/components/pipelines/legacy-v1/sdk/install-sdk/).
1. Activate your Python 3 environment if you haven't done so already:

    ```
    source activate <YOUR-PYTHON-ENVIRONMENT-NAME>
    ```

    For example:

    ```
    source activate mlpipeline
    ```

### Choose and compile a pipeline

Examine the pipeline samples that you downloaded and choose one to work with.
The 
[`sequential.py` sample pipeline](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/sequential/sequential.py):
is a good one to start with.

Each pipeline is defined as a Python program. Before you can submit a pipeline
to the Kubeflow Pipelines service, you must compile the 
pipeline to an intermediate representation. The intermediate representation
takes the form of a YAML file compressed into a 
`.tar.gz` file.

Use the `dsl-compile` command to compile the pipeline that you chose:

```bash
dsl-compile --py [path/to/python/file] --output [path/to/output/tar.gz]
```

For example, to compile the
[`sequential.py` sample pipeline](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/sequential/sequential.py):

```bash
export DIR=[YOUR PIPELINES REPO DIRECTORY]/samples/core/sequential
dsl-compile --py ${DIR}/sequential.py --output ${DIR}/sequential.tar.gz
```

### Deploy the pipeline

Upload the generated `.tar.gz` file through the Kubeflow Pipelines UI. See the
guide to [getting started with the UI](/docs/components/pipelines/legacy-v1/overview/quickstart).

## Building a pipeline in a Jupyter notebook

You can choose to build your pipeline in a Jupyter notebook. The
[sample notebooks](https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core)
walk you through the process.

It's easiest to use the Jupyter services that are installed in the same cluster as 
the Kubeflow Pipelines system. 

Note: The notebook samples don't work on Jupyter notebooks outside the same 
cluster, because the Python library communicates with the Kubeflow Pipelines 
system through in-cluster service names.

Follow these steps to start a notebook:

1. Deploy Kubeflow:

    * Follow the [GCP deployment guide](https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/deploy/), including the step 
      to deploy Kubeflow using the 
      [Kubeflow deployment UI](https://deploy.kubeflow.cloud/).

    * When Kubeflow is running, access the Kubeflow UI at a URL of the form
      `https://<deployment-name>.endpoints.<project>.cloud.goog/`.

1. Follow the [Kubeflow notebooks setup guide](/docs/components/notebooks/quickstart-guide/) to
  create a Jupyter notebook server and open the Jupyter UI.

1. Download the sample notebooks from
  https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core.

1. Upload these notebooks from the Jupyter UI: In Jupyter, go to the tree view
  and find the **upload** button in the top right-hand area of the screen.

1. Open one of the uploaded notebooks.

1. Make sure the notebook kernel is set to Python 3. The Python version is at 
  the top right-hand corner in the Jupyter notebook view. 
  
1. Follow the instructions in the notebook.

The following notebooks are available:

* [KubeFlow pipeline using TFX OSS components](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/tfx-oss/TFX%20Example.ipynb):
  This notebook demonstrates how to build a machine learning pipeline based on
  [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx/) components. 
  The pipeline includes a TFDV step to infer the schema, a TFT preprocessor, a 
  TensorFlow trainer, a TFMA analyzer, and a model deployer which deploys the 
  trained model to `tf-serving` in the same cluster. The notebook also 
  demonstrates how to build a component based on Python 3 inside the notebook, 
  including how to build a Docker container.

* [Lightweight Python components](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/lightweight_component/lightweight_component.ipynb): 
  This notebook demonstrates how to build simple Python components based on 
  Python 3 and use them in a pipeline with fast iterations. If you use this
  technique, you don't need to build a Docker container when you build a
  component. Note that the container image may not be self contained because the 
  source code is not built into the container.

## Next steps

* Learn the various ways to use the [Kubeflow Pipelines 
  SDK](/docs/components/pipelines/legacy-v1/sdk/sdk-overview/).
* See how to 
  [build your own pipeline components](/docs/components/pipelines/legacy-v1/sdk/component-development/).
* Read more about 
  [building lightweight components](/docs/components/pipelines/user-guides/components/lightweight-python-components/).



================================================
File: content/en/docs/components/pipelines/legacy-v1/tutorials/cloud-tutorials.md
================================================
+++
title = "Run a Cloud-specific Pipelines Tutorial"
description = "Choose the Kubeflow Pipelines tutorial to suit your deployment"
weight = 40
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

{{% alert title="Opportunity to add cloud tutorials" color="info" %}}
<p><b>Invitation:</b> Create a cloud-specific tutorial and link it here.
See the <a href="/docs/about/style-guide/">style guide for the Kubeflow docs</a>.</p>
{{% /alert %}}

* [Pipelines End-to-end on Azure](https://azure.github.io/kubeflow-aks/main/): An end-to-end tutorial for Kubeflow Pipelines on Microsoft Azure.
* [Pipelines on Google Cloud Platform](https://g.co/codelabs/kfp-gis) : This GCP tutorial walks through a Kubeflow Pipelines example that shows training a Tensor2Tensor model for GitHub issue summarization, both via the Pipelines Dashboard UI, and from a Jupyter notebook



================================================
File: content/en/docs/components/pipelines/legacy-v1/tutorials/sdk-examples.md
================================================
+++
title = "Using the Kubeflow Pipelines SDK"
description = "How to use the Kubeflow Pipelines SDK"
weight = 10
                    
+++
{{% alert title="Old Version" color="warning" %}}
This page is about __Kubeflow Pipelines V1__, please see the [V2 documentation](/docs/components/pipelines) for the latest information.

Note, while the V2 backend is able to run pipelines submitted by the V1 SDK, we strongly recommend [migrating to the V2 SDK](/docs/components/pipelines/user-guides/migration).
For reference, the final release of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), and its reference documentation is [available here](https://kubeflow-pipelines.readthedocs.io/en/1.8.22/).
{{% /alert %}}

This guide provides examples that demonstrate how to use the Kubeflow Pipelines SDK.

## Before you start

To follow the examples in this guide, you must have Kubeflow Pipelines SDK
version 0.2.5 or higher installed. Use the following instructions to install
the Kubeflow Pipelines SDK and check the SDK version.

1. Install the [Kubeflow Pipelines SDK](/docs/components/pipelines/legacy-v1/sdk/install-sdk/)
1. Run the following command to check the version of the SDK
   ```
   pip list | grep kfp
   ```
   The response should be something like this:
   ```
   kfp                      0.2.5
   kfp-server-api           0.2.5
   ```

## Examples

Use the following examples to learn more about the Kubeflow Pipelines SDK.

### Example 1: Creating a pipeline and a pipeline version using the SDK

The following example demonstrates how to use the Kubeflow Pipelines SDK to
create a pipeline and a pipeline version.

In this example, you:

* Use `kfp.Client` to create a pipeline from a local file. When the pipeline
  is created, a default pipeline version is automatically created.
* Use `kfp.Client` to add a pipeline version to the pipeline that was created
  in the previous step.

```python
import kfp
import os

host = <host>
pipeline_file_path = <path to pipeline file>
pipeline_name = <pipeline name>
pipeline_version_file_path = <path to pipeline version file>
pipeline_version_name = <pipeline version name>

client = kfp.Client(host)
pipeline_file = os.path.join(pipeline_file_path)
pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file, name=pipeline_name)
pipeline_version_file = os.path.join(pipeline_version_file_path)
pipeline_version = client.pipeline_uploads.upload_pipeline_version(pipeline_version_file,
                                                                   name=pipeline_version_name,
                                                                   pipelineid=pipeline.id)
```

* **host**: Your Kubeflow Pipelines cluster's host name.
* **path to pipeline file**: The path to the directory where your pipeline YAML
  is stored.
* **pipeline name**: Your pipeline's file name.
* **path to pipeline version file**: The path to the directory where the new
  version of your pipeline YAML is stored.
* **pipeline version name**: Your pipeline version's file name.

**Note:** Pipeline names need to be unique across your Kubeflow Pipelines
cluster. Pipeline version names need to be unique within each pipeline.

#### Adding a version to an existing pipeline using the SDK

To add a pipeline version for an existing pipeline, you must find the
pipeline's ID and use it with the `upload_pipeline_version` method. To
find a pipeline's ID:

1. Open the Kubeflow Pipelines UI. A list of your pipelines appears.
1. Click the **name of your pipeline**. The pipeline details page appears.
1. The pipeline ID is listed in the summary card, as shown below.

<img src="/docs/images/sdk-examples-snapshot-1.png"
alt="Pipeline ID in Summary Card"
class="mt-3 mb-3 border border-info rounded">

### Example 2: Listing pipelines with a filter

The following example demonstrates how to use the Kubeflow Pipelines SDK to
list pipelines with a particular pipeline name. If list_pipelines method is
called without any input parameters, it will list all the pipelines. However,
you can specify a filter as an input parameter to list pipelines with a
particular name. Given that Kubeflow Pipelines requires pipeline names to be
unique, listing pipelines with a particular name returns at most one pipeline.

```python
import kfp
import json

# 'host' is your Kubeflow Pipelines API server's host address.
host = <host>
# 'pipeline_name' is the name of the pipeline you want to list.
pipeline_name = <pipeline name>

client = kfp.Client(host)
# To filter on pipeline name, you can use a predicate indicating that the pipeline
# name is equal to the given name.
# A predicate includes 'key', 'op' and 'string_value' fields.
# The 'key' specifies the property you want to apply the filter to. For example,
# if you want to filter on the pipeline name, then 'key' is set to 'name' as
# shown below.
# The 'op' specifies the operator used in a predicate. The operator can be
# EQUALS, NOT_EQUALS, GREATER_THAN, etc. The complete list is at [filter.proto](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/backend/api/filter.proto#L32)
# When using the operator in a string-typed predicate, you need to use the
# corresponding integer value of the enum. For Example, you can use the integer
# value 1 to indicate EQUALS as shown below.
# The 'string_value' specifies the value you want to filter with.
filter = json.dumps({'predicates': [{'key': 'name', 'op': 1, 'string_value': '{}'.format(pipeline_name)}]})
pipelines = client.pipelines.list_pipelines(filter=filter)
# The pipeline with the given pipeline_name, if exists, is in pipelines.pipelines[0].
```

### Example 3: Creating a run using a pipeline version

Examine the run_service_api.ipynb notebook to [learn more about creating a run using a pipeline version](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/tools/benchmarks/run_service_api.ipynb).



================================================
File: content/en/docs/components/pipelines/operator-guides/_index.md
================================================
+++
title = "Operator Guides"
description = "Documentation for operators of Kubeflow Pipelines."
weight = 6
+++



================================================
File: content/en/docs/components/pipelines/operator-guides/configure-object-store.md
================================================
+++  
title = "Object Store Configuration"  
weight = 3  
+++

{{% kfp-v2-keywords %}}

In Kubeflow Pipelines (KFP), there are two components that utilize Object store:

* KFP API Server
* KFP Launcher (aka KFP executor)

The default object store that is shipped as part of the Kubeflow Platform is Minio. However, you can configure a different object store provider with your KFP deployment.

The following diagram provides an simplified overview of how object storage is utilized and configured: 

<img src="/docs/components/pipelines/operator-guides/images/kfp-obj-store-configurations.png"
alt="KFP Object Store Configuration Overview"
class="mt-3 mb-3"
style="width: 75%; height: 75%;">


## Prerequisites

* Admin level access to KFP Kubernetes namespace
* Object Store credentials for a supported provider (see below)

> **Note**: in this doc "**KFP Namespace**" refers to the namespace where KFP is deployed. If KFP is deployed as part of the Kubeflow Platform deployment, this is the `kubeflow` namespace.

## KFP API Server

The KFP API Server uses the object store to store the Pipeline Intermediate Representation (IR).

The list below describe the type of Object Store configurations supported today for API Server. Static credentials here
refers to long term credentials provided by Object Store providers. 

For [AWS Static Credentials] and other S3 compliant object storage, this consists of an Access Key ID and Secret Access Key ID embedded into the run time environment or, passed as secure parameters to some API. In Google Cloud Storage, this refers to a JSON containing the [GCS APP Credentials].

### API Server Supported providers

| Provider                                     | Supported |
|----------------------------------------------|-----------|
| Minio with Static Credentials                | Yes       |
| AWS S3 with Static Credentials               | Yes       |
| AWS S3 with IRSA                             | Yes       |
| S3-Compliant Storage with Static Credentials | Yes       |
| Google Cloud Storage with Static Credentials | No        |        
| Google Cloud Storage with App Credentials    | No        |  

### API Server Object Store Configuration

To configure the object store used by the KFP API Server, the configuration depends on whether you are using static credentials, or AWS S3 with IAM roles for service accounts (IRSA).

**Static credentials**

To configure an AWS S3 bucket with static credentials you will need to add the following environment variables to your KFP API Server deployment:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-pipeline
  namespace: kubeflow
spec:
  ...
  template:
    ...
    spec:
      containers:
      - name: ml-pipeline-api-server
        serviceAccountName: "ml-pipeline"
        env:
          ...
          - name: OBJECTSTORECONFIG_HOST
            value: "your-bucket" # e.g. s3.amazonaws.com
          - name: OBJECTSTORECONFIG_PORT
            value: "port" # e.g. 443   
          - name: OBJECTSTORECONFIG_REGION
            value: "region" # e.g. us-east-1
            # true if object store is on a secure connection 
          - name: OBJECTSTORECONFIG_SECURE
            value: "true"        
            # These env vars reference the values from a Kubernetes secret
            # this requires deploying the secret ahead of time, and filling out the
            # following values accordingly.
          - name: OBJECTSTORECONFIG_ACCESSKEY
            valueFrom:
              secretKeyRef:
                key: "some-key-1"
                name: "secret-name"
          - name: OBJECTSTORECONFIG_SECRETACCESSKEY
            valueFrom:
              secretKeyRef:
                key: "some-key-2"
                name: "secret-name"
```

**AWS IRSA (IAM Roles for Service Accounts)**

To utilize AWS IRSA for KFP API Server, you will need to omit any Static credential configuration from your deployment, namely `OBJECTSTORECONFIG_ACCESSKEY` and `OBJECTSTORECONFIG_SECRETACCESSKEY` , as these take precedence. If they are left in, API Server will ignore any IRSA configuration.

Next, ensure the appropriate IAM roles associated with the Kubernetes Service Account `ml-pipeline` in the KFP namespace. This is highly dependent on your platform provider, for example for EKS see the [IRSA docs].

## KFP Launcher

The KFP launcher uses the object store to store KFP Input and Output Artifacts.

The list below describe the type of Object Store configurations supported today for API Server.

Refer to the API Server configuration section [here](#api-server-supported-providers) for more information on what Static Credentials are.

### KFP Launcher Supported providers

| Provider                                     | Supported |
|----------------------------------------------|-----------|
| Minio with Static Credentials                | Yes       |
| AWS S3 with Static Credentials               | Yes       |
| AWS S3 with IRSA                             | Yes       |
| S3-Compliant Storage with Static Credentials | Yes       |
| Google Cloud Storage with Static Credentials | Yes       |        
| Google Cloud Storage with App Credentials    | Yes       |  


### KFP Launcher Object Store Configuration

To configure the object store utilized by the KFP Launcher, you will need to edit the `kfp-launcher` Kubernetes ConfigMap.

In a default KFP deployment, this typically looks like: 

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace
data:
  defaultPipelineRoot: ""
```

> **Note**: **If this configmap is not provided, you will need to deploy this in the Kubernetes Namespace where the Pipelines will be 
executed**. This is not necessarily the same namespace as where Kubeflow Pipeline itself is deployed.

The `defaultPipelineRoot` is a path within the Object store bucket where Artifact Input/Outputs in a given pipeline are 
stored. Note that this field can also be configured via the KFP SDK, see [SDK PipelineRoot Docs]. It can also be configured via the 
KFP UI when creating a Run.

By default `defaultPipelineRoot` is `minio://mlpipeline/v2/artifacts` and artifacts are stored in the default Minio deployment.
The first value in the path `mlpipeline` refers to the bucket name. 

If you want artifacts to be stored in a different path in the bucket that is not `/v2/artifacts`, you can simply change the `defaultPipelineRoot`.
For example to store artifacts in `/some/other/path` within the default Minio install, use the following KFP Launcher configmap: 

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace
data:
  defaultPipelineRoot: "minio://mlpipeline/some/other/path"
```

> **Note**: that when utilizing the **KFP Launcher configmap it needs to be deployed in the same namespace where the Pipelines 
will be created**. In a standalone KFP deployment this is the KFP namespace. In a Kubeflow Platform deployment, this will 
be the user Kubeflow Profile namespace.

#### Configure other Providers

To utilize a different object store provider entirely, you will need to add a new field `providers` to the KFP launcher configmap.
How to configure this field depends on your object store provider. See below for details.

Note that the provider is determined by the PipelineRoot value. If `PipelineRoot=s3://mlpipeline` then this is matched 
with the `s3` provider. If `PipelineRoot=g3://mlpipeline` then this is atched with the `gs` provider (GCS) and so on.

#### S3 and S3-compatible Provider

To configure an AWS S3 bucket with static credentials, update your KFP Launcher configmap to the following:

```yaml
apiVersion: v1
data:
  defaultPipelineRoot: s3://mlpipeline
  providers: |-
    s3:
      default:
        endpoint: s3.amazonaws.com
        disableSSL: false
        region: us-east-2
        credentials:
          fromEnv: false
          secretRef:
            secretName: your-k8s-secret
            accessKeyKey: some-key-1
            secretKeyKey: some-key-2
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace
```

The `s3` provider field is valid for any s3 compliant storage. The `default` indicates that this configuration is used 
by default if no matching `overrides` are provided (read about [overrides]).

#### S3 IRSA and Environment based Credentials

If you are using AWS IRSA, or you have embedded your bucket credentials in the task environment (e.g. via [SDK Secret Env]), 
then you can set `fromEnv: true` and omit `secretRef`. This would look something like: 

```yaml
apiVersion: v1
data:
  defaultPipelineRoot: s3://mlpipeline
  providers: |-
    s3:
      default:
        endpoint: s3.amazonaws.com
        disableSSL: false
        region: us-east-2
        credentials:
          fromEnv: true
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace
```

Ensure that the service account being used to run your pipeline is configured with IRSA (see [IRSA docs]).

You can also configure static credentials via AWS Environment variables directly in your pipeline, for example: 

```python
kubernetes.use_secret_as_env(
    your_task,
    secret_name='aws-s3-creds',
    secret_key_to_env={'AWS_SECRET_ACCESS_KEY': 'AWS_SECRET_ACCESS_KEY'})
kubernetes.use_secret_as_env(
    your_task,
    secret_name='aws-s3-creds',
    secret_key_to_env={'AWS_ACCESS_KEY_ID': 'AWS_ACCESS_KEY_ID'})
kubernetes.use_secret_as_env(
    your_task,
    secret_name='aws-s3-creds',
    secret_key_to_env={'AWS_REGION': 'AWS_REGION'})
...
```

#### Google Cloud Storage (GCS) provider

To configure a GCS provider with static credentials you need to only provide the reference to the App Credentials file via a Kubernetes Secret: 

```yaml
apiVersion: v1
data:
  defaultPipelineRoot: gs://mlpipeline
  providers: |-
    gs:
      default:
        credentials:
          fromEnv: false
          secretRef:
            secretName: your-k8s-secret
            tokenKey: some-key-1
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace
```

You can also configure static credentials via GCS Environment variables directly in your pipeline, for example:

```python
# Specify the default APP Credential path
your_task.set_env_variable(name='GOOGLE_APPLICATION_CREDENTIALS', value='/gcloud/credentials.json')
# Mount the GCS Credentials JSON
kubernetes.use_secret_as_volume(your_task, secret_name='gcs-secret', mount_path='/gcloud')
```

#### GCS Environment based Credentials

If you have embedded GCS credentials in your Pipeline Task environment (e.g. via [SDK Secret Env]), you can set 
`fromEnv: true` and omit the `secretRef`: 

```yaml
apiVersion: v1
data:
  defaultPipelineRoot: gs://mlpipeline
  providers: |-
    gs:
      default:
        credentials:
          fromEnv: true
kind: ConfigMap
metadata:
  name: kfp-launcher
  namespace: user-namespace
```

### KFP Launcher Overrides

KFP Launcher overrides allow users to specify different Provider sources for different paths within a given PipelineRoot. 

The following example shows a comprehensive example of how to do this for GCS and S3 compatible providers: 

```yaml
gs:
  default:
    credentials:
      fromEnv: false
      secretRef:
        secretName: gs-secret-1
        tokenKey: gs-tokenKey
  overrides:
    # Matches pipeline root: gs://your-bucket/some/subfolder
    - bucketName: your-bucket
      keyPrefix: some/subfolder
      credentials:
        fromEnv: false
        secretRef:
          secretName: gcs-secret-2
          tokenKey: gs-tokenKey-2
    # Matches pipeline root: gs://your-bucket/some/othersubfolder
    - bucketName: your-bucket
      keyPrefix: some/othersubfolder
      credentials:
        fromEnv: true
s3:
  default:
    endpoint: http://some-s3-compliant-store-endpoint.com
    disableSSL: true
    region: minio
    credentials:
      fromEnv: false
      secretRef:
        secretName: your-secret
        accessKeyKey: accesskey
        secretKeyKey: secretkey
  overrides:
    # Matches pipeline root: s3://your-bucket/subfolder
    # aws-s3-creds secret is used for static credentials
    - bucketName: your-bucket
      keyPrefix: subfolder
      endpoint: s3.amazonaws.com
      region: us-east-2
      disableSSL: false
      credentials:
        fromEnv: false
        secretRef:
          secretName: aws-s3-creds
          accessKeyKey: AWS_ACCESS_KEY_ID
          secretKeyKey: AWS_SECRET_ACCESS_KEY
    # Matches pipeline root: s3://your-bucket/some/s3/path/a/b
    - bucketName: your-bucket
      keyPrefix: some/s3/path/a/b
      endpoint: s3.amazonaws.com
      region: us-east-2
      credentials:
        fromEnv: true
    # Matches pipeline root: s3://your-bucket/some/s3/path/a/c
    - bucketName: your-bucket
      keyPrefix: some/s3/path/a/c
      endpoint: s3.amazonaws.com
      region: us-east-2
      credentials:
        fromEnv: false
        secretRef:
          secretName: aws-s3-creds
          accessKeyKey: AWS_ACCESS_KEY_ID
          secretKeyKey: AWS_SECRET_ACCESS_KEY
    # Matches pipeline root: s3://your-bucket/some/s3/path/b/a
    - bucketName: your-bucket
      keyPrefix: some/s3/path/b/a
      endpoint: https://s3.amazonaws.com
      region: us-east-2
      credentials:
        fromEnv: false
        secretRef:
          secretName: aws-s3-creds
          accessKeyKey: AWS_ACCESS_KEY_ID
          secretKeyKey: AWS_SECRET_ACCESS_KEY
```

The `keyPrefix` are matched with the path prescribed in the `PipelineRoot`. For example if  `PipelineRoot` is `s3://your-bucket/some/s3/path/b/a` Then the following provider config is used: 

```yaml
- bucketName: your-bucket
  keyPrefix: some/s3/path/b/a
  endpoint: https://s3.amazonaws.com
  region: us-east-2
  credentials:
    fromEnv: false
    secretRef:
      secretName: aws-s3-creds
      accessKeyKey: AWS_ACCESS_KEY_ID
      secretKeyKey: AWS_SECRET_ACCESS_KEY
```
If a field is not provided, then the default configuration is utilized. 

[IRSA docs]: https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html
[SDK PipelineRoot Docs]: /docs/components/pipelines/concepts/pipeline-root.md
[overrides]: #kfp-launcher-overrides
[SDK Secret Env]: https://kfp-kubernetes.readthedocs.io/en/kfp-kubernetes-1.2.0/source/kubernetes.html#kfp.kubernetes.use_secret_as_env
[GCS APP Credentials]: https://cloud.google.com/docs/authentication/application-default-credentials
[AWS Static Credentials]: https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html
  



================================================
File: content/en/docs/components/pipelines/operator-guides/multi-user.md
================================================
+++
title = "Multi-user Isolation"
description = "How multi-user isolation works in Kubeflow Pipelines"
weight = 3
+++

Multi-user isolation for Kubeflow Pipelines is part of Kubeflow's overall profile and namespace isolation strategy.

{{% alert title="Tip" color="info" %}}
Kubeflow Pipelines multi-user isolation is only supported in [Kubeflow Platform](/docs/started/installing-kubeflow/#kubeflow-platform) deployments.
Refer to [profiles and namespaces](/docs/components/central-dash/profiles/) for common operations like [managing profile contributors](/docs/components/central-dash/profiles/#manage-profile-contributors).
{{% /alert %}}

## How are resources separated?

Kubeflow Pipelines separates resources using Kubernetes namespaces that are managed by [Kubeflow Profiles](/docs/components/central-dash/profiles/).
Other users cannot see resources in your Profile/Namespace without permission, because the Kubeflow Pipelines API server 
rejects requests for namespaces that the current user is not authorized to access.

"Experiments" belong to namespaces directly, runs and recurring runs belong to their parent experiment's namespace.

"Pipeline Runs" are executed in user namespaces, so that users can leverage Kubernetes namespace isolation. 
For example, they can configure different secrets for other services in different namespaces.

{{% alert title="Warning" color="warning" %}}
Kubeflow makes no hard security guarantees about Profile isolation.
<br>
User profiles have no additional isolation beyond what is provided by Kubernetes Namespaces.
{{% /alert %}}

## When using the UI

When you visit the Kubeflow Pipelines UI from the Kubeflow Dashboard, it only shows "experiments", "runs", and "recurring runs" in your chosen namespace. 
Similarly, when you create resources from the UI, they also belong to the namespace you have chosen.

{{% alert title="Warning" color="warning" %}}
Pipeline definitions are not isolated right now, and are shared across all namespaces, see [Current Limitations](#current-limitations) for more details.
{{% /alert %}}

## When using the SDK

How to connect Pipelines SDK to Kubeflow Pipelines will depend on __what kind__ of Kubeflow deployment you have, and __from where you are running your code__.

* [Full Kubeflow (from inside cluster)](/docs/components/pipelines/user-guides/core-functions/connect-api/#kubeflow-platform---inside-the-cluster)
* [Full Kubeflow (from outside cluster)](/docs/components/pipelines/user-guides/core-functions/connect-api/#kubeflow-platform---outside-the-cluster)
* [Standalone Kubeflow Pipelines (from inside cluster)](/docs/components/pipelines/user-guides/core-functions/connect-api/#standalone-kfp---inside-the-cluster)
* [Standalone Kubeflow Pipelines (from outside cluster)](/docs/components/pipelines/user-guides/core-functions/connect-api/#standalone-kfp---outside-the-cluster)

The following Python code will create an experiment (and associated run) from a Pod inside a full Kubeflow cluster.

```python
import kfp

# the namespace in which you deployed Kubeflow Pipelines
kubeflow_namespace = "kubeflow"

# the namespace of your pipelines user (where the pipeline will be executed)
user_namespace = "jane-doe"

# the KF_PIPELINES_SA_TOKEN_PATH environment variable is used when no `path` is set
# the default KF_PIPELINES_SA_TOKEN_PATH is /var/run/secrets/kubeflow/pipelines/token
credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)

# create a client
client = kfp.Client(host=f"http://ml-pipeline-ui.{kubeflow_namespace}", credentials=credentials)

# create an experiment
client.create_experiment(name="<YOUR_EXPERIMENT_ID>", namespace=user_namespace)
print(client.list_experiments(namespace=user_namespace))

# create a pipeline run
client.run_pipeline(
    experiment_id="<YOUR_EXPERIMENT_ID>",  # the experiment determines the namespace
    job_name="<YOUR_RUN_NAME>",
    pipeline_id="<YOUR_PIPELINE_ID>"  # the pipeline definition to run
)
print(client.list_runs(experiment_id="<YOUR_EXPERIMENT_ID>"))
print(client.list_runs(namespace=user_namespace))
```

{{% alert title="Tip" color="info" %}}
* To set a default namespace for Pipelines SDK commands, use the [`kfp.Client().set_user_namespace()`](https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html#kfp.Client.set_user_namespace) method, 
  this method stores your user namespace in a configuration file at `$HOME/.config/kfp/context.json`.
* Detailed documentation for `kfp.Client()` can be found in the [Kubeflow Pipelines SDK Reference](https://kubeflow-pipelines.readthedocs.io/en/stable/source/client.html).
{{% /alert %}}

## When using the REST API

When calling the [Kubeflow Pipelines REST API](/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec/), a namespace argument is required for experiment APIs.
<br>
The namespace is specified by a "resource reference" with `type` of `NAMESPACE` and `key.id` equal to the namespace name.

The following code uses the [generated python API client](https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.server_api.html) to create an experiment and pipeline run.

```python
import kfp
from kfp_server_api import *

# the namespace in which you deployed Kubeflow Pipelines
kubeflow_namespace = "kubeflow"

# the namespace of your pipelines user (where the pipeline will be executed)
user_namespace = "jane-doe"

# the KF_PIPELINES_SA_TOKEN_PATH environment variable is used when no `path` is set
# the default KF_PIPELINES_SA_TOKEN_PATH is /var/run/secrets/kubeflow/pipelines/token
credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)

# create a client
client = kfp.Client(host=f"http://ml-pipeline-ui.{kubeflow_namespace}", credentials=credentials)

# create an experiment
experiment: ApiExperiment = client._experiment_api.create_experiment(
    body=ApiExperiment(
        name="<YOUR_EXPERIMENT_ID>",
        resource_references=[
            ApiResourceReference(
                key=ApiResourceKey(
                    id=user_namespace,
                    type=ApiResourceType.NAMESPACE,
                ),
                relationship=ApiRelationship.OWNER,
            )
        ],
    )
)
print("-------- BEGIN: EXPERIMENT --------")
print(experiment)
print("-------- END: EXPERIMENT ----------")

# get the experiment by name (only necessary if you comment out the `create_experiment()` call)
# experiment: ApiExperiment = client.get_experiment(
#     experiment_name="<YOUR_EXPERIMENT_ID>",
#     namespace=user_namespace
# )

# create a pipeline run
run: ApiRunDetail = client._run_api.create_run(
    body=ApiRun(
        name="<YOUR_RUN_NAME>",
        pipeline_spec=ApiPipelineSpec(
            # replace <YOUR_PIPELINE_ID> with the UID of a pipeline definition you have previously uploaded
            pipeline_id="<YOUR_PIPELINE_ID>",
        ),
        resource_references=[ApiResourceReference(
            key=ApiResourceKey(
                id=experiment.id,
                type=ApiResourceType.EXPERIMENT,
            ),
            relationship=ApiRelationship.OWNER,
        )
        ],
    )
)
print("-------- BEGIN: RUN --------")
print(run)
print("-------- END: RUN ----------")

# view the pipeline run
runs: ApiListRunsResponse = client._run_api.list_runs(
    resource_reference_key_type=ApiResourceType.EXPERIMENT,
    resource_reference_key_id=experiment.id,
)
print("-------- BEGIN: RUNS --------")
print(runs)
print("-------- END: RUNS ----------")
```

## Current limitations

### Resources without isolation

The following resources do not currently support isolation and are shared without access control:

* Pipelines (Pipeline definitions).
* Artifacts, Executions, and other metadata entities in [Machine Learning Metadata (MLMD)](https://www.tensorflow.org/tfx/guide/mlmd).
* [Minio artifact storage](https://min.io/) which contains pipeline runs' input/output artifacts.



================================================
File: content/en/docs/components/pipelines/operator-guides/server-config.md
================================================
+++
title = "Server Configuration"
description = "Guidance on managing your Kubeflow Pipelines instances"
weight = 2
+++


By default, you can use Kubeflow Pipelines deployment manifests as provided,
which aim to offer a standard configuration for most use cases. At the meantime,
customizations are available for more advanced usage.

When deploying Kubeflow Pipelines servers, you can pass various environment variables
to customize the behavior of servers.

## Frontend Server

When deploying frontend server called `ml-pipeline-ui`, you can pass various environment
variables to customize the server behavior for your namespace. Some examples are shown
in the [ml-pipeline-ui-deployment.yaml](https://github.com/kubeflow/pipelines/blob/b630d5c8ae7559be0011e67f01e3aec1946ef765/manifests/kustomize/base/pipeline/ml-pipeline-ui-deployment.yaml#L32-L50).

### Artifact storage endpoint allowlist

You can configure `ALLOWED_ARTIFACT_DOMAIN_REGEX` to allowlist object storage endpoint
that your frontend server will fetch artifacts from. If the domain that frontend server
tries to fetch does not match the regular expression defined in
`ALLOWED_ARTIFACT_DOMAIN_REGEX`, it will return error to users that the requested domain
is not allowed.

#### Standalone Kubeflow Pipelines deployment

By default, the value for `ALLOWED_ARTIFACT_DOMAIN_REGEX` is `"^.*$"`. You can customize
this value for your users, for example: `^.*.yourdomain$` in the
[ml-pipeline-ui-deployment.yaml](https://github.com/kubeflow/pipelines/blob/b630d5c8ae7559be0011e67f01e3aec1946ef765/manifests/kustomize/base/pipeline/ml-pipeline-ui-deployment.yaml#L32-L50).


#### Full fledged Kubeflow deployment

For full fledged Kubeflow, each namespace corresponds to a project with the same name.
To configure the `ALLOWED_ARTIFACT_DOMAIN_REGEX` value for user namespace, add an entry in `ml-pipeline-ui-artifact`
just like this example in [sync.py](https://github.com/kubeflow/pipelines/blob/b630d5c8ae7559be0011e67f01e3aec1946ef765/manifests/kustomize/base/installs/multi-user/pipelines-profile-controller/sync.py#L304-L310) for `ALLOWED_ARTIFACT_DOMAIN_REGEX` environment variable,
the entry is identical to the environment variable instruction in Standalone Kubeflow Pipelines
deployment.



================================================
File: content/en/docs/components/pipelines/operator-guides/installation/_index.md
================================================
+++
title = "Installation"
weight = 1
+++

{{% kfp-v2-keywords %}}

As an alternative to deploying Kubeflow Pipelines (KFP) as part of the
[Kubeflow deployment](/docs/started/installing-kubeflow), you also have a choice
to deploy only Kubeflow Pipelines. Follow the instructions below to deploy
Kubeflow Pipelines standalone using the supplied kustomize manifests.

You should be familiar with [Kubernetes](https://kubernetes.io/docs/home/),
[kubectl](https://kubernetes.io/docs/reference/kubectl/overview/), and [kustomize](https://kustomize.io/).

## Deploying Kubeflow Pipelines

1. Deploy the Kubeflow Pipelines:

     ```
     export PIPELINE_VERSION={{% pipelines/latest-version %}}
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
     kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
     kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"
     ```

     The Kubeflow Pipelines deployment requires approximately 3 minutes to complete.

2. Run the following to port-forward the Kubeflow Pipelines UI:
     ```
     kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80
     ```

3. Open http://localhost:8080 on your browser to see the Kubeflow Pipelines UI.



================================================
File: content/en/docs/components/pipelines/reference/_index.md
================================================
+++
title = "Reference"
description = "Reference docs for Kubeflow Pipelines Version 2"
weight = 8
+++



================================================
File: content/en/docs/components/pipelines/reference/community-and-support.md
================================================
+++
title = "Community and Support"
description = "Where to get help, contribute, and learn more"
weight = 1
+++

{{% kfp-v2-keywords %}}

## Help
There are several places to get help with KFP:

* [kubeflow-pipelines][kfp-stack-overflow] on Stack Overflow
* The #kubeflow-pipelines channel in the [Kubeflow Slack Workspace](https://kubeflow.slack.com/)
* Via a GitHub Issue describing your problem in the [kubeflow/pipelines repository][create-github-issue]
* In the Kubeflow Pipelines Community Meeting
## Kubeflow Pipelines Community Meeting
There is a Kubeflow Pipelines Community meeting every other Wednesday. This is an opportunity to learn about changes to KFP developments, discuss feature requests, and ask questions. When making a feature request, it is best to include a [GitHub Issue][create-github-issue] as a written accompaniment to your request.

To participate, join [kubeflow-discuss][kubeflow-discuss-google-group] on Google Groups.

## Bugs and Feature Requests
Bugs and feature requests should be recorded as GitHub Issues in the [kubeflow/pipelines repository][create-github-issue].

## Contributing
KFP is an open source project and welcomes community contribution. KFP has several components, each with its own contributing guidelines:

* [Backend contributing guidelines][backend-contributing-guidelines]
* [Frontend contributing guidelines][frontend-contributing-guidelines]
* [SDK contributing guidelines][sdk-contributing-guidelines]

[kfp-stack-overflow]: https://stackoverflow.com/questions/tagged/kubeflow-pipelines
[create-github-issue]: https://github.com/kubeflow/pipelines/issues/new/choose
[kubeflow-discuss-google-group]: https://groups.google.com/g/kubeflow-discuss
[backend-contributing-guidelines]: https://github.com/kubeflow/pipelines/blob/master/backend/README.md
[frontend-contributing-guidelines]: https://github.com/kubeflow/pipelines/blob/master/frontend/README.md
[sdk-contributing-guidelines]: https://github.com/kubeflow/pipelines/blob/master/sdk/CONTRIBUTING.md



================================================
File: content/en/docs/components/pipelines/reference/component-spec.md
================================================
+++
title = "Component Specification"
description = "Definition of a Kubeflow Pipelines component"
weight = 4
                    
+++

This specification describes the container component data model for Kubeflow
Pipelines. The data model is serialized to a file in YAML format for sharing.

Below are the main parts of the component definition:

* **Metadata:** Name, description, and other metadata.
* **Interface (inputs and outputs):** Name, type, default value.
* **Implementation:** How to run the component, given the input arguments.

## Example of a component specification

A component specification takes the form of a YAML file, `component.yaml`. Below
is an example:

```yaml
name: xgboost4j - Train classifier
description: Trains a boosted tree ensemble classifier using xgboost4j

inputs:
- {name: Training data}
- {name: Rounds, type: Integer, default: '30', description: 'Number of training rounds'}

outputs:
- {name: Trained model, type: XGBoost model, description: 'Trained XGBoost model'}

implementation:
  container:
    image: gcr.io/ml-pipeline/xgboost-classifier-train@sha256:b3a64d57
    command: [
      /ml/train.py,
      --train-set, {inputPath: Training data},
      --rounds,    {inputValue: Rounds},
      --out-model, {outputPath: Trained model},
    ]
```

See some examples of real-world 
[component specifications](https://github.com/search?q=repo%3Akubeflow%2Fpipelines+path%3A**%2Fcomponent.yaml&type=code).

## Detailed specification (ComponentSpec)

This section describes the 
[ComponentSpec](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_structures.py).

### Metadata

* `name`: Human-readable name of the component.
* `description`: Description of the component.
* `metadata`: Standard object's metadata:

    * `annotations`: A string key-value map used to add information about the component.
        Currently, the annotations get translated to Kubernetes annotations when the component task is executed on Kubernetes. Current limitation: the key cannot contain more that one slash ("/"). See more information in the
        [Kubernetes user guide](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/).
    * `labels`: Deprecated. Use `annotations`.

### Interface

* `inputs` and `outputs`:
    Specifies the list of inputs/outputs and their properties. Each input or
    output has the following properties:

    * `name`: Human-readable name of the input/output. Name must be
        unique inside the inputs or outputs section, but an output may have the
        same name as an input.
    * `description`: Human-readable description of the input/output.
    * `default`: Specifies the default value for an input. **Only
        valid for inputs.**
    * `type`: Specifies the type of input/output. The types are used
        as hints for pipeline authors and can be used by the pipeline system/UI
        to validate arguments and connections between components. Basic types
        are **String**, **Integer**, **Float**, and **Bool**. See the full list
        of [types](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/dsl/types.py)
        defined by the Kubeflow Pipelines SDK.
    * `optional`: Specifies if input is optional or not. This is of type
        **Bool**, and defaults to **False**. **Only valid for inputs.**

### Implementation

* `implementation`: Specifies how to execute the component instance.
    There are two implementation types,  `container` and `graph`. (The latter is
    not in scope for this document.) In future we may introduce more 
    implementation types like `daemon` or `K8sResource`.

    * `container`:
        Describes the Docker container that implements the component. A portable 
        subset of the Kubernetes
        [Container v1 spec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#container-v1-core).

        * `image`: Name of the Docker image.
        * `command`: Entrypoint array. The Docker image's
            ENTRYPOINT is used if this is not provided. Each item is either a
            string or a placeholder. The most common placeholders are
            `{inputValue: Input name}`, `{inputPath: Input name}` and `{outputPath: Output name}`.
        * `args`: Arguments to the entrypoint. The Docker
            image's CMD is used if this is not provided. Each item is either a
            string or a placeholder. The most common placeholders are
            `{inputValue: Input name}`, `{inputPath: Input name}` and `{outputPath: Output name}`.
        * `env`: Map of environment variables to set in the container.
        * `fileOutputs`: Legacy property that is only needed in
            cases where the container always stores the output data in some
            hard-coded non-configurable local location. This property specifies
            a map between some outputs and local file paths where the program
            writes the output data files. Only needed for components that have
            hard-coded output paths. Such containers need to be fixed by
            modifying the program or adding a wrapper script that copies the
            output to a configurable location. Otherwise the component may be
            incompatible with future storage systems.

You can set all other Kubernetes container properties when you
use the component inside a pipeline.

## Using placeholders for command-line arguments

### Consuming input by value

The `{inputValue: <Input name>}` placeholder is replaced by the value of the input argument:

* In `component.yaml`:
  
  ```yaml
  command: [program.py, --rounds, {inputValue: Rounds}]
  ```

* In the pipeline code:

  ```python
  task1 = component1(rounds=150)
  ```

* Resulting command-line code (showing the value of the input argument that
  has replaced the placeholder):

  ```shell
  program.py --rounds 150
  ```

### Consuming input by file

The `{inputPath: <Input name>}` placeholder is replaced by the (auto-generated) local file path where the system has put the argument data passed for the "Input name" input.

* In `component.yaml`:

  ```yaml
  command: [program.py, --train-set, {inputPath: training_data}]
  ```

* In the pipeline code:

  ```python
  task2 = component1(training_data=some_task1.outputs['some_data'])
  ```

* Resulting command-line code (the placeholder is replaced by the 
  generated path):

  ```shell
  program.py --train-set /inputs/train_data/data
  ```


### Producing outputs

The `{outputPath: <Output name>}` placeholder is replaced by a (generated) local file path where the component program is supposed to write the output data.
The parent directories of the path may or may not not exist. Your
program must handle both cases without error.

* In `component.yaml`:

  ```yaml
  command: [program.py, --out-model, {outputPath: trained_model}]
  ```

* In the pipeline code:

  ```python
  task1 = component1()
  # You can now pass `task1.outputs['trained_model']` to other components as argument.
  ```

* Resulting command-line code (the placeholder is replaced by the 
  generated path):

  ```shell
  program.py --out-model /outputs/trained_model/data
  ```



================================================
File: content/en/docs/components/pipelines/reference/kfp-kubernetes.md
================================================
+++
title = "Kubernetes Platform-specific Features"
description = "Reference documentation for the `kfp-kubernetes` Python library"
weight = 6
                    
+++

The `kfp-kubernetes` Python library enables authoring Kubeflow pipelines with Kubernetes-specific features. 
See the [reference documentation](https://kfp-kubernetes.readthedocs.io).



================================================
File: content/en/docs/components/pipelines/reference/sdk.md
================================================
+++
title = "Pipelines SDK Reference"
description = "Reference documentation for the Kubeflow Pipelines SDK Version 2"
weight = 5
                    
+++

See the [KFP SDK v2 reference documentation](https://kubeflow-pipelines.readthedocs.io/en/stable/).



================================================
File: content/en/docs/components/pipelines/reference/version-compatibility.md
================================================
+++
title = "Version Compatibility"
description = "Version compatibility between KFP Runtime and KFP SDK"
weight = 2
+++

{{% kfp-v2-keywords %}}

The following table presents a comprehensive overview of the version compatibility between the Kubeflow Pipelines (KFP) Runtime and the KFP SDK.

| KFP Runtime | KFP SDK | Notes |
|---|---|---|
| v2.0.* | v2.0.* | Active development. Support for certain features may be staged between the Runtime and the SDK. |
| v2.0.* | v1.8.* | Backward compatibility maintained for v1 features. No v2 features support. |
| v1.8.* | v1.8.* | Maintenance mode. Full compatibility for v1 features. No v2 features support. |
| v1.7.* | * | Not recommended due to the age of the release. |
| * | v1.7.* | Not recommended due to the age of the release. |

### Notes

* **v1 features** refer to the features available when running v1 pipelines--these are pipelines produced by v1 versions of the KFP SDK (excluding the v2 compiler available in KFP SDK v1.8), they are persisted as Argo workflow in YAML format.

* **v2 features** refer to the features available when running v2 pipelines--these are pipelines produced using v2 versions of the KFP SDK, they are persisted as Intermediate Representation (IR) in YAML format.

* Pipelines produced using the v2 namespace (`kfp.v2`) in the SDK v1.8 were partially and momentarily supported by the KFP Runtime v1.8 via *v2-compatible* mode. The support for v2-compatible mode has been discontinued.

* The KFP Runtime v2.0.* supports v2 features and v1 features at the same time. Depending on whether users are running v1 pipelines vs. v2 pipelines, the KFP Runtime behaves differently. The most noticeable difference users can perceive is v1 pipelines are rendered using the old v1-style DAG UI, while v2 pipelines are rendered in the v2 modern DAG UI.

Please note that while we aim to sure backward compatibility when possible, it is always recommended to use the latest version of both KFP Runtime and KFP SDK to take advantage of the full range of features and improvements.

For more detailed information on feature support, please refer to the version-specific user documentation:

* [Kubeflow Pipelines v1][kfp-v1-doc]
* [Kubeflow Pipelines v2][kfp-v2-doc]

[kfp-v1-doc]: /docs/components/pipelines/legacy-v1
[kfp-v2-doc]: /docs/components/pipelines/



================================================
File: content/en/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec.md
================================================
+++
title = "Pipelines API Reference (v2beta1)"
description = "API Reference for Kubeflow Pipelines API - v2beta1"
weight = 3
+++

This document describes the API specification for the `v2beta1` Kubeflow Pipelines REST API.

## About the REST API

In most deployments of the [Kubeflow Platform](/docs/started/installing-kubeflow/#kubeflow-platform), the Kubeflow Pipelines REST API is available under the `/pipeline/` HTTP path.
For example, if you host Kubeflow at `https://kubeflow.example.com`, the API will be available at `https://kubeflow.example.com/pipeline/`.

{{% alert title="Tip" color="dark" %}}
We recommend using the [Kubeflow Pipelines Python SDK](docs/components/pipelines/reference/sdk/) as it provides a more user-friendly interface.
See the [Connect SDK to the API](/docs/components/pipelines/user-guides/core-functions/connect-api/) guide for more information.
{{% /alert %}}

### Authentication

How requests are authenticated and authorized will depend on the distribution you are using.
Typically, you will need to provide a token or cookie in the request headers.

Please refer to the documentation of your [Kubeflow distribution](/docs/started/installing-kubeflow/#kubeflow-platform) for more information.

### Example Usage

To use the API, you will need to send HTTP requests to the appropriate endpoints.

For example, to list pipeline runs in the `team-1` namespace, send a `GET` request to the following URL:

```
https://kubeflow.example.com/pipeline/apis/v2beta1/runs?namespace=team-1
```

## Swagger UI

The following [Swagger UI](https://github.com/swagger-api/swagger-ui) is automatically generated from the [`{{% pipelines/latest-version %}}`](https://github.com/kubeflow/pipelines/releases/tag/{{% pipelines/latest-version %}}) version of Kubeflow Pipelines for the [`v2beta1` REST API](https://github.com/kubeflow/pipelines/blob/{{% pipelines/latest-version %}}/backend/api/v2beta1/swagger/kfp_api_single_file.swagger.json).

{{% alert title="Note" color="info" %}}
The _try it out_ feature of Swagger UI does not work due to authentication and CORS, but it can help you construct the correct API calls.
{{% /alert %}}

{{< swaggerui-inline component_name="Kubeflow Pipelines" default_input_url="https://kubeflow.example.com/pipeline/" >}}
https://raw.githubusercontent.com/kubeflow/pipelines/{{% pipelines/latest-version %}}/backend/api/v2beta1/swagger/kfp_api_single_file.swagger.json
{{< /swaggerui-inline >}}



================================================
File: content/en/docs/components/pipelines/user-guides/_index.md
================================================
+++
title = "User Guides"
description = "Documentation for users of Kubeflow Pipelines."
weight = 7
+++



================================================
File: content/en/docs/components/pipelines/user-guides/migration.md
================================================
+++
title = "Migrate to Kubeflow Pipelines v2"
description = "Migrate to the new Kubeflow Pipelines v2 backend and SDK."
weight = 1
+++

{{% kfp-v2-keywords %}}

## Overview 

Kubeflow Pipelines V2 is a significant update to the Kubeflow Pipelines (KFP) platform.

The key features introduced by KFP V2 are:

- A more pythonic SDK - use decorators like ([`@dsl.pipeline`][dsl-pipeline], [`@dsl.component`][dsl-component], [`@dsl.container_component`][dsl-container-component])
- Decouple from Argo Workflows - compile pipelines to a generic [IR YAML][ir-yaml] rather than Argo `Workflow` YAML
- Enhanced Workflow GUI - visualize pipelines, sub-DAGs (nested pipelines), loops, and artifacts (datasets, models, and metrics) to understand and debug your pipelines

## Version Matrix

The first version of [Kubeflow Platform](/docs/started/introduction/#what-is-kubeflow-platform) to include the Kubeflow Pipelines V2 backend was [Kubeflow 1.8](/docs/releases/kubeflow-1.8/).

The following table shows which versions of KFP backend are included in each version of Kubeflow Platform:

Release Date | Kubeflow Platform Version | KFP Backend Version | SDK Mode: [`v1`](/docs/components/pipelines/legacy-v1/sdk/) | SDK Mode: [`v2`](/docs/components/pipelines/user-guides/core-functions/compile-a-pipeline/) | SDK Mode: [`v2-compatible`](https://v1-7-branch.kubeflow.org/docs/components/pipelines/v1/sdk-v2/)
--- | --- | --- | --- | --- | ---
2024-07-22 | [Kubeflow 1.9](/docs/releases/kubeflow-1.9/) | [2.2.0](https://github.com/kubeflow/pipelines/releases/tag/2.2.0) | <i class="fa-solid fa-check"></i> | <i class="fa-solid fa-check"></i> | <i class="fa-solid fa-xmark"></i>
2023-11-01 | [Kubeflow 1.8](/docs/releases/kubeflow-1.8/) | [2.0.3](https://github.com/kubeflow/pipelines/releases/tag/2.0.3) | <i class="fa-solid fa-check"></i> | <i class="fa-solid fa-check"></i> | <i class="fa-solid fa-xmark"></i>
2023-03-29 | [Kubeflow 1.7](/docs/releases/kubeflow-1.7/) | [2.0.0-alpha.7](https://github.com/kubeflow/pipelines/releases/tag/2.0.0-alpha.7) | <i class="fa-solid fa-check"></i> | <i class="fa-solid fa-xmark"></i> | <i class="fa-solid fa-check"></i> 
2022-10-10 | [Kubeflow 1.6](/docs/releases/kubeflow-1.6/) | [2.0.0-alpha.5](https://github.com/kubeflow/pipelines/releases/tag/2.0.0-alpha.5) | <i class="fa-solid fa-check"></i> | <i class="fa-solid fa-xmark"></i> | <i class="fa-solid fa-check"></i> 
2022-06-15 | [Kubeflow 1.5](/docs/releases/kubeflow-1.5/) | [1.8.2](https://github.com/kubeflow/pipelines/releases/tag/1.8.2)  | <i class="fa-solid fa-check"></i> | <i class="fa-solid fa-xmark"></i> | <i class="fa-solid fa-xmark"></i>

## Backward Compatibility

If you have existing KFP Pipelines that you compiled with the V1 SDK, you can run them on the new KFP V2 backend without any changes.
If you wish to author new pipelines, there are some recommended and required steps to migrate which are detailed below.

{{% alert title="Warning" color="warning" %}}
Running V1 pipelines on KFP V2 requires that you compile and submit them using the V1 SDK.
The last version of the V1 SDK was [`kfp==1.8.22`](https://pypi.org/project/kfp/1.8.22/), there will be no further releases.
{{% /alert %}}

## Terminology

Term | Definition
--- | ---
SDK v1 | The `1.x.x` versions of the [`kfp`](https://pypi.org/project/kfp/1.8.22/) Python SDK.
SDK v2 | The `2.x.x` versions of the [`kfp`](https://pypi.org/project/kfp/) Python SDK.
SDK v1 (v2-namespace) | The preview V2 module that was available in the V1 SDK (e.g. `from kfp.v2 import *`).<br>_Only ever used by [Google Cloud Vertex AI Pipelines][vertex-pipelines] users._

## Migration Paths

How you migrate to KFP V2 will depend on your current SDK version and usage.

There are two common migration paths:

1. [__SDK v1__ → __SDK v2__](#migrate-from-sdk-v1-to-sdk-v2)
2. [__SDK v1 (v2-namespace) → SDK v2__](#migrate-from-sdk-v1-v2-namespace-to-sdk-v2)

<br>

### **Migrate from 'SDK v1' to 'SDK v2'**

KFP SDK v2 is generally not backward compatible with user code that uses the KFP SDK v1 main namespace. This section describes some of the important breaking changes and migration steps to upgrade to KFP SDK v2.

We indicate whether each breaking change affects [KFP OSS backend][oss-be-v1] users or [Google Cloud Vertex AI Pipelines][vertex-pipelines] users.

#### **Breaking changes**

<details>
<summary>Click to expand</summary>
<hr>

##### **create_component_from_func and func_to_container_op support**

**Affects:** KFP OSS users and Vertex AI Pipelines users

`create_component_from_func` and `func_to_container_op` are both used in KFP SDK v1 to create lightweight Python function-based components.

Both functions are removed in KFP SDK v2.

**Change:** Use the [`@dsl.component`][dsl-component] decorator, as described in [Lightweight Python Components][lightweight-python-components] and [Containerized Python Components][containerized-python-components].

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
from kfp.components import create_component_from_func
from kfp.components import func_to_container_op

@create_component_from_func
def component1(...):
    ...

def component2(...):
    ...

component2 = create_component_from_func(component2)

@func_to_container_op
def component3(...):
    ...

@dsl.pipeline(name='my-pipeline')
def pipeline():
    component1(...)
    component2(...)
    component3(...)
```

</td>
<td>

```python
from kfp import dsl

@dsl.component
def component1(...):
    ...

@dsl.component
def component2(...):
    ...

@dsl.component
def component3(...):
    ...

@dsl.pipeline(name='my-pipeline')
def pipeline():
    component1(...)
    component2(...)
    component3(...)
```

</td>
</tr>
</table>

---

##### **Keyword arguments required**

**Affects:** KFP OSS users and Vertex AI Pipelines users

Keyword arguments are required when instantiating components as tasks within a pipeline definition.

**Change:** Use keyword arguments.

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
def my_pipeline():
    trainer_component(100, 0.1)
```

</td>
<td>

```python
def my_pipeline():
    trainer_component(epochs=100, learning_rate=0.1)
```

</td>
</tr>
</table>

---

##### **ContainerOp support**

**Affects:** KFP OSS users

`ContainerOp` has been deprecated since mid-2020. `ContainerOp` instances do not carry a description of inputs and outputs and therefore cannot be compiled to [IR YAML][ir-yaml].

`ContainerOp` is removed from v2.

**Change:** Use the [`@dsl.container_component`][dsl-container-component] decorator as described in [Container Components][container-components].

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
from kfp import dsl

# v1 ContainerOp will not be supported.
component_op = dsl.ContainerOp(...)

# v1 ContainerOp from class will not be supported.
class FlipCoinOp(dsl.ContainerOp):
```

</td>
<td>

```python
from kfp import dsl

@dsl.container_component
def flip_coin(rand: int, result: dsl.OutputPath(str)):
  return ContainerSpec(
    image='gcr.io/flip-image'
    command=['flip'],
    arguments=['--seed', rand, '--result-file', result])
```

</td>
</tr>
</table>

---

##### **VolumeOp and ResourceOp support**

**Affects:** KFP OSS users

`VolumeOp` and `ResourceOp` expose direct access to Kubernetes resources within a pipeline definition. There is no support for these features on a non-Kubernetes platforms.

KFP v2 enables support for [platform-specific features](/docs/components/pipelines/user-guides/core-functions/platform-specific-features/) via KFP SDK extension libraries. Kubernetes-specific features are supported in KFP v2 via the [`kfp-kubernetes`](https://kfp-kubernetes.readthedocs.io/) extension library.

---

##### **v1 component YAML support**

**Affects:** KFP OSS users and Vertex AI Pipelines users

KFP v1 supported authoring components directly in YAML via the v1 component YAML format ([example][v1-component-yaml-example]). This authoring style enabled component authors to set their component's image, command, and args directly.

In KFP v2, both components and pipelines are compiled to the same [IR YAML][ir-yaml] format, which is different than the v1 component YAML format.

KFP v2 will continue to support loading existing v1 component YAML using the [`components.load_component_from_file`][components-load-component-from-file] function and [similar functions][load] for backward compatibility.

**Change:** To author components via custom image, command, and args, use the [`@dsl.container_component`][dsl-container-component] decorator as described in [Container Components][container-components]. Note that unlike when authoring v1 component YAML, Container Components do not support setting environment variables on the component itself. Environment variables should be set on the task instantiated from the component within a pipeline definition using the [`.set_env_variable`][dsl-pipelinetask-set-env-variable] task [configuration method][task-configuration-methods].

---

##### **v1 lightweight component types InputTextFile, InputBinaryFile, OutputTextFile and OutputBinaryFile support**

**Affects:** KFP OSS users and Vertex AI Pipelines users

These types ensure files are written either in text mode or binary mode in components authored using the KFP SDK v1.

KFP SDK v2 does not support authoring with these types since users can easily do this themselves.

**Change:** Component authors should inputs and outputs using KFP's [artifact][artifacts] and [parameter][parameters] types.

---

##### **AIPlatformClient support**

**Affects:** Vertex AI Pipelines users

KFP SDK v1 included an `AIPlatformClient` for submitting pipelines to [Vertex AI Pipelines][vertex-pipelines].

KFP SDK v2 does not include this client.

**Change:** Use the official [Python Vertex SDK][vertex-sdk]'s `PipelineJob` class.

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
from kfp.v2.google.client import AIPlatformClient

api_client = AIPlatformClient(
    project_id=PROJECT_ID,
    region=REGION,
)

response = api_client.create_run_from_job_spec(
    job_spec_path=PACKAGE_PATH, pipeline_root=PIPELINE_ROOT,
)
```

</td>
<td>

```python
# pip install google-cloud-aiplatform
from google.cloud import aiplatform

aiplatform.init(
    project=PROJECT_ID,
    location=REGION,
)

job = aiplatform.PipelineJob(
    display_name=DISPLAY_NAME,
    template_path=PACKAGE_PATH,
    pipeline_root=PIPELINE_ROOT,
)

job.submit()
```

</td>
</tr>
</table>

---

##### **run_as_aiplatform_custom_job support**

**Affects:** Vertex AI Pipelines users

KFP v1's `run_as_aiplatform_custom_job` was an experimental feature that allowed converting any component into a [Vertex AI CustomJob][vertex-customjob].

KFP v2 does not include this feature.

**Change:** Use [Google Cloud Pipeline Component][gcpc]'s [create_custom_training_job_from_component][create-custom-training-job-from-component] function.

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
from kfp import components
from kfp.v2 import dsl
from kfp.v2.google.experimental import run_as_aiplatform_custom_job

training_op = components.load_component_from_url(...)

@dsl.pipeline(name='my-pipeline')
def pipeline():
  training_task = training_op(...)
  run_as_aiplatform_custom_job(
      training_task, ...)
```

</td>
<td>

```python
# pip install google-cloud-pipeline-components
from kfp import components
from kfp import dsl
from google_cloud_pipeline_components.v1.custom_job import utils

training_op = components.load_component_from_url(...)

@dsl.pipeline(name='my-pipeline')
def pipeline():
    utils.create_custom_training_job_from_component(training_op, ...)
```

</td>
</tr>
</table>

---

##### **Typecasting behavior change**

**Affects:** KFP OSS users and Vertex AI Pipelines users

KFP SDK v1 had more lenient pipeline typechecking than does KFP SDK v2. Some pipelines that utilized this leniency may not be compilable using KFP SDK v2. For example, parameters typed with `float` would accept the string `"0.1"`:

```python
from kfp.v2 import compiler
from kfp.v2 import dsl
from kfp import components


@dsl.component
def train(
    number_of_epochs: int,
    learning_rate: float,
):
    print(f"number_of_epochs={number_of_epochs}")
    print(f"learning_rate={learning_rate}")


def training_pipeline(number_of_epochs: int = 1):
    train(
        number_of_epochs=number_of_epochs,
        learning_rate="0.1",  # string cannot be passed to float parameter using KFP SDK v2
    )
```

**Change:** We recommend updating your components and pipelines to use types strictly.

---

</details>

<br>
<br>

### **Migrate from 'SDK v1 (v2-namespace)' to 'SDK v2'**

With few exceptions, KFP SDK v2 is backward compatible with user code that uses the KFP SDK v1 v2-namespace.

{{% alert title="Note" color="dark" %}}
This migration path ONLY affects v1 SDK users that were running pipelines on Google Cloud's Vertex AI Pipelines.
{{% /alert %}}

#### **Non-breaking changes**

This section documents non-breaking changes in SDK v2 relative to the SDK v1 v2-namespace.
We suggest you migrate your code to the "New usage", even though the "Previous usage" will still work with warnings.

<details>
<summary>Click to expand</summary>
<hr>

##### **Import namespace**

KFP SDK v1 v2-namespace imports (`from kfp.v2 import *`) should be converted to imports from the primary namespace (`from kfp import *`).

**Change:** Remove the `.v2` module from any KFP SDK v1 v2-namespace imports.

<style>
    th {
        text-align: center;
    }
</style>

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
from kfp.v2 import dsl
from kfp.v2 import compiler

@dsl.pipeline(name='my-pipeline')
def pipeline():
  ...

compiler.Compiler().compile(...)
```

</td>
<td>

```python
from kfp import dsl
from kfp import compiler

@dsl.pipeline(name='my-pipeline')
def pipeline():
  ...

compiler.Compiler().compile(...)
```

</td>
</tr>
</table>

---

##### **output_component_file parameter**

In KFP SDK v2, components can be [compiled][compile] to and [loaded][load] from [IR YAML][ir-yaml] in the same way as pipelines.

KFP SDK v1 v2-namespace supported compiling components via the [`@dsl.component`][dsl-component] decorator's `output_component_file` parameter. This is deprecated in KFP SDK v2. If you choose to still use this parameter, your pipeline will be compiled to [IR YAML][ir-yaml] instead of v1 component YAML.

**Change:** Remove uses of `output_component_file`. Replace with a call to [`Compiler().compile()`][compiler-compile].

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
from kfp.v2.dsl import component

@component(output_component_file='my_component.yaml')
def my_component(input: str):
   ...
```

</td>
<td>

```python
from kfp.dsl import component
from kfp import compiler

@component()
def my_component(input: str):
   ...

compiler.Compiler().compile(my_component, 'my_component.yaml')
```

</td>
</tr>
</table>

---

##### **Pipeline package file extension**

The KFP compiler will compile your pipeline according to the extension provided to the compiler (`.yaml` or `.json`).

In KFP SDK v2, YAML is the preferred serialization format.

**Change:** Convert `package_path` arguments that use a `.json` extension to use a `.yaml` extension.

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
from kfp.v2 import compiler
# .json extension, deprecated format
compiler.Compiler().compile(pipeline, package_path='my_pipeline.json')
```

</td>
<td>

```python
from kfp import compiler
# .yaml extension, preferred format
compiler.Compiler().compile(pipeline, package_path='my_pipeline.yaml')
```

</td>
</tr>
</table>

---

</details>

#### **Breaking changes**

There are only a few subtle breaking changes in SDK v2 relative to the SDK v1 v2-namespace.

<details>
<summary>Click to expand</summary>
<hr>

##### **Drop support for Python 3.6**

KFP SDK v1 supported Python 3.6. KFP SDK v2 supports Python >=3.7.0,\<3.12.0.

---

##### **CLI output change**

The v2 [KFP CLI][cli] is more consistent, readable, and parsable. Code that parsed the v1 CLI output may fail to parse the v2 CLI output.

---

##### **.after referencing upstream task in a dsl.ParallelFor loop**

The following pipeline cannot be compiled in KFP SDK v2:

```python
with dsl.ParallelFor(...):
    t1 = comp()
t2 = comp().after(t1)
```

This usage was primarily used by KFP SDK v1 users who implemented a custom `dsl.ParallelFor` fan-in. KFP SDK v2 natively supports fan-in from [`dsl.ParallelFor`][dsl-parallelfor] using [`dsl.Collected`][dsl-collected]. See [Control Flow][parallelfor-control-flow] user docs for instructions.

---

##### **Importer component import statement**

The location of the `importer_node` object has changed.

**Change:** Import from `kfp.dsl`.

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
from kfp.components import importer_node
```

</td>
<td>

```python
from kfp.dsl import importer_node
```

</td>
</tr>
</table>

---

##### **Adding node selector constraint/accelerator**

The task method `.add_node_selector_constraint` is deprecated in favor of `.set_accelerator_type`. Compared to the previous implementation of `.add_node_selector_constraint`, both methods have the `label_name` parameter removed and the `value` parameter is replaced by the parameter `accelerator`.

**Change:** Use `task.set_accelerator_type(accelerator=...)`. Provide the previous `value` argument to the `accelerator` parameter. Omit the `label_name`.

<table>
<tr>
<th>Previous usage</th>
<th>New usage</th>
</tr>
<tr>
<td>

```python
@dsl.pipeline
def my_pipeline():
    task.add_node_selector_constraint(
        label_name='cloud.google.com/gke-accelerator',
        value='NVIDIA_TESLA_A100',
    )
```

</td>
<td>

```python
@dsl.pipeline
def my_pipeline():
    task.set_accelerator_type(accelerator="NVIDIA_TESLA_K80")
```

</td>
</tr>
</table>

---

</details>

<br>

## Did we miss something?

If you believe we missed a breaking change or an important migration step, please [create an issue][new-issue] describing the change in the [`kubeflow/pipelines` repository][pipelines-repo].

[artifacts]: /docs/components/pipelines/user-guides/data-handling/artifacts
[cli]: /docs/components/pipelines/user-guides/core-functions/cli/
[compile]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline
[compiler-compile]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/compiler.html#kfp.compiler.Compiler.compile
[components-load-component-from-file]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/components.html#kfp.components.load_component_from_file
[container-components]: /docs/components/pipelines/user-guides/components/containerized-python-components/
[containerized-python-components]: /docs/components/pipelines/user-guides/components/containerized-python-components/
[create-custom-training-job-from-component]: https://cloud.google.com/vertex-ai/docs/pipelines/customjob-component
[dsl-collected]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.Collected
[dsl-component]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.component
[dsl-container-component]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.container_component
[dsl-pipeline]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.pipeline
[dsl-parallelfor]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.ParallelFor
[gcpc]: https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction
[ir-yaml]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline/#ir-yaml
[lightweight-python-components]: /docs/components/pipelines/user-guides/components/lightweight-python-components//
[load]: /docs/components/pipelines/user-guides/components/load-and-share-components//
[new-issue]: https://github.com/kubeflow/pipelines/issues/new
[oss-be-v1]: /docs/components/pipelines/legacy-v1/
[oss-be-v2]: /docs/components/pipelines/operator-guides/installation/
[parallelfor-control-flow]: /docs/components/pipelines/user-guides/core-functions/control-flow/#dslparallelfor
[parameters]: /docs/components/pipelines/user-guides/data-handling/parameters
[pipelines-repo]: https://github.com/kubeflow/pipelines
[semver-minor-version]: https://semver.org/#:~:text=MINOR%20version%20when%20you%20add%20functionality%20in%20a%20backwards%20compatible%20manner
[v1-component-yaml-example]: https://github.com/kubeflow/pipelines/blob/01c87f8a032e70a6ca92cdbefa974a7da387f204/sdk/python/test_data/v1_component_yaml/add_component.yaml
[vertex-customjob]: https://cloud.google.com/vertex-ai/docs/training/create-custom-job
[vertex-pipelines]: https://cloud.google.com/vertex-ai/docs/pipelines/introduction
[vertex-sdk]: https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#vertex-ai-sdk-for-python
[dsl-pipelinetask-set-env-variable]: https://kubeflow-pipelines.readthedocs.io/en/2.0.0b13/source/dsl.html#kfp.dsl.PipelineTask.set_env_variable
[task-configuration-methods]: /docs/components/pipelines/user-guides/components/compose-components-into-pipelines/#task-configurations



================================================
File: content/en/docs/components/pipelines/user-guides/components/_index.md
================================================
+++
title = "Create components"
description = "Create pipelines with reusable components."
weight = 3
+++

{{% kfp-v2-keywords %}}

Components are the building blocks of KFP pipelines. A component is a remote function definition; it specifies inputs, has user-defined logic in its body, and can create outputs. When the component template is instantiated with input parameters, we call it a task.

KFP provides two high-level ways to author components: **Python Components** and **Container Components.**

Python Components are a convenient way to author components implemented in pure Python. There are two specific types of Python components: **Lightweight Python Components** and **Containerized Python Components.**

Container Components expose a more flexible, advanced authoring approach by allowing you to define a component using an arbitrary container definition. This is the recommended approach for components that are not implemented in pure Python.

**Importer Components** are a special "pre-baked" component provided by KFP which allows you to import an artifact into your pipeline when that artifact was not created by tasks within the pipeline.


================================================
File: content/en/docs/components/pipelines/user-guides/components/additional-functionality.md
================================================
+++
title = "Additional Functionality"
description = "More information about authoring KFP components"
weight = 6
+++

### Component docstring format
KFP allows you to document your components and pipelines using Python docstrings. The KFP SDK automatically parses your docstrings and include certain fields in [IR YAML][ir-yaml] when you compile components and pipelines.

For components, KFP can extract your component **input descriptions** and **output descriptions**.

For pipelines, KFP can extract your pipeline **input descriptions** and **output descriptions**, as well as a **description of your full pipeline**.

For the KFP SDK to correctly parse your docstrings, you should write your docstrings in the KFP docstring style. The KFP docstring style is a particular variant on the [Google docstring style][google-docstring-style], with the following changes:
* The `Returns:` section takes the same structure as the `Args:` section, where each return value in the `Returns:` section should take the form `<name>: <description>`. This is distinct from the typical Google docstring `Returns:` section which takes the form `<type>: <description>`, with no names for return values.
* Component outputs should be included in the `Returns:` section, even though they are declared via component function input parameters. This applies to function parameters annotated with [`dsl.OutputPath`][dsl-outputpath] and the [`Output[<Artifact>]`][output-type-marker] type marker for declaring [output artifacts][output-artifacts].
* *Suggested:* Type information, including which inputs are optional/required, should be omitted from the input/output descriptions. This information is duplicative of the annotations.

For example, the KFP SDK can extract input and output descriptions from the following component docstring which uses the KFP docstring style:


```python
@dsl.component
def join_datasets(
    dataset_a: Input[Dataset],
    dataset_b: Input[Dataset],
    out_dataset: Output[Dataset],
) -> str:
    """Concatenates two datasets.

    Args:
        dataset_a: First dataset.
        dataset_b: Second dataset.

    Returns:
        out_dataset: The concatenated dataset.
        Output: The concatenated string.
    """
    ...
```

Similarly, KFP can extract the component input descriptions, the component output descriptions, and the pipeline description from the following pipeline docstring:

```python
@dsl.pipeline(display_name='Concatenation pipeline')
def dataset_concatenator(
    string: str,
    in_dataset: Input[Dataset],
) -> Dataset:
    """Pipeline to convert string to a Dataset, then concatenate with
    in_dataset.

    Args:
        string: String to concatenate to in_artifact.
        in_dataset: Dataset to which to concatenate string.

    Returns:
        Output: The final concatenated dataset.
    """
    ...
```

Note that if you provide a `description` argument to the [`@dsl.pipeline`][dsl-pipeline] decorator, KFP will use this description instead of the docstring description.

[ir-yaml]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline#ir-yaml
[google-docstring-style]: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html
[dsl-pipeline]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.pipeline
[output-artifacts]: /docs/components/pipelines/user-guides/data-handling/artifacts
[dsl-outputpath]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.OutputPath
[output-type-marker]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Output


================================================
File: content/en/docs/components/pipelines/user-guides/components/compose-components-into-pipelines.md
================================================
+++
title = "Compose components into pipelines"
weight = 2
+++

{{% kfp-v2-keywords %}}

While components have three authoring approaches, pipelines have one authoring approach: they are defined with a pipeline function decorated with the [`@dsl.pipeline`][dsl-pipeline] decorator. Take the following pipeline, `pythagorean`, which implements the Pythagorean theorem as a pipeline via simple arithmetic components:

```python
from kfp import dsl

@dsl.component
def square(x: float) -> float:
    return x ** 2

@dsl.component
def add(x: float, y: float) -> float:
    return x + y

@dsl.component
def square_root(x: float) -> float:
    return x ** .5

@dsl.pipeline
def pythagorean(a: float, b: float) -> float:
    a_sq_task = square(x=a)
    b_sq_task = square(x=b)
    sum_task = add(x=a_sq_task.output, y=b_sq_task.output)
    return square_root(x=sum_task.output).output
```

Although a KFP pipeline decoratored with the `@dsl.pipeline` decorator looks like a normal Python function, it is actually an expression of pipeline topology and [control flow][control-flow] semantics, constructed using the [KFP domain-specific language][dsl-reference-docs] (DSL).

A pipeline definition has four parts:

1. The pipeline decorator
2. Inputs and outputs declared in the function signature
3. Data passing and task dependencies
4. Task configurations
5. Pipeline control flow

This section covers the first four parts. [Control flow][control-flow] is covered in the next section.

### The pipeline decorator

KFP pipelines are defined inside functions decorated with the `@dsl.pipeline` decorator. The decorator takes three optional arguments:

* `name` is the name of your pipeline. If not provided, the name defaults to a sanitized version of the pipeline function name.
* `description` is a description of the pipeline.
* `pipeline_root` is the root path of the remote storage destination within which the tasks in your pipeline will create outputs. `pipeline_root` may also be set or overridden by pipeline submission clients.
* `display_name` is a human-readable for your pipeline.

You can modify the definition of `pythagorean` to use these arguments:

```python
@dsl.pipeline(name='pythagorean-theorem-pipeline',
              description='Solve for the length of a hypotenuse of a triangle with sides length `a` and `b`.',
              pipeline_root='gs://my-pipelines-bucket',
              display_name='Pythagorean pipeline.')
def pythagorean(a: float, b: float) -> float:
    ...
```

Also see [Additional Functionality: Component docstring format][component-docstring-format] for information on how to provide pipeline metadata via docstrings.

### Pipeline inputs and outputs

Like [components][components], pipeline inputs and outputs are defined by the parameters and annotations in the pipeline function signature.

In the preceding example, `pythagorean` accepts inputs `a` and `b`, each typed `float`, and creates one `float` output.

Pipeline inputs are declaried via function input parameters/annotations and pipeline outputs are declared via function output annotations. Pipeline outputs will _never be declared via pipeline function input parameters_, unlike for components that use [output artifacts][output-artifacts] or [Container Components that use `dsl.OutputPath`][container-component-outputs].

For more information on how to declare pipeline function inputs and outputs, see [Data Types][data-types].

### Data passing and task dependencies

When you call a component in a pipeline definition, it constructs a [`PipelineTask`][pipelinetask] instance. You can pass data between tasks using the `PipelineTask`'s `.output` and `.outputs` attributes.

For a task with a single unnamed output indicated by a single return annotation, access the output using `PipelineTask.output`. This the case for the components `square`, `add`, and `square_root`, which each have one unnamed output.

For tasks with multiple outputs or named outputs, access the output using `PipelineTask.outputs['<output-key>']`. Using named output parameters is described in more detail in [Data Types: Parameters][parameters-namedtuple].

In the absence of data exchange, tasks will run in parallel for efficient pipeline executions. This is the case for `a_sq_task` and `b_sq_task` which do not exchange data.

When tasks exchange data, an execution ordering is established between those tasks. This is to ensure that upstream tasks create their outputs before downstream tasks attempt to consume those outputs. For example, in `pythagorean`, the backend will execute `a_sq_task` and `b_sq_task` before it executes `sum_task`. Similarly, it will execute `sum_task` before it executes the final task created from the `square_root` component.

In some cases, you may wish to establish execution ordering in the absence of data exchange. In these cases, you can call one task's `.after()` method on another task. For example, while `a_sq_task` and `b_sq_task` do not exchange data, we can specify `a_sq_task` to run before `b_sq_task`:

```python
@dsl.pipeline
def pythagorean(a: float, b: float) -> float:
    a_sq_task = square(x=a)
    b_sq_task = square(x=b)
    b_sq_task.after(a_sq_task)
    ...
```

#### Special input types
There are a few special input values that you can pass to a component within your pipeline definition to give the component access to some metadata about itself. These values can be passed to input parameters typed `str`.

For example, the following `print_op` component prints the pipeline job name at component runtime using [`dsl.PIPELINE_JOB_NAME_PLACEHOLDER`][dsl-pipeline-job-name-placeholder]:

```python
from kfp import dsl

@dsl.pipeline
def my_pipeline():
    print_op(text=dsl.PIPELINE_JOB_NAME_PLACEHOLDER)
```

There several special values that may be used in this style, including:
* `dsl.PIPELINE_JOB_NAME_PLACEHOLDER`
* `dsl.PIPELINE_JOB_RESOURCE_NAME_PLACEHOLDER`
* `dsl.PIPELINE_JOB_ID_PLACEHOLDER`
* `dsl.PIPELINE_TASK_NAME_PLACEHOLDER`
* `dsl.PIPELINE_TASK_ID_PLACEHOLDER`
* `dsl.PIPELINE_JOB_CREATE_TIME_UTC_PLACEHOLDER`
* `dsl.PIPELINE_JOB_SCHEDULE_TIME_UTC_PLACEHOLDER`
* `dsl.PIPELINE_ROOT_PLACEHOLDER`

{{% oss-be-unsupported feature_name="`PIPELINE_JOB_CREATE_TIME_UTC_PLACEHOLDER`, `PIPELINE_JOB_SCHEDULE_TIME_UTC_PLACEHOLDER`, and `PIPELINE_ROOT_PLACEHOLDER`" gh_issue_link=https://github.com/kubeflow/pipelines/issues/6155 %}}

See the [KFP SDK DSL reference docs][dsl-reference-docs] for more information about the data provided by each special input.

### Task configurations

The KFP SDK exposes several platform-agnostic task-level configurations via task methods. Platform-agnostic configurations are those that are expected to exhibit similar execution behavior on all KFP-conformant backends, such as the [open source KFP backend][oss-be] or [Google Cloud Vertex AI Pipelines][vertex-pipelines].

All platform-agnostic task-level configurations are set using [`PipelineTask`][pipelinetask] methods. Take the following environment variable example:

```python
from kfp import dsl

@dsl.component
def print_env_var():
    import os
    print(os.environ.get('MY_ENV_VAR'))

@dsl.pipeline()
def my_pipeline():
    task = print_env_var()
    task.set_env_variable('MY_ENV_VAR', 'hello')
```

When executed, the `print_env_var` component should print `'hello'`.

Task-level configuration methods can also be chained:

```python
print_env_var().set_env_variable('MY_ENV_VAR', 'hello').set_env_variable('OTHER_VAR', 'world')
```

The KFP SDK provides the following task methods for setting task-level configurations:

* `.add_accelerator_type`
* `.set_accelerator_limit`
* `.set_cpu_limit`
* `.set_memory_limit`
* `.set_env_variable`
* `.set_caching_options`
* `.set_display_name`
* `.set_retry`
* `.ignore_upstream_failure`

{{% oss-be-unsupported feature_name="`.ignore_upstream_failure`" gh_issue_link=https://github.com/kubeflow/pipelines/issues/9459 %}}

See the [`PipelineTask` reference documentation][pipelinetask] for more information about these methods.

### Pipelines as components

Pipelines can themselves be used as components in other pipelines, just as you would use any other single-step component in a pipeline. For example, we could easily recompose the preceding `pythagorean` pipeline to use an inner helper pipeline `square_and_sum`:

```python
from kfp import dsl

@dsl.component
def square(x: float) -> float:
    return x ** 2

@dsl.component
def add(x: float, y: float) -> float:
    return x + y

@dsl.component
def square_root(x: float) -> float:
    return x ** .5

@dsl.pipeline
def square_and_sum(a: float, b: float) -> float:
    a_sq_task = square(x=a)
    b_sq_task = square(x=b)
    return add(x=a_sq_task.output, y=b_sq_task.output).output

@dsl.pipeline
def pythagorean(a: float = 1.2, b: float = 1.2) -> float:
    sq_and_sum_task = square_and_sum(a=a, b=b)
    return square_root(x=sq_and_sum_task.output).output
```

<!-- TODO: make this reference more precise throughout -->
[dsl-reference-docs]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html
[dsl-pipeline]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.pipeline
[control-flow]: /docs/components/pipelines/user-guides/core-functions/control-flow
[components]: /docs/components/pipelines/user-guides/components
[pipelinetask]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.PipelineTask
[vertex-pipelines]: https://cloud.google.com/vertex-ai/docs/pipelines/introduction
[oss-be]: /docs/components/pipelines/operator-guides/installation/
[data-types]: /docs/components/pipelines/user-guides/data-handling/data-types
[output-artifacts]: /docs/components/pipelines/user-guides/data-handling/artifacts
[container-component-outputs]: /docs/components/pipelines/user-guides/components/container-components#create-component-outputs
[parameters-namedtuple]: /docs/components/pipelines/user-guides/data-handling/parameters#multiple-output-parameters
[dsl-pipeline-job-name-placeholder]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.PIPELINE_JOB_NAME_PLACEHOLDER
[component-docstring-format]: /docs/components/pipelines/user-guides/components/additional-functionality#component-docstring-format



================================================
File: content/en/docs/components/pipelines/user-guides/components/container-components.md
================================================
+++
title = "Container Components"
description = "Create a component via an arbitrary container definition"
weight = 4
+++

{{% kfp-v2-keywords %}}

In KFP, each task execution corresponds to a container execution. This means that all components, even Python Components, are defined by an `image`, `command`, and `args`.

Python Components are unique because they abstract most aspects of the container definition away from the user, making it convenient to construct components that use pure Python. Under the hood, the KFP SDK sets the `image`, `commands`, and `args` to the values needed to execute the Python component for the user.

**Container Components, unlike Python Components, enable component authors to set the `image`, `command`, and `args` directly.** This makes it possible to author components that execute shell scripts, use other languages and binaries, etc., all from within the KFP Python SDK.

### A simple Container Component

The following starts with a simple `say_hello` Container Component and gradually modifies it until it is equivalent to our `say_hello` component from the [Hello World Pipeline example][hello-world-pipeline]. Here is a simple Container Component:

```python
from kfp import dsl

@dsl.container_component
def say_hello():
    return dsl.ContainerSpec(image='alpine', command=['echo'], args=['Hello'])
```

To create a Container Components, use the [`dsl.container_component`][dsl-container-component] decorator and create a function that returns a [`dsl.ContainerSpec`][dsl-containerspec] object. `dsl.ContainerSpec` accepts three arguments: `image`, `command`, and `args`. The component above runs the command `echo` with the argument `Hello` in a container running the image [`alpine`][alpine].

Container Components can be used in pipelines just like Python Components:

```python
from kfp import dsl
from kfp import compiler

@dsl.pipeline
def hello_pipeline():
    say_hello()

compiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')
```

If you run this pipeline, you'll see the string `Hello` in `say_hello`'s logs.

### Use component inputs

To be more useful, `say_hello` should be able to accept arguments. You can modify `say_hello` so that it accepts an input argument `name`:

```python
from kfp import dsl

@dsl.container_component
def say_hello(name: str):
    return dsl.ContainerSpec(image='alpine', command=['echo'], args=[f'Hello, {name}!'])
```

The parameters and annotations in the Container Component function declare the component's interface. In this case, the component has one input parameter `name` and no output parameters.

When you compile this component, `name` will be replaced with a placeholder. At runtime, this placeholder is replaced with the actual value for `name` provided to the `say_hello` component.

Another way to implement this component is to use `sh -c` to read the commands from a single string and pass the name as an argument. This approach tends to be more flexible, as it readily allows chaining multiple commands together.

```python
from kfp import dsl

@dsl.container_component
def say_hello(name: str):
    return dsl.ContainerSpec(image='alpine', command=['sh', '-c', 'echo Hello, $0!'], args=[name])
```

When you run the component with the argument `name='World'`, you’ll see the string `'Hello, World!'` in `say_hello`’s logs.

### Create component outputs

Unlike Python functions, containers do not have a standard mechanism for returning values. To enable Container Components to have outputs, KFP requires you to write outputs to a file inside the container. KFP will read this file and persist the output.

To return an output string from the say `say_hello` component, you can add an output parameter to the function using a `dsl.OutputPath(str)` annotation:

```python
@dsl.container_component
def say_hello(name: str, greeting: dsl.OutputPath(str)):
    ...
```

This component now has one input parameter named `name` typed `str` and one output parameter named `greeting` also typed `str`. At runtime, parameters annotated with [`dsl.OutputPath`][dsl-outputpath] will be provided a system-generated path as an argument. Your component logic should write the output value to this path as JSON. The argument `str` in `greeting: dsl.OutputPath(str)` describes the type of the output `greeting` (e.g., the JSON written to the path `greeting` will be a string). You can fill in the `command` and `args` to write the output:

```python
@dsl.container_component
def say_hello(name: str, greeting: dsl.OutputPath(str)):
    """Log a greeting and return it as an output."""

    return dsl.ContainerSpec(
        image='alpine',
        command=[
            'sh', '-c', '''RESPONSE="Hello, $0!"\
                            && echo $RESPONSE\
                            && mkdir -p $(dirname $1)\
                            && echo $RESPONSE > $1
                            '''
        ],
        args=[name, greeting])
```

### Use in a pipeline

Finally, you can use the updated `say_hello` component in a pipeline:

```python
from kfp import dsl
from kfp import compiler

@dsl.pipeline
def hello_pipeline(person_to_greet: str) -> str:
    # greeting argument is provided automatically at runtime!
    hello_task = say_hello(name=person_to_greet)
    return hello_task.outputs['greeting']

compiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')
```

Note that you will never provide output parameters to components when constructing your pipeline; output parameters are always provided automatically by the backend at runtime.

This should look very similar to the [Hello World pipeline][hello-world-pipeline] with one key difference: since `greeting` is a named output parameter, we access it and return it from the pipeline using `hello_task.outputs['greeting']`, instead of `hello_task.output`. Data passing is discussed in more detail in [Pipelines Basics][pipeline-basics].

### Special placeholders
Each of the three component authoring styles automatically handle data passing into your component via placeholders in the container `command` and `args`. In general, you do not need to know how this works. Container Components also enable you to directly utilize two special placeholders if you wish: [`dsl.ConcatPlaceholder`][dsl-concatplaceholder] and [`dsl.IfPresentPlaceholder`][dsl-ifpresentplaceholder].

You may only use these placeholders in the [`dsl.ContainerSpec`][dsl-containerspec] returned from a Container Component authored via the [`@dsl.container_component`][dsl-container-component] decorator.

#### dsl.ConcatPlaceholder

When you provide a container `command` or container `args` as a list of strings, each element in the list is concatenated using a space separator, then issued to the container at runtime. Concatenating one input to another string without a space separator requires special handling provided by the [`dsl.ConcatPlaceholder`][dsl-concatplaceholder].

`dsl.ConcatPlaceholder` takes one argument, `items`, which may be a list of any combination of static strings, upstream outputs, pipeline parameters, or other instances of `dsl.ConcatPlaceholder` or `dsl.IfPresentPlaceholder`. At runtime, these strings will be concatenated together without a separator.

For example, you can use `dsl.ConcatPlaceholder` to concatenate a file path prefix, suffix, and extension:

```python
from kfp import dsl

@dsl.container_component
def concatenator(prefix: str, suffix: str):
    return dsl.ContainerSpec(
        image='alpine',
        command=[
            'my_program.sh'
        ],
        args=['--input', dsl.ConcatPlaceholder([prefix, suffix, '.txt'])]
    )
```

#### dsl.IfPresentPlaceholder
[`dsl.IfPresentPlaceholder`][dsl-ifpresentplaceholder] is used to conditionally provide command line arguments. The `dsl.IfPresentPlaceholder` takes three arguments: `input_name`, `then`, and optionally `else_`. This placeholder is easiest to understand through an example:

```python
@dsl.container_component
def hello_someone(optional_name: str = None):
    return dsl.ContainerSpec(
        image='python:3.7',
        command=[
            'say_hello',
            dsl.IfPresentPlaceholder(
                input_name='optional_name', then=['--name', optional_name])
        ])
```

If the `hello_someone` component is passed `'world'` as an argument for `optional_name`, the component will pass `--name world` to the executable `say_hello`. If `optional_name` is not provided, `--name world` is omitted.

The third parameter `else_` can be used to provide a default value to fall back to if `input_name` is not provided. For example:

```python
@dsl.container_component
def hello_someone(optional_name: str = None):
    return dsl.ContainerSpec(
        image='python:3.7',
        command=[
            'say_hello',
            dsl.IfPresentPlaceholder(
                input_name='optional_name',
                then=['--name', optional_name],
                else_=['--name', 'friend'])
        ])
```

Arguments to `then` and `else_` may be a list of any combination of static strings, upstream outputs, pipeline parameters, or other instances of `dsl.ConcatPlaceholder` or `dsl.IfPresentPlaceholder`


[hello-world-pipeline]: /docs/components/pipelines/getting-started
[pipeline-basics]: /docs/components/pipelines/user-guides/components/compose-components-into-pipelines
[alpine]: https://hub.docker.com/_/alpine
[dsl-outputpath]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.OutputPath
[dsl-container-component]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.container_component
[dsl-containerspec]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.ContainerSpec
[dsl-concatplaceholder]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.ConcatPlaceholder
[dsl-ifpresentplaceholder]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.IfPresentPlaceholder


================================================
File: content/en/docs/components/pipelines/user-guides/components/containerized-python-components.md
================================================
+++
title = "Containerized Python Components"
description = "Create Python components with more complex dependencies"
weight = 3
+++

{{% kfp-v2-keywords %}}

The following assumes a basic familiarity with [Lightweight Python Components][lightweight-python-components].

Containerized Python Components extend [Lightweight Python Components][lightweight-python-components] by relaxing the constraint that Lightweight Python Components be hermetic (i.e., fully self-contained). This means Containerized Python Component functions can depend on symbols defined outside of the function, imports outside of the function, code in adjacent Python modules, etc. To achieve this, the KFP SDK provides a convenient way to package your Python code into a container.

As a production software best practice, component authors should prefer Containerized Python Components to [Lightweight Python Components][lightweight-python-components] when their component specifies [`packages_to_install`][packages-to-install], since the KFP SDK will install these dependencies into the component's image when it is built, rather than at task runtime.

The following shows how to use Containerized Python Components by modifying the `add` component from the [Lightweight Python Components][lightweight-python-components] example:

```python
from kfp import dsl

@dsl.component
def add(a: int, b: int) -> int:
    return a + b
```

### 1. Source code setup
Start by creating an empty `src/` directory to contain your source code:

```txt
src/
```

Next, add the following simple module, `src/math_utils.py`, with one helper function:

```python
# src/math_utils.py
def add_numbers(num1, num2):
    return num1 + num2
```

Lastly, move your component to `src/my_component.py` and modify it to use the helper function:

```python
# src/my_component.py
from kfp import dsl

@dsl.component
def add(a: int, b: int) -> int:
    from math_utils import add_numbers
    return add_numbers(a, b)
```

`src` now looks like this:

```txt
src/
├── my_component.py
└── math_utils.py
```

### 2. Modify the dsl.component decorator

In this step, you'll provide `base_image` and `target_image` arguments to the `@dsl.component` decorator of your component in `src/my_component.py`:

```python
@dsl.component(base_image='python:3.11',
               target_image='gcr.io/my-project/my-component:v1')
def add(a: int, b: int) -> int:
    from math_utils import add_numbers
    return add_numbers(a, b)
```

Setting `target_image` both (a) specifies the [tag][image-tag] for the image you'll build in Step 3, and (b) instructs KFP to run the decorated Python function in a container that uses the image with that tag.

In a Containerized Python Component, `base_image` specifies the base image that KFP will use when building your new container image. Specifically, KFP uses the `base_image` argument for the [`FROM`][docker-from] instruction in the Dockerfile used to build your image.

The previous example includes `base_image` for clarity, but this is not necessary as `base_image` will default to `'python:3.11'` if omitted.

### 3. Build the component
Now that your code is in a standalone directory and you've specified a target image, you can conveniently build an image using the [`kfp component build`][kfp-component-build] CLI command:

```sh
kfp component build src/ --component-filepattern my_component.py --no-push-image
```

If you have a [configured Docker to use a private image registry](https://docs.docker.com/engine/reference/commandline/login/), you can replace the `--no-push-image` flag with `--push-image` to automatically push the image after building.

### 4. Use the component in a pipeline

Finally, you can use the component in a pipeline:

```python
# pipeline.py
from kfp import compiler, dsl
from src.my_component import add

@dsl.pipeline
def addition_pipeline(x: int, y: int) -> int:
    task1 = add(a=x, b=y)
    task2 = add(a=task1.output, b=x)
    return task2.output

compiler.Compiler().compile(addition_pipeline, 'pipeline.yaml')
```

Your directory now looks like this:

```txt
pipeline.py
src/
├── my_component.py
└── math_utils.py
```

Since `add`'s `target_image` uses [Google Cloud Artifact Registry][artifact-registry] (indicated by the `gcr.io` URI), the pipeline shown here assumes you have pushed your image to Google Cloud Artifact Registry, you are running your pipeline on [Google Cloud Vertex AI Pipelines][vertex-pipelines], and you have configured [IAM permissions][iam] so that Vertex AI Pipelines can pull images from Artifact Registry.


[kfp-component-build]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/cli.html#kfp-component-build
[lightweight-python-components]: /docs/components/pipelines/user-guides/components/lightweight-python-components/
[image-tag]: https://docs.docker.com/engine/reference/commandline/tag/
[docker-from]: https://docs.docker.com/engine/reference/builder/#from
[artifact-registry]: https://cloud.google.com/artifact-registry/docs/docker/authentication
[vertex-pipelines]: https://cloud.google.com/vertex-ai/docs/pipelines/introduction
[iam]: https://cloud.google.com/iam
[packages-to-install]: /docs/components/pipelines/user-guides/components/lightweight-python-components#packages_to_install



================================================
File: content/en/docs/components/pipelines/user-guides/components/importer-component.md
================================================
+++
title = "Special Case: Importer Components"
description = "Import artifacts from outside your pipeline"
weight = 5
+++

{{% kfp-v2-keywords %}}

Unlike the other three authoring approaches, an importer component is not a general authoring style but a pre-baked component for a specific use case: loading a machine learning artifact from from a URI into the current pipeline and, as a result, into [ML Metadata][ml-metadata]. This section assumes basic familiarity with KFP [artifacts][artifacts].

As described in [Pipeline Basics][pipeline-basics], inputs to a task are typically outputs of an upstream task. When this is the case, artifacts are easily accessed on the upstream task using `my_task.outputs['<output-key>']`. The artifact is also registered in ML Metadata when it is created by the upstream task.

If you wish to use an existing artifact that was not generated by a task in the current pipeline, you can use a [`dsl.importer`][dsl-importer] component to load the artifact from its URI.

You do not need to write an importer component; it can be imported from the `dsl` module and used directly:

```python
from kfp import dsl

@dsl.pipeline
def my_pipeline():
    task = get_date_string()
    importer_task = dsl.importer(
        artifact_uri='gs://ml-pipeline-playground/shakespeare1.txt',
        artifact_class=dsl.Dataset,
        reimport=True,
        metadata={'date': task.output})
    other_component(dataset=importer_task.output)
```

In addition to an `artifact_uri` argument, you must provide an `artifact_class` argument to specify the type of the artifact.

The `importer` component permits setting artifact metadata via the `metadata` argument. Metadata can be constructed with outputs from upstream tasks, as is done for the `'date'` value in the example pipeline.

You may also specify a boolean `reimport` argument. If `reimport` is `False`, KFP will check to see if the artifact has already been imported to ML Metadata and, if so, use it. This is useful for avoiding duplicative artifact entries in ML Metadata when multiple pipeline runs import the same artifact. If `reimport` is `True`, KFP will reimport the artifact as a new artifact in ML Metadata regardless of whether it was previously imported.

[pipeline-basics]: /docs/components/pipelines/user-guides/components/compose-components-into-pipelines
[dsl-importer]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.importer
[artifacts]: /docs/components/pipelines/user-guides/data-handling/artifacts
[ml-metadata]: https://github.com/google/ml-metadata



================================================
File: content/en/docs/components/pipelines/user-guides/components/lightweight-python-components.md
================================================
+++
title = "Lightweight Python Components"
description = "Create a component from a self-contained Python function"
weight = 1
+++

{{% kfp-v2-keywords %}}

The easiest way to get started authoring components is by creating a Lightweight Python Component. We saw an example of a Lightweight Python Component with `say_hello` in the [Hello World pipeline example][hello-world-pipeline]. Here is another Lightweight Python Component that adds two integers together:

```python
from kfp import dsl

@dsl.component
def add(a: int, b: int) -> int:
    return a + b
```

Lightweight Python Components are constructed by decorating Python functions with the [`@dsl.component`][dsl-component] decorator. The `@dsl.component` decorator transforms your function into a KFP component that can be executed as a remote function by a KFP conformant-backend, either independently or as a single step in a larger pipeline.

### Python function requirements
To decorate a function with the `@dsl.component` decorator it must meet two requirements:

1. **Type annotations:** The function inputs and outputs must have valid KFP [type annotations][data-types].

    There are two categories of inputs and outputs in KFP: [parameters][parameters] and [artifacts][artifacts]. There are specific types of parameters and artifacts within each category. Every input and output will have a specific type indicated by its type annotation.

    In the preceding `add` component, both inputs `a` and `b` are parameters typed `int`. There is one output, also typed `int`.

    Valid parameter annotations include Python's built-in `int`, `float`, `str`, `bool`, `typing.Dict`, and `typing.List`. Artifact annotations are discussed in detail in [Data Types: Artifacts][artifacts].

2. **Hermetic:** The Python function may not reference any symbols defined outside of its body.

    For example, if you wish to use a constant, the constant must be defined inside the function:

    ```python
    @dsl.component
    def double(a: int) -> int:
        """Succeeds at runtime."""
        VALID_CONSTANT = 2
        return VALID_CONSTANT * a
    ```

    By comparison, the following is invalid and will fail at runtime:

    ```python
    # non-example!
    INVALID_CONSTANT = 2

    @dsl.component
    def errored_double(a: int) -> int:
        """Fails at runtime."""
        return INVALID_CONSTANT * a
    ```

    Imports must also be included in the function body:

    ```python
    @dsl.component
    def print_env():
        import os
        print(os.environ)
    ```

    For many realistic components, hermeticism can be a fairly constraining requirement. [Containerized Python Components][containerized-python-components] is a more flexible authoring approach that drops this requirement.

### dsl.component decorator arguments
In the above examples, we used the [`@dsl.component`][dsl-component] decorator with only one argument: the Python function. The decorator accepts some additional arguments.

#### packages_to_install

Most realistic Lightweight Python Components will depend on other Python libraries. You can pass a list of requirements to `packages_to_install` and the component will install these packages at runtime before executing the component function.

This is similar to including requirements in a [`requirements.txt`][requirements-txt] file.

```python
@dsl.component(packages_to_install=['numpy==1.21.6'])
def sin(val: float = 3.14) -> float:
    return np.sin(val).item()
```

**Note:** As a production software best practice, prefer using [Containerized Python Components][containerized-python-components] when your component specifies `packages_to_install` to eliminate installation of your dependencies at runtime.

#### pip_index_urls

`pip_index_urls` exposes the ability to pip install `packages_to_install` from package indices other than the default [PyPI.org][pypi-org].

When you set `pip_index_urls`, KFP passes these indices to [`pip install`][pip-install]'s [`--index-url`][pip-index-url] and [`--extra-index-url`][pip-extra-index-url] options. It also sets each index as a `--trusted-host`.

Take the following component:

```python
@dsl.component(packages_to_install=['custom-ml-package==0.0.1', 'numpy==1.21.6'],
               pip_index_urls=['http://myprivaterepo.com/simple', 'http://pypi.org/simple'],
)
def comp():
    from custom_ml_package import model_trainer
    import numpy as np
    ...
```

These arguments approximately translate to the following `pip install` command:

```sh
pip install custom-ml-package==0.0.1 numpy==1.21.6 kfp==2 --index-url http://myprivaterepo.com/simple --trusted-host http://myprivaterepo.com/simple --extra-index-url http://pypi.org/simple --trusted-host http://pypi.org/simple
```

Note that when you set `pip_index_urls`, KFP does not include `'https://pypi.org/simple'` automatically. If you wish to pip install packages from a private repository _and_ the default public repository, you should include both the private and default URLs as shown in the preceding component `comp`.

#### base_image

When you create a Lightweight Python Component, your Python function code is extracted by the KFP SDK to be executed inside a container at pipeline runtime. By default, the container image used is [`python:3.7`](https://hub.docker.com/_/python). You can override this image by providing an argument to `base_image`. This can be useful if your code requires a specific Python version or other dependencies not included in the default image.

```python
@dsl.component(base_image='python:3.8')
def print_py_version():
    import sys
    print(sys.version)
```

#### install_kfp_package

`install_kfp_package` can be used together with `pip_index_urls` to provide granular control over installation of the `kfp` package at component runtime.

By default, Python Components install `kfp` at runtime. This is required to define symbols used by your component (such as [artifact annotations][artifacts]) and to access additional KFP library code required to execute your component remotely. If `install_kfp_package` is `False`, `kfp` will not be installed via the normal automatic mechanism. Instead, you can use `packages_to_install` and `pip_index_urls` to install a different version of `kfp`, possibly from a non-default pip index URL.

Note that setting `install_kfp_package` to `False` is rarely necessary and is discouraged for the majority of use cases.

[hello-world-pipeline]: /docs/components/pipelines/getting-started
[containerized-python-components]: /docs/components/pipelines/user-guides/components/containerized-python-components
[dsl-component]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.component
[data-types]: /docs/components/pipelines/user-guides/data-handling/data-types
[parameters]: /docs/components/pipelines/user-guides/data-handling/parameters
[artifacts]: /docs/components/pipelines/user-guides/data-handling/artifacts
[requirements-txt]: https://pip.pypa.io/en/stable/reference/requirements-file-format/
[pypi-org]: https://pypi.org/
[pip-install]: https://pip.pypa.io/en/stable/cli/pip_install/
[pip-index-url]: https://pip.pypa.io/en/stable/cli/pip_install/#cmdoption-0
[pip-extra-index-url]: https://pip.pypa.io/en/stable/cli/pip_install/#cmdoption-extra-index-url


================================================
File: content/en/docs/components/pipelines/user-guides/components/load-and-share-components.md
================================================
+++
title = "Load and Share Components"
description = "Load and use an ecosystem of components"
weight = 8
+++

{{% kfp-v2-keywords %}}

This section describes how to load and use existing components. In this section, "components" refers to both single-step components and pipelines, which can also be [used as components][pipeline-as-component].

IR YAML serves as a portable, sharable computational template. This allows you compile and share your components with others, as well as leverage an ecosystem of existing components.

To use an existing component, you can load it using the [`components`][components-module] module and use it with other components in a pipeline:

```python
from kfp import components

loaded_comp = components.load_component_from_file('component.yaml')

@dsl.pipeline
def my_pipeline():
    loaded_comp()
```

You can also load a component directly from a URL, such as a GitHub URL:

```python
loaded_comp = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2.0.0/sdk/python/test_data/components/add_numbers.yaml')
```

Lastly, you can load a component from a string using [`components.load_component_from_text`][components-load-component-from-text]:

```python
with open('component.yaml') as f:
    component_str = f.read()

loaded_comp = components.load_component_from_text(component_str)
```

As components and pipelines are persisted in the same format (IR YAML), you can also load a pipeline from a local file, URL, or string, just like you load components. Once loaded, a pipeline can be used in another pipeline:

```python
from kfp import components

loaded_pipeline = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2.0.0/sdk/python/test_data/pipelines/pipeline_in_pipeline_complex.yaml')

@dsl.pipeline
def my_pipeline():
    loaded_pipeline(msg='Hello KFP v2!')
```

Some libraries, such as [Google Cloud Pipeline Components][gcpc] package and provide reusable components in a pip-installable [Python package][gcpc-pypi].

[pipeline-as-component]: /docs/components/pipelines/user-guides/components/compose-components-into-pipelines#pipelines-as-components
[gcpc]: https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction
[gcpc-pypi]: https://pypi.org/project/google-cloud-pipeline-components/
[components-module]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html
[components-load-component-from-text]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.load_component_from_text



================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/_index.md
================================================
+++
title = "Core Functions"
description = "Learn about the core functions of Kubeflow Pipelines."
weight = 2
+++



================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/build-advanced-pipeline.md
================================================
+++
title = "Build a More Advanced ML Pipeline"
description = "Create a more advanced pipeline that leverages additional KFP features."
weight = 199
+++

{{% kfp-v2-keywords %}}

This step demonstrates how to build a more advanced machine learning (ML) pipeline that leverages additional KFP pipeline composition features.

The following ML pipeline creates a dataset, normalizes the features of the dataset as a preprocessing step, and trains a simple ML model on the data using different hyperparameters:

```python
from typing import List

from kfp import client
from kfp import dsl
from kfp.dsl import Dataset
from kfp.dsl import Input
from kfp.dsl import Model
from kfp.dsl import Output


@dsl.component(packages_to_install=['pandas==1.3.5'])
def create_dataset(iris_dataset: Output[Dataset]):
    import pandas as pd

    csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
    col_names = [
        'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'
    ]
    df = pd.read_csv(csv_url, names=col_names)

    with open(iris_dataset.path, 'w') as f:
        df.to_csv(f)


@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])
def normalize_dataset(
    input_iris_dataset: Input[Dataset],
    normalized_iris_dataset: Output[Dataset],
    standard_scaler: bool,
    min_max_scaler: bool,
):
    if standard_scaler is min_max_scaler:
        raise ValueError(
            'Exactly one of standard_scaler or min_max_scaler must be True.')

    import pandas as pd
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.preprocessing import StandardScaler

    with open(input_iris_dataset.path) as f:
        df = pd.read_csv(f)
    labels = df.pop('Labels')

    if standard_scaler:
        scaler = StandardScaler()
    if min_max_scaler:
        scaler = MinMaxScaler()

    df = pd.DataFrame(scaler.fit_transform(df))
    df['Labels'] = labels
    with open(normalized_iris_dataset.path, 'w') as f:
        df.to_csv(f)


@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])
def train_model(
    normalized_iris_dataset: Input[Dataset],
    model: Output[Model],
    n_neighbors: int,
):
    import pickle

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsClassifier

    with open(normalized_iris_dataset.path) as f:
        df = pd.read_csv(f)

    y = df.pop('Labels')
    X = df

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    clf.fit(X_train, y_train)
    with open(model.path, 'wb') as f:
        pickle.dump(clf, f)


@dsl.pipeline(name='iris-training-pipeline')
def my_pipeline(
    standard_scaler: bool,
    min_max_scaler: bool,
    neighbors: List[int],
):
    create_dataset_task = create_dataset()

    normalize_dataset_task = normalize_dataset(
        input_iris_dataset=create_dataset_task.outputs['iris_dataset'],
        standard_scaler=True,
        min_max_scaler=False)

    with dsl.ParallelFor(neighbors) as n_neighbors:
        train_model(
            normalized_iris_dataset=normalize_dataset_task
            .outputs['normalized_iris_dataset'],
            n_neighbors=n_neighbors)


endpoint = '<KFP_UI_URL>'
kfp_client = client.Client(host=endpoint)
run = kfp_client.create_run_from_pipeline_func(
    my_pipeline,
    arguments={
        'min_max_scaler': True,
        'standard_scaler': False,
        'neighbors': [3, 6, 9]
    },
)
url = f'{endpoint}/#/runs/details/{run.run_id}'
print(url)
```

This example introduces the following new features in the pipeline:

* Some Python **packages to install** are added at component runtime, using the `packages_to_install` argument on the `@dsl.component` decorator, as follows:

    `@dsl.component(packages_to_install=['pandas==1.3.5'])`

    To use a library after installing it, you must include its import statements within the scope of the component function, so that the library is imported at component runtime.

* **Input and output artifacts** of types `Dataset` and `Model` are introduced in the component signature to describe the input and output artifacts of the components. This is done using the type annotation generics `Input[]` and `Output[]` for input and output artifacts respectively.

  Within the scope of a component, artifacts can be read (for inputs) and written (for outputs) via the `.path` attribute. The KFP backend ensures that *input* artifact files are copied *to* the executing pod's local file system from the remote storage at runtime, so that the component function can read input artifacts from the local file system. By comparison, *output* artifact files are copied *from* the local file system of the pod to remote storage, when the component finishes running. This way, the output artifacts persist outside the pod. In both cases, the component author needs to interact with the local file system only to create persistent artifacts.

  The arguments for the parameters annotated with `Output[]` are not passed to components by the pipeline author. The KFP backend passes this artifact during component runtime, so that component authors don't need to be concerned about the path to which the output artifacts are written. After an output artifact is written, the backend executing the component recognizes the KFP artifact types (`Dataset` or `Model`), and organizes them on the Dashboard.

  An output artifact can be passed as an input to a downstream component using the `.outputs` attribute of the source task and the output artifact parameter name, as follows:

  `create_dataset_task.outputs['iris_dataset']`

* One of the **DSL control flow features**, `dsl.ParallelFor`, is used. It is a context manager that lets pipeline authors create tasks. These tasks execute in parallel in a loop. Using `dsl.ParallelFor` to iterate over the `neighbors` pipeline argument lets you execute the  `train_model` component with different arguments and test multiple hyperparameters in one pipeline run. Other control flow features include `dsl.Condition` and `dsl.ExitHandler`.

Congratulations! You now have a KFP deployment, an end-to-end ML pipeline, and an introduction to the UI. That's just the beginning of KFP pipeline and Dashboard features.


================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/caching.md
================================================
+++
title = "Use Caching"
description = "Learn about caching in Kubeflow Pipelines."
weight = 104
+++

Kubeflow Pipelines support caching to eliminate redundant executions and improve
the efficiency of your pipeline runs. This page provides an overview of caching
in KFP and how to use it in your pipelines.

## Overview

Caching in KFP is a feature that allows you to cache the results of a component
execution and reuse them in subsequent runs. When caching is enabled for a
component, KFP will reuse the component's outputs if the component
is executed again with the same inputs and parameters (and the output is still
available).

Caching is particularly useful when you have components that take a long time to
execute or when you have components that are executed multiple times with the
same inputs and parameters.

If a task's results are retrieved from cache, its representation in the UI will
be marked with a green "arrow from cloud" icon.

## How to use caching

Caching is enabled by default for all components in KFP. You can disable caching
for a component by calling [`.set_caching_options(enable_caching=False)`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.PipelineTask.set_caching_options) on a task object.

```python
from kfp import dsl

@dsl.component
def say_hello(name: str) -> str:
    hello_text = f'Hello, {name}!'
    print(hello_text)
    return hello_text

@dsl.pipeline
def hello_pipeline(recipient: str = 'World!') -> str:
    hello_task = say_hello(name=recipient)
    hello_task.set_caching_options(False)
    return hello_task.output
```

You can also enable or disable caching for all components in a pipeline by
setting the argument `caching` when submitting a pipeline for execution.
This will override the caching settings for all components in the pipeline.

```python
from kfp.client import Client

client = Client()
client.create_run_from_pipeline_func(
    hello_pipeline,
    enable_caching=True,  # overrides the above disabling of caching
)
```

The `--disable-execution-caching-by-default` flag disables caching for all pipeline tasks by default.

Example:

```
kfp dsl compile --py my_pipeline.py --output my_pipeline.yaml --disable-execution-caching-by-default
```

You can also set the default caching behavior using the `KFP_DISABLE_EXECUTION_CACHING_BY_DEFAULT` environment variable. When set to `true`, `1`, or other truthy values, it will disable execution caching by default for all pipelines. When set to `false` or when absent, the default of caching enabled remains.

Example:

```
KFP_DISABLE_EXECUTION_CACHING_BY_DEFAULT=true \
kfp dsl compile --py my_pipeline.py --output my_pipeline.yaml
```
This environment variable also works for `Compiler().compile()`.

Given the following pipeline file:
```
@dsl.pipeline(name='my-pipeline')
def my_pipeline():
    task_1 = create_dataset()
    task_2 = create_dataset()
    task_1.set_caching_options(False)

Compiler().compile(
    pipeline_func=my_pipeline,
    package_path='my_pipeline.yaml',

)
```
Executing the following:
```
KFP_DISABLE_EXECUTION_CACHING_BY_DEFAULT=true \
python my_pipeline.py
```
will result in `task_2` having caching disabled.

**NOTE**: Since Python initializes configurations during the import process, setting the `KFP_DISABLE_EXECUTION_CACHING_BY_DEFAULT` environment variable after importing pipeline components will not affect the caching behavior. Therefore, always set it before importing any Kubeflow Pipelines components.


================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/cli.md
================================================
+++
title = "Use the KFP CLI"
description = "Learn how to interact with Kubeflow Pipelines using the KFP CLI."
weight = 203
+++

{{% kfp-v2-keywords %}}

<!-- TODO: Improve or standardize rendering of variables and placeholders -->
<!-- TODO: Standardize inline references to KFP CLI SDK -->

This section provides a summary of the available commands in the KFP CLI. For more comprehensive documentation about all the available commands in the KFP CLI, see [Command Line Interface][cli-reference-docs] in the [KFP SDK reference documentation][kfp-sdk-api-ref].

## Installation
The KFP CLI is installed when you install the KFP SDK: `pip install kfp`.

### Check availability of KFP CLI

To check whether KFP CLI is installed in your environment, run the following command:

```shell
kfp --version
```

### General syntax

All commands in the KFP CLI use the following general syntax:

```shell
kfp [OPTIONS] COMMAND [ARGS]...
```

For example, to list all runs for a specific endpoint, run the following command:

```shell
kfp --endpoint http://my_kfp_endpoint.com run list
```

### Get help for a command

To get help for a specific command, use the argument `--help` directly in the command line. For example, to view guidance about the `kfp run` command, type the following command:

```shell
kfp run --help
```
## Main functons of the KFP CLI

You can use the KFP CLI to do the following:

- [Installation](#installation)
  - [Check availability of KFP CLI](#check-availability-of-kfp-cli)
  - [General syntax](#general-syntax)
  - [Get help for a command](#get-help-for-a-command)
- [Main functons of the KFP CLI](#main-functons-of-the-kfp-cli)
  - [Interact with KFP resources](#interact-with-kfp-resources)
  - [Compile pipelines](#compile-pipelines)
  - [Build containerized Python components](#build-containerized-python-components)
    - [Before you begin](#before-you-begin)
    - [Build the component](#build-the-component)

### Interact with KFP resources

The majority of the KFP CLI commands let you create, read, update, or delete KFP resources from the KFP backend. All of these commands use the following general syntax:

```shell
kfp <resource_name> <action>
```

The `<resource_name>` argument can be one of the following:
* `run`
* `recurring-run`
* `pipeline`
* `experiment`

For all values of the `<resource_name>` argument, the `<action>` argument can be one of the following:
* `create`
* `list`
* `get`
* `delete`

Some resource names have additional resource-specific actions. The following table lists a few examples of resource-specific actions:

| Resource name | Additional resource-specific actions
|---------------|--------
| `run` | <ul><li>`archive`</li><li>`unarchive`</li></ul>
| `recurring-run` | <ul><li>`disable`</li><li>`enable`</li></ul>
| `experiment` | <ul><li>`archive`</li><li>`unarchive`</li></ul>
| `pipeline` | <ul><li>`create-version`</li><li>`list-versions`</li><li>`get-versions`</li><li>`delete-versions`</li></ul>

### Compile pipelines

You can use the `kfp dsl compile` command to compile pipelines or components defined in a Python file to IR YAML.

* To compile a pipeline definition defined in a Python file, run the following command.

  ```shell
  kfp dsl compile --py [PATH_TO_INPUT_PYTHON] --output [PATH_TO_OUTPUT_YAML] --function [PIPELINE_NAME]
  ```
  
  For example:
  
  ```shell
  kfp dsl compile --py path/to/pipeline.py --output path/to/output.yaml
  ```
  
  To compile a single pipeline or component from a Python file containing multiple pipeline or component definitions, use the `--function` argument.
  
  For example:
  
  ```shell
  kfp dsl compile --py path/to/pipeline.py --output path/to/output.yaml --function my_pipeline
  ```
  
  ```shell
  kfp dsl compile --py path/to/pipeline.py --output path/to/output.yaml --function my_component
  ```

* To specify pipeline parameters, use the `--pipeline-parameters` argument and provide the parameters as JSON.

  ```shell
  kfp dsl compile [PATH_TO_INPUT_PYTHON] --output [PATH_TO_OUTPUT_YAML] --pipeline-parameters [PIPELINE_PARAMETERS_JSON]
  ```

  For example:
  
  ```shell
  kfp dsl compile --py path/to/pipeline.py --output path/to/output.yaml --pipeline-parameters '{"param1": 2.0, "param2": "my_val"}'
  ```

### Build containerized Python components

You can author [Containerized Python Components][containerized-python-components] in the KFP SDK. This lets you use handle more source code with better code organization than the simpler [Lightweight Python Component][lightweight-python-component] authoring experience.


#### Before you begin

Run the following command to install the KFP SDK with the additional Docker dependency:

```shell
pip install kfp[all]
```

#### Build the component

To build a containerized Python component, use the following convenience command in the KFP CLI. Using this command, you can build an image with all the source code found in `COMPONENTS_DIRECTORY`. The command uses the component found in the directory as the component runtime entrypoint.

```shell
kfp component build [OPTIONS] [COMPONENTS_DIRECTORY] [ARGS]...
```

For example:

```shell
kfp component build src/ --component-filepattern my_component --push-image
```

For more information about the arguments and flags supported by the `kfp component build` command, see [build](https://kubeflow-pipelines.readthedocs.io/en/stable/source/cli.html#kfp-component-build) in the [KFP SDK API reference][kfp-sdk-api-ref]. For more information about creating containerized Python components, see [Authoring Python Containerized Components][containerized-python-components].

[cli-reference-docs]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/cli.html
[kfp-sdk-api-ref]: https://kubeflow-pipelines.readthedocs.io/en/stable/index.html
[lightweight-python-component]: /docs/components/pipelines/user-guides/components/lightweight-python-components/
[containerized-python-components]: /docs/components/pipelines/user-guides/components/containerized-python-components



================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/compile-a-pipeline.md
================================================
+++
title = "Compile a Pipeline"
description = "Define and compile a basic pipeline using the KFP SDK."
weight = 101
+++

{{% kfp-v2-keywords %}}

## Overview

To [submit a pipeline for execution](/docs/components/pipelines/user-guides/core-functions/run-a-pipeline/), you must compile it to YAML with the KFP SDK compiler.

In the following example, the compiler creates a file called `pipeline.yaml`, which contains a hermetic representation of your pipeline.
The output is called an [Intermediate Representation (IR) YAML](#ir-yaml), which is a serialized [`PipelineSpec`][pipeline-spec] protocol buffer message.

```python
from kfp import compiler, dsl

@dsl.component
def comp(message: str) -> str:
    print(message)
    return message

@dsl.pipeline
def my_pipeline(message: str) -> str:
    """My ML pipeline."""
    return comp(message=message).output

compiler.Compiler().compile(my_pipeline, package_path='pipeline.yaml')
```

Because components are actually pipelines, you may also compile them to IR YAML:

```python
@dsl.component
def comp(message: str) -> str:
    print(message)
    return message

compiler.Compiler().compile(comp, package_path='component.yaml')
```

You can view an [example of IR YAML][compiled-output-example] on GitHub. 
The contents of the file are not intended to be human-readable, however the comments at the top of the file provide a summary of the pipeline:

```yaml
# PIPELINE DEFINITION
# Name: my-pipeline
# Description: My ML pipeline.
# Inputs:
#    message: str
# Outputs:
#    Output: str
...
```

## Type checking

By default, the DSL compiler statically type checks your pipeline to ensure type consistency between components that pass data between one another. 
Static type checking helps identify component I/O inconsistencies without having to run the pipeline, shortening development iterations.

Specifically, the type checker checks for type equality between the type of data a component input expects and the type of the data provided. 
See [Data Types][data-types] for more information about KFP data types.

For example, for parameters, a list input may only be passed to parameters with a `typing.List` annotation. 
Similarly, a float may only be passed to parameters with a `float` annotation.

Input data types and annotations must also match for artifacts, with one exception: the `Artifact` type is compatible with all other artifact types. 
In this sense, the `Artifact` type is both the default artifact type and an artifact "any" type.

As described in the following section, you can disable type checking.

## Compiler arguments

The [`Compiler.compile`][compiler-compile] method accepts the following arguments:

| Name | Type | Description |
|------|------|-------------|
| `pipeline_func` | `function` | _Required_<br/>Pipeline function constructed with the `@dsl.pipeline` or component constructed with the @dsl.component decorator.
| `package_path` | `string` | _Required_<br/>Output YAML file path. For example, `~/my_pipeline.yaml` or `~/my_component.yaml`.
| `pipeline_name` | `string` | _Optional_<br/>If specified, sets the name of the pipeline template in the `pipelineInfo.name` field in the compiled IR YAML output. Overrides the name of the pipeline or component specified by the `name` parameter in the `@dsl.pipeline` decorator.
| `pipeline_parameters` | `Dict[str, Any]` | _Optional_<br/>Map of parameter names to argument values. This lets you provide default values for pipeline or component parameters. You can override these default values during pipeline submission.
| `type_check` | `bool` | _Optional_<br/>Indicates whether static type checking is enabled during compilation.<br/>

## IR YAML

The IR YAML is an intermediate representation of a compiled pipeline or component. 
It is an instance of the [`PipelineSpec`][pipeline-spec] protocol buffer message type, which is a platform-agnostic pipeline representation protocol. 
It is considered an intermediate representation because the KFP backend compiles `PipelineSpec` to [Argo Workflow][argo-workflow] YAML as the final pipeline definition for execution.

Unlike the v1 component YAML, the IR YAML is not intended to be written directly.
While IR YAML is not intended to be easily human-readable, you can still inspect it if you know a bit about its contents:

| Section | Description | Example |
|-------|-------------|---------|
| [`components`][components-schema] | This section is a map of the names of all components used in the pipeline to [`ComponentSpec`][component-spec]. `ComponentSpec` defines the interface, including inputs and outputs, of a component.<br/>For primitive components, `ComponentSpec` contains a reference to the executor containing the component implementation.<br/><br/>For pipelines used as components, `ComponentSpec` contains a [DagSpec][dag-spec] instance, which includes references to the underlying primitive components. | [View on Github][components-example]
| [`deployment_spec`][deployment-spec-schema] | This section contains a map of executor name to [`ExecutorSpec`][executor-spec]. `ExecutorSpec` contains the implementation for a primitive component. | [View on Github][deployment-spec-example]
| [`root`][root-schema] | This section defines the steps of the outermost pipeline definition, also called the pipeline root definition. The root definition is the workflow executed when you submit the IR YAML. It is an instance of [`ComponentSpec`][component-spec]. | [View on Github][root-example]
| [`pipeline_info`][pipeline-info-schema] <a id="kfp_iryaml_pipelineinfo"></a> | This section contains pipeline metadata, including the `pipelineInfo.name` field. This field contains the name of your pipeline template. When you upload your pipeline, a pipeline context name is created based on this template name. The pipeline context lets the backend and the dashboard associate artifacts and executions from pipeline runs using the pipeline template. You can use a pipeline context to determine the best model by comparing metrics and artifacts from multiple pipeline runs based on the same training pipeline. | [View on Github][pipeline-info-example]
| [`sdk_version`][sdk-version-schema] | This section records the version of the KFP SDK used to compile the pipeline. | [View on Github][sdk-version-example]
| [`schema_version`][schema-version-schema] | This section records the version of the `PipelineSpec` schema used for the IR YAML. | [View on Github][schema-version-example]
| [`default_pipeline_root`][default-pipeline-root-schema] | This section records the remote storage root path, such as a MinIO URI or Google Cloud Storage URI, where the pipeline output is written. | [View on Github][default-pipeline-root-example]

[pipeline-spec]: https://github.com/kubeflow/pipelines/blob/master/api/v2alpha1/pipeline_spec.proto#L50
[argo-workflow]: https://argoproj.github.io/argo-workflows/
[compiled-output-example]: https://github.com/kubeflow/pipelines/blob/984d8a039d2ff105ca6b21ab26be057b9552b51d/sdk/python/test_data/pipelines/two_step_pipeline.yaml
[components-example]: https://github.com/kubeflow/pipelines/blob/984d8a039d2ff105ca6b21ab26be057b9552b51d/sdk/python/test_data/pipelines/two_step_pipeline.yaml#L1-L21
[deployment-spec-example]: https://github.com/kubeflow/pipelines/blob/984d8a039d2ff105ca6b21ab26be057b9552b51d/sdk/python/test_data/pipelines/two_step_pipeline.yaml#L23-L49
[root-example]: https://github.com/kubeflow/pipelines/blob/984d8a039d2ff105ca6b21ab26be057b9552b51d/sdk/python/test_data/pipelines/two_step_pipeline.yaml#L52-L85
[pipeline-info-example]: https://github.com/kubeflow/pipelines/blob/984d8a039d2ff105ca6b21ab26be057b9552b51d/sdk/python/test_data/pipelines/two_step_pipeline.yaml#L50-L51
[sdk-version-example]: https://github.com/kubeflow/pipelines/blob/984d8a039d2ff105ca6b21ab26be057b9552b51d/sdk/python/test_data/pipelines/two_step_pipeline.yaml#L87
[schema-version-example]: https://github.com/kubeflow/pipelines/blob/984d8a039d2ff105ca6b21ab26be057b9552b51d/sdk/python/test_data/pipelines/two_step_pipeline.yaml#L86
[default-pipeline-root-example]: https://github.com/kubeflow/pipelines/blob/984d8a039d2ff105ca6b21ab26be057b9552b51d/sdk/python/test_data/pipelines/two_step_pipeline.yaml#L22
[components-schema]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L74-L75
[deployment-spec-schema]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L56
[root-schema]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L77-L79
[pipeline-info-schema]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L51-L52
[sdk-version-schema]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L58-L59
[schema-version-schema]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L61-L62
[default-pipeline-root-schema]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L81-L82
[component-spec]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L85-L96
[executor-spec]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L788-L803
[dag-spec]: https://github.com/kubeflow/pipelines/blob/41b69fd90da812005965f2209b64fd1278f1cdc9/api/v2alpha1/pipeline_spec.proto#L98-L105
[data-types]: /docs/components/pipelines/user-guides/data-handling/data-types
[compiler-compile]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/compiler.html#kfp.compiler.Compiler.compile



================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md
================================================
+++
title = "Connect the SDK to the API"
description = "Learn how to connect the Kubeflow Pipelines SDK to the API."
weight = 201
+++

## Overview

The [Kubeflow Pipelines SDK](https://kubeflow-pipelines.readthedocs.io/en/stable/) provides a Python interface to interact with the Kubeflow Pipelines API. 
This guide will show you how to connect the SDK to the Pipelines API in various scenarios.


## Kubeflow Platform

When running Kubeflow Pipelines as part of a multi-user [Kubeflow Platform](/docs/started/introduction/#what-is-kubeflow-platform), how you authenticate the Pipelines SDK will depend on whether you are running your code __inside__ or __outside__ the cluster.

### **Kubeflow Platform - Inside the Cluster**

<details>
<summary>Click to expand</summary>
<hr>

A [ServiceAccount token volume](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection) can be mounted to a Pod running in the same cluster as Kubeflow Pipelines.
The Kubeflow Pipelines SDK can use this token to authenticate itself with the Kubeflow Pipelines API.

The following Python code will create a `kfp.Client()` using a ServiceAccount token for authentication:

```python
import kfp

# by default, when run from inside a Kubernetes cluster:
#  - the token is read from the `KF_PIPELINES_SA_TOKEN_PATH` path
#  - the host is set to `http://ml-pipeline-ui.kubeflow.svc.cluster.local`
kfp_client = kfp.Client()

# test the client by listing experiments
experiments = kfp_client.list_experiments(namespace="my-profile")
print(experiments)
```

#### ServiceAccount Token Volume

To use the preceding code, you will need to run it from a Pod that has a ServiceAccount token volume mounted.
You may manually add a `volume` and `volumeMount` to your PodSpec or use Kubeflow's [`PodDefaults`](https://github.com/kubeflow/kubeflow/tree/master/components/admission-webhook) to inject the required volume.

__Option 1 - manually add a volume to your PodSpec:__

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: access-kfp-example
spec:
  containers:
  - image: hello-world:latest
    name: hello-world
    env:
      - ## this environment variable is automatically read by `kfp.Client()`
        ## this is the default value, but we show it here for clarity
        name: KF_PIPELINES_SA_TOKEN_PATH
        value: /var/run/secrets/kubeflow/pipelines/token
    volumeMounts:
      - mountPath: /var/run/secrets/kubeflow/pipelines
        name: volume-kf-pipeline-token
        readOnly: true
  volumes:
    - name: volume-kf-pipeline-token
      projected:
        sources:
          - serviceAccountToken:
              path: token
              expirationSeconds: 7200
              ## defined by the `TOKEN_REVIEW_AUDIENCE` environment variable on the `ml-pipeline` deployment
              audience: pipelines.kubeflow.org      
```

__Option 2 - use a `PodDefault` to inject the volume:__

```yaml
apiVersion: kubeflow.org/v1alpha1
kind: PodDefault
metadata:
  name: access-ml-pipeline
  namespace: "<YOUR_USER_PROFILE_NAMESPACE>"
spec:
  desc: Allow access to Kubeflow Pipelines
  selector:
    matchLabels:
      access-ml-pipeline: "true"
  env:
    - ## this environment variable is automatically read by `kfp.Client()`
      ## this is the default value, but we show it here for clarity
      name: KF_PIPELINES_SA_TOKEN_PATH
      value: /var/run/secrets/kubeflow/pipelines/token
  volumes:
    - name: volume-kf-pipeline-token
      projected:
        sources:
          - serviceAccountToken:
              path: token
              expirationSeconds: 7200
              ## defined by the `TOKEN_REVIEW_AUDIENCE` environment variable on the `ml-pipeline` deployment
              audience: pipelines.kubeflow.org      
  volumeMounts:
    - mountPath: /var/run/secrets/kubeflow/pipelines
      name: volume-kf-pipeline-token
      readOnly: true
```

{{% alert title="Tip" color="info" %}}
* `PodDefaults` are namespaced resources, so you need to create one inside __each__ of your Kubeflow `Profile` namespaces.
* The Notebook Spawner UI will be aware of any `PodDefaults` in the user's namespace (they are selectable under the "configurations" section).
{{% /alert %}}

#### RBAC Authorization

The Kubeflow Pipelines API respects Kubernetes RBAC, and will check RoleBindings assigned to the ServiceAccount before allowing it to take Pipelines API actions.

For example, this RoleBinding allows Pods with the `default-editor` ServiceAccount in `namespace-2` to manage Kubeflow Pipelines in `namespace-1`:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: allow-namespace-2-kubeflow-edit
  ## this RoleBinding is in `namespace-1`, because it grants access to `namespace-1`
  namespace: namespace-1
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubeflow-edit
subjects:
  - kind: ServiceAccount
    name: default-editor
    ## the ServiceAccount lives in `namespace-2`
    namespace: namespace-2
```

{{% alert title="Tip" color="info" %}}
* Review the ClusterRole called [`aggregate-to-kubeflow-pipelines-edit`](https://github.com/kubeflow/pipelines/blob/efb96135033fc6e6e55078d33814c45a98566e68/manifests/kustomize/base/installs/multi-user/view-edit-cluster-roles.yaml#L36-L99) 
for a list of some important `pipelines.kubeflow.org` RBAC verbs.
* Kubeflow Notebooks pods run as the `default-editor` ServiceAccount by default, so the RoleBindings for `default-editor` apply to them
and give them access to submit pipelines in their own namespace.
* For more information about profiles, see the [Manage Profile Contributors](/docs/components/central-dash/profiles/#manage-profile-contributors) guide.
{{% /alert %}}

</details>

### **Kubeflow Platform - Outside the Cluster**

<details>
<summary>Click to expand</summary>
<hr>

{{% alert title="Kubeflow Notebooks" color="warning" %}}
As Kubeflow Notebooks run on Pods _inside the cluster_, they can NOT use the following method to authenticate the Pipelines SDK, see the [inside the cluster](#kubeflow-platform---inside-the-cluster) method.
{{% /alert %}}

The precise method to authenticate from _outside the cluster_ will depend on how you [deployed Kubeflow Platform](/docs/started/installing-kubeflow/#kubeflow-platform). 
Because most distributions use [Dex](https://dexidp.io/) as their identity provider, this example will show you how to authenticate with Dex using a Python script.

You will need to make the Kubeflow Pipelines API accessible on the remote machine.
If your Kubeflow Istio gateway is already exposed, skip this step and use that URL directly.

The following command will expose the `istio-ingressgateway` service on `localhost:8080`:

```bash
# TIP: svc/istio-ingressgateway may be called something else, 
#      or use different ports in your distribution
kubectl port-forward --namespace istio-system svc/istio-ingressgateway 8080:80
```

The following Python code defines a `KFPClientManager()` class that creates an authenticated `kfp.Client()` by interacting with Dex:

```python
import re
from urllib.parse import urlsplit, urlencode

import kfp
import requests
import urllib3


class KFPClientManager:
    """
    A class that creates `kfp.Client` instances with Dex authentication.
    """

    def __init__(
        self,
        api_url: str,
        dex_username: str,
        dex_password: str,
        dex_auth_type: str = "local",
        skip_tls_verify: bool = False,
    ):
        """
        Initialize the KfpClient

        :param api_url: the Kubeflow Pipelines API URL
        :param skip_tls_verify: if True, skip TLS verification
        :param dex_username: the Dex username
        :param dex_password: the Dex password
        :param dex_auth_type: the auth type to use if Dex has multiple enabled, one of: ['ldap', 'local']
        """
        self._api_url = api_url
        self._skip_tls_verify = skip_tls_verify
        self._dex_username = dex_username
        self._dex_password = dex_password
        self._dex_auth_type = dex_auth_type
        self._client = None

        # disable SSL verification, if requested
        if self._skip_tls_verify:
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        # ensure `dex_default_auth_type` is valid
        if self._dex_auth_type not in ["ldap", "local"]:
            raise ValueError(
                f"Invalid `dex_auth_type` '{self._dex_auth_type}', must be one of: ['ldap', 'local']"
            )

    def _get_session_cookies(self) -> str:
        """
        Get the session cookies by authenticating against Dex
        :return: a string of session cookies in the form "key1=value1; key2=value2"
        """

        # use a persistent session (for cookies)
        s = requests.Session()

        # GET the api_url, which should redirect to Dex
        resp = s.get(
            self._api_url, allow_redirects=True, verify=not self._skip_tls_verify
        )
        if resp.status_code == 200:
            pass
        elif resp.status_code == 403:
            # if we get 403, we might be at the oauth2-proxy sign-in page
            # the default path to start the sign-in flow is `/oauth2/start?rd=<url>`
            url_obj = urlsplit(resp.url)
            url_obj = url_obj._replace(
                path="/oauth2/start", query=urlencode({"rd": url_obj.path})
            )
            resp = s.get(
                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify
            )
        else:
            raise RuntimeError(
                f"HTTP status code '{resp.status_code}' for GET against: {self._api_url}"
            )

        # if we were NOT redirected, then the endpoint is unsecured
        if len(resp.history) == 0:
            # no cookies are needed
            return ""

        # if we are at `../auth` path, we need to select an auth type
        url_obj = urlsplit(resp.url)
        if re.search(r"/auth$", url_obj.path):
            url_obj = url_obj._replace(
                path=re.sub(r"/auth$", f"/auth/{self._dex_auth_type}", url_obj.path)
            )

        # if we are at `../auth/xxxx/login` path, then we are at the login page
        if re.search(r"/auth/.*/login$", url_obj.path):
            dex_login_url = url_obj.geturl()
        else:
            # otherwise, we need to follow a redirect to the login page
            resp = s.get(
                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify
            )
            if resp.status_code != 200:
                raise RuntimeError(
                    f"HTTP status code '{resp.status_code}' for GET against: {url_obj.geturl()}"
                )
            dex_login_url = resp.url

        # attempt Dex login
        resp = s.post(
            dex_login_url,
            data={"login": self._dex_username, "password": self._dex_password},
            allow_redirects=True,
            verify=not self._skip_tls_verify,
        )
        if resp.status_code != 200:
            raise RuntimeError(
                f"HTTP status code '{resp.status_code}' for POST against: {dex_login_url}"
            )

        # if we were NOT redirected, then the login credentials were probably invalid
        if len(resp.history) == 0:
            raise RuntimeError(
                f"Login credentials are probably invalid - "
                f"No redirect after POST to: {dex_login_url}"
            )

        # if we are at `../approval` path, we need to approve the login
        url_obj = urlsplit(resp.url)
        if re.search(r"/approval$", url_obj.path):
            dex_approval_url = url_obj.geturl()

            # approve the login
            resp = s.post(
                dex_approval_url,
                data={"approval": "approve"},
                allow_redirects=True,
                verify=not self._skip_tls_verify,
            )
            if resp.status_code != 200:
                raise RuntimeError(
                    f"HTTP status code '{resp.status_code}' for POST against: {url_obj.geturl()}"
                )

        return "; ".join([f"{c.name}={c.value}" for c in s.cookies])

    def _create_kfp_client(self) -> kfp.Client:
        try:
            session_cookies = self._get_session_cookies()
        except Exception as ex:
            raise RuntimeError(f"Failed to get Dex session cookies") from ex

        # monkey patch the kfp.Client to support disabling SSL verification
        # kfp only added support in v2: https://github.com/kubeflow/pipelines/pull/7174
        original_load_config = kfp.Client._load_config

        def patched_load_config(client_self, *args, **kwargs):
            config = original_load_config(client_self, *args, **kwargs)
            config.verify_ssl = not self._skip_tls_verify
            return config

        patched_kfp_client = kfp.Client
        patched_kfp_client._load_config = patched_load_config

        return patched_kfp_client(
            host=self._api_url,
            cookies=session_cookies,
        )

    def create_kfp_client(self) -> kfp.Client:
        """Get a newly authenticated Kubeflow Pipelines client."""
        return self._create_kfp_client()
```

The following Python code shows how to use the `KFPClientManager()` class to create a `kfp.Client()`:

```python
# initialize a KFPClientManager
kfp_client_manager = KFPClientManager(
    api_url="http://localhost:8080/pipeline",
    skip_tls_verify=True,

    dex_username="user@example.com",
    dex_password="12341234",

    # can be 'ldap' or 'local' depending on your Dex configuration
    dex_auth_type="local",
)

# get a newly authenticated KFP client
# TIP: long-lived sessions might need to get a new client when their session expires
kfp_client = kfp_client_manager.create_kfp_client()

# test the client by listing experiments
experiments = kfp_client.list_experiments(namespace="my-profile")
print(experiments)
```

</details>

## Standalone Kubeflow Pipelines

When running Kubeflow Pipelines in [standalone mode](/docs/components/pipelines/operator-guides/installation/), there will be no concept of multi-user authentication or RBAC.
The specific steps will depend on whether you are running your code __inside__ or __outside__ the cluster.

### **Standalone KFP - Inside the Cluster**

<details>
<summary>Click to expand</summary>
<hr>

When running inside the Kubernetes cluster, you may connect Pipelines SDK directly to the `ml-pipeline-ui` service via [cluster-internal service DNS resolution](https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services).

{{% alert title="Warning" color="warning" %}}
In [standalone deployments](/docs/components/pipelines/operator-guides/installation/) of Kubeflow Pipelines, there is no authentication enforced on the `ml-pipeline-ui` service.
{{% /alert %}}

When running in the __same namespace__ as Kubeflow:

```python
import kfp

client = kfp.Client(host="http://ml-pipeline-ui:80")

print(client.list_experiments())
```

When running in a __different namespace__ to Kubeflow:

```python
import kfp

# the namespace in which you deployed Kubeflow Pipelines
namespace = "kubeflow" 

client = kfp.Client(host=f"http://ml-pipeline-ui.{namespace}")

print(client.list_experiments())
```

</details>

### **Standalone KFP - Outside the Cluster**

<details>
<summary>Click to expand</summary>
<hr>

When running outside the Kubernetes cluster, you may connect Pipelines SDK to the `ml-pipeline-ui` service by using [kubectl port-forwarding](https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/).

{{% alert title="Warning" color="warning" %}}
In [standalone deployments](/docs/components/pipelines/operator-guides/installation/) of Kubeflow Pipelines, there is no authentication enforced on the `ml-pipeline-ui` service.
{{% /alert %}}

__Step 1:__ run the following command on your external system to initiate port-forwarding:

```bash
# change `--namespace` if you deployed Kubeflow Pipelines into a different namespace
kubectl port-forward --namespace kubeflow svc/ml-pipeline-ui 3000:80
```

__Step 2:__ the following code will create a `kfp.Client()` against your port-forwarded `ml-pipeline-ui` service:

```python
import kfp

client = kfp.Client(host="http://localhost:3000")

print(client.list_experiments())
```

</details>

<br>


================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md
================================================
+++
title = "Control Flow"
description = "Use control flow such as conditionals, loops, and exit handling in Kubeflow Pipelines."
weight = 103
+++

{{% kfp-v2-keywords %}}

## Overview

Although a KFP pipeline decorated with the `@dsl.pipeline` decorator looks like a normal Python function, it is actually an expression of pipeline topology and control flow semantics, constructed using the KFP domain-specific language (DSL). 

The [components guide][pipeline-basics] shows how pipeline topology is expressed by [data passing and task dependencies][data-passing]. 
This section describes how to introduce control flow in your pipelines to create more complex workflows.

The core types of control flow in KFP pipelines are:

1. [__Conditions__](#conditions)
2. [__Loops__](#loops)
3. [__Exit handling__](#exit-handling)

## Conditions

Kubeflow Pipelines supports common conditional control flow constructs. 
You can use these constructs to conditionally execute tasks based on the output of an upstream task or pipeline input parameter.

{{% alert title="Deprecated" color="warning" %}}
The `dsl.Condition` is deprecated in favor of the functionally identical `dsl.If`, which is concise, Pythonic, and consistent with the `dsl.Elif` and `dsl.Else` objects.
{{% /alert %}}

### **dsl.If** / **dsl.Elif** / **dsl.Else**

The [`dsl.If`][dsl-if] context manager enables conditional execution of tasks within its scope based on the output of an upstream task or pipeline input parameter. 

The context manager takes two arguments: a required `condition` and an optional `name`. 
The `condition` is a comparative expression where at least one of the two operands is an output from an upstream task or a pipeline input parameter.

In the following pipeline, `conditional_task` only executes if `coin_flip_task` has the output `'heads'`.

```python
from kfp import dsl

@dsl.component
def flip_coin() -> str:
    import random
    return random.choice(['heads', 'tails'])

#@dsl.component
#def my_comp():
#    print('Conditional task executed!')

@dsl.pipeline
def my_pipeline():
    coin_flip_task = flip_coin()
    with dsl.If(coin_flip_task.output == 'heads'):
        conditional_task = my_comp()
```

You may also use [`dsl.Elif`][dsl-elif] and [`dsl.Else`][dsl-else] context managers **immediately downstream** of `dsl.If` for additional conditional control flow functionality:

```python
from kfp import dsl

@dsl.component
def flip_three_sided_coin() -> str:
    import random
    return random.choice(['heads', 'tails', 'draw'])

@dsl.component
def print_comp(text: str):
    print(text)

@dsl.pipeline
def my_pipeline():
    coin_flip_task = flip_three_sided_coin()
    with dsl.If(coin_flip_task.output == 'heads'):
        print_comp(text='Got heads!')
    with dsl.Elif(coin_flip_task.output == 'tails'):
        print_comp(text='Got tails!')
    with dsl.Else():
        print_comp(text='Draw!')
```

### **dsl.OneOf**

[`dsl.OneOf`][dsl-oneof] can be used to gather outputs from mutually exclusive branches into a single task output which can be consumed by a downstream task or outputted from a pipeline.
Branches are mutually exclusive if exactly one will be executed. 
To enforce this, the KFP SDK compiler requires `dsl.OneOf` consume from tasks within a logically associated group of conditional branches and that one of the branches is a `dsl.Else` branch.

{{% oss-be-unsupported feature_name="`dsl.OneOf`" %}}

For example, the following pipeline uses `dsl.OneOf` to gather outputs from mutually exclusive branches:

```python
from kfp import dsl

@dsl.component
def flip_three_sided_coin() -> str:
    import random
    return random.choice(['heads', 'tails', 'draw'])

@dsl.component
def print_and_return(text: str) -> str:
    print(text)
    return text

@dsl.component
def announce_result(result: str):
    print(f'The result is: {result}')

@dsl.pipeline
def my_pipeline() -> str:
    coin_flip_task = flip_three_sided_coin()
    with dsl.If(coin_flip_task.output == 'heads'):
        t1 = print_and_return(text='Got heads!')
    with dsl.Elif(coin_flip_task.output == 'tails'):
        t2 = print_and_return(text='Got tails!')
    with dsl.Else():
        t3 = print_and_return(text='Draw!')
    
    oneof = dsl.OneOf(t1.output, t2.output, t3.output)
    announce_result(oneof)
    return oneof
```

You should provide task outputs to the `dsl.OneOf` using `.output` or `.outputs[<key>]`, just as you would pass an output to a downstream task. 
The outputs provided to `dsl.OneOf` must be of the same type and cannot be other instances of `dsl.OneOf` or [`dsl.Collected`][dsl-collected].

## Loops

Kubeflow Pipelines supports loops which cause fan-out and fan-in of tasks.

### **dsl.ParallelFor**

The [`dsl.ParallelFor`][dsl-parallelfor] context manager allows parallel execution of tasks over a static set of items. 

The context manager takes three arguments:

- `items`: static set of items to loop over
- `name` (optional): is the name of the loop context
- `parallelism` (optional): is the maximum number of concurrent iterations while executing the `dsl.ParallelFor` group
     - note, `parallelism=0` indicates unconstrained parallelism

{{% oss-be-unsupported feature_name="Setting `parallelism`" gh_issue_link=https://github.com/kubeflow/pipelines/issues/8718 %}}

In the following pipeline, `train_model` will train a model for 1, 5, 10, and 25 epochs, with no more than two training tasks running at one time:

```python
from kfp import dsl

#@dsl.component
#def train_model(epochs: int) -> Model:
#    ...

@dsl.pipeline
def my_pipeline():
    with dsl.ParallelFor(
        items=[1, 5, 10, 25],
        parallelism=2
    ) as epochs:
        train_model(epochs=epochs)
```

### **dsl.Collected**

Use [`dsl.Collected`][dsl-collected] with `dsl.ParallelFor` to gather outputs from a parallel loop of tasks.

{{% oss-be-unsupported feature_name="`dsl.Collected`" gh_issue_link=https://github.com/kubeflow/pipelines/issues/6161 %}}

#### **Example:** Using `dsl.Collected` as an input to a downstream task

Downstream tasks might consume `dsl.Collected` outputs via an input annotated with a `List` of parameters or a `List` of artifacts. 

For example, in the following pipeline, `max_accuracy` has the input `models` with type `Input[List[Model]]`, and will find the model with the highest accuracy from the models trained in the parallel loop:

```python
from kfp import dsl
from kfp.dsl import Model, Input

#def score_model(model: Model) -> float:
#    return ...

#@dsl.component
#def train_model(epochs: int) -> Model:
#    ...

@dsl.component
def max_accuracy(models: Input[List[Model]]) -> float:
    return max(score_model(model) for model in models)

@dsl.pipeline
def my_pipeline():
    
    # Train a model for 1, 5, 10, and 25 epochs
    with dsl.ParallelFor(
        items=[1, 5, 10, 25],
    ) as epochs:
        train_model_task = train_model(epochs=epochs)
        
    # Find the model with the highest accuracy
    max_accuracy(
        models=dsl.Collected(train_model_task.outputs['model'])
    )
```

#### **Example:** Nested lists of parameters

You can use `dsl.Collected` to collect outputs from nested loops in a *nested list* of parameters. 

For example, output parameters from two nested `dsl.ParallelFor` groups are collected in a multilevel nested list of parameters, where each nested list contains the output parameters from one of the `dsl.ParallelFor` groups. 
The number of nested levels is based on the number of nested `dsl.ParallelFor` contexts.

By comparison, *artifacts* created in nested loops are collected in a *flat* list.

#### **Example:** Returning `dsl.Collected` from a pipeline

You can also return a `dsl.Collected` from a pipeline.

Use a `List` of parameters or a `List` of artifacts in the return annotation, as shown in the following example:

```python
from typing import List

from kfp import dsl
from kfp.dsl import Model

#@dsl.component
#def train_model(epochs: int) -> Model:
#    ...

@dsl.pipeline
def my_pipeline() -> List[Model]:
    with dsl.ParallelFor(
        items=[1, 5, 10, 25],
    ) as epochs:
        train_model_task = train_model(epochs=epochs)
    return dsl.Collected(train_model_task.outputs['model'])
```

## Exit handling

Kubeflow Pipelines supports exit handlers for implementing cleanup and error handling tasks that run after the main pipeline tasks finish execution.

### **dsl.ExitHandler**

The [`dsl.ExitHandler`][dsl-exithandler] context manager allows pipeline authors to specify an exit task which will run after the tasks within the context manager's scope finish execution, even if one of those tasks fails. 

This construct is analogous to using a `try:` block followed by a `finally:` block in normal Python, where the exit task is in the `finally:` block. 
The context manager takes two arguments: a required `exit_task` and an optional `name`. `exit_task` accepts an instantiated [`PipelineTask`][dsl-pipelinetask].

#### **Example:** Basic cleanup task

The most common use case for `dsl.ExitHandler` is to run a cleanup task after the main pipeline tasks finish execution.

In the following pipeline, `clean_up_task` will execute after both `create_dataset` and `train_and_save_models` finish (regardless of whether they succeed or fail):

```python
from kfp import dsl
from kfp.dsl import Dataset

#@dsl.component
#def clean_up_resources():
#    ...

#@dsl.component
#def create_datasets():
#    ...

#@dsl.component
#def train_and_save_models(dataset: Dataset):
#    ...

@dsl.pipeline
def my_pipeline():
    clean_up_task = clean_up_resources()
    with dsl.ExitHandler(exit_task=clean_up_task):
        dataset_task = create_datasets()
        train_task = train_and_save_models(dataset=dataset_task.output)
```

### **Example:** Accessing pipeline and task status metadata

The task you use as an exit task may use a special input that provides access to pipeline and task status metadata, including pipeline failure or success status. 

You can use this special input by annotating your exit task with the [`dsl.PipelineTaskFinalStatus`][dsl-pipelinetaskfinalstatus] annotation. 
The argument for this parameter will be provided by the backend automatically at runtime. 
You should not provide any input to this annotation when you instantiate your exit task.

The following pipeline uses `dsl.PipelineTaskFinalStatus` to obtain information about the pipeline and task failure, even after `fail_op` fails:

```python
from kfp import dsl
from kfp.dsl import PipelineTaskFinalStatus

@dsl.component
def print_op(message: str):
    print(message)

@dsl.component
def exit_op(user_input: str, status: PipelineTaskFinalStatus):
    """Prints pipeline run status."""
    print(user_input)
    print('Pipeline status: ', status.state)
    print('Job resource name: ', status.pipeline_job_resource_name)
    print('Pipeline task name: ', status.pipeline_task_name)
    print('Error code: ', status.error_code)
    print('Error message: ', status.error_message)

@dsl.component
def fail_op():
    import sys
    sys.exit(1)

@dsl.pipeline
def my_pipeline():
    print_op(message='Starting pipeline...')
    print_status_task = exit_op(user_input='Task execution status:')
    with dsl.ExitHandler(exit_task=print_status_task):
        fail_op()
```

{{% oss-be-unsupported feature_name="Setting `PipelineTaskFinalStatus`" gh_issue_link=https://github.com/kubeflow/pipelines/issues/10917 %}}

#### **Example:** Ignoring upstream task failures

The [`.ignore_upstream_failure()`][ignore-upstream-failure] task method on [`PipelineTask`][dsl-pipelinetask] enables another approach to author pipelines with exit handling behavior. 
Calling this method on a task causes the task to ignore failures of any specified upstream tasks (as established by data exchange or by use of [`.after()`][dsl-pipelinetask-after]). 
If the task has no upstream tasks, this method has no effect.

In the following pipeline definition, `clean_up_task` is executed after `fail_task`, regardless of whether `fail_op` succeeds:

```python
from kfp import dsl

@dsl.component
def cleanup_op(message: str = 'Cleaning up...'):
    print(message)

@dsl.component
def fail_op(message: str):
    print(message)
    raise ValueError('Task failed!')

@dsl.pipeline()
def my_pipeline(text: str = 'message'):
    fail_task = fail_op(message=text)
    clean_up_task = cleanup_op(
        message=fail_task.output
    ).ignore_upstream_failure()
```

Note that the component used for the caller task (`cleanup_op` in the example above) requires a default value for all inputs it consumes from an upstream task. 
The default value is applied if the upstream task fails to produce the outputs that are passed to the caller task. 
Specifying default values ensures that the caller task always succeeds, regardless of the status of the upstream task.

[data-passing]: /docs/components/pipelines/user-guides/components/compose-components-into-pipelines#data-passing-and-task-dependencies
[pipeline-basics]: /docs/components/pipelines/user-guides/components/compose-components-into-pipelines
[dsl-condition]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Condition
[dsl-exithandler]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.ExitHandler
[dsl-parallelfor]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.ParallelFor
[dsl-pipelinetaskfinalstatus]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.PipelineTaskFinalStatus
[ignore-upstream-failure]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.PipelineTask.ignore_upstream_failure
[dsl-pipelinetask]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.PipelineTask
[dsl-pipelinetask-after]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.PipelineTask.after
[dsl-if]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.If
[dsl-elif]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Elif
[dsl-else]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Else
[dsl-oneof]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.OneOf
[dsl-collected]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Collected


================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/execute-kfp-pipelines-locally.md
================================================
+++
title = "Execute KFP pipelines locally"
description = "Learn how to run Kubeflow Pipelines locally."
weight = 200
+++

{{% kfp-v2-keywords %}}

## Overview

KFP supports executing components and pipelines locally, enabling a tight development loop before running your code remotely.

Executing components and pipelines locally is easy. Simply initialize a local session using `local.init()`, then call the component or pipeline like a normal Python function. 
KFP will log information about the execution. 
Once execution completes, you can access the task outputs just as you would when composing a pipeline; the only difference is that the outputs are now materialized values, not references to future outputs.

## Limitations

Local execution is designed to help quickly *test* components and pipelines locally before testing in a remote environment.

Local execution comes with several limitations:

- Local execution does not feature optimizations and additional features such as caching, retry, etc. While these feature are important for production pipelines, they are less critical for a local testing environment. You will find that task methods like `.set_retry`, `.set_caching_options`, etc. have no effect locally.
- Local execution makes simple assumptions about the resources available on your machine. Local execution does not support specifying resource requests/limits/affinities related to memory, cores, accelerators, etc. You will find that task methods like `.set_memory_limit`, `.set_memory_request`, `.set_accelerator_type` etc. have no effect locally.
- Local execution doesn't support authentication mechanisms. If your component interacts with cloud resources or requires other privileged actions, you must test your pipeline in the cloud.
- While local pipeline execution has full support for sequential and nested pipelines, it does not yet support `dsl.Condition`, `dsl.ParallelFor`, or `dsl.ExitHandler`.

## Basic Example

In the following example, we use the `DockerRunner` type, the [runner types](#runner-types) are covered in more detail below.

```python
from kfp import local
from kfp import dsl

local.init(runner=local.DockerRunner())

@dsl.component
def add(a: int, b: int) -> int:
    return a + b

# run a single component
task = add(a=1, b=2)
assert task.output == 3

# or run it in a pipeline
@dsl.pipeline
def math_pipeline(x: int, y: int, z: int) -> int:
    t1 = add(a=x, b=y)
    t2 = add(a=t1.output, b=z)
    return t2.output

pipeline_task = math_pipeline(x=1, y=2, z=3)
assert pipeline_task.output == 6
```

Similarly, you can create artifacts and read the contents:
```python
from kfp import local
from kfp import dsl
from kfp.dsl import Output, Artifact
import json

local.init(runner=local.SubprocessRunner())

@dsl.component
def add(a: int, b: int, out_artifact: Output[Artifact]):
    import json

    result = json.dumps(a + b)

    with open(out_artifact.path, 'w') as f:
        f.write(result)

    out_artifact.metadata['operation'] = 'addition'


task = add(a=1, b=2)

# can read artifact contents
with open(task.outputs['out_artifact'].path) as f:
    contents = f.read()

assert json.loads(contents) == 3
assert task.outputs['out_artifact'].metadata['operation'] == 'addition'
```

By default, KFP will raise an exception if your component exits with a failure status. You can toggle this behavior using `raise_on_error`. You can also specify a new local "pipeline root" using `pipeline_root`. This is the local directory to which component outputs, including artifacts, are written.

```python
local.init(runner=...,
           raise_on_error=False,
           pipeline_root='~/my/component/outputs')
```

## Runner Types

Kubeflow pipelines has two local runners that you can use to execute your components and pipelines locally: `DockerRunner` and `SubprocessRunner`.

__We strongly recommended using `DockerRunner` whenever possible.__

### **Runner:** DockerRunner

The `DockerRunner` requires [Docker to be installed](https://docs.docker.com/engine/install/), but requires essentially no knowledge of Docker to use.

For example, to use the `DockerRunner`:

```python
from kfp import local

local.init(runner=local.DockerRunner())
```

Since the local `DockerRunner` executes each task in a separate container, the `DockerRunner`:

- Offers the strongest form of local runtime environment isolation
- Is most faithful to the remote runtime environment
- Allows execution of all component types: [Lightweight Python Component][lightweight-python-component], [Containerized Python Components][containerized-python-components], and [Container Components][container-components]

When you use the `DockerRunner`, KFP mounts your local pipeline root to the container to write outputs outside of the container. 
This means that your component outputs will still be available for inspection even after the container exits.

### **Runner:** SubprocessRunner

The `SubprocessRunner` is only recommended where Docker cannot be installed, such as in some notebook environments.

For example, to use the `SubprocessRunner`:

```python
from kfp import local

local.init(runner=local.SubprocessRunner())
```

Since `SubprocessRunner` runs your code in a subprocess, the `SubprocessRunner`:
- Offers less local runtime environment isolation than the `DockerRunner`
- Does not support custom images or easily support tasks with complex environment dependencies
- Only allows execution of [Lightweight Python Component][lightweight-python-component]

{{% alert title="Tip" color="info" %}}
By default, the `SubprocessRunner` will install your dependencies into a virtual environment.

This is recommended, but can be disabled by setting `use_venv=False`:

```python
from kfp import local

local.init(runner=local.SubprocessRunner(use_venv=False))
```
{{% /alert %}}

[lightweight-python-component]: /docs/components/pipelines/user-guides/components/lightweight-python-components/
[containerized-python-components]: /docs/components/pipelines/user-guides/components/containerized-python-components
[container-components]: /docs/components/pipelines/user-guides/components/container-components






================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/platform-specific-features.md
================================================
+++
title = "Use Platform-Specific Features"
description = "Learn how to use platform-specific features in Kubeflow Pipelines."
weight = 105
+++

## Overview

One of the benefits of KFP is cross-platform portability. 
The KFP SDK compiles pipeline definitions to [IR YAML][ir-yaml] which can be read and executed by different backends, including the Kubeflow Pipelines [open source backend][oss-be] and [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction).

For cases where features are not portable across platforms, users may author pipelines with platform-specific functionality via KFP SDK platform-specific plugin libraries.
In general, platform-specific plugin libraries provide functions that act on tasks similarly to [task-level configuration methods][task-level-config-methods] provided by the KFP SDK directly. 

<!-- TODO: add docs on how to create a platform-specific authoring library -->

## kfp-kubernetes

Currently, the only KFP SDK platform-specific plugin library is [`kfp-kubernetes`][kfp-kubernetes-pypi], which is supported by the Kubeflow Pipelines [open source backend][oss-be] and enables direct access to some Kubernetes resources and functionality.

For more information, see the [`kfp-kubernetes` documentation ][kfp-kubernetes-docs].

### **Kubernetes PersistentVolumeClaims**

In this example we will use `kfp-kubernetes` to create a [PersistentVolumeClaim (PVC)][persistent-volume], use the PVC to pass data between tasks, and then delete the PVC.

We will assume you have basic familiarity with `PersistentVolume` and `PersistentVolumeClaim` resources in Kubernetes, in addition to [authoring components][authoring-components], and [authoring pipelines][authoring-pipelines] in KFP.

#### **Step 1:** Install the `kfp-kubernetes` library

Run the following command to install the `kfp-kubernetes` library:

```sh
pip install kfp[kubernetes]
```

#### **Step 2:** Create components that read/write to the mount path

Create two simple components that read and write to a file in the `/data` directory. 

In a later step, we will mount a PVC volume to the `/data` directory.

```python
from kfp import dsl

@dsl.component
def producer() -> str:
    with open('/data/file.txt', 'w') as file:
        file.write('Hello world')
    with open('/data/file.txt', 'r') as file:
        content = file.read()
    print(content)
    return content

@dsl.component
def consumer() -> str:
    with open('/data/file.txt', 'r') as file:
        content = file.read()
    print(content)
    return content
```

#### **Step 3:** Dynamically provision a PVC using CreatePVC

Now that we have our components, we can begin constructing a pipeline. 

We need a PVC to mount, so we will create one using the `kubernetes.CreatePVC` pre-baked component:

```python
from kfp import kubernetes

@dsl.pipeline
def my_pipeline():
    pvc1 = kubernetes.CreatePVC(
        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC
        pvc_name_suffix='-my-pvc',
        access_modes=['ReadWriteMany'],
        size='5Gi',
        storage_class_name='standard',
    )
```

This component provisions a 5GB PVC from the [StorageClass][storage-class] `'standard'` with the `ReadWriteMany` [access mode][access-mode].
The PVC will be named after the underlying Argo workflow that creates it, concatenated with the suffix `-my-pvc`. The `CreatePVC` component returns this name as the output `'name'`.

#### **Step 4:** Read and write data to the PVC

Next, we'll use the `mount_pvc` task modifier with the `producer` and `consumer` components. 

We schedule `task2` to run after `task1` so the components don't read and write to the PVC at the same time.

```python
    # write to the PVC
    task1 = producer()
    kubernetes.mount_pvc(
        task1,
        pvc_name=pvc1.outputs['name'],
        mount_path='/data',
    )

    # read to the PVC
    task2 = consumer()
    kubernetes.mount_pvc(
        task2,
        pvc_name=pvc1.outputs['name'],
        mount_path='/reused_data',
    )
    task2.after(task1)
```

#### **Step 5:** Delete the PVC

Finally, we can schedule deletion of the PVC after `task2` finishes to clean up the Kubernetes resources we created.

```python
    delete_pvc1 = kubernetes.DeletePVC(
        pvc_name=pvc1.outputs['name']
    ).after(task2)
```

For the full pipeline and more information, see a [similar example][full-example] in the [`kfp-kubernetes` documentation][kfp-kubernetes-docs].


[ir-yaml]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline#ir-yaml
[oss-be]: /docs/components/pipelines/operator-guides/installation/
[kfp-kubernetes-pypi]: https://pypi.org/project/kfp-kubernetes/
[task-level-config-methods]: /docs/components/pipelines/user-guides/components/compose-components-into-pipelines/#task-configurations
[kfp-kubernetes-docs]: https://kfp-kubernetes.readthedocs.io/
[persistent-volume]: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
[storage-class]: https://kubernetes.io/docs/concepts/storage/storage-classes/
[access-mode]: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
[full-example]: https://kfp-kubernetes.readthedocs.io/en/kfp-kubernetes-0.0.1/#persistentvolumeclaim-dynamically-create-pvc-mount-then-delete
[authoring-components]: /docs/components/pipelines/user-guides/components/
[authoring-pipelines]: /docs/components/pipelines/user-guides/



================================================
File: content/en/docs/components/pipelines/user-guides/core-functions/run-a-pipeline.md
================================================
+++
title = "Run a Pipeline"
description = "Execute a pipeline on the KFP backend"
weight = 100
+++

{{% kfp-v2-keywords %}}

## Overview

Kubeflow Pipelines (KFP) provides several ways to trigger a pipeline run:

1. [__KFP Dashboard__](#run-pipeline---kfp-dashboard)
2. [__KFP SDK Client__](#run-pipeline---kfp-sdk-client)
3. [__KFP CLI__](#run-pipeline---kfp-cli)

{{% alert title="Tip" color="info" %}}
This guide only covers how to trigger an immediate pipeline run. 
For more advanced scheduling options please see the "Recurring Runs" section of the dashboard and API.
{{% /alert %}}

## Run Pipeline - KFP Dashboard

The first and easiest way to run a pipeline is by submitting it via the KFP dashboard.

To submit a pipeline for an immediate run:

1. [Compile a pipeline][compile-a-pipeline] to IR YAML.

2. From the "Pipelines" tab in the dashboard, select `+ Upload pipeline`:

   <img src="/docs/images/pipelines/submit-a-pipeline-on-dashboard.png" 
        alt="Upload pipeline button"
        class="mt-3 mb-3 border border-info rounded">

3. Upload the pipeline `.yaml`, `.zip` or `.tar.gz` file, populate the upload form, then click `Create`.

   <img src="/docs/images/pipelines/upload-a-pipeline.png" 
        alt="Upload pipeline screen" 
        class="mt-3 mb-3 border border-info rounded">

4. From the "Runs" tab, select `+ Create run`:

   <img src="/docs/images/pipelines/create-run.png" 
        alt="Create run button"
        class="mt-3 mb-3 border border-info rounded">

5. Select the pipeline you want to run, populate the run form, then click `Start`:

   <img src="/docs/images/pipelines/start-a-run.png" 
        alt="Start a run screen"
        class="mt-3 mb-3 border border-info rounded">

## Run Pipeline - KFP SDK Client

You may also programmatically submit pipeline runs from the KFP SDK client. 
The client supports two ways of submitting runs: _from IR YAML_ or _from a Python pipeline function_. 

{{% alert title="Note" color="dark" %}}
See the [Connect the SDK to the API](/docs/components/pipelines/user-guides/core-functions/connect-api/) guide for more information about creating a KFP client.
{{% /alert %}}

For either approach, start by instantiating a [`kfp.Client()`][kfp-client]:

```python
import kfp

# TIP: you may need to authenticate with the KFP instance
kfp_client = kfp.Client()
```

To submit __IR YAML__ for execution use the [`.create_run_from_pipeline_package()`][kfp-client-create_run_from_pipeline_package] method:

```python
#from kfp import compiler, dsl
#
#@dsl.component
#def add(a: float, b: float) -> float:
#   return a + b
#
#@dsl.pipeline(name="Add two Numbers")
#def add_pipeline(a: float, b: float):
#   add_task = add(a=a, b=b)
#
#compiler.Compiler().compile(
#    add_pipeline, 
#    package_path="./add-pipeline.yaml"
#)

kfp_client.create_run_from_pipeline_package(
    "./add-pipeline.yaml", 
    arguments={
        "a": 1,
        "b": 2,
    }
)
```

To submit a python __pipeline function__ for execution use the [`.create_run_from_pipeline_func()`][kfp-client-create_run_from_pipeline_func] convenience method, which wraps compilation and run submission into one method:

```python
#from kfp import dsl
#
#@dsl.component
#def add(a: float, b: float) -> float:
#    return a + b
#
#@dsl.pipeline(name="Add two Numbers")
#def add_pipeline(a: float, b: float):
#    add_task = add(a=a, b=b)

kfp_client.create_run_from_pipeline_func(
    add_pipeline,
    arguments={
        "a": 1,
        "b": 2,
    }
)
```

## Run Pipeline - KFP CLI

The [`kfp run create`][kfp-cli-run-create] command allows you to submit a pipeline from the command line. 

Here is the output of `kfp run create --help`:

```shell
kfp run create [OPTIONS] [ARGS]...
```

For example, the following command submits the `./path/to/pipeline.yaml` IR YAML to the KFP backend:

```shell
kfp run create \
  --experiment-name "my-experiment" \
  --package-file "./path/to/pipeline.yaml"
```

For more information about the `kfp` CLI, please see:

- [Use the CLI][use-the-cli]
- [CLI Reference][kfp-cli]

[compile-a-pipeline]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline/
[use-the-cli]: /docs/components/pipelines/user-guides/core-functions/cli/
[kfp-cli]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/cli.html
[kfp-cli-run-create]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/cli.html#kfp-run-create
[kfp-client]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/client.html#kfp.client.Client
[kfp-client-create_run_from_pipeline_func]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/client.html#kfp.client.Client.create_run_from_pipeline_func
[kfp-client-create_run_from_pipeline_package]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/client.html#kfp.client.Client.create_run_from_pipeline_package


================================================
File: content/en/docs/components/pipelines/user-guides/data-handling/_index.md
================================================
+++
title = "Data Handling"
description = "Learn how to handle data in Kubeflow Pipelines."
weight = 4
+++



================================================
File: content/en/docs/components/pipelines/user-guides/data-handling/artifacts.md
================================================
+++
title = "Create, use, pass, and track ML artifacts"
weight = 3
+++

{{% kfp-v2-keywords %}}

Most machine learning pipelines aim to create one or more machine learning artifacts, such as a model, dataset, evaluation metrics, etc.

KFP provides first-class support for creating machine learning artifacts via the [`dsl.Artifact`][dsl-artifact] class and other artifact subclasses. KFP maps these artifacts to their underlying [ML Metadata][ml-metadata] schema title, the canonical name for the artifact type.

In general, artifacts and their associated annotations serve several purposes:
* To provide logical groupings of component/pipeline input/output types
* To provide a convenient mechanism for writing to object storage via the task's local filesystem
* To enable [type checking][type-checking] of pipelines that create ML artifacts
* To make the contents of some artifact types easily observable via special UI rendering

The following `training_component` demonstrates usage of both input and output artifacts using the [traditional artifact syntax][traditional-artifact-syntax]:

```python
from kfp.dsl import Input, Output, Dataset, Model

@dsl.component
def training_component(dataset: Input[Dataset], model: Output[Model]):
    """Trains an output Model on an input Dataset."""
    with open(dataset.path) as f:
        contents = f.read()

    # ... train tf_model model on contents of dataset ...

    tf_model.save(model.path)
    model.metadata['framework'] = 'tensorflow'
```

This `training_component` does the following:
1. Accepts an input dataset and declares an output model
2. Reads the input dataset's content from the local filesystem
3. Trains a model (omitted)
4. Saves the model as a component output
5. Sets some metadata about the saved model

As illustrated by `training_component`, artifacts are simply a thin wrapper around some artifact properties, including the `.path` from which the artifact can be read/written and the artifact's `.metadata`. The following sections describe these properties and other aspects of artifacts in detail.

### Artifact properties

To use create and consume artifacts from components, you'll use the available properties on [artifact instances](#artifact-types). Artifacts feature four properties:  
* `name`, the name of the artifact (cannot be overwritten on Vertex Pipelines).  
* `.uri`, the location of your artifact object. For input artifacts, this is where the object resides currently. For output artifacts, this is where you will write the artifact from within your component.  
* `.metadata`, additional key-value pairs about the artifact.  
* `.path`, a local path that corresponds to the artifact's `.uri`.

The artifact `.path` attribute is particularly helpful. When you write the contents of your artifact to the location provided by the artifact's `.path` attribute, the pipelines backend will handle copying the file at `.path` to the URI at `.uri` automatically, allowing you to create artifact files within a component by only interacting with the task's local filesystem.

As you will see more in the other examples in this section, each of these properties are accessible on artifacts inside components:

```python
from kfp import dsl
from kfp.dsl import Dataset
from kfp.dsl import Input

@dsl.component
def print_artifact_properties(dataset: Input[Dataset]):
    with open(dataset.path) as f:
        lines = f.readlines()
    
    print('Information about the artifact')
    print('Name:', dataset.name)
    print('URI:', dataset.uri)
    print('Path:', dataset.path)
    print('Metadata:', dataset.metadata)
    
    return len(lines)
```

Note that input artifacts should be treated as immutable. You should not try to modify the contents of the file at `.path` and any changes to the artifact's properties will not affect the artifact's metadata in [ML Metadata][ml-metadata].

### Artifacts in components

The KFP SDK supports two forms of artifact authoring syntax for components: traditional and Pythonic.

The **traditional artifact** authoring syntax is the original artifact authoring style provided by the KFP SDK. The traditional artifact authoring syntax is supported for both [Python Components][python-components] and [Container Components][container-components]. It is supported at runtime by the open source KFP backend and the Google Cloud Vertex Pipelines backend.

The **Pythonic artifact** authoring syntax provides an alterative artifact I/O syntax that is familiar to Python developers. The Pythonic artifact authoring syntax is supported for [Python Components][python-components] only. This syntax is not supported for [Container Components][container-components]. It is currently only supported at runtime by the Google Cloud Vertex Pipelines backend.

#### Traditional artifact syntax

When using the traditional artifact authoring syntax, all artifacts are provided to the component function as an input wrapped in an `Input` or `Output` type marker.

```python
def my_component(in_artifact: Input[Artifact], out_artifact: Output[Artifact]):
    ...
```

For _input artifacts_, you can read the artifact using its `.uri` or `.path` attribute.

For _output artifacts_, a pre-constructed output artifact will be passed into the component. You can update the output artifact's [properties](#artifact-properties) in place and write the artifact's contents to the artifact's `.path` or `.uri` attribute. You should not return the artifact instance from your component. For example:

```python
from kfp import dsl
from kfp.dsl import Dataset, Input, Model, Output

@dsl.component
def train_model(dataset: Input[Dataset], model: Output[Model]):
    with open(dataset.path) as f:
        dataset_lines = f.readlines()

    # train a model
    trained_model = ...
    
    trained_model.save(model.path)
    model.metadata['samples'] = len(dataset_lines)
```

#### **New** Pythonic artifact syntax

To use the Pythonic artifact authoring syntax, simply annotate your components with the artifact class as you would when writing normal Python.

```python
def my_component(in_artifact: Artifact) -> Artifact:
    ...
```

Inside the body of your component, you can read artifacts passed in as input (no change from the traditional artifact authoring syntax). For artifact outputs, you'll construct the artifact in your component code, then return the artifact as an output. For example:

```python
from kfp import dsl
from kfp.dsl import Dataset, Model

@dsl.component
def train_model(dataset: Dataset) -> Model:
    with open(dataset.path) as f:
        dataset_lines = f.readlines()

    # train a model
    trained_model = ...

    model_artifact = Model(uri=dsl.get_uri(), metadata={'samples': len(dataset_lines)})
    trained_model.save(model_artifact.path)
    
    return model_artifact
```

For a typical output artifact which is written to one or more files, the `dsl.get_uri` function can be used at runtime to obtain a unique object storage URI that corresponds to the current task. The optional `suffix` parameter is useful for avoiding path collisions when your component has multiple artifact outputs.

Multiple output artifacts should be specified similarly to [multiple output parameters][multiple-outputs]:

```python
from kfp import dsl
from kfp.dsl import Dataset, Model
from typing import NamedTuple

@dsl.component
def train_multiple_models(
    dataset: Dataset,
) -> NamedTuple('outputs', model1=Model, model2=Model):
    with open(dataset.path) as f:
        dataset_lines = f.readlines()

    # train a model
    trained_model1 = ...
    trained_model2 = ...
    
    model_artifact1 = Model(uri=dsl.get_uri(suffix='model1'), metadata={'samples': len(dataset_lines)})
    trained_model1.save(model_artifact1.path)
    
    model_artifact2 = Model(uri=dsl.get_uri(suffix='model2'), metadata={'samples': len(dataset_lines)})
    trained_model2.save(model_artifact2.path)
    
    outputs = NamedTuple('outputs', model1=Model, model2=Model)
    return outputs(model1=model_artifact1, model2=model_artifact2)
```

{{% oss-be-unsupported feature_name="The Pythonic artifact authoring syntax" %}}

### Artifacts in pipelines

Irrespective of whether your components use the Pythonic or traditional artifact authoring syntax, pipelines that use artifacts should be annotated with the [Pythonic artifact syntax][pythonic-artifact-syntax]:

```python
def my_pipeline(in_artifact: Artifact) -> Artifact:
    ...
```

See the following pipeline which accepts a `Dataset` as input and outputs a `Model`, surfaced from the inner component `train_model`:

```python
from kfp import dsl
from kfp.dsl import Dataset, Model

@dsl.pipeline
def augment_and_train(dataset: Dataset) -> Model:
    augment_task = augment_dataset(dataset=dataset)
    return train_model(dataset=augment_task.output).output
```

The [KFP SDK compiler][compiler] will type check artifact usage according to the rules described in [Type Checking][type-checking].

Please see [Pipeline Basics][pipelines] for comprehensive documentation on how to author a pipeline.


### Lists of artifacts

KFP supports input lists of artifacts, annotated as `List[Artifact]` or `Input[List[Artifact]]`. This is useful for collecting output artifacts from a loop of tasks using the [`dsl.ParallelFor`][dsl-parallelfor] and [`dsl.Collected`][dsl-collected] control flow objects.

Pipelines can also return an output list of artifacts by using a `-> List[Artifact]` return annotation and returning a [`dsl.Collected`][dsl-collected] instance. 

Both consuming an input list of artifacts and returning an output list of artifacts from a pipeline are described in [Pipeline Control Flow: Parallel looping][parallel-looping]. Creating output lists of artifacts from a single-step component is not currently supported.


### Artifact types

The artifact annotation indicates the type of the artifact. KFP provides several artifact types within the DSL:

| DSL object                    | Artifact schema title              |
| ----------------------------- | ---------------------------------- |
| [`Artifact`][dsl-artifact]                    | system.Artifact                    |
| [`Dataset`][dsl-dataset]                     | system.Dataset                     |
| [`Model`][dsl-model]                       | system.Model                       |
| [`Metrics`][dsl-metrics]                     | system.Metrics                     |
| [`ClassificationMetrics`][dsl-classificationmetrics]       | system.ClassificationMetrics       |
| [`SlicedClassificationMetrics`][dsl-slicedclassificationmetrics] | system.SlicedClassificationMetrics |
| [`HTML`][dsl-html]                        | system.HTML                        |****
| [`Markdown`][dsl-markdown]                    | system.Markdown                    |


`Artifact`, `Dataset`, `Model`, and `Metrics` are the most generic and commonly used artifact types. `Artifact` is the default artifact base type and should be used in cases where the artifact type does not fit neatly into another artifact category. `Artifact` is also compatible with all other artifact types. In this sense, the `Artifact` type is also an artifact "any" type.

On the [KFP open source][oss-be] UI, `ClassificationMetrics`, `SlicedClassificationMetrics`, `HTML`, and `Markdown` provide special UI rendering to make the contents of the artifact easily observable.


[ml-metadata]: https://github.com/google/ml-metadata
[compiler]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/compiler.html#kfp.compiler.Compiler
[dsl-artifact]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Artifact
[dsl-dataset]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Dataset
[dsl-model]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Model
[dsl-metrics]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Metrics
[dsl-classificationmetrics]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.ClassificationMetrics
[dsl-slicedclassificationmetrics]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.SlicedClassificationMetrics
[dsl-html]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.HTML
[dsl-markdown]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Markdown
[type-checking]: /docs/components/pipelines/user-guides/core-functions/compile-a-pipeline#type-checking
[oss-be]: /docs/components/pipelines/operator-guides/installation/
[pipelines]: /docs/components/pipelines/user-guides/components/compose-components-into-pipelines/
[container-components]: /docs/components/pipelines/user-guides/components/container-components
[python-components]: /docs/components/pipelines/user-guides/components/lightweight-python-components
[dsl-parallelfor]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.ParallelFor
[dsl-collected]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.Collected
[parallel-looping]: /docs/components/pipelines/user-guides/core-functions/control-flow/#dslparallelfor
[traditional-artifact-syntax]: /docs/components/pipelines/user-guides/data-handling/artifacts/#traditional-artifact-syntax
[multiple-outputs]: /docs/components/pipelines/user-guides/data-handling/parameters/#multiple-output-parameters
[pythonic-artifact-syntax]: /docs/components/pipelines/user-guides/data-handling/artifacts/#new-pythonic-artifact-syntax


================================================
File: content/en/docs/components/pipelines/user-guides/data-handling/data-types.md
================================================
+++
title = "Data Types"
description = "Component and pipeline I/O types"
weight = 1
+++

{{% kfp-v2-keywords %}}

KFP components and pipelines can accept inputs and create outputs. To do so, they must declare typed interfaces through their function signatures and annotations.

There are two groups of types in KFP: parameters and artifacts. Parameters are useful for passing small amounts of data between components. Artifacts types are the mechanism by which KFP provides first-class support for ML artifact outputs, such as datasets, models, metrics, etc.

So far [Hello World pipeline][hello-world] and the examples in [Components][components] have demonstrated how to use input and output parameters.

KFP automatically tracks the way parameters and artifacts are passed between components and stores the this data passing history in [ML Metadata][ml-metadata]. This enables out-of-the-box ML artifact lineage tracking and easily reproducible pipeline executions. Furthermore, KFP's strongly-typed components provide a data contract between tasks in a pipeline.

[hello-world]: /docs/components/pipelines/getting-started
[components]: /docs/components/pipelines/user-guides/components
[ml-metadata]: https://github.com/google/ml-metadata


================================================
File: content/en/docs/components/pipelines/user-guides/data-handling/parameters.md
================================================
+++
title = "Pass small amounts of data between components"
weight = 2
+++

{{% kfp-v2-keywords %}}

Parameters are useful for passing small amounts of data between components and when the data created by a component does not represent a machine learning artifact such as a model, dataset, or more complex data type.

Specify parameter inputs and outputs using built-in Python type annotations:

```python
from kfp import dsl

@dsl.component
def join_words(word: str, count: int = 10) -> str:
    return ' '.join(word for _ in range(count))
```


KFP maps Python type annotations to the types stored in [ML Metadata][ml-metadata] according to the following table:

| Python object          | KFP type |
| ---------------------- | -------- |
| `str`                  | string   |
| `int`                  | number   |
| `float`                | number   |
| `bool`                 | boolean  |
| `typing.List` / `list` | object   |
| `typing.Dict` / `dict` | object   |

As with normal Python function, input parameters can have default values, indicated in the standard way: `def func(my_string: str = 'default'):` 

Under the hood KFP passes all parameters to and from components by serializing them as JSON.

For all Python Components ([Lightweight Python Components][lightweight-python-components] and [Containerized Python Components][containerized-python-components]), parameter serialization and deserialization is invisible to the user; KFP handles this automatically.

For [Container Components][container-component], input parameter deserialization is invisible to the user; KFP passes inputs to the component automatically. For Container Component *outputs*, the user code in the Container Component must handle serializing the output parameters as described in [Container Components: Create component outputs][container-component-outputs].

### Input parameters
Using input parameters is very easy. Simply annotate your component function with the types and, optionally, defaults. This is demonstrated by the following pipeline, which uses a Python Component, a Container Component, and a pipeline with all parameter types as inputs:

<!-- TODO: document None default -->

```python
from typing import Dict, List
from kfp import dsl

@dsl.component
def python_comp(
    string: str = 'hello',
    integer: int = 1,
    floating_pt: float = 0.1,
    boolean: bool = True,
    dictionary: Dict = {'key': 'value'},
    array: List = [1, 2, 3],
):
    print(string)
    print(integer)
    print(floating_pt)
    print(boolean)
    print(dictionary)
    print(array)


@dsl.container_component
def container_comp(
    string: str = 'hello',
    integer: int = 1,
    floating_pt: float = 0.1,
    boolean: bool = True,
    dictionary: Dict = {'key': 'value'},
    array: List = [1, 2, 3],
):
    return dsl.ContainerSpec(
        image='alpine',
        command=['sh', '-c', """echo $0 $1 $2 $3 $4 $5 $6"""],
        args=[
            string,
            integer,
            floating_pt,
            boolean,
            dictionary,
            array,
        ])

@dsl.pipeline
def my_pipeline(
    string: str = 'Hey!',
    integer: int = 100,
    floating_pt: float = 0.1,
    boolean: bool = False,
    dictionary: Dict = {'key': 'value'},
    array: List = [1, 2, 3],
):
    python_comp(
        string='howdy',
        integer=integer,
        array=[4, 5, 6],
    )
    container_comp(
        string=string,
        integer=20,
        dictionary={'other key': 'other val'},
        boolean=boolean,
    )
```

### Output parameters

For Python Components and pipelines, output parameters are indicated via return annotations:

```python
from kfp import dsl

@dsl.component
def my_comp() -> int:
    return 1

@dsl.pipeline
def my_pipeline() -> int:
    task = my_comp()
    return task.output
```

For Container Components, output parameters are indicated using a [`dsl.OutputPath`][dsl-outputpath] annotation:

```python
from kfp import dsl

@dsl.container_component
def my_comp(int_path: dsl.OutputPath(int)):
    return dsl.ContainerSpec(
        image='alpine',
        command=[
            'sh', '-c', f"""mkdir -p $(dirname {int_path})\
                            && echo 1 > {int_path}"""
        ])

@dsl.pipeline
def my_pipeline() -> int:
    task = my_comp()
    return task.outputs['int_path']
```

See [Container Components: Create component outputs][container-component-outputs] for more information on how to use `dsl.OutputPath`

### Multiple output parameters
You can specify multiple named output parameters using a [`typing.NamedTuple`][typing-namedtuple]. You can access a named output using `.outputs['<output-key>']` on [`PipelineTask`][pipelinetask]:

```python
from kfp import dsl
from typing import NamedTuple

@dsl.component
def my_comp() -> NamedTuple('outputs', a=int, b=str):
    outputs = NamedTuple('outputs', a=int, b=str)
    return outputs(1, 'hello')

@dsl.pipeline
def my_pipeline() -> NamedTuple('pipeline_outputs', c=int, d=str):
    task = my_comp()
    pipeline_outputs = NamedTuple('pipeline_outputs', c=int, d=str)
    return pipeline_outputs(task.outputs['a'], task.outputs['b'])
```


[ml-metadata]: https://github.com/google/ml-metadata
[lightweight-python-components]: /docs/components/pipelines/user-guides/components/lightweight-python-components/
[containerized-python-components]: /docs/components/pipelines/user-guides/components/containerized-python-components
[container-component]: /docs/components/pipelines/user-guides/components/container-components
[container-component-outputs]: /docs/components/pipelines/user-guides/components/container-components#create-component-outputs
[pipelinetask]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.PipelineTask
[dsl-outputpath]: https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.OutputPath
[ml-metadata]: https://github.com/google/ml-metadata
[typing-namedtuple]: https://docs.python.org/3/library/typing.html#typing.NamedTuple



================================================
File: content/en/docs/components/spark-operator/OWNERS
================================================
approvers:
  - ChenYi015
  - mwielgus
  - yuchaoran2011
  - vara-bonthu
reviewers:
  - jacobsalway



================================================
File: content/en/docs/components/spark-operator/_index.md
================================================
---
title: Spark Operator
description: Documentation for Spark Operator
weight: 70
---



================================================
File: content/en/docs/components/spark-operator/developer-guide.md
================================================
---
title: Developer Guide
description: Developer guide
weight: 60
---

## Clone the Repository

Clone the Spark operator repository and change to the directory:

```bash
git clone git@github.com:kubeflow/spark-operator.git

cd spark-operator
```

## (Optional) Configure Git Pre-Commit Hooks

Git hooks are useful for identifying simple issues before submission to code review. We run hooks on every commit to automatically generate helm chart `README.md` file from `README.md.gotmpl` file. Before you can run git hooks, you need to have the pre-commit package manager installed as follows:

```shell
# Using pip
pip install pre-commit

# Using conda
conda install -c conda-forge pre-commit

# Using Homebrew
brew install pre-commit
```

To set up the pre-commit hooks, run the following command:

```shell
pre-commit install

pre-commit install-hooks
```

## Use Makefile

We use Makefile to automate common tasks. For example, to build the operator, run the `build-operator` target as follows, and `spark-operator` binary will be build and placed in the `bin` directory:

```bash
make build-operator
```

Dependencies will be automatically downloaded locally to `bin` directory as needed. For example, if you run `make manifests` target, then `controller-gen` tool will be automatically downloaded using `go install` command and then it will be renamed like `controller-gen-vX.Y.Z` and placed in the `bin` directory.

To see the full list of available targets, run the following command:

```bash
$ make help         

Usage:
  make <target>

General
  help                            Display this help.
  version                         Print version information.

Development
  manifests                       Generate CustomResourceDefinition, RBAC and WebhookConfiguration manifests.
  generate                        Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.
  update-crd                      Update CRD files in the Helm chart.
  go-clean                        Clean up caches and output.
  go-fmt                          Run go fmt against code.
  go-vet                          Run go vet against code.
  lint                            Run golangci-lint linter.
  lint-fix                        Run golangci-lint linter and perform fixes.
  unit-test                       Run unit tests.
  e2e-test                        Run the e2e tests against a Kind k8s instance that is spun up.

Build
  build-operator                  Build Spark operator.
  build-sparkctl                  Build sparkctl binary.
  install-sparkctl                Install sparkctl binary.
  clean                           Clean spark-operator and sparkctl binaries.
  build-api-docs                  Build api documentation.
  docker-build                    Build docker image with the operator.
  docker-push                     Push docker image with the operator.
  docker-buildx                   Build and push docker image for the operator for cross-platform support

Helm
  detect-crds-drift               Detect CRD drift.
  helm-unittest                   Run Helm chart unittests.
  helm-lint                       Run Helm chart lint test.
  helm-docs                       Generates markdown documentation for helm charts from requirements and values files.

Deployment
  kind-create-cluster             Create a kind cluster for integration tests.
  kind-load-image                 Load the image into the kind cluster.
  kind-delete-custer              Delete the created kind cluster.
  install-crd                     Install CRDs into the K8s cluster specified in ~/.kube/config.
  uninstall-crd                   Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
  deploy                          Deploy controller to the K8s cluster specified in ~/.kube/config.
  undeploy                        Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.

Dependencies
  kustomize                       Download kustomize locally if necessary.
  controller-gen                  Download controller-gen locally if necessary.
  kind                            Download kind locally if necessary.
  envtest                         Download setup-envtest locally if necessary.
  golangci-lint                   Download golangci-lint locally if necessary.
  gen-crd-api-reference-docs      Download gen-crd-api-reference-docs locally if necessary.
  helm                            Download helm locally if necessary.
  helm-unittest-plugin            Download helm unittest plugin locally if necessary.
  helm-docs-plugin                Download helm-docs plugin locally if necessary.
```

## Develop with Spark Operator

### Build the Binary

To build the operator, run the following command:

```shell
make build-operator
```

### Build the Docker Image

In case you want to build the operator from the source code, e.g., to test a fix or a feature you write, you can do so following the instructions below.

The easiest way to build the operator without worrying about its dependencies is to just build an image using the [Dockerfile](https://github.com/kubeflow/spark-operator/Dockerfile).

```shell
make docker-build IMAGE_TAG=<image-tag>
```

The operator image is built upon a base Spark image that defaults to `spark:3.5.2`. If you want to use your own Spark image (e.g., an image with a different version of Spark or some custom dependencies), specify the argument `SPARK_IMAGE` as the following example shows:

```shell
docker build --build-arg SPARK_IMAGE=<your Spark image> -t <image-tag> .
```

### Update the API definition

If you have updated the API definition, then you also need to update the auto-generated code. To update the auto-generated code which contains DeepCopy, DeepCopyInto, and DeepCopyObject method implementations, run the following command:

```shell
make generate
```

To update the auto-generated CustomResourceDefinition (CRD), RBAC and WebhookConfiguration manifests, run the following command:

```shell
make manifests
```

After updating the CRD files, run the following command to copy the CRD files to the helm chart directory:

```shell
make update-crd
```

Besides, the API specification documentation `docs/api-docs.md` also needs to be updated. To update the doc, run the following command:

```shell
make build-api-docs
```

### Run Unit Tests

To run unit tests, run the following command:

```shell
make unit-test
```

### Run E2E Tests

To run e2e tests, run the following command:

```shell
# Create a kind cluster
make kind-create-cluster

# Build docker image
make docker-build IMAGE_TAG=local

# Load docker image to kind cluster
make kind-load-image

# Run e2e tests
make e2e-test

# Delete the kind cluster
make kind-delete-cluster
```

## Develop with the Helm Chart

### Run Helm Chart Lint Tests

To run Helm chart lint tests, run the following command:

```shell
$ make helm-lint
Linting charts...

------------------------------------------------------------------------------------------------------------------------
 Charts to be processed:
------------------------------------------------------------------------------------------------------------------------
 spark-operator => (version: "1.2.4", path: "charts/spark-operator-chart")
------------------------------------------------------------------------------------------------------------------------

Linting chart "spark-operator => (version: \"1.2.4\", path: \"charts/spark-operator-chart\")"
Checking chart "spark-operator => (version: \"1.2.4\", path: \"charts/spark-operator-chart\")" for a version bump...
Old chart version: 1.2.1
New chart version: 1.2.4
Chart version ok.
Validating /Users/user/go/src/github.com/kubeflow/spark-operator/charts/spark-operator-chart/Chart.yaml...
Validation success! 👍
Validating maintainers...

Linting chart with values file "charts/spark-operator-chart/ci/ci-values.yaml"...

==> Linting charts/spark-operator-chart
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, 0 chart(s) failed

------------------------------------------------------------------------------------------------------------------------
 ✔︎ spark-operator => (version: "1.2.4", path: "charts/spark-operator-chart")
------------------------------------------------------------------------------------------------------------------------
All charts linted successfully
```

### Run Helm chart unit tests

For detailed information about how to write Helm chart unit tests, please refer to [helm-unittest](https://github.com/helm-unittest/helm-unittest). To run the Helm chart unit tests, run the following command:

```shell
$ make helm-unittest 

### Chart [ spark-operator ] charts/spark-operator-chart

 PASS  Test controller deployment       charts/spark-operator-chart/tests/controller/deployment_test.yaml
 PASS  Test controller pod disruption budget    charts/spark-operator-chart/tests/controller/poddisruptionbudget_test.yaml
 PASS  Test controller rbac     charts/spark-operator-chart/tests/controller/rbac_test.yaml
 PASS  Test controller deployment       charts/spark-operator-chart/tests/controller/service_test.yaml
 PASS  Test controller service account  charts/spark-operator-chart/tests/controller/serviceaccount_test.yaml
 PASS  Test prometheus pod monitor      charts/spark-operator-chart/tests/prometheus/podmonitor_test.yaml
 PASS  Test Spark RBAC  charts/spark-operator-chart/tests/spark/rbac_test.yaml
 PASS  Test spark service account       charts/spark-operator-chart/tests/spark/serviceaccount_test.yaml
 PASS  Test webhook deployment  charts/spark-operator-chart/tests/webhook/deployment_test.yaml
 PASS  Test mutating webhook configuration      charts/spark-operator-chart/tests/webhook/mutatingwebhookconfiguration_test.yaml
 PASS  Test webhook pod disruption budget       charts/spark-operator-chart/tests/webhook/poddisruptionbudget_test.yaml
 PASS  Test webhook rbac        charts/spark-operator-chart/tests/webhook/rbac_test.yaml
 PASS  Test webhook service     charts/spark-operator-chart/tests/webhook/service_test.yaml
 PASS  Test validating webhook configuration    charts/spark-operator-chart/tests/webhook/validatingwebhookconfiguration_test.yaml

Charts:      1 passed, 1 total
Test Suites: 14 passed, 14 total
Tests:       137 passed, 137 total
Snapshot:    0 passed, 0 total
Time:        477.748ms
```

### Build the Helm Docs

The Helm chart `README.md` file is generated by [helm-docs](https://github.com/norwoodj/helm-docs) tool. If you want to update the Helm docs, remember to modify `README.md.gotmpl` rather than `README.md`, then run `make helm-docs` to generate the `README.md` file:

```shell
$ make helm-docs
INFO[2024-04-14T07:29:26Z] Found Chart directories [charts/spark-operator-chart] 
INFO[2024-04-14T07:29:26Z] Generating README Documentation for chart charts/spark-operator-chart 
```

Note that if git pre-commit hooks are set up, `helm-docs` will automatically run before committing any changes. If there are any changes to the `README.md` file, the commit process will be aborted.

## Sign off your commits

After you have made changes to the code, please sign off your commits with `-s` or `--signoff` flag so that the DCO check CI will pass:

```bash
git commit -s -m "Your commit message"
```



================================================
File: content/en/docs/components/spark-operator/getting-started.md
================================================
---
title: Getting Started
description: Getting started with Spark Operator
weight: 30
---

For a more detailed guide on how to use, compose, and work with `SparkApplication`s, please refer to the
User Guide. If you are running the Kubernetes Operator for Apache Spark on Google Kubernetes Engine and want to use Google Cloud Storage (GCS) and/or BigQuery for reading/writing data, also refer to the [GCP guide](/docs/components/spark-operator/user-guide/gcp/). The Kubernetes Operator for Apache Spark will simply be referred to as the operator for the rest of this guide.

## Prerequisites

- Helm >= 3
- Kubernetes >= 1.16

## Installation

### Add Helm Repo

```shell
helm repo add spark-operator https://kubeflow.github.io/spark-operator

helm repo update
```

See [helm repo](https://helm.sh/docs/helm/helm_repo) for command documentation.

### Install the chart

```shell
helm install [RELEASE_NAME] spark-operator/spark-operator
```

For example, if you want to create a release with name `spark-operator` in the `spark-operator` namespace:

```shell
helm install spark-operator spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace
```

See [helm install](https://helm.sh/docs/helm/helm_install) for command documentation.

Installing the chart will create a namespace `spark-operator` if it doesn't exist, and helm will set up RBAC for the operator to run in the namespace. It will also set up RBAC in the `default` namespace for driver pods of your Spark applications to be able to manipulate executor pods. In addition, the chart will create a Deployment in the namespace `spark-operator`. The chart by default does not enable [Mutating Admission Webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) for Spark pod customization. When enabled, a webhook service and a secret storing the x509 certificate called `spark-webhook-certs` are created for that purpose. To install the operator with the mutating admission webhook on a Kubernetes cluster, install the chart with the flag `webhook.enable=true`:

```shell
helm install my-release spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace \
    --set webhook.enable=true
```

If you want to deploy the chart to GKE cluster, you will first need to [grant yourself cluster-admin privileges](https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control#defining_permissions_in_a_role) before you can create custom roles and role bindings on a GKE cluster versioned 1.6 and up. Run the following command before installing the chart on GKE:

```shell
kubectl create clusterrolebinding <user>-cluster-admin-binding --clusterrole=cluster-admin --user=<user>@<domain>
```

Now you should see the operator running in the cluster by checking the status of the Helm release.

```shell
helm status --namespace spark-operator my-release
```

### Upgrade the Chart

```shell
helm upgrade [RELEASE_NAME] spark-operator/spark-operator [flags]
```

See [helm upgrade](https://helm.sh/docs/helm/helm_upgrade) for command documentation.

### Uninstall the Chart

```shell
helm uninstall [RELEASE_NAME]
```

This removes all the Kubernetes resources associated with the chart and deletes the release, except for the `crds`, those will have to be removed manually.

See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall) for command documentation.

## Running the Examples

To run the Spark PI example, run the following command:

```shell
kubectl apply -f examples/spark-pi.yaml
```

Note that `spark-pi.yaml` configures the driver pod to use the `spark` service account to communicate with the Kubernetes API server. You might need to replace it with the appropriate service account before submitting the job. If you installed the operator using the Helm chart and overrode `spark.jobNamespaces`, the service account name ends with `-spark` and starts with the Helm release name. For example, if you would like to run your Spark jobs to run in a namespace called `test-ns`, first make sure it already exists, and then install the chart with the command:

```shell
helm install my-release spark-operator/spark-operator --namespace spark-operator --set "spark.jobNamespaces={test-ns}"
```

Then the chart will set up a service account for your Spark jobs to use in that namespace.

See the section on the [Spark Job Namespace](#about-spark-job-namespaces) for details on the behavior of the default Spark Job Namespace.

Running the above command will create a `SparkApplication` object named `spark-pi`. Check the object by running the following command:

```shell
kubectl get sparkapplication spark-pi -o=yaml
```

This will show something similar to the following:

```yaml
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  ...
spec:
  deps: {}
  driver:
    coreLimit: 1200m
    cores: 1
    labels:
      version: 2.3.0
    memory: 512m
    serviceAccount: spark
  executor:
    cores: 1
    instances: 1
    labels:
      version: 2.3.0
    memory: 512m
  image: gcr.io/ynli-k8s/spark:v3.1.1
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar
  mainClass: org.apache.spark.examples.SparkPi
  mode: cluster
  restartPolicy:
      type: OnFailure
      onFailureRetries: 3
      onFailureRetryInterval: 10
      onSubmissionFailureRetries: 5
      onSubmissionFailureRetryInterval: 20
  type: Scala
status:
  sparkApplicationId: spark-5f4ba921c85ff3f1cb04bef324f9154c9
  applicationState:
    state: COMPLETED
  completionTime: 2018-02-20T23:33:55Z
  driverInfo:
    podName: spark-pi-83ba921c85ff3f1cb04bef324f9154c9-driver
    webUIAddress: 35.192.234.248:31064
    webUIPort: 31064
    webUIServiceName: spark-pi-2402118027-ui-svc
    webUIIngressName: spark-pi-ui-ingress
    webUIIngressAddress: spark-pi.ingress.cluster.com
  executorState:
    spark-pi-83ba921c85ff3f1cb04bef324f9154c9-exec-1: COMPLETED
  LastSubmissionAttemptTime: 2018-02-20T23:32:27Z
```

To check events for the `SparkApplication` object, run the following command:

```shell
kubectl describe sparkapplication spark-pi
```

This will show the events similarly to the following:

```text
Events:
  Type    Reason                      Age   From            Message
  ----    ------                      ----  ----            -------
  Normal  SparkApplicationAdded       5m    spark-operator  SparkApplication spark-pi was added, enqueued it for submission
  Normal  SparkApplicationTerminated  4m    spark-operator  SparkApplication spark-pi terminated with state: COMPLETED
```

The operator submits the Spark Pi example to run once it receives an event indicating the `SparkApplication` object was added.

## Configuration

The operator is typically deployed and run using the Helm chart. However, users can still run it outside a Kubernetes cluster and make it talk to the Kubernetes API server of a cluster by specifying path to `kubeconfig`, which can be done using the `-kubeconfig` flag.

The operator uses multiple workers in the `SparkApplication` controller. The number of worker threads are controlled using command-line flag `-controller-threads` which has a default value of 10.

The operator enables cache resynchronization so periodically the informers used by the operator will re-list existing objects it manages and re-trigger resource events. The resynchronization interval in seconds can be configured using the flag `-resync-interval`, with a default value of 30 seconds.

By default, the operator will install the [CustomResourceDefinitions](https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/) for the custom resources it manages. This can be disabled by setting the flag `-install-crds=false`, in which case the CustomResourceDefinitions can be installed manually using `kubectl apply -f manifest/spark-operator-crds.yaml`.

The mutating admission webhook is an **optional** component and can be enabled or disabled using the `-enable-webhook` flag, which defaults to `false`.

By default, the operator will manage custom resource objects of the managed CRD types for the whole cluster. It can be configured to manage only the custom resource objects in a specific namespace with the flag `-namespace=<namespace>`

## Upgrade

To upgrade the operator, e.g., to use a newer version container image with a new tag, run the following command with updated parameters for the Helm release:

```shell
helm upgrade <YOUR-HELM-RELEASE-NAME> --set image.repository=org/image --set image.tag=newTag
```

Refer to the Helm [documentation](https://helm.sh/docs/helm/helm_upgrade/) for more details on `helm upgrade`.

## About Spark Job Namespaces

The Spark Job Namespaces value defines the namespaces where `SparkApplications` can be deployed. The Helm chart value for the Spark Job Namespaces is `spark.jobNamespaces`, and its default value is `[]`. When the list of namespaces is empty the Helm chart will create a service account in the namespace where the spark-operator is deployed.

If you installed the operator using the Helm chart and overrode the `spark.jobNamespaces` to some other, pre-existing namespace, the Helm chart will create the necessary service account and RBAC in the specified namespace.

The Spark Operator uses the Spark Job Namespace to identify and filter relevant events for the `SparkApplication` CRD. If you specify a namespace for Spark Jobs, and then submit a SparkApplication resource to another namespace, the Spark Operator will filter out the event, and the resource will not get deployed. If you don't specify a namespace, the Spark Operator will see only `SparkApplication` events for the Spark Operator namespace.

## About the Service Account for Driver Pods

A Spark driver pod need a Kubernetes service account in the pod's namespace that has permissions to create, get, list, and delete executor pods, and create a Kubernetes headless service for the driver. The driver will fail and exit without the service account, unless the default service account in the pod's namespace has the needed permissions. To submit and run a `SparkApplication` in a namespace, please make sure there is a service account with the permissions in the namespace and set `.spec.driver.serviceAccount` to the name of the service account. Please refer to [spark-rbac.yaml](https://github.com/kubeflow/spark-operator/blob/master/config/rbac/spark-application-rbac.yaml) for an example RBAC setup that creates a driver service account named `spark-operator-spark` in the `default` namespace, with a RBAC role binding giving the service account the needed permissions.

## About the Service Account for Executor Pods

A Spark executor pod may be configured with a Kubernetes service account in the pod namespace. To submit and run a `SparkApplication` in a namespace, please make sure there is a service account with the permissions required in the namespace and set `.spec.executor.serviceAccount` to the name of the service account.

## Enable Metric Exporting to Prometheus

The operator exposes a set of metrics via the metric endpoint to be scraped by `Prometheus`. The Helm chart by default installs the operator with the additional flag to enable metrics (`-enable-metrics=true`) as well as other annotations used by Prometheus to scrape the metric endpoint. If `podMonitor.enable` is enabled, the helm chart will submit a pod monitor for the operator's pod. To install the operator  **without** metrics enabled, pass the appropriate flag during `helm install`:

```shell
helm install my-release spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace \
    --set metrics.enable=false
```

If enabled, the operator generates the following metrics:

### Spark Application Metrics

| Metric | Description |
| ------------- | ------------- |
| `spark_application_count`  | Total number of SparkApplication handled by the Operator.|
| `spark_application_submit_count`  | Total number of SparkApplication spark-submitted by the Operator.|
| `spark_application_success_count` | Total number of SparkApplication which completed successfully.|
| `spark_application_failure_count` | Total number of SparkApplication which failed to complete. |
| `spark_application_running_count` | Total number of SparkApplication which are currently running.|
| `spark_application_success_execution_time_seconds` | Execution time for applications which succeeded.|
| `spark_application_failure_execution_time_seconds` | Execution time for applications which failed. |
| `spark_application_start_latency_seconds` | Start latency of SparkApplication as type of [Prometheus Summary](https://prometheus.io/docs/concepts/metric_types/#summary). |
| `spark_application_start_latency_seconds` | Start latency of SparkApplication as type of [Prometheus Histogram](https://prometheus.io/docs/concepts/metric_types/#histogram). |
| `spark_executor_success_count` | Total number of Spark Executors which completed successfully. |
| `spark_executor_failure_count` | Total number of Spark Executors which failed. |
| `spark_executor_running_count` | Total number of Spark Executors which are currently running. |

#### Work Queue Metrics

| Metric | Description |
| ------------- | ------------- |
| `workqueue_depth` | Current depth of workqueue |
| `workqueue_adds_total` | Total number of adds handled by workqueue |
| `workqueue_queue_duration_seconds_bucket` | How long in seconds an item stays in workqueue before being requested |
| `workqueue_work_duration_seconds_bucket` | How long in seconds processing an item from workqueue takes |
| `workqueue_retries_total` | Total number of retries handled by workqueue |
| `workqueue_unfinished_work_seconds` | Unfinished work in seconds |
| `workqueue_longest_running_processor_seconds` | Longest running processor in seconds |

The following is a list of all the configurations the operators supports for metrics:

```shell
-enable-metrics=true
-metrics-port=10254
-metrics-endpoint=/metrics
-metrics-prefix=myServiceName
-metrics-label=label1Key
-metrics-label=label2Key
```

All configs except `-enable-metrics` are optional. If port and/or endpoint are specified, please ensure that the annotations `prometheus.io/port`,  `prometheus.io/path` and `containerPort` in `spark-operator-with-metrics.yaml` are updated as well.

A note about `metrics-labels`: In `Prometheus`, every unique combination of key-value label pairs represents a new time series, which can dramatically increase the amount of data stored. Hence, labels should not be used to store dimensions with high cardinality with potentially a large or unbounded value range.

Additionally, these metrics are best-effort for the current operator run and will be reset on an operator restart. Also, some of these metrics are generated by listening to pod state updates for the driver/executors and deleting the pods outside the operator might lead to incorrect metric values for some of these metrics.

## Driver UI Access and Ingress

The operator, by default, makes the Spark UI accessible by creating a service of type `ClusterIP` which exposes the UI. This is only accessible from within the cluster.

The operator also supports creating an optional Ingress for the UI. This can be turned on by setting the `ingress-url-format` command-line flag. The `ingress-url-format` should be a template like `{{$appName}}.{ingress_suffix}/{{$appNamespace}}/{{$appName}}`. The `{ingress_suffix}` should be replaced by the user to indicate the cluster's ingress url and the operator will replace the `{{$appName}}` & `{{$appNamespace}}` with the appropriate value. Please note that Ingress support requires that cluster's ingress url routing is correctly set-up. For e.g. if the `ingress-url-format` is `{{$appName}}.ingress.cluster.com`, it requires that anything `*ingress.cluster.com` should be routed to the ingress-controller on the K8s cluster.

The operator also sets both `WebUIAddress` which is accessible from within the cluster as well as `WebUIIngressAddress` as part of the `DriverInfo` field of the `SparkApplication`.

The operator generates ingress resources intended for use with the [Ingress NGINX Controller](https://kubernetes.github.io/ingress-nginx/). Include this in your application spec for the controller to ensure it recognizes the ingress and provides appropriate routes to your Spark UI.

```yaml
spec:
  sparkUIOptions:
    ingressAnnotations:
        kubernetes.io/ingress.class: nginx
```

## About the Mutating Admission Webhook

The Kubernetes Operator for Apache Spark comes with an optional mutating admission webhook for customizing Spark driver and executor pods based on the specification in `SparkApplication` objects, e.g., mounting user-specified ConfigMaps and volumes, and setting pod affinity/anti-affinity, and adding tolerations.

The webhook requires a X509 certificate for TLS for pod admission requests and responses between the Kubernetes API server and the webhook server running inside the operator. For that, the certificate and key files must be accessible by the webhook server. The location of these certs is configurable and they will be reloaded on a configurable period.
The Kubernetes Operator for Spark ships with a tool at `hack/gencerts.sh` for generating the CA and server certificate and putting the certificate and key files into a secret named `spark-webhook-certs` in the namespace `spark-operator`. This secret will be mounted into the operator pod.

Run the following command to create the secret with a certificate and key files using a batch Job, and install the operator Deployment with the mutating admission webhook:

```shell
kubectl apply -f manifest/spark-operator-with-webhook.yaml
```

This will create a Deployment named `sparkoperator` and a Service named `spark-webhook` for the webhook in namespace `spark-operator`.

### Mutating Admission Webhooks on a private GKE or EKS cluster

If you are deploying the operator on a GKE cluster with the [Private cluster](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters) setting enabled, or on an enterprise AWS EKS cluster and you wish to deploy the cluster with the [Mutating Admission Webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/), then make sure to change the `webhookPort` to `443`. Alternatively you can choose to allow connections to the default port (8080).

> By default, firewall rules restrict your cluster master to only initiate TCP connections to your nodes on ports 443 (HTTPS) and 10250 (kubelet). For some Kubernetes features, you might need to add firewall rules to allow access on additional ports. For example, in Kubernetes 1.9 and older, kubectl top accesses heapster, which needs a firewall rule to allow TCP connections on port 8080. To grant such access, you can add firewall rules.
For GCP, refer to [this link](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules)

To install the operator with a custom port, pass the appropriate flag during `helm install`:

```shell
helm install my-release spark-operator/spark-operator \
   --namespace spark-operator  \
   --create-namespace \
   --set "spark.jobNamespaces={spark}" \
   --set webhook.enable=true \
   --set webhook.port=443
```


================================================
File: content/en/docs/components/spark-operator/overview/_index.md
================================================
---
title: Overview
description: An overview for Spark Operator
weight: 20
---

## What is Kubeflow Spark Operator?

The Kubernetes Operator for Apache Spark aims to make specifying and running Spark applications as easy and idiomatic as running other workloads on Kubernetes. It uses Kubernetes custom resources for specifying, running, and surfacing status of Spark applications.

## Introduction

In Spark 2.3, Kubernetes becomes an official scheduler backend for Spark, additionally to the standalone scheduler, Mesos, and Yarn. Compared with the alternative approach of deploying a standalone Spark cluster on top of Kubernetes and submit applications to run on the standalone cluster, having Kubernetes as a native scheduler backend offers some important benefits as discussed in [SPARK-18278](https://issues.apache.org/jira/browse/SPARK-18278) and is a huge leap forward. However, the way life cycle of Spark applications are managed, e.g., how applications get submitted to run on Kubernetes and how application status is tracked, are vastly different from that of other types of workloads on Kubernetes, e.g., Deployments, DaemonSets, and StatefulSets. The Kubernetes Operator for Apache Spark reduces the gap and allow Spark applications to be specified, run, and monitored idiomatically on Kubernetes.

Specifically, the Kubernetes Operator for Apache Spark follows the recent trend of leveraging the [operator](https://coreos.com/blog/introducing-operators.html) pattern for managing the life cycle of Spark applications on a Kubernetes cluster. The operator allows Spark applications to be specified in a declarative manner (e.g., in a YAML file) and run without the need to deal with the spark submission process. It also enables status of Spark applications to be tracked and presented idiomatically like other types of workloads on Kubernetes. This document discusses the design and architecture of the operator. For documentation of the [CustomResourceDefinition](https://kubernetes.io/docs/concepts/api-extension/custom-resources/) for specification of Spark applications, please refer to [API Definition](https://github.com/kubeflow/spark-operator/blob/master/docs/api-docs.md).

The Kubernetes Operator for Apache Spark currently supports the following list of features:

- Supports Spark 2.3 and up.
- Enables declarative application specification and management of applications through custom resources.
- Automatically runs `spark-submit` on behalf of users for each `SparkApplication` eligible for submission.
- Provides native cron support for running scheduled applications.
- Supports customization of Spark pods beyond what Spark natively is able to do through the mutating admission webhook, e.g., mounting ConfigMaps and volumes, and setting pod affinity/anti-affinity.
- Supports automatic application re-submission for updated `SparkApplication` objects with updated specification.
- Supports automatic application restart with a configurable restart policy.
- Supports automatic retries of failed submissions with optional linear back-off.
- Supports mounting local Hadoop configuration as a Kubernetes ConfigMap automatically via `sparkctl`.
- Supports automatically staging local application dependencies to Google Cloud Storage (GCS) via `sparkctl`.
- Supports collecting and exporting application-level metrics and driver/executor metrics to Prometheus.

## Architecture

The operator consists of:

- a `SparkApplication` controller that watches events of creation, updates, and deletion of
`SparkApplication` objects and acts on the watch events,
- a *submission runner* that runs `spark-submit` for submissions received from the controller,
- a *Spark pod monitor* that watches for Spark pods and sends pod status updates to the controller,
- a [Mutating Admission Webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) that handles customizations for Spark driver and executor pods based on the annotations on the pods added by the controller,
- and also a command-line tool named `sparkctl` for working with the operator.

The following diagram shows how different components interact and work together.

![Architecture Diagram](architecture-diagram.png)

Specifically, a user uses the `sparkctl` (or `kubectl`) to create a `SparkApplication` object. The `SparkApplication` controller receives the object through a watcher from the API server, creates a submission carrying the `spark-submit` arguments, and sends the submission to the *submission runner*. The submission runner submits the application to run and creates the driver pod of the application. Upon starting, the driver pod creates the executor pods. While the application is running, the *Spark pod monitor* watches the pods of the application and sends status updates of the pods back to the controller, which then updates the status of the application accordingly.

## The CRD Controller

The `SparkApplication` controller, or CRD controller in short, watches events of creation, updates, and deletion of `SparkApplication` objects in any namespaces in a Kubernetes cluster, and acts on the watch events. When a new `SparkApplication` object is added (i.e., when the `AddFunc` callback function of the `ResourceEventHandlerFuncs` is called), it enqueues the object into an internal work queue, from which a worker picks it up prepares a submission and sends the submission to the submission runner, which actually submits the application to run in the Kubernetes cluster. The submission includes the list of arguments for the `spark-submit` command. The submission runner has a configurable number of workers for submitting applications to run in the cluster. When a `SparkApplication` object is deleted, the object is dequeued from the internal work queue and all the Kubernetes resources associated with the application get deleted or garbage collected.

When a `SparkApplication` object gets updated (i.e., when the `UpdateFunc` callback function of the `ResourceEventHandlerFuncs` is called), e.g., from the user who used `kubectl apply` to apply the update. The controller checks if the application specification in `SparkApplicationSpec` has changed. If the application specification remains the same, the controller simply ignores the update. This ensures that updates without application specification changes, e.g., those triggered by cache re-synchronization, won't result in application a re-submission. If otherwise the update was made to the application specification, the controller cancels the current run of the application by deleting the driver pod of the current run, and submits a new run of the application with the updated specification. Note that deleting the driver pod of the old run of the application effectively kills the run and causes the executor pods to be deleted as well because the driver is the owner of the executor pods.

The controller is also responsible for updating the status of a `SparkApplication` object with the help of the Spark pod monitor, which watches Spark pods and update the `SparkApplicationStatus` field of corresponding `SparkApplication` objects based on the status of the pods. The Spark pod monitor watches events of creation, updates, and deletion of Spark pods, creates status update messages based on the status of the pods, and sends the messages to the controller to process. When the controller receives a status update message, it gets the corresponding `SparkApplication` object from the cache store and updates the the `Status` accordingly.

As described in [API Definition](https://github.com/kubeflow/spark-operator/blob/master/docs/api-docs.md), the `Status` field (of type `SparkApplicationStatus`) records the overall state of the application as well as the state of each executor pod. Note that the overall state of an application is determined by the driver pod state, except when submission fails, in which case no driver pod gets launched. Particularly, the final application state is set to the termination state of the driver pod when applicable, i.e., `COMPLETED` if the driver pod completed or `FAILED` if the driver pod failed. If the driver pod gets deleted while running, the final application state is set to `FAILED`. If submission fails, the application state is set to `FAILED_SUBMISSION`.  There are two terminal states: `COMPLETED` and `FAILED` which means that any Application in these states will never be retried by the Operator. All other states are non-terminal and based on the State as well as RestartPolicy (discussed below) can be retried.

As part of preparing a submission for a newly created `SparkApplication` object, the controller parses the object and adds configuration options for adding certain annotations to the driver and executor pods of the application. The annotations are later used by the mutating admission webhook to configure the pods before they start to run. For example,if a Spark application needs a certain Kubernetes ConfigMap to be mounted into the driver and executor pods, the controller adds an annotation that specifies the name of the ConfigMap to mount. Later the mutating admission webhook sees the annotation on the pods and mount the ConfigMap to the pods.

## Handling Application Restart And Failures

The operator provides a configurable option through the `RestartPolicy` field of `SparkApplicationSpec` (see the [Configuring Automatic Application Restart and Failure Handling](/docs/components/spark-operator/user-guide/working-with-sparkapplication#configuring-automatic-application-restart-and-failure-handling) for more details) for specifying the application restart policy. The operator determines if an application should be restarted based on its termination state and the restart policy. As discussed above, the termination state of an application is based on the termination state of the driver pod. So effectively the decision is based on the termination state of the driver pod and the restart policy. Specifically, one of the following conditions applies:

- If the restart policy type is `Never`, the application is not restarted upon terminating.
- If the restart policy type is `Always`, the application gets restarted regardless of the termination state of the application. Please note that such an Application will never end up in a terminal state of `COMPLETED` or `FAILED`.
- If the restart policy type  is `OnFailure`, the application gets restarted if and only if the application failed and the retry limit is not reached. Note that in case the driver pod gets deleted while running, the application is considered being failed as discussed above. In this case, the application gets restarted if the restart policy is `OnFailure`.

When the operator decides to restart an application, it cleans up the Kubernetes resources associated with the previous terminated run of the application and enqueues the `SparkApplication` object of the application into the internal work queue, from which it gets picked up by a worker who will handle the submission. Note that instead of restarting the driver pod, the operator simply re-submits the application and lets the submission client create a new driver pod.

## Mutating Admission Webhook

The operator comes with an optional mutating admission webhook for customizing Spark driver and executor pods based on certain annotations on the pods added by the CRD controller. The annotations are set by the operator based on the application specifications. All Spark pod customization needs except for those natively support by Spark on Kubernetes are handled by the mutating admission webhook.

## Command-line Tool: Sparkctl

[sparkctl](https://github.com/kubeflow/spark-operator/blob/master/cmd/sparkctl/README.md) is a command-line tool for working with the operator. It supports creating a `SparkApplication`object from a YAML file, listing existing `SparkApplication` objects, checking status of a `SparkApplication`, forwarding from a local port to the remote port on which the Spark driver runs, and deleting a `SparkApplication` object. For more details on `sparkctl`, please refer to [README](https://github.com/kubeflow/spark-operator/blob/master/cmd/sparkctl/README.md).



================================================
File: content/en/docs/components/spark-operator/performance/_index.md
================================================
---
title: Performance
description: Documentation for Spark Operator Benchmarking
weight: 70
---



================================================
File: content/en/docs/components/spark-operator/performance/benchmarking.md
================================================
---
title: Kubeflow Spark Operator Benchmarks
description: Benchmarking the Kubeflow Spark Operator
weight: 10
---

## Overview

As organizations increasingly adopt [Kubernetes](https://kubernetes.io/) for large-scale data processing, efficiently executing **[Apache Spark](https://spark.apache.org/)** workloads on Kubernetes has become a critical need. The **[Kubeflow Spark Operator](https://github.com/kubeflow/spark-operator)** streamlines Spark job submission and management within Kubernetes, yet its capacity to handle high-concurrency workloads involving thousands of job submissions and tens of thousands of pods requires thorough validation. To assess the operator’s current performance, we developed benchmark scripts and conducted tests using Kubeflow Spark Operator. These tests are repeatable, enabling users to leverage this benchmarking toolkit to evaluate future releases as performance optimizations are introduced.

Benchmarking is essential to determine how effectively the Spark Operator performs under heavy load, particularly with thousands of concurrent job submissions and constrained resource allocations. Key objectives include:

* Identifying bottlenecks in the controller’s processing of SparkApplication CRs.
* Optimizing resource allocation for the Spark Operator controller pod.
* Assessing whether a single operator instance is sufficient or if multiple instances are required for enhanced performance.
* Pinpointing bottlenecks in Kubernetes job scheduling, API responsiveness, and resource management.

This guide offers a detailed framework for benchmarking the Kubeflow Spark Operator, adaptable to any Kubernetes cluster. While the configurations and results presented here are based on [Amazon EKS](https://aws.amazon.com/eks/), the methodology is flexible and can be applied to other cloud providers or on-premises Kubernetes environments.

{{< note >}}
These benchmarks focus on **evaluating the Spark Operator’s performance**, not the Spark jobs themselves. To isolate the operator’s ability to manage high job submission rates and pod launches, minimal resources are allocated to driver and executor pods. This approach reduces the number of nodes needed, allowing a clear assessment of the operator’s efficiency in scheduling and launching jobs under high concurrency.
{{< /note >}}


## How the Spark Operator Works
The Kubeflow Spark Operator extends Kubernetes by introducing the `SparkApplication` custom resource definition (CRD), which defines Spark job configurations such as driver and executor pod specifications, dependencies, and runtime arguments. A controller, deployed as a pod and managed via Helm, oversees these CRDs through a reconciliation loop that includes submitting jobs by translating SparkApplication objects into Kubernetes pods via the API, managing their lifecycle by tracking states (e.g., **Pending**, **Running**, **Completed**, **Failed**) and updating statuses, scheduling resources in coordination with the Kubernetes scheduler (optionally using webhooks for validation), and cleaning up by terminating completed or failed jobs unless explicitly disabled. 

Built on the **controller-runtime** library, the operator processes submissions through a single work queue per instance.  Each controller instance operates independently with its own queue, and performance hinges on CPU availability, memory for JVM processes spawned per submission, and Kubernetes API responsiveness. 

## Available Tuning Parameters in Helm Values
Spark Operator [Helm chart](https://github.com/kubeflow/spark-operator/tree/master/charts/spark-operator-chart) provides several parameters to tune the Spark Operator’s performance:

* **controller.workers**: Sets the number of reconciler threads (`default: 10`). Increasing this (e.g., to 20 or 30) enhances concurrency, potentially boosting throughput if CPU resources allow, though API latency may limit gains.
* **workqueueRateLimiter**:
    * **bucketQPS** (default: `50`): This limits the average rate (items per second) at which the queue processes jobs. Increasing it might seem like it would speed things up, but if the operator’s controller pod is already maxing out its CPU or waiting on the Kubernetes API server, the queue won’t process faster.
    * **bucketSize** (default: `500`): This sets the maximum number of jobs that can be queued. Raising it allows more jobs to pile up during bursts, but if the processing rate doesn’t increase, those extra jobs just wait longer.
    * **maxDelay.enable** and **maxDelay.duration** (default: `true`, 6h): Manages overflow behavior. Reducing duration (e.g., to 1h) may expedite requeuing but risks event loss if the queue remains full.
* **webhook.enable**: Toggles webhook usage (default: `true`). Disabling this (`false`) reduces latency (~60s per job) but requires leveraging Spark Pod Templates feature.
* **batchScheduler.enable**: Enables a batch scheduler like **Volcano** or **Yunikorn** (default: `false`). Setting to true optimizes pod scheduling for large-scale workloads.
* **Controller resources**: Defines CPU/memory requests and limits for the controller pod (default: `unset`). Allocating more resources (e.g., 64 vCPUs, 20Gi) supports higher concurrency.

## Infrastructure Setup

### 1. Cluster Configuration

* **Kubernetes Cluster**: Tested on **Amazon EKS 1.31** (eks.19), adaptable to any **Kubernetes** environment.
* **Networking**:
    * Two Subnets: Deployed to `100.64.128.0/171` and `100.64.0.0/17`  subnets (32766 IP addresses each) spread across two availability zones in a VPC with two CIDRs attached. Spark Jobs targeted to run one a single AZ.
* **Node Configuration**:
    * **Dedicated Node Group for Spark Pods**: Optimized for pod bin-packing efficiency to minimize costs, using `200` `m6a.4xlarge nodes` (16 vCPUs, 64 GB RAM each). Default pod capacity is **110** per node, increased to **220** via kubelet settings (`maxPods`) to handle more Spark pods, each consuming one IP address.
    * **Dedicated Node Group for Spark Operator**: Ensures that the Spark Operator runs in isolation on a compute intensive c5.9xlarge instance, preventing interference from other workloads. This approach follows best practices for system-critical components, reducing the risk of eviction or resource contention.
    * **Spark Operator Controller and Webhook Pod Resources**: Requested `33 vCPUs` and `50 Gi` memory for the controller pod, `1vCPU` and `10Gi` memory for webook pod, tailored for benchmarks tests.
    * **Dedicated Node Group for Prometheus**: Used to capture and monitor metrics with the Kube Prometheus Stack, which includes **Prometheus** and **Grafana** for visualization. At scale, Prometheus can consume significant CPU and memory, so running it on dedicated node ensures it doesn’t compete with Spark Pods. It’s common to dedicate a node or node pool solely to monitoring components (Prometheus, Grafana, etc.) using node selectors or taints.
    
{{% alert title="Warning" color="warning" %}}
**Provisioning 200 nodes can be costly (e.g., ~$0.6912 per node on EKS). Users should assess expenses before replication.**
{{% /alert %}}

### 2. Workload Configuration
* **Software Versions**:
    * Spark Operator Version: Latest stable release (e.g., `v2.1.0`).
    * Spark Version: `Apache Spark 3.5.3` (compatible with the operator).
* **Job Type**: Spark job that calculates Pi and includes a sleep interval of 1 hour to simulate a fixed runtime.
* **Concurrency**: Jobs are submitted concurrently to simulate high-concurrency workloads. Key details include:
    * **Total Applications**: `6000` Spark applications.
    * **Applications per Virtual User**: `2000` applications per user, with three virtual users submitting jobs using **[Locust](https://locust.io/)** load test tool.
    * **Submission Rate**: Jobs are submitted at a rate of `1000` applications per minute to the Kubernetes cluster.
    * **Submission Window**: All `6000` jobs are submitted within a 6-minute period, leaving the Spark Operator to process and manage the workload over time.
    * **Total Pods**: Each Spark job consists of `1` driver pod and `5` executor pods, totaling 6 pods, resulting in `36000` pods across `6000` applications, supported by a cluster provisioned with `200` nodes to ensure sufficient resources for execution.

### 3. Metrics to Measure

To effectively evaluate the performance of the Kubeflow Spark Operator under high-concurrency workloads, we have developed a new custom Grafana dashboard ([Spark-Operator Scale Test Dashboard](https://grafana.com/grafana/dashboards/23032-spark-operator-scale-test-dashboard/)) as part of this benchmark. This dashboard is open-source and freely available, allowing you to monitor both Spark Operator and Kubernetes metrics in your own environment. Below, we outline the key metrics tracked by the dashboard, categorized into three main areas: **Spark Operator Pod Metrics**, **Spark Application Metrics**, and **Kubernetes Metrics**. These metrics provide a comprehensive view of resource usage, job processing efficiency, and cluster health.

#### Spark Operator Pod Metrics

* **Max CPU Usage**: Tracks the highest CPU consumption of the controller pod, helping you determine if it becomes CPU-bound under heavy load.
* **Max Memory Usage**: Monitors peak memory usage to ensure the pod has adequate memory for job processing.
* **Network Transmit Bytes**: Tracks the volume of data sent by the controller pod, useful for spotting network bottlenecks.
* **Network Receive Bytes**: Monitors incoming data to assess if network latency affects performance.
* **Work Queue Depth**: Measures the number of pending SparkApplication submissions in the queue, indicating whether the operator can keep pace with incoming jobs. e.g., `workqueue_depth{container="spark-operator-controller"}`
* **Work Queue Total Adds**: Counts the total number of jobs added to the work queue over time, reflecting the workload’s intensity.
* **Work Queue Duration (Secs)**: Measures how long jobs wait in the queue before processing, highlighting potential delays.
* **Work Duration (Secs)**: Tracks the time taken to process each job, showing the efficiency of the operator’s reconciliation loop.
* **Work Queue Total Retries**: Counts job retries, indicating issues with job submission or pod creation.
* **Longest Running Controller Thread (Secs)**: Measures the maximum duration of a single reconciler thread, helping detect slow or stuck operations.
* **Unfinished Work**: Tracks jobs that remain in progress, ensuring all submissions are completed eventually.

#### Spark Application Metrics

* **Start Latency Percentiles (Secs)**: Measures the time from job submission to execution start, with percentiles (e.g., `p50`, `p90`, `p99`) to show latency distribution.
* **Spark Job Count**: Tracks the total number of submitted, running, and completed Spark jobs, offering a snapshot of workload progress.
* **Job Submission Rate (Jobs per min)**: Monitors the rate of job submissions, ensuring it matches the intended concurrency level.
* **Job Success Rate**: Calculates the percentage of jobs that complete successfully, helping identify failures or errors.
* **Job Failure Trend**: Tracks job failures over time, allowing you to spot periods of instability or resource contention.

#### Kubernetes Metrics

* **API Server Latency (p90)**: Measures the 90th percentile latency for API server requests, indicating potential delays in API responsiveness.
* **API Server Request Duration (Secs)**: Tracks the total duration of API requests, helping pinpoint slow operations.
* **Admission Webhook Admission Duration (Secs)**: Monitors the time taken by admission webhooks, which can slow pod creation if enabled.
* **Etcd Latency (p99)**: Measures the 99th percentile latency for etcd operations, crucial for understanding database performance under load.
* **API Server Total DB Size**: Tracks the size of the etcd database, ensuring it remains manageable and doesn’t degrade performance.
* **Spark Application Object Count**: Counts the number of SparkApplication custom resources (CRs) in the cluster, reflecting workload scale.
* **Node Object Count**: Monitors the number of nodes, verifying that the cluster scales appropriately for the workload.
* **Pod Object Count**: Tracks the total number of pods, ensuring the cluster can support the expected volume (e.g., up to 36000 pods).

## How to run Benchmark test

### Test Setup

**Benchmarking Tool**: Locust is used to simulate concurrent job submissions and measure performance metrics. The Locust script dynamically generates and submits SparkApplication YAML files to the Kubernetes cluster.

* **Dynamic Job Naming**: Each job is assigned a unique name using `uuid.uuid4()`.
* **Rate Limiting**: Jobs are submitted at a controlled rate (e.g., 1 job per sec).
* **Job Status Monitoring**: The script monitors the status of submitted jobs to ensure they are running as expected.
* **Cleanup**: Jobs are deleted after the test unless the  `--no-delete-jobs` flag is set.

**Locust Script Configuration**: The following parameters are available in the Locust script.

* **Users (-u)**: Number of concurrent virtual users for Locust script
* **Jobs Per Minute (--jobs-per-min)**: Controls the rate of job submissions.
* **Jobs Limit (--job-limit-per-user)**: Maximum number of jobs to submit for each user.
* **Namespaces (--spark-namespaces)**: Jobs submitted across specified namespaces.
* **Spark Job Template (--spark_job_template)**: Path to the SparkApplication YAML template.
* **Delete Jobs (--no-delete-jobs)**: If set, jobs are not deleted after the test (useful for debugging).

The benchmarking process uses Locust, which creates user processes that execute tasks based on configurations in the `locustfile.py` file. This allows us to generate and submit SparkApplication CRDs at a controlled rate, enabling us to measure performance at scale.

#### Installation

​The benchmarking test kit and scripts are available at the [Data on EKS](https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/spark-k8s-operator/examples/benchmark/spark-operator-benchmark-kit) GitHub repository.

To begin, install Locust and its dependencies in a virtual environment:

```sh
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

#### Running Locust

For this benchmark, we’ve pre-created three Spark job namespaces `spark-team-a`, `spark-team-b`, and `spark-team-c` to distribute the workload evenly across them. These namespaces are configured in the Spark Operator’s Helm values under `controller.spark.jobNamespaces`, ensuring the operator watches only these namespaces for job submissions. The following command runs Locust in headless mode, submitting `6000` jobs at a rate of `1000` jobs per minute across these namespaces:

```sh
locust --headless --only-summary -u 3 -r 1 \
--job-limit 2000 \
--jobs-per-min 1000 \
--spark-namespaces spark-team-a,spark-team-b,spark-team-c
```

## Benchmark results
To assess the performance of the Kubeflow Spark Operator under high-concurrency workloads, we conducted two key tests: one with webhooks enabled and another with webhooks disabled. These tests involved submitting `6000` Spark applications each with 1 driver and 5 executor pods, totaling `36000` pods at a rate of 1000 jobs per minute across three namespaces. Below, we detail the test configurations, observations, and key findings, presented in a clear and structured manner to highlight the operator’s behavior and limitations.

### Test1: 6000 Spark Applications (With Webhook Enabled)

#### Test Configuration:

* **Spark Operator Controller Pod**:
    * **Instance Type**: `c5.9xlarge`
    * **Resources**: `36` vCPUs, `72` GB memory.
    * **Deployment**: Helm chart deployed with default values.
    * **Controller Threads**: `10` threads.
    * The operator was deployed with default helm configuration except for `metrics-job-start-latency-buckets` because the default maximum bucket size of 500 was too small to meaningfully measure job start time. Checkout the [PR](https://github.com/kubeflow/spark-operator/pull/2450)
* **Spark Application Pods**:
    * **Number of Nodes**: `200`
    * **Instance Type**: `m6a.4xlarge`
    * **Resources**: `16` vCPUs, `64` GB memory per node.
    * **Driver and Executors**: 1 Driver and 5 executor pods per application.
    * **Job Behavior**: Executors primarily sleep to simulate minimal resource usage.
* **Workload**:
    * **Job Type**: Simple Spark jobs with a fixed runtime (sleeping for most of the duration).
    * **Total Jobs**: 6000 jobs across three namespaces (`spark-team-a`, `spark-team-b`, `spark-team-c`) at a rate of `1000 job` submissions per minute:
    * This test is limited by CPU constraints and ensures consistent load across multiple namespaces to simulate real-world usage scenarios.

#### Observations from Spark Operator Scale Test

* **CPU Utilization**:
  * The Spark Operator controller pod is **CPU-bound**, utilizing all `36 cores` during peak processing. 
  * CPU constraints limit job processing speed, making compute power a key factor for scalability.
  * Spark Operator's throughput indicated by the `spark_application_submit_count metric`, is soley influenced by CPU single core speed, available number of cores, and configured number goroutines(`20`).
  * Increasing goroutines from `10` to `20` on a `36 core` machine improved the job submission rate from `~130-140` to `~140-155` jobs per minute (`~7-11%` gain), but showed diminishing returns likely due to Spark Operator queue processing bottlenecks.

![Spark Job Submission rate](/docs/components/spark-operator/performance/spark-job-submission-rate.png)

  - The number of applications submitted to the cluster does not affect the processing speed. There was no difference between `2000` apps and `6000` apps.
  - Memory footprint does not change between the number of apps submitted. However, with it does increase with the number of goroutine.
  - Memory consumption remains stable, regardless of the number of applications processed.
  - This indicates that memory is not a bottleneck, and increasing RAM would not improve performance.
  
* **Job Processing Rate**:
    * The Spark Operator processes applications at `~130` apps per minute.
    * The processing rate is capped by CPU limitations, preventing further scaling without additional compute resources.
    * Because the operator can only process `130` - `150` applications per minute, later Spark jobs took `~1` hour for their driver pods to spawn.

![Job Strat Latency Rate](/docs/components/spark-operator/performance/job-start-latency.jpg)

* **Time to Process Jobs**:
    * `~15` minutes to process `2000` applications.
    * `~30` minutes to process `4000` applications.
    * These numbers align with the observed 130 apps per minute processing rate.
    * Because Spark operator was busy processing new applications, updates to status of submitted spark application were slow. Some applications took `~30` minutes for their status to be updated.
* **Work Queue Duration Metric Reliability**:
    * The default work queue duration metric becomes unreliable once it exceeds `16` minutes. 
        * The default configuration for the `metrics-job-start-latency-buckets` option for the spark operator has a max bucket of `300s`, we increased the buckets for our testing but the metric did not record latency beyond 300s. 
    * Under high concurrency, this metric fails to provide accurate insights into queue processing times.
* **API Server Performance Impact**:
    * Kubernetes API request duration increases significantly under high workload conditions.
    * Consistent significant delay in `LIST` calls. This is not caused by the Operator, it's caused by Spark listing pods in its own namespace to find executor pods.
    * This is caused by Spark querying executor pods frequently, not a limitation of the Spark Operator itself.
    * The increased API server load affects job submission latency and monitoring performance across the cluster.

![P90 API Server Latency](/docs/components/spark-operator/performance/p90-apiserver-latency.jpg)

Note that setting `spark.kubernetes.executor.enablePollingWithResourceVersion: "true"` in SparkApplication config greatly alleviates this issue.

![P90 API Server Latency with Version](/docs/components/spark-operator/performance/p90-apiserver-latency1.jpg)

However, this means the API server could return any version of pods especially in HA configurations. This could lead to an inconsistent state that Spark cannot recover from.

* `6000 Spark Jobs`. Spread across three namespaces with 2000 applications in each namespace.
* Controller workers set to `10`
* `130 apps` processed per minute.
* `~25 cores` in use.
* Significantly increased API latency.

![alt text](/docs/components/spark-operator/performance/spark-operator-pod-metrics.jpg) 

![alt text](/docs/components/spark-operator/performance/kubernetes-metrics.jpg) 

![alt text](/docs/components/spark-operator/performance/spark-application-metrics.jpg)

#### Test2:  6000 Spark Applications with 20 Controller Workers
- 6000 Spark Jobs. Spread across three namespaces with 2000 applications in each namespace.

**Changes:**
- Increased available configured number of goroutine to `20` from `10` (default).

**Observation:**
- `140` - `150` apps processed per minute.
- `35` cores in use. 
- Otherwise results are similar to the default configuration results.

![alt text](/docs/components/spark-operator/performance/spark-operator-pod-metrics1.jpg)

![alt text](/docs/components/spark-operator/performance/kubernetes-metrics1.jpg) 

![alt text](/docs/components/spark-operator/performance/spark-application-metrics1.jpg)

#### Test3: 6000 Spark Applications with 20 Controller Workers and resource version set to 0.

**Changes:**
* Increased available configured number of goroutine to `20` from `10` (default).
* Set `spark.kubernetes.executor.enablePollingWithResourceVersion: "true"` in SparkApplication config.
* `6000` Spark Jobs. Spread across three namespaces with `2000` applications in each namespace.

**Observation:**
* Increase in API latency is virtually gone as expected.
* `140` - `150` apps processed per minute.
* `35 cores` in use. 

![alt text](/docs/components/spark-operator/performance/spark-operator-pod-metrics3.jpg) 

![alt text](/docs/components/spark-operator/performance/kubernetes-metrics3.jpg) 

![alt text](/docs/components/spark-operator/performance/spark-application-metrics3.jpg)

#### Test4: 6000 Spark Applications (With Webhook Disabled)

**Test Configuration:**
* Identical to Test 1, except webhooks were disabled to evaluate their impact on performance.

**Observation:**
* Disabling webhooks reduced job start latency by approximately 60 seconds per job, enhancing overall throughput.

## Key Takeaways

### CPU Constraints in the Spark Operator
* The Spark Operator’s controller pod is heavily CPU-bound. Increasing the number of controller workers (goroutines) from `10` to `20` improved the processing rate from `~130` to `~150` applications per minute, but CPU usage peaked, indicating that CPU resources are the limiting factor.
* Memory usage remained stable, suggesting that memory is not a bottleneck for the operator.

### Kubernetes API Server Latency
* High API server latency, peaking at ~600 ms under heavy load (e.g., `36000` pod creations), significantly impacted job submission and status updates. This latency was primarily caused by Spark’s frequent queries for executor pods, not by the operator itself.
* Enabling `spark.kubernetes.executor.enablePollingWithResourceVersion: "true"` reduced API latency but introduced the risk of inconsistent pod states in high-availability (HA) setups, potentially leading to job failures.

### Webhook Overhead
* Enabling webhooks added `~60` seconds of latency per job, significantly reducing throughput. Disabling webhooks improved performance by leveraging Pod Templates but sacrificed validation and mutation features.

### Namespace Management Issues
* Running 6000 SparkApplications in a single namespace caused pod failures with the error: `exec /opt/entrypoint.sh: argument list too long`
* This occurred because each SparkApplication creates a service object, and the accumulation of services overwhelmed environment variables storing host and port details. Service objects persist after job completion unless SparkApplication objects are manually removed. See this article for more information.

### Processing Delays
* The operator processed `~130–150` applications per minute, leading to significant delays for large workloads (e.g., `~1` hour for driver pods to spawn in a `6000` job submission). Status updates for some jobs lagged by up to `30 minutes` as the operator prioritized new submissions over updates.

## Recommendations

To optimize the Kubeflow Spark Operator for high-concurrency workloads, consider the following strategies:

### Deploy Multiple Spark Operator Instances

* **Why**: A single operator instance struggles with large workloads due to CPU and processing limits. Distributing the load across multiple instances leverages parallelism to reduce processing time.
* **How**:
    * Deploy a separate Spark Operator instance per namespace (e.g., `spark-team-a`, `spark-team-b`, `spark-team-c`) using Helm.
    * Configure each instance with `spark.jobNamespaces` set to its respective namespaces.
* **Benefit**: With three instances, each handling `2000 jobs` from a `6000` job submission, processing could complete in `~15` minutes (assuming even distribution), compared to `~1` hour with a single instance.
* **Considerations**:
    * Assign each instance a unique service account with namespace-scoped RBAC rules for isolation.
    * The Helm chart defaults to cluster-wide permissions, so custom configurations may be required.

### Disable Webhooks

* **Why**: Webhooks add `~60` seconds of latency per job, severely impacting throughput under high concurrency.
* **How**: Set `webhook.enable` to false in the Helm chart.
* **Benefit**: Eliminating this overhead accelerates job starts, as demonstrated in benchmark tests.
* **Considerations**:
    * Use `Pod Templates` for volume configuration, node selectors, and taints to replace webhook functionality.

### Increase the Number of Workers

* **Why**: More controller workers (goroutines) allow the operator to process jobs concurrently, boosting throughput if CPU resources are available.
* **How**: Increase controller.workers from `10` to `20` or `30` via the Helm chart. Choose compute intensive nodes for running Spark Operator (e.g., `c5.9xlarge` used in this test)
* **Benefit**: Tests showed a processing rate increase from `~130` to `~150` applications per minute with `20` workers. With `36 vCPUs` available, each worker could use `~1.8 vCPUs` at `20` workers, potentially scaling further.
* **Considerations**:
    * Monitor CPU usage (e.g., via Prometheus with `prometheus.metrics.enable`) to avoid saturation.
    * Effectiveness may be limited if API latency, not CPU, is the primary bottleneck.

### Enable a Batch Scheduler

* **Why**: These benchmarks didn’t cover the usage of Batch schedulers but `Volcano` or `Yunikorn` custom batch schedulers optimize resource allocation, reducing pod creation times for large-scale jobs.
* **How**:
    * Set `batchScheduler.enable` to true in the Helm chart.
    * Specify a scheduler (e.g., `Volcano` or `Yunikorn`) and install it in the cluster
* **Benefit**: Enhanced scheduling efficiency improves performance under high concurrency.

### Optimize Kubernetes Cluster Performance

* **Why**: High API server latency under load (e.g., `36000` pod creations across `200 nodes`) hampers job processing. Existing node capacity (`m6a.4xlarge`, `16 vCPUs`, `64GB` RAM, up to `220` pods per node) is sufficient, but API server autoscaling and scheduler efficiency requires attention.
* **How**:
    * Scale API server replicas or allocate more resources (API server scaling is automatically handled by many cloud providers, including Amazon EKS).

### Manage Namespace Usage

* **Why**: Excessive SparkApplications in one namespace (e.g., `6000`) cause pod failures due to environment variable overflow from persistent service objects.
* **How**:
    * Distribute Spark jobs across multiple namespaces (e.g., `spark-team-a`, `spark-team-b`, `spark-team-c`).
    * Set a `timeToLiveSeconds` duration for SparkApplication objects or deploy a custom garbage collection (GC) service to remove completed applications and their services.
* **Benefit**: Prevents failures and ensures reliable operation under high job volumes.
* **Considerations**: Service objects aren’t automatically deleted upon job completion, so proactive cleanup is essential.

### Monitor and Tune

* **Why**: Continuous monitoring identifies bottlenecks and validates optimizations.
* **How**: Use the provided Grafana dashboard (enabled via Helm) to track metrics like work queue depth, CPU usage, API latency, and processing rates.
* **Benefit**: Data-driven tuning ensures the operator scales efficiently for large workloads.
* **Considerations**: Focus on CPU saturation and API responsiveness to guide adjustments.

### Future work
We will continue running benchmarks to evaluate the effectiveness of these enhancements. Our goal is to improve the Spark Operator's performance by exploring the following community-suggested strategies:

- **Go-based Spark Submit:** Transitioning from the current Java-based `spark-submit` to a Go-based implementation to reduce JVM startup latency and improve job submission efficiency.

- **Controller Sharding:** Enhancing the Spark Operator's controller to enable parallel processing and improve scalability.​

### Acknowledgments

We extend our gratitude to [AWS](https://aws.amazon.com) for providing the infrastructure necessary for these benchmarks. We also thank Spark Operator community contributors *[Manabu McCloskey](https://github.com/nabuskey)*, *[Vara Bonthu](https://github.com/vara-bonthu)*, *[Alan Halcyon](https://github.com/alanty)* and *[Ratnopam Chakrabarti](https://github.com/ratnopamc)* for their efforts in executing these benchmarks.


================================================
File: content/en/docs/components/spark-operator/reference/_index.md
================================================
---
title: Reference
description: Reference documentation for Spark Operator
weight: 50
---



================================================
File: content/en/docs/components/spark-operator/reference/api-docs.md
================================================
---
title: sparkoperator.k8s.io/v1beta2
description: Spark Operator `v1beta2` API documentation
manualLink: https://github.com/kubeflow/spark-operator/blob/master/docs/api-docs.md
manualLinkTarget: _blank
weight: 50
---



================================================
File: content/en/docs/components/spark-operator/user-guide/_index.md
================================================
---
title: User Guide
description: User guides for Spark Operator
weight: 40
---



================================================
File: content/en/docs/components/spark-operator/user-guide/customizing-spark-operator.md
================================================
---
title: Customizing Spark Operator
description: Customizing Spark Operator
weight: 80
---

To customize the operator, you can follow the steps below:

1. Compile Spark distribution with Kubernetes support as per [Spark documentation](https://spark.apache.org/docs/latest/building-spark.html#building-with-kubernetes-support).
2. Create docker images to be used for Spark with [docker-image tool](https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images).
3. Create a new operator image based on the above image. You need to modify the `FROM` tag in the [Dockerfile](https://github.com/kubeflow/spark-operator/blob/master/Dockerfile) with your Spark image.

4. Build and push multi-arch operator image to your own image registry by running the following command ([docker buildx](https://github.com/docker/buildx) is needed):

    ```bash
    make docker-build IMAGE_REGISTRY=docker.io IMAGE_REPOSITORY=kubeflow/spark-operator IMAGE_TAG=latest PLATFORMS=linux/amd64,linux/arm64
    ```

5. Deploy the Spark operator Helm chart by specifying your own operator image:

    ```bash
    helm repo add --force-update spark-operator https://kubeflow.github.io/spark-operator

    helm install spark-operator spark-operator/spark-operator \
        --namespace spark-operator \
        --create-namespace \
        --set image.registry=docker.io \
        --set image.repository=kubeflow/spark-operator \
        --set image.tag=latest
    ```



================================================
File: content/en/docs/components/spark-operator/user-guide/gcp.md
================================================
---
title: Integration with Google Cloud Storage and BigQuery
description: Integration with Google Cloud Storage and BigQuery
weight: 90
---

This document describes how to use Google Cloud services, e.g., Google Cloud Storage (GCS) and BigQuery as data sources
or sinks in `SparkApplication`s. For a detailed tutorial on building Spark applications that access GCS and BigQuery,
please refer to [Using Spark on Kubernetes Engine to Process Data in BigQuery](https://cloud.google.com/solutions/spark-on-kubernetes-engine).

A Spark application requires the [GCS](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage) and
[BigQuery](https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery) connectors to access GCS and BigQuery
using the Hadoop `FileSystem` API. One way to make the connectors available to the driver and executors is to use a
custom Spark image with the connectors built-in, as this example [Dockerfile](https://github.com/GoogleCloudPlatform/spark-on-k8s-gcp-examples/blob/master/dockerfiles/spark-gcs/Dockerfile) shows.
An image built from this Dockerfile is located at `gcr.io/ynli-k8s/spark:v2.3.0-gcs`.

The connectors require certain Hadoop properties to be set properly to function. Setting Hadoop properties can be done
both through a custom Hadoop configuration file, namely, `core-site.xml` in a custom image, or via the `spec.hadoopConf`
section in a `SparkApplication`. The example Dockerfile mentioned above shows the use of a custom `core-site.xml` and a
custom `spark-env.sh` that points the environment variable `HADOOP_CONF_DIR` to the directory in the container where
`core-site.xml` is located. The example `core-site.xml` and `spark-env.sh` can be found
[here](https://github.com/GoogleCloudPlatform/spark-on-k8s-gcp-examples/tree/master/conf).

The GCS and BigQuery connectors need to authenticate with the GCS and BigQuery services before they can use the services.
The connectors support using a [GCP service account JSON key file](https://cloud.google.com/iam/docs/creating-managing-service-account-keys)
for authentication. The service account must have the necessary IAM roles for access GCS and/or BigQuery granted. The
[tutorial](https://cloud.google.com/solutions/spark-on-kubernetes-engine) has detailed information on how to create a
service account, grant it the right roles, furnish a key, and download a JSON key file. To tell the connectors to use
a service JSON key file for authentication, the following Hadoop configuration properties
must be set:

```properties
google.cloud.auth.service.account.enable=true
google.cloud.auth.service.account.json.keyfile=<path to the service account JSON key file in the container>
```

The most common way of getting the service account JSON key file into the driver and executor containers is mount the key
file in through a Kubernetes secret volume. Detailed information on how to create a secret can be found in the
[tutorial](https://cloud.google.com/solutions/spark-on-kubernetes-engine).

Below is an example `SparkApplication` using the custom image at `gcr.io/ynli-k8s/spark:v2.3.0-gcs` with the GCS/BigQuery
connectors and the custom Hadoop configuration files above built-in. Note that some of the necessary Hadoop configuration
properties are set using `spec.hadoopConf`. Those Hadoop configuration properties are additional to the ones set in the
built-in `core-site.xml`. They are set here instead of in `core-site.xml` because of their application-specific nature.
The ones set in `core-site.xml` apply to all applications using the image. Also note how the Kubernetes secret named
`gcs-bg` that stores the service account JSON key file gets mounted into both the driver and executors. The environment
variable `GCS_PROJECT_ID` must be set when using the image at `gcr.io/ynli-k8s/spark:v2.3.0-gcs`.

```yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: foo-gcs-bg
spec:
  type: Java
  mode: cluster
  image: gcr.io/ynli-k8s/spark:v2.3.0-gcs
  imagePullPolicy: Always
  hadoopConf:
    "fs.gs.project.id": "foo"
    "fs.gs.system.bucket": "foo-bucket"
    "google.cloud.auth.service.account.enable": "true"
    "google.cloud.auth.service.account.json.keyfile": "/mnt/secrets/key.json"
  driver:
    cores: 1
    secrets:
    - name: "gcs-bq"
      path: "/mnt/secrets"
      secretType: GCPServiceAccount
    envVars:
      GCS_PROJECT_ID: foo
    serviceAccount: spark
  executor:
    instances: 2
    cores: 1
    memory: "512m"
    secrets:
    - name: "gcs-bq"
      path: "/mnt/secrets"
      secretType: GCPServiceAccount
    envVars:
      GCS_PROJECT_ID: foo
```



================================================
File: content/en/docs/components/spark-operator/user-guide/leader-election.md
================================================
---
title: Enabling Leader Election
description: Enabling Leader Election for High Availability
weight: 50
---

The operator supports a high-availability (HA) mode, in which there can be more than one replicas of the operator, with only one of the replicas (the leader replica) actively operating. If the leader replica fails, the leader election process is engaged again to determine a new leader from the replicas available. The HA mode can be enabled through an optional leader election process. Leader election is disabled by default but can be enabled via a command-line flag. The following table summarizes the command-line flags relevant to leader election:

| Flag | Default Value | Description |
| ------------- | ------------- | ------------- |
| `leader-election` | `false` | Whether to enable leader election (or the HA mode) or not. |
| `leader-election-lock-namespace` | `spark-operator` | Kubernetes namespace of the lock resource used for leader election. |
| `leader-election-lock-name` | `spark-operator-lock` | Name of the lock resource used for leader election. |
| `leader-election-lease-duration` | 15 seconds | Leader election lease duration. |
| `leader-election-renew-deadline` | 14 seconds | Leader election renew deadline. |
| `leader-election-retry-period` | 4 seconds | Leader election retry period. |



================================================
File: content/en/docs/components/spark-operator/user-guide/resource-quota-enforcement.md
================================================
---
title: Enabling Resource Quota Enforcement
description: Enabling Resource Quota Enforcement
weight: 60
---

The Spark Operator provides limited support for resource quota enforcement using a validating webhook. It will count the resources of non-terminal-phase SparkApplications and Pods, and determine whether a requested SparkApplication will fit given the remaining resources. ResourceQuota scope selectors are not supported, any ResourceQuota object that does not match the entire namespace will be ignored. Like the native Pod quota enforcement, current usage is updated asynchronously, so some overscheduling is possible.

If you are running Spark applications in namespaces that are subject to resource quota constraints, consider enabling this feature to avoid driver resource starvation. Quota enforcement can be enabled with the command line arguments `-enable-resource-quota-enforcement=true`. It is recommended to also set `-webhook-fail-on-error=true`.



================================================
File: content/en/docs/components/spark-operator/user-guide/running-multiple-instances-of-the-operator.md
================================================
---
title: Running Multiple Instances of the Spark Operator
description: |
    Running Multiple Instances of the Spark Operator within the Same K8s Cluster
weight: 70
---

If you need to run multiple instances of the Spark operator within the same k8s cluster, then you need to ensure that the running instances should not watch the same spark job namespace.
For example, you can deploy two Spark operator instances in the `spark-operator` namespace, one with release name `spark-operator-1` which watches the `spark-1` namespace:

```bash
# Create the spark-1 namespace if it does not exist
kubectl create ns spark-1

# Install the Spark operator with release name spark-operator-1
helm install spark-operator-1 spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace \
    --set 'spark.jobNamespaces={spark-1}'
```

And then deploy another one with release name `spark-operator-2` which watches the `spark-2` namespace:

```bash
# Create the spark-2 namespace if it does not exist
kubectl create ns spark-2

# Install the Spark operator with release name spark-operator-2
helm install spark-operator-2 spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace \
    --set 'spark.jobNamespaces={spark-2}'
```



================================================
File: content/en/docs/components/spark-operator/user-guide/running-sparkapplication-on-schedule.md
================================================
---
title: "Running Spark Applications on a Schedule"
description: "Running Spark Applications on a Schedule"
weight: 40
---

The operator supports running a Spark application on a standard [cron](https://en.wikipedia.org/wiki/Cron) schedule using objects of the `ScheduledSparkApplication` custom resource type. A `ScheduledSparkApplication` object specifies a cron schedule on which the application should run and a `SparkApplication` template from which a `SparkApplication` object for each run of the application is created. The following is an example `ScheduledSparkApplication`:

```yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: ScheduledSparkApplication
metadata:
  name: spark-pi-scheduled
  namespace: default
spec:
  schedule: "@every 5m"
  concurrencyPolicy: Allow
  successfulRunHistoryLimit: 1
  failedRunHistoryLimit: 3
  template:
    type: Scala
    mode: cluster
    image: gcr.io/spark/spark:v3.1.1
    mainClass: org.apache.spark.examples.SparkPi
    mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar
    driver:
      cores: 1
      memory: 512m
    executor:
      cores: 1
      instances: 1
      memory: 512m
    restartPolicy:
      type: Never
```

The concurrency of runs of an application is controlled by `.spec.concurrencyPolicy`, whose valid values are `Allow`, `Forbid`, and `Replace`, with `Allow` being the default. The meanings of each value is described below:

* `Allow`: more than one run of an application are allowed if for example the next run of the application is due even though the previous run has not completed yet.
* `Forbid`: no more than one run of an application is allowed. The next run of the application can only start if the previous run has completed.
* `Replace`: no more than one run of an application is allowed. When the next run of the application is due, the previous run is killed and the next run starts as a replacement.

A scheduled `ScheduledSparkApplication` can be temporarily suspended (no future scheduled runs of the application will be triggered) by setting `.spec.suspend` to `true`. The schedule can be resumed by removing `.spec.suspend` or setting it to `false`. A `ScheduledSparkApplication` can have names of `SparkApplication` objects for the past runs of the application tracked in the `Status` section as discussed below. The numbers of past successful runs and past failed runs to keep track of are controlled by field `.spec.successfulRunHistoryLimit` and field `.spec.failedRunHistoryLimit`, respectively. The example above allows 1 past successful run and 3 past failed runs to be tracked.

The `Status` section of a `ScheduledSparkApplication` object shows the time of the last run and the proposed time of the next run of the application, through `.status.lastRun` and `.status.nextRun`, respectively. The names of the `SparkApplication` object for the most recent run (which may  or may not be running) of the application are stored in `.status.lastRunName`. The names of `SparkApplication` objects of the past successful runs of the application are stored in `.status.pastSuccessfulRunNames`. Similarly, the names of `SparkApplication` objects of the past failed runs of the application are stored in `.status.pastFailedRunNames`.

Note that certain restart policies (specified in `.spec.template.restartPolicy`) may not work well with the specified schedule and concurrency policy of a `ScheduledSparkApplication`. For example, a restart policy of `Always` should never be used with a `ScheduledSparkApplication`. In most cases, a restart policy of `OnFailure` may not be a good choice as the next run usually picks up where the previous run left anyway. For these reasons, it's often the right choice to use a restart policy of `Never` as the example above shows.



================================================
File: content/en/docs/components/spark-operator/user-guide/using-sparkapplication.md
================================================
---
title: "Using SparkApplications"
description: "Using SparkApplications"
weight: 10
---

The operator runs Spark applications specified in Kubernetes objects of the `SparkApplication` custom resource type. The most common way of using a `SparkApplication` is store the `SparkApplication` specification in a YAML file and use the `kubectl` command or alternatively the `sparkctl` command to work with the `SparkApplication`. The operator automatically submits the application as configured in a `SparkApplication` to run on the Kubernetes cluster and uses the `SparkApplication` to collect and surface the status of the driver and executors to the user.

As with all other Kubernetes API objects, a `SparkApplication` needs the `apiVersion`, `kind`, and `metadata` fields. For general information about working with manifests, see [object management using kubectl](https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/).

A `SparkApplication` also needs a [`.spec` section](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status). This section contains fields for specifying various aspects of an application including its type (`Scala`, `Java`, `Python`, or `R`), deployment mode (`cluster` or `client`), main application resource URI (e.g., the URI of the application jar), main class, arguments, etc. Node selectors are also supported via the optional field `.spec.nodeSelector`.

It also has fields for specifying the unified container image (to use for both the driver and executors) and the image pull policy, namely, `.spec.image` and `.spec.imagePullPolicy` respectively. If a custom init-container (in both the driver and executor pods) image needs to be used, the optional field `.spec.initContainerImage` can be used to specify it. If set, `.spec.initContainerImage` overrides `.spec.image` for the init-container image. Otherwise, the image specified by `.spec.image` will be used for the init-container. It is invalid if both `.spec.image` and `.spec.initContainerImage` are not set.

Below is an example showing part of a `SparkApplication` specification:

```yaml
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: spark:3.5.1
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar
```



================================================
File: content/en/docs/components/spark-operator/user-guide/volcano-integration.md
================================================
---
title: Integration with Volcano for Batch Scheduling
description: Integration with Volcano for Batch Scheduling
weight: 90
---

[Volcano](https://github.com/volcano-sh/volcano) is a batch system built on Kubernetes. It provides a suite of mechanisms
currently missing from Kubernetes that are commonly required by many classes
of batch & elastic workloads.
With the integration with Volcano, Spark application pods can be scheduled for better scheduling efficiency.

## Volcano components

Before using Kubernetes Operator for Apache Spark, with Volcano enabled, user need to ensure Volcano has been successfully installed in the
same environment, please refer [Quick Start Guide](https://github.com/volcano-sh/volcano#quick-start-guide) for Volcano installation.

## Install Kubernetes Operator for Apache Spark with Volcano enabled

Within the help of Helm chart, Kubernetes Operator for Apache Spark with Volcano can be easily installed with the command below:

```shell
helm repo add spark-operator https://kubeflow.github.io/spark-operator

helm install my-release spark-operator/spark-operator \
    --namespace spark-operator \
    --set webhook.enable=true \
    --set batchScheduler.enable=true
```

## Run Spark Application with Volcano scheduler

Now, we can run an updated version of spark application (with `batchScheduler` configured), for instance:

```yaml
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: spark:3.5.1
  imagePullPolicy: Always
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-v3.5.1.jar
  sparkVersion: 3.5.1
  batchScheduler: volcano # Note: the batch scheduler name must be specified with `volcano`
  restartPolicy:
    type: Never
  volumes:
    - name: test-volume
      hostPath:
        path: /tmp
        type: Directory
  driver:
    cores: 1
    coreLimit: 1200m
    memory: 512m
    labels:
      version: 3.5.1
    serviceAccount: spark
    volumeMounts:
      - name: test-volume
        mountPath: /tmp
  executor:
    cores: 1
    instances: 1
    memory: 512m
    labels:
      version: 3.5.1
    volumeMounts:
      - name: test-volume
        mountPath: "/tmp"
```

When running, the Pods Events can be used to verify that whether the pods have been scheduled via Volcano.

```
Type    Reason     Age   From                          Message
----    ------     ----  ----                          -------
Normal  Scheduled  23s   volcano                       Successfully assigned default/spark-pi-driver to integration-worker2
```

# Technological detail

If SparkApplication is configured to run with Volcano, there are some details underground that make the two systems integrated:

1. Kubernetes Operator for Apache Spark's webhook will patch pods' `schedulerName` according to the `batchScheduler` in SparkApplication Spec.
2. Before submitting spark application, Kubernetes Operator for Apache Spark will create a Volcano native resource
   `PodGroup`[here](https://github.com/volcano-sh/volcano/blob/a8fb05ce6c6902e366cb419d6630d66fc759121e/pkg/apis/scheduling/v1alpha2/types.go#L93) for the whole application.
   and as a brief introduction, most of the Volcano's advanced scheduling features, such as pod delay creation, resource fairness and gang scheduling are all depend on this resource.
   Also, a new pod annotation named `scheduling.k8s.io/group-name` will be added.
3. Volcano scheduler will take over all of the pods that both have schedulerName and annotation correctly configured for scheduling.

Kubernetes Operator for Apache Spark enables end user to have fine-grained controlled on batch scheduling via attribute `BatchSchedulerOptions`. `BatchSchedulerOptions` is a string dictionary
that different batch scheduler can utilize it to expose different attributes.
For now, volcano support these attributes below:

| Name  | Description                                                                | example                                                        |
|-------|----------------------------------------------------------------------------|----------------------------------------------------------------|
| queue | Used to specify which volcano queue will this spark application belongs to |  batchSchedulerOptions:<br/>  &nbsp; &nbsp; queue: "queue1" |
| priorityClassName | Used to specify which priorityClass this spark application will use        |  batchSchedulerOptions:<br/>  &nbsp; &nbsp; priorityClassName: "pri1" |



================================================
File: content/en/docs/components/spark-operator/user-guide/working-with-sparkapplication.md
================================================
---
title: "Working with SparkApplications"
description: "Working with SparkApplications"
weight: 30
---

## Creating a New SparkApplication

A `SparkApplication` can be created from a YAML file storing the `SparkApplication` specification using either the `kubectl apply -f <YAML file path>` command or the `sparkctl create <YAML file path>` command. Please refer to the `sparkctl` [README](https://github.com/kubeflow/spark-operator/blob/master/cmd/sparkctl/README.md#create) for usage of the `sparkctl create` command. Once a `SparkApplication` is successfully created, the operator will receive it and submits the application as configured in the specification to run on the Kubernetes cluster. Please note, that `SparkOperator` submits `SparkApplication` in `Cluster` mode only. 

## Deleting a SparkApplication

A `SparkApplication` can be deleted using either the `kubectl delete <name>` command or the `sparkctl delete <name>` command. Please refer to the `sparkctl` [README](https://github.com/kubeflow/spark-operator/blob/master/cmd/sparkctl/README.md#delete) for usage of the `sparkctl delete`
command. Deleting a `SparkApplication` deletes the Spark application associated with it. If the application is running when the deletion happens, the application is killed and all Kubernetes resources associated with the application are deleted or garbage collected.

## Updating a SparkApplication

A `SparkApplication` can be updated using the `kubectl apply -f <updated YAML file>` command. When a `SparkApplication`  is successfully updated, the operator will receive both the updated and old `SparkApplication` objects. If the specification of the `SparkApplication` has changed, the operator submits the application to run, using the updated specification. If the application is currently running, the operator kills the running application before submitting a new run with the updated specification. There is planned work to enhance the way `SparkApplication` updates are handled. For example, if the change was to increase the number of executor instances, instead of killing the currently running application and starting a new run, it is a much better user experience to incrementally launch the additional executor pods.

## Checking a SparkApplication

A `SparkApplication` can be checked using the `kubectl describe sparkapplications <name>` command. The output of the command shows the specification and status of the `SparkApplication` as well as events associated with it. The events communicate the overall process and errors of the `SparkApplication`.

## Configuring Automatic Application Restart and Failure Handling

The operator supports automatic application restart with a configurable `RestartPolicy` using the optional field
`.spec.restartPolicy`. The following is an example of a sample `RestartPolicy`:

```yaml
  restartPolicy:
     type: OnFailure
     onFailureRetries: 3
     onFailureRetryInterval: 10
     onSubmissionFailureRetries: 5
     onSubmissionFailureRetryInterval: 20
```

The valid types of restartPolicy include `Never`, `OnFailure`, and `Always`. Upon termination of an application,
the operator determines if the application is subject to restart based on its termination state and the
`RestartPolicy` in the specification. If the application is subject to restart, the operator restarts it by
submitting a new run of it. For `OnFailure`, the Operator further supports setting limits on number of retries
via the `onFailureRetries` and `onSubmissionFailureRetries` fields. Additionally, if the  submission retries has not been reached,
the operator retries submitting the application using a linear backoff with the interval specified by
`onFailureRetryInterval` and `onSubmissionFailureRetryInterval` which are required for both `OnFailure` and `Always` `RestartPolicy`.
The old resources like driver pod, ui service/ingress etc. are deleted if it still exists before submitting the new run, and a new  driver pod is created by the submission
client so effectively the driver gets restarted.

## Setting TTL for a SparkApplication

The `v1beta2` version of the `SparkApplication` API starts having TTL support for `SparkApplication`s through a new optional field named `.spec.timeToLiveSeconds`, which if set, defines the Time-To-Live (TTL) duration in seconds for a SparkApplication after its termination. The `SparkApplication` object will be garbage collected if the current time is more than the `.spec.timeToLiveSeconds` since its termination. The example below illustrates how to use the field:

```yaml
spec:
  timeToLiveSeconds: 3600
```

Note that this feature requires that informer cache resync to be enabled, which is true by default with a resync internal of 30 seconds. You can change the resync interval by setting the flag `-resync-interval=<interval>`.



================================================
File: content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md
================================================
---
title: "Writing a SparkApplication"
description: "Writing a SparkApplication"
weight: 20
---

As with all other Kubernetes API objects, a `SparkApplication` needs the `apiVersion`, `kind`, and `metadata` fields. For general information about working with manifests, see [object management using kubectl](https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/).

A `SparkApplication` also needs a [`.spec` section](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status). This section contains fields for specifying various aspects of an application including its type (`Scala`, `Java`, `Python`, or `R`), deployment mode (`cluster` or `client`), main application resource URI (e.g., the URI of the application jar), main class, arguments, etc. Node selectors are also supported via the optional field `.spec.nodeSelector`.

It also has fields for specifying the unified container image (to use for both the driver and executors) and the image pull policy, namely, `.spec.image` and `.spec.imagePullPolicy` respectively. If a custom init-container (in both the driver and executor pods) image needs to be used, the optional field `.spec.initContainerImage` can be used to specify it. If set, `.spec.initContainerImage` overrides `.spec.image` for the init-container image. Otherwise, the image specified by `.spec.image` will be used for the init-container. It is invalid if both `.spec.image` and `.spec.initContainerImage` are not set.

Below is an example showing part of a `SparkApplication` specification:

```yaml
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: spark:3.5.1
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar
```

## Specifying Deployment Mode

A `SparkApplication` should set `.spec.deployMode` to `cluster`, as `client` is not currently implemented. The driver pod will then run `spark-submit` in `client` mode internally to run the driver program. Additional details of how `SparkApplication`s are run can be found in the [design documentation](/docs/components/spark-operator/overview#architecture).

## Specifying Application Dependencies

Often Spark applications need additional files additionally to the main application resource to run. Such application dependencies can include for example jars and data files the application needs at runtime. When using the `spark-submit` script to submit a Spark application, such dependencies are specified using the `--jars` and `--files` options. To support specification of application dependencies, a `SparkApplication` uses an optional field `.spec.deps` that in turn supports specifying jars and files, respectively. More specifically, the optional fields `.spec.deps.jars` and`.spec.deps.files` correspond to the `--jars` and `--files` options of the `spark-submit` script, respectively.

Additionally, `.spec.deps` also has fields for specifying the locations in the driver and executor containers where jars and files should be downloaded to, namely, `.spec.deps.jarsDownloadDir` and `.spec.deps.filesDownloadDir`. The optional fields `.spec.deps.downloadTimeout` and `.spec.deps.maxSimultaneousDownloads` are used to control the timeout and maximum parallelism of downloading dependencies that are hosted remotely, e.g., on an HTTP server, or in external storage such as HDFS, Google Cloud Storage, or AWS S3.

The following is an example specification with both container-local (i.e., within the container) and remote dependencies:

```yaml
spec:
  deps:
    jars:
      - local:///opt/spark-jars/gcs-connector.jar
    files:
      - gs://spark-data/data-file-1.txt
      - gs://spark-data/data-file-2.txt
```

It's also possible to specify additional jars to obtain from a remote repository by adding maven coordinates to `.spec.deps.packages`. Conflicting transitive dependencies can be addressed by adding to the exclusion list with `.spec.deps.excludePackages`. Additional repositories can be added to the `.spec.deps.repositories` list. These directly translate to the `spark-submit` parameters `--packages`, `--exclude-packages`, and `--repositories`.

NOTE:

- Each package in the `packages` list must be of the form "groupId:artifactId:version"
- Each package in the `excludePackages` list must be of the form "groupId:artifactId"

The following example shows how to use these parameters.

```yaml
spec:
  deps:
    repositories:
      - https://repository.example.com/prod
    packages:
      - com.example:some-package:1.0.0
    excludePackages:
      - com.example:other-package
```

## Specifying Spark Configuration

There are two ways to add Spark configuration: setting individual Spark configuration properties using the optional field `.spec.sparkConf` or mounting a special Kubernetes ConfigMap storing Spark configuration files (e.g. `spark-defaults.conf`, `spark-env.sh`, `log4j.properties`) using the optional field `.spec.sparkConfigMap`. If `.spec.sparkConfigMap` is used, additionally to mounting the ConfigMap into the driver and executors, the operator additionally sets the environment variable `SPARK_CONF_DIR` to point to the mount path of the ConfigMap.

```yaml
spec:
  sparkConf:
    spark.ui.port: "4045"
    spark.eventLog.enabled: "true"
    spark.eventLog.dir: "hdfs://hdfs-namenode-1:8020/spark/spark-events"
```

## Specifying Hadoop Configuration

There are two ways to add Hadoop configuration: setting individual Hadoop configuration properties using the optional field `.spec.hadoopConf` or mounting a special Kubernetes ConfigMap storing Hadoop configuration files (e.g.  `core-site.xml`) using the optional field `.spec.hadoopConfigMap`. The operator automatically adds the prefix `spark.hadoop.` to the names of individual Hadoop configuration properties in `.spec.hadoopConf`. If  `.spec.hadoopConfigMap` is used, additionally to mounting the ConfigMap into the driver and executors, the operator additionally sets the environment variable `HADOOP_CONF_DIR` to point to the mount path of the ConfigMap.

The following is an example showing the use of individual Hadoop configuration properties:

```yaml
spec:
  hadoopConf:
    "fs.gs.project.id": spark
    "fs.gs.system.bucket": spark
    "google.cloud.auth.service.account.enable": true
    "google.cloud.auth.service.account.json.keyfile": /mnt/secrets/key.json
```

## Writing Driver Specification

The `.spec` section of a `SparkApplication` has a `.spec.driver` field for configuring the driver. It allows users to set the memory and CPU resources to request for the driver pod, and the container image the driver should use. It also has fields for optionally specifying labels, annotations, and environment variables for the driver pod. By default, the driver pod name of an application is automatically generated by the Spark submission client. If instead you want to use a particular name for the driver pod, the optional field `.spec.driver.podName` can be used. The driver pod by default uses the `default` service account in the namespace it is running in to talk to the Kubernetes API server. The `default` service account, however, may or may not have sufficient permissions to create executor pods and the headless service used by the executors to connect to the driver. If it does not and a custom service account that has the right permissions should be used instead, the optional field `.spec.driver.serviceAccount` can be used to specify the name of the custom service account. When a custom container image is needed for the driver, the field `.spec.driver.image` can be used to specify it. This overrides the image specified in `.spec.image` if it is also set. It is invalid if both `.spec.image` and `.spec.driver.image` are not set.

For applications that need to mount Kubernetes [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/) or [ConfigMaps](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) into the driver pod, fields `.spec.driver.secrets` and `.spec.driver.configMaps` can be used. For more details, please refer to
[Mounting Secrets](#mounting-secrets) and [Mounting ConfigMaps](#mounting-configmaps).

The following is an example driver specification:

```yaml
spec:
  driver:
    cores: 1
    coreLimit: 200m
    memory: 512m
    labels:
      version: 3.1.1
    serviceAccount: spark
```

## Writing Executor Specification

The `.spec` section of a `SparkApplication` has a `.spec.executor` field for configuring the executors. It allows users to set the memory and CPU resources to request for the executor pods, and the container image the executors should use. It also has fields for optionally specifying labels, annotations, and environment variables for the executor pods. By default, a single executor is requested for an application. If more than one executor are needed, the optional field `.spec.executor.instances` can be used to specify the number of executors to request. When a custom container image is needed for the executors, the field `.spec.executor.image` can be used to specify it. This overrides the image specified in `.spec.image` if it is also set. It is invalid if both `.spec.image` and `.spec.executor.image` are not set.

For applications that need to mount Kubernetes [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/) or [ConfigMaps](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) into the executor pods, fields `.spec.executor.secrets` and `.spec.executor.configMaps` can be used. For more details, please refer to
[Mounting Secrets](#mounting-secrets) and [Mounting ConfigMaps](#mounting-configmaps).

An example executor specification is shown below:

```yaml
spec:
  executor:
    cores: 1
    instances: 1
    memory: 512m
    labels:
      version: 3.1.1
    serviceAccount: spark
```

## Specifying Extra Java Options

A `SparkApplication` can specify extra Java options for the driver or executors, using the optional field `.spec.driver.javaOptions` for the driver and `.spec.executor.javaOptions` for executors. Below is an example:

```yaml
spec:
  executor:
    javaOptions: "-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
```

Values specified using those two fields get converted to Spark configuration properties `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`, respectively. **Prefer using the above two fields over configuration properties `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`** as the fields work well with other fields that might modify what gets set for `spark.driver.extraJavaOptions` or `spark.executor.extraJavaOptions`.

## Specifying Environment Variables

There are two fields for specifying environment variables for the driver and/or executor containers, namely `.spec.driver.env` (or `.spec.executor.env` for the executor container) and `.spec.driver.envFrom` (or `.spec.executor.envFrom` for the executor container). Specifically, `.spec.driver.env` (and `.spec.executor.env`) takes a list of [EnvVar](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#envvar-v1-core), each of which specifies an environment variable or the source of an environment variable, e.g., a name-value pair, a ConfigMap key, a Secret key, etc. Alternatively, `.spec.driver.envFrom` (and `.spec.executor.envFrom`) takes a list of [EnvFromSource](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#envfromsource-v1-core) and allows [using all key-value pairs in a ConfigMap or Secret as environment variables](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables). The `SparkApplication` snippet below shows the use of both fields:

```yaml
spec:
  driver:
    env:
      - name: ENV1
        value: VAL1
      - name: ENV2
        value: VAL2
      - name: ENV3
        valueFrom:
          configMapKeyRef:
            name: some-config-map
            key: env3-key
      - name: AUTH_KEY
        valueFrom:
          secretKeyRef:
            name: some-secret
            key: auth-key
    envFrom:
      - configMapRef:
          name: env-config-map
      - secretRef:
          name: env-secret
  executor:
    env:
      - name: ENV1
        value: VAL1
      - name: ENV2
        value: VAL2
      - name: ENV3
        valueFrom:
          configMapKeyRef:
            name: some-config-map
            key: env3-key
      - name: AUTH_KEY
        valueFrom:
          secretKeyRef:
            name: some-secret
            key: auth-key
    envFrom:
      - configMapRef:
          name: my-env-config-map
      - secretRef:
          name: my-env-secret
```

**Note: legacy field `envVars` that can also be used for specifying environment variables is deprecated and will be removed in a future API version.**

## Requesting GPU Resources

A `SparkApplication` can specify GPU resources for the driver or executor pod, using the optional field `.spec.driver.gpu` or `.spec.executor.gpu`. Below is an example:

```yaml
spec:
  driver:
    cores: 0.1
    coreLimit: "200m"
    memory: "512m"
    gpu:
      name: "amd.com/gpu"   # GPU resource name
      quantity: 1           # number of GPUs to request
    labels:
      version: 3.1.1
    serviceAccount: spark
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    serviceAccount: spark
    gpu:
      name: "nvidia.com/gpu"
      quantity: 1
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the [Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Host Network

A `SparkApplication` can specify `hostNetwork` for the driver or executor pod, using the optional field `.spec.driver.hostNetwork` or `.spec.executor.hostNetwork`. When `hostNetwork` is `true`, the operator sets pods' `spec.hostNetwork` to `true` and sets pods' `spec.dnsPolicy` to `ClusterFirstWithHostNet`. Below is an example:

```yaml
spec:
  driver:
    cores: 0.1
    coreLimit: "200m"
    memory: "512m"
    hostNetwork: true
    labels:
      version: 3.1.1
    serviceAccount: spark
  executor:
    cores: 1
    instances: 1
    memory: "512m"
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the [Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Mounting Secrets

As mentioned above, both the driver specification and executor specification have an optional field `secrets` for configuring the list of Kubernetes Secrets to be mounted into the driver and executors, respectively. The field is a map with the names of the Secrets as keys and values specifying the mount path and type of each Secret. For instance, the following example shows a driver specification with a Secret named `gcp-svc-account` of type `GCPServiceAccount` to be mounted to `/mnt/secrets` in the driver pod.

```yaml
spec:
  driver:
    secrets:
      - name: gcp-svc-account
        path: /mnt/secrets
        secretType: GCPServiceAccount
```

The type of a Secret as specified by the `secretType` field is a hint to the operator on what extra configuration it needs to take care of for the specific type of Secrets. For example, if a Secret is of type **`GCPServiceAccount`**, the operator additionally sets the environment variable **`GOOGLE_APPLICATION_CREDENTIALS`** to point to the JSON key file stored in the secret. Please refer to
[Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started) for more information on how to authenticate with GCP services using a service account JSON key file. Note that the operator assumes that the key of the service account JSON key file in the Secret data map is **`key.json`** so it is able to set the environment variable automatically. Similarly, if the type of a Secret is **`HadoopDelegationToken`**, the operator additionally sets the environment variable **`HADOOP_TOKEN_FILE_LOCATION`** to point to the file storing the Hadoop delegation token. In this case, the operator assumes that the key of the delegation token file in the Secret data map is **`hadoop.token`**.
The `secretType` field should have the value `Generic` if no extra configuration is required.

Note that the mutating admission webhook is needed to use this feature. Please refer to the [Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Mounting ConfigMaps

Both the driver specification and executor specifications have an optional field for configuring
the list of Kubernetes ConfigMaps to be mounted into the driver and executors, respectively. The field is a map with keys being the names of the ConfigMaps and values specifying the mount path of each ConfigMap. For instance, the following example shows a driver specification with a ConfigMap named `configmap1` to be mounted to `/mnt/config-maps` in the driver pod.

```yaml
spec:
  driver:
    configMaps:
      - name: configmap1
        path: /mnt/config-maps
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the [Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Mounting a ConfigMap storing Spark Configuration Files

A `SparkApplication` can specify a Kubernetes ConfigMap storing Spark configuration files such as `spark-env.sh` or `spark-defaults.conf` using the optional field `.spec.sparkConfigMap` whose value is the name of the ConfigMap. The ConfigMap is assumed to be in the same namespace as that of the `SparkApplication`. The operator mounts the ConfigMap onto path `/etc/spark/conf` in both the driver and executors. Additionally, it also sets the environment variable `SPARK_CONF_DIR` to point to `/etc/spark/conf` in the driver and executors.

Note that the mutating admission webhook is needed to use this feature. Please refer to the
[Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Mounting a ConfigMap storing Hadoop Configuration Files

A `SparkApplication` can specify a Kubernetes ConfigMap storing Hadoop configuration files such as `core-site.xml` using the optional field `.spec.hadoopConfigMap` whose value is the name of the ConfigMap. The ConfigMap is assumed to be in the same namespace as that of the `SparkApplication`. The operator mounts the ConfigMap onto path  `/etc/hadoop/conf` in both the driver and executors. Additionally, it also sets the environment variable `HADOOP_CONF_DIR` to point to `/etc/hadoop/conf` in the driver and executors.

Note that the mutating admission webhook is needed to use this feature. Please refer to the [Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Mounting Volumes

The operator also supports mounting user-specified Kubernetes volumes into the driver and executors. A
`SparkApplication` has an optional field `.spec.volumes` for specifying the list of [volumes](https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/volume/) the driver and the executors need collectively. Then both the driver and executor specifications have an optional field `volumeMounts`  that specifies the [volume mounts](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#volumes-1) for the volumes needed by the driver and executors, respectively. The following is an example showing a `SparkApplication` with both driver and executor volume mounts.

```yaml
spec:
  volumes:
    - name: spark-data
      persistentVolumeClaim:
        claimName: my-pvc
    - name: spark-work
      emptyDir:
        sizeLimit: 5Gi
  driver:
    volumeMounts:
      - name: spark-work
        mountPath: /mnt/spark/work
  executor:
    volumeMounts:
      - name: spark-data
        mountPath: /mnt/spark/data
      - name: spark-work
        mountPath: /mnt/spark/work

```

Note that the mutating admission webhook is needed to use this feature. Please refer to the [Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Using Secrets As Environment Variables

**Note: `envSecretKeyRefs` is deprecated and will be removed in a future API version.**

A `SparkApplication` can use [secrets as environment variables](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables), through the optional field `.spec.driver.envSecretKeyRefs` for the driver pod and the optional field
`.spec.executor.envSecretKeyRefs` for the executor pods. A `envSecretKeyRefs` is a map from environment variable names to pairs consisting of a secret name and a secret key. Below is an example:

```yaml
spec:
  driver:
    envSecretKeyRefs:
      SECRET_USERNAME:
        name: mysecret
        key: username
      SECRET_PASSWORD:
        name: mysecret
        key: password
```

## Using Image Pull Secrets

**Note that this feature requires an image based on the latest Spark master branch.**

For images that need image-pull secrets to be pulled, a `SparkApplication` has an optional field `.spec.imagePullSecrets` for specifying a list of image-pull secrets. Below is an example:

```yaml
spec:
  imagePullSecrets:
    - secret1
    - secret2
```

## Using Pod Affinity

A `SparkApplication` can specify an `Affinity` for the driver or executor pod, using the optional field `.spec.driver.affinity` or `.spec.executor.affinity`. Below is an example:

```yaml
spec:
  driver:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          ...
  executor:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          ...
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the [Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Using Tolerations

A `SparkApplication` can specify an `Tolerations` for the driver or executor pod, using the optional field `.spec.driver.tolerations` or `.spec.executor.tolerations`. Below is an example:

```yaml
spec:
  driver:
    tolerations:
    - key: Key
      operator: Exists
      effect: NoSchedule

  executor:
    tolerations:
    - key: Key
      operator: Equal
      value: Value
      effect: NoSchedule
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the
[Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Using Security Context

A `SparkApplication` can specify a `SecurityContext` for the driver or executor containers, using the optional field `.spec.driver.securityContext` or `.spec.executor.securityContext`.
`SparkApplication` can also specify a `PodSecurityContext` for the driver or executor pod, using the optional field `.spec.driver.podSecurityContext` or `.spec.executor.podSecurityContext`. Below is an example:

```yaml
spec:
  driver:
    podSecurityContext:
      runAsUser: 1000
    securityContext:
      allowPrivilegeEscalation: false
      runAsUser: 2000
  executor:
    podSecurityContext:
      runAsUser: 1000
    securityContext:
      allowPrivilegeEscalation: false
      runAsUser: 2000
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the
[Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Using Sidecar Containers

A `SparkApplication` can specify one or more optional sidecar containers for the driver or executor pod, using the optional field `.spec.driver.sidecars` or `.spec.executor.sidecars`. The specification of each sidecar container follows the [Container](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#container-v1-core) API definition. Below is an example:

```yaml
spec:
  driver:
    sidecars:
    - name: "sidecar1"
      image: "sidecar1:latest"
      ...
  executor:
    sidecars:
    - name: "sidecar1"
      image: "sidecar1:latest"
      ...
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the
[Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Using Init-Containers

A `SparkApplication` can optionally specify one or more [init-containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) for the driver or executor pod, using the optional field `.spec.driver.initContainers` or `.spec.executor.initContainers`, respectively. The specification of each init-container follows the [Container](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#container-v1-core) API definition. Below is an example:

```yaml
spec:
  driver:
    initContainers:
    - name: "init-container1"
      image: "init-container1:latest"
      ...
  executor:
    initContainers:
    - name: "init-container1"
      image: "init-container1:latest"
      ...
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the
[Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Using DNS Settings

A `SparkApplication` can define DNS settings for the driver and/or executor pod, by adding the standard [DNS](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-config) kubernetes settings. Fields to add such configuration are `.spec.driver.dnsConfig` and `.spec.executor.dnsConfig`. Example:

```yaml
spec:
  driver:
    dnsConfig:
      nameservers:
        - 1.2.3.4
      searches:
        - ns1.svc.cluster.local
        - my.dns.search.suffix
      options:
        - name: ndots
          value: "2"
        - name: edns0
```

Note that the mutating admission webhook is needed to use this feature. Please refer to the
[Getting Started](/docs/components/spark-operator/getting-started) on how to enable the mutating admission webhook.

## Using Volume For Scratch Space

By default, Spark uses temporary scratch space to spill data to disk during shuffles and other operations.
The scratch directory defaults to `/tmp` of the container.
If that storage isn't enough or you want to use a specific path, you can use one or more volumes.
The volume names should start with `spark-local-dir-`.

```yaml
spec:
  volumes:
    - name: "spark-local-dir-1"
      hostPath:
        path: "/tmp/spark-local-dir"
  executor:
    volumeMounts:
      - name: "spark-local-dir-1"
        mountPath: "/tmp/spark-local-dir"
    ...
```

Then you will get `SPARK_LOCAL_DIRS` set to `/tmp/spark-local-dir` in the pod like below.

```yaml
Environment:
  SPARK_USER:                 root
  SPARK_DRIVER_BIND_ADDRESS:  (v1:status.podIP)
  SPARK_LOCAL_DIRS:           /tmp/spark-local-dir
  SPARK_CONF_DIR:             /opt/spark/conf
```

> Note: Multiple volumes can be used together

```yaml
spec:
  volumes:
    - name: "spark-local-dir-1"
      hostPath:
        path: "/mnt/dir1"
    - name: "spark-local-dir-2"
      hostPath:
        path: "/mnt/dir2"
  executor:
    volumeMounts:
      - name: "spark-local-dir-1"
        mountPath: "/tmp/dir1"
      - name: "spark-local-dir-2"
        mountPath: "/tmp/dir2"
    ...
```

> Note: Besides `hostPath`, `persistentVolumeClaim` can be used as well.

```yaml
spec:
  volumes:
    - name: "spark-local-dir-1"
      persistentVolumeClaim:
        claimName: network-file-storage
  executor:
    volumeMounts:
      - name: "spark-local-dir-1"
        mountPath: "/tmp/dir1"
```

## Using Termination Grace Period

A Spark Application can optionally specify a termination grace Period seconds to the driver and executor pods. More [info](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods)

```yaml
spec:
  driver:
    terminationGracePeriodSeconds: 60
```

## Using Container LifeCycle Hooks

A Spark Application can optionally specify a [Container Lifecycle Hooks](https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks) for a driver. It is useful in cases where you need a PreStop or PostStart hooks to driver and executor.

```yaml
spec:
  driver:
    lifecycle:
      preStop:
        exec:
          command:
          - /bin/bash
          - -c
          - touch /var/run/killspark && sleep 65
```

In cases like Spark Streaming or Spark Structured Streaming applications, you can test if a file exists to start a graceful shutdown and stop all streaming queries manually.

## Python Support

Python support can be enabled by setting `.spec.mainApplicationFile` with path to your python application. Optionally, the `.spec.pythonVersion` field can be used to set the major Python version of the docker image used to run the driver and executor containers. Below is an example showing part of a `SparkApplication` specification:

```yaml
spec:
  type: Python
  pythonVersion: 2
  mainApplicationFile: local:///opt/spark/examples/src/main/python/pyfiles.py
```

Some PySpark applications need additional Python packages to run. Such dependencies are specified using the optional field `.spec.deps.pyFiles`, which translates to the `--py-files` option of the spark-submit command.

```yaml
spec:
  deps:
    pyFiles:
       - local:///opt/spark/examples/src/main/python/py_container_checks.py
       - gs://spark-data/python-dep.zip
```

In order to use the dependencies that are hosted remotely, the following PySpark code can be used in Spark 2.4.

```python
python_dep_file_path = SparkFiles.get("python-dep.zip")
spark.sparkContext.addPyFile(dep_file_path)
```

Note that Python binding for PySpark is available in Apache Spark 2.4.

## Monitoring

The operator supports using the Spark metric system to expose metrics to a variety of sinks. Particularly, it is able to automatically configure the metric system to expose metrics to [Prometheus](https://prometheus.io/). Specifically, the field `.spec.monitoring` specifies how application monitoring is handled and particularly how metrics are to be reported. The metric system is configured through the configuration file `metrics.properties`, which gets its content from the field `.spec.monitoring.metricsProperties`. The content of [metrics.properties](https://github.com/kubeflow/spark-operator/blob/master/spark-docker/conf/metrics.properties) will be used by default if `.spec.monitoring.metricsProperties` is not specified. `.spec.monitoring.metricsPropertiesFile` overwrite the value `spark.metrics.conf` in spark.properties, and will not use content from `.spec.monitoring.metricsProperties`. You can choose to enable or disable reporting driver and executor metrics using the fields `.spec.monitoring.exposeDriverMetrics` and `.spec.monitoring.exposeExecutorMetrics`, respectively.

Further, the field `.spec.monitoring.prometheus` specifies how metrics are exposed to Prometheus using the [Prometheus JMX exporter](https://github.com/prometheus/jmx_exporter). When `.spec.monitoring.prometheus` is specified, the operator automatically configures the JMX exporter to run as a Java agent. The only required field of `.spec.monitoring.prometheus` is `jmxExporterJar`, which specified the path to the Prometheus JMX exporter Java agent jar in the container. If you use the image `gcr.io/spark-operator/spark:v3.1.1-gcs-prometheus`, the jar is located at `/prometheus/jmx_prometheus_javaagent-0.11.0.jar`. The field `.spec.monitoring.prometheus.port` specifies the port the JMX exporter Java agent binds to and defaults to `8090` if not specified. The field `.spec.monitoring.prometheus.configuration` specifies the content of the configuration to be used with the JMX exporter. The content of [prometheus.yaml](https://github.com/kubeflow/spark-operator/blob/master/spark-docker/conf/prometheus.yaml) will be used by default if `.spec.monitoring.prometheus.configuration` is not specified.

Below is an example that shows how to configure the metric system to expose metrics to Prometheus using the Prometheus JMX exporter. Note that the JMX exporter Java agent jar is listed as a dependency and will be downloaded to where `.spec.dep.jarsDownloadDir` points to in Spark 2.3.x, which is `/var/spark-data/spark-jars` by default. Things are different in Spark 2.4 as dependencies will be downloaded to the local working directory instead in Spark 2.4. A complete example can be found in [examples/spark-pi-prometheus.yaml](https://github.com/kubeflow/spark-operator/blob/master/examples/spark-pi-prometheus.yaml).

```yaml
spec:
  deps:
    jars:
    - http://central.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.11.0/jmx_prometheus_javaagent-0.11.0.jar
  monitoring:
    exposeDriverMetrics: true
    prometheus:
      jmxExporterJar: "/var/spark-data/spark-jars/jmx_prometheus_javaagent-0.11.0.jar"
```

The operator automatically adds the annotations such as `prometheus.io/scrape=true` on the driver and/or executor pods (depending on the values of  `.spec.monitoring.exposeDriverMetrics` and `.spec.monitoring.exposeExecutorMetrics`) so the metrics exposed on the pods can be scraped by the Prometheus server in the same cluster.

## Dynamic Allocation

The operator supports a limited form of [Spark Dynamic Resource Allocation](http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) through the shuffle tracking enhancement introduced in Spark 3.0.0 *without needing an external shuffle service* (not available in the Kubernetes mode). See this [issue](https://issues.apache.org/jira/browse/SPARK-27963) for details on the enhancement. To enable this limited form of dynamic allocation, follow the example below:

```yaml
spec:
  dynamicAllocation:
    enabled: true
    initialExecutors: 2
    minExecutors: 2
    maxExecutors: 10
```

Note that if dynamic allocation is enabled, the number of executors to request initially is set to the bigger of `.spec.dynamicAllocation.initialExecutors` and `.spec.executor.instances` if both are set.



================================================
File: content/en/docs/components/spark-operator/user-guide/yunikorn-integration.md
================================================
---
title: Integration with YuniKorn
description: Using the Spark operator and YuniKorn together for batch scheduling
weight: 100
---

{{% alert title="Warning" color="warning" %}}
This feature is only supported on version 2.0.0 of the operator and above.
{{% /alert %}}

[YuniKorn](https://yunikorn.apache.org/) is an alternative scheduler for Kubernetes that provides batch scheduling capabilities over the default scheduler and can notably improve the experience of running Spark on Kubernetes. These capabilities include gang scheduling, application-aware job queueing, hierarchical resource quotas and improved binpacking.

## How to use

### 1. Install YuniKorn

Install YuniKorn on your Kubernetes cluster by following the [Get Started](https://yunikorn.apache.org/docs/) guide on the YuniKorn docs.

**Note:** By default, YuniKorn will install an admission controller that will set the `schedulerName` field on all pods to YuniKorn. Please consult the YuniKorn docs for instructions on disabling this if this is not desired.

### 2. Enable batch scheduling in the controller

Set the following values on your operator installation to enable batch scheduling in the controller. You can also optionally set the default batch scheduler for all `SparkApplication` definitions if not specified by the user in `.spec.batchScheduler`.

```yaml
controller:
  batchScheduler:
    enable: true
    # Setting the default batch scheduler is optional. The default only
    # applies if the batchScheduler field on the SparkApplication spec is not set
    default: yunikorn
```

### 3. Submit an application

Specify the batch scheduler on your `SparkApplication` as shown below. You can find a full example in the repo under [`examples/spark-pi-yunikorn.yaml`](https://github.com/kubeflow/spark-operator/blob/master/examples/spark-pi-yunikorn.yaml).

```yaml
spec:
  ...
  batchScheduler: yunikorn
  batchSchedulerOptions:
    queue: root.default
```

Using the above example, the Spark operator will do the following:

1. Annotate the driver pod with task group annotations
2. Set the `schedulerName` field on the driver and executor pods to `yunikorn`
3. Add a queue label to the driver and executor pods if specified under `batchSchedulerOptions`

For more information on gang scheduling, task groups and queue routing, please consult the following YuniKorn doc pages:

* [Gang scheduling](https://yunikorn.apache.org/docs/next/user_guide/gang_scheduling/)
* [Placement rules](https://yunikorn.apache.org/docs/next/user_guide/placement_rules)

You should see the following pod events that show the pod was gang scheduled with YuniKorn:

```
Type    Reason             Age   From      Message
----    ------             ----  ----      -------
Normal  Scheduling         20s   yunikorn  default/spark-pi-yunikorn-driver is queued and waiting for allocation
Normal  GangScheduling     20s   yunikorn  Pod belongs to the taskGroup spark-driver, it will be scheduled as a gang member
Normal  Scheduled          19s   yunikorn  Successfully assigned default/spark-pi-yunikorn-driver to node spark-operator-worker
Normal  PodBindSuccessful  19s   yunikorn  Pod default/spark-pi-yunikorn-driver is successfully bound to node spark-operator-worker
Normal  TaskCompleted      4s    yunikorn  Task default/spark-pi-yunikorn-driver is completed
Normal  Pulling            20s   kubelet   Pulling image "spark:3.5.2"
Normal  Pulled             13s   kubelet   Successfully pulled image "spark:3.5.2" in 6.162s (6.162s including waiting)
Normal  Created            13s   kubelet   Created container spark-kubernetes-driver
Normal  Started            13s   kubelet   Started container spark-kubernetes-driver
```

The following annotations and labels should also be present on the driver pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    yunikorn.apache.org/allow-preemption: "true"
    yunikorn.apache.org/task-group-name: spark-driver
    yunikorn.apache.org/task-groups: '[{"name":"spark-driver","minMember":1,"minResource":{"cpu":"1","memory":"896Mi"},"labels":{"queue":"root.default","version":"3.5.2"}},{"name":"spark-executor","minMember":2,"minResource":{"cpu":"1","memory":"896Mi"},"labels":{"queue":"root.default","version":"3.5.2"}}]'
    yunikorn.apache.org/user.info: '{"user":"system:serviceaccount:spark-operator:spark-operator-controller","groups":["system:serviceaccounts","system:serviceaccounts:spark-operator","system:authenticated"]}'
  creationTimestamp: "2024-09-10T04:40:37Z"
  labels:
    queue: root.default
    spark-app-name: spark-pi-yunikorn
    spark-app-selector: spark-1bfe85bb77df4d5594337249b38c9648
    spark-role: driver
    spark-version: 3.5.2
    sparkoperator.k8s.io/app-name: spark-pi-yunikorn
    sparkoperator.k8s.io/launched-by-spark-operator: "true"
    sparkoperator.k8s.io/submission-id: 1a71de55-cdc7-4e62-b997-197883dc4cbe
    version: 3.5.2
  name: spark-pi-yunikorn-driver
  ...
```



================================================
File: content/en/docs/components/trainer/OWNERS
================================================
approvers:
  - andreyvelich
  - ChanYiLin
  - gaocegege
  - Jeffwan
  - johnugeorge
  - terrytangyuan



================================================
File: content/en/docs/components/trainer/_index.md
================================================
+++
title = "Kubeflow Trainer"
description = "Documentation for Kubeflow Trainer"
weight = 20
+++



================================================
File: content/en/docs/components/trainer/getting-started.md
================================================
+++
title = "Getting Started"
description = "Get Started with Kubeflow Trainer"
weight = 30
+++

This guide describes how to get started with Kubeflow Trainer and run distributed training
with PyTorch.

## Prerequisites

Ensure that you have access to a Kubernetes cluster with Kubeflow Trainer
control plane installed. If it is not set up yet, follow
[the installation guide](/docs/components/trainer/operator-guides/installation) to quickly deploy
Kubeflow Trainer.

### Installing the Kubeflow Python SDK

Install the latest Kubeflow Python SDK version directly from the source repository:

```bash
pip install git+https://github.com/kubeflow/trainer.git@master#subdirectory=sdk
```

## Getting Started with PyTorch

Before creating a Kubeflow TrainJob, defines the training function that handles end-to-end model
training. Each PyTorch node will execute this function within the configured distributed environment.
Typically, this function includes steps to download the dataset, initialize the model, and train it.

Kubeflow Trainer automatically sets up the distributed environment for PyTorch, enabling
[Distributed Data Parallel (DDP)](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).

```python
def train_pytorch():
    import os

    import torch
    from torch import nn
    import torch.nn.functional as F

    from torchvision import datasets, transforms
    import torch.distributed as dist
    from torch.utils.data import DataLoader, DistributedSampler

    # [1] Configure CPU/GPU device and distributed backend.
    # Kubeflow Trainer will automatically configure the distributed environment.
    device, backend = ("cuda", "nccl") if torch.cuda.is_available() else ("cpu", "gloo")
    dist.init_process_group(backend=backend)

    local_rank = int(os.getenv("LOCAL_RANK", 0))
    print(
        "Distributed Training with WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}.".format(
            dist.get_world_size(),
            dist.get_rank(),
            local_rank,
        )
    )

    # [2] Define PyTorch CNN Model to be trained.
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(1, 20, 5, 1)
            self.conv2 = nn.Conv2d(20, 50, 5, 1)
            self.fc1 = nn.Linear(4 * 4 * 50, 500)
            self.fc2 = nn.Linear(500, 10)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2, 2)
            x = x.view(-1, 4 * 4 * 50)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return F.log_softmax(x, dim=1)

    # [3] Attach model to the correct device.
    device = torch.device(f"{device}:{local_rank}")
    model = nn.parallel.DistributedDataParallel(Net().to(device))
    model.train()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)

    # [4] Get the Fashion-MNIST dataset and distributed it across all available devices.
    dataset = datasets.FashionMNIST(
        "./data",
        train=True,
        download=True,
        transform=transforms.Compose([transforms.ToTensor()]),
    )
    train_loader = DataLoader(
        dataset,
        batch_size=100,
        sampler=DistributedSampler(dataset),
    )

    # [5] Define the training loop.
    for epoch in range(3):
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            # Attach tensors to the device.
            inputs, labels = inputs.to(device), labels.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = F.nll_loss(outputs, labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if batch_idx % 10 == 0 and dist.get_rank() == 0:
                print(
                    "Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}".format(
                        epoch,
                        batch_idx * len(inputs),
                        len(train_loader.dataset),
                        100.0 * batch_idx / len(train_loader),
                        loss.item(),
                    )
                )

    # Wait for the training to complete and destroy to PyTorch distributed process group.
    dist.barrier()
    if dist.get_rank() == 0:
        print("Training is finished")
    dist.destroy_process_group()
```

After configuring the training function, check the available Kubeflow Training Runtimes:

```python
from kubeflow.trainer import TrainerClient, CustomTrainer

for r in TrainerClient().list_runtimes():
    print(f"Runtime: {r.name}")
```

You should be able to see list of available Training Runtimes:

```python
Runtime: torch-distributed
```

Create a TrainJob using the `torch-distributed` Runtime, which scales your training function across
4 PyTorch nodes, every node has 1 GPU.

```python
job_id = TrainerClient().train(
    trainer=CustomTrainer(
        func=train_pytorch,
        num_nodes=4,
        resources_per_node={
            "cpu": 5,
            "memory": "16Gi",
            "gpu": 1, # Comment this line if you don't have GPUs.
        },
    ),
    runtime_ref="torch-distributed",
)
```

You can check the components of the TrainJob and the number of devices each PyTorch node is using:

```python
for c in TrainerClient().get_job(name=job_id).components:
    print(f"Component: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}")
```

The output:

```python
Component: trainer-node-0, Status: Succeeded, Devices: gpu x 1
Component: trainer-node-1, Status: Succeeded, Devices: gpu x 1
Component: trainer-node-2, Status: Succeeded, Devices: gpu x 1
Component: trainer-node-3, Status: Succeeded, Devices: gpu x 1
```

Finally, you can check the training logs from the master node:

```python
logs = TrainerClient().get_job_logs(name=job_id)

print(logs["trainer-node-0"])
```

Since training was run on 4 GPUs, each PyTorch node processes 60,000 / 4 = 15,000 images
from the dataset:

```python

Distributed Training with WORLD_SIZE: 4, RANK: 0, LOCAL_RANK: 0.
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
...
Train Epoch: 0 [0/60000 (0%)]	Loss: 2.300872
Train Epoch: 0 [1000/60000 (7%)]	Loss: 1.641364
Train Epoch: 0 [2000/60000 (13%)]	Loss: 1.210384
Train Epoch: 0 [3000/60000 (20%)]	Loss: 0.994264
Train Epoch: 0 [4000/60000 (27%)]	Loss: 0.831711
Train Epoch: 0 [5000/60000 (33%)]	Loss: 0.613464
Train Epoch: 0 [6000/60000 (40%)]	Loss: 0.739163
Train Epoch: 0 [7000/60000 (47%)]	Loss: 0.843191
Train Epoch: 0 [8000/60000 (53%)]	Loss: 0.447067
Train Epoch: 0 [9000/60000 (60%)]	Loss: 0.538711
Train Epoch: 0 [10000/60000 (67%)]	Loss: 0.386125
Train Epoch: 0 [11000/60000 (73%)]	Loss: 0.545219
Train Epoch: 0 [12000/60000 (80%)]	Loss: 0.452070
Train Epoch: 0 [13000/60000 (87%)]	Loss: 0.551063
Train Epoch: 0 [14000/60000 (93%)]	Loss: 0.409985
Train Epoch: 1 [0/60000 (0%)]	Loss: 0.485615
Train Epoch: 1 [1000/60000 (7%)]	Loss: 0.414390
Train Epoch: 1 [2000/60000 (13%)]	Loss: 0.449027
Train Epoch: 1 [3000/60000 (20%)]	Loss: 0.262625
Train Epoch: 1 [4000/60000 (27%)]	Loss: 0.471265
Train Epoch: 1 [5000/60000 (33%)]	Loss: 0.353005
Train Epoch: 1 [6000/60000 (40%)]	Loss: 0.570305
Train Epoch: 1 [7000/60000 (47%)]	Loss: 0.574882
Train Epoch: 1 [8000/60000 (53%)]	Loss: 0.393912
Train Epoch: 1 [9000/60000 (60%)]	Loss: 0.346508
Train Epoch: 1 [10000/60000 (67%)]	Loss: 0.311427
Train Epoch: 1 [11000/60000 (73%)]	Loss: 0.336713
Train Epoch: 1 [12000/60000 (80%)]	Loss: 0.321332
Train Epoch: 1 [13000/60000 (87%)]	Loss: 0.348189
Train Epoch: 1 [14000/60000 (93%)]	Loss: 0.360835
Train Epoch: 2 [0/60000 (0%)]	Loss: 0.416435
Train Epoch: 2 [1000/60000 (7%)]	Loss: 0.364135
Train Epoch: 2 [2000/60000 (13%)]	Loss: 0.392644
Train Epoch: 2 [3000/60000 (20%)]	Loss: 0.265317
Train Epoch: 2 [4000/60000 (27%)]	Loss: 0.400089
Train Epoch: 2 [5000/60000 (33%)]	Loss: 0.333744
Train Epoch: 2 [6000/60000 (40%)]	Loss: 0.515001
Train Epoch: 2 [7000/60000 (47%)]	Loss: 0.489475
Train Epoch: 2 [8000/60000 (53%)]	Loss: 0.304395
Train Epoch: 2 [9000/60000 (60%)]	Loss: 0.274867
Train Epoch: 2 [10000/60000 (67%)]	Loss: 0.273643
Train Epoch: 2 [11000/60000 (73%)]	Loss: 0.303883
Train Epoch: 2 [12000/60000 (80%)]	Loss: 0.268735
Train Epoch: 2 [13000/60000 (87%)]	Loss: 0.277623
Train Epoch: 2 [14000/60000 (93%)]	Loss: 0.247948
Training is finished
```



================================================
File: content/en/docs/components/trainer/overview.md
================================================
+++
title = "Overview"
description = "An overview of Kubeflow Trainer"
weight = 10
+++

{{% alert title="Note" color="dark" %}}
Kubeflow Trainer project is currently in <strong>alpha</strong> status, and APIs may change.
If you are using Kubeflow Training Operator V1, refer [to this migration document](/docs/components/trainer/operator-guides/migration).

For legacy Kubeflow Training Operator V1 documentation, check [these guides](/docs/components/trainer/legacy-v1)
{{% /alert %}}

## What is Kubeflow Trainer

Kubeflow Trainer is a Kubernetes-native project designed for
large language models (LLMs) fine-tuning and enabling scalable, distributed training of
machine learning (ML) models across various frameworks, including PyTorch, JAX, TensorFlow, and XGBoost.

You can integrate other ML libraries such as [HuggingFace](https://huggingface.co),
[DeepSpeed](https://github.com/microsoft/DeepSpeed), or [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
with Kubeflow Trainer to orchestrate their ML training on Kubernetes.

Kubeflow Trainer allows you to effortlessly develop your LLMs with the Kubeflow Python SDK and
build Kubernetes-native Training Runtimes with Kubernetes Custom Resources APIs.

<img src="/docs/components/trainer/images/trainer-tech-stack.drawio.svg"
  alt="Kubeflow Trainer Tech Stack"
  class="mt-3 mb-3">

## Who is this for

Kubeflow Trainer is designed for two primary user personas, each with specific resources and
responsibilities:

<img src="/docs/components/trainer/images/user-personas.drawio.svg"
  alt="Kubeflow Trainer Personas"
  class="mt-3 mb-3">

### User Personas

Kubeflow Trainer documentation is separated between these user personas:

- [ML Users](/docs/components/trainer/user-guides): engineers and scientists who develop AI models
  using the Kubeflow Python SDK and TrainJob.
- [Cluster Operators](/docs/components/trainer/operator-guides): administrators responsible for managing
  Kubernetes clusters and Kubeflow Training Runtimes.
- [Contributors](/docs/components/trainer/contributor-guides): open source contributors working on
  [Kubeflow Trainer project](https://github.com/kubeflow/trainer).

## Kubeflow Trainer Introduction

Watch the following KubeCon + CloudNativeCon 2024 talk which provides an overview of Kubeflow Trainer:

{{< youtube id="Lgy4ir1AhYw" title="Kubeflow Trainer V2">}}

## Why use Kubeflow Trainer

The Kubeflow Trainer supports key phases on the AI/ML lifecycle, including model training and LLMs
fine-tuning, as shown in the diagram below:

<img src="/docs/components/trainer/images/ml-lifecycle-trainer.drawio.svg"
  alt="AI/ML Lifecycle Trainer"
  class="mt-3 mb-3">

### Key Benefits

- **Simple and Scalable for Distributed Training and LLMs Fine-Tuning**

Effortlessly scale your model training from a single machine to large distributed Kubernetes
clusters using Kubeflow Python APIs and supported Training Runtimes.

- **Extensible and Portable**

Deploy Kubeflow Trainer on any cloud platform with a Kubernetes cluster and integrate your own
ML frameworks in any programming language.

- **Blueprints for LLMs Fine-Tuning**

Fine-tune the latest LLMs on Kubernetes with ready-to-use Kubeflow LLM blueprints.

- **Reduce GPU Cost**

- Kubeflow Trainer implements custom dataset and model initializers to reduce GPU cost by
  offloading I/O tasks to CPU workloads and to streamline assets initialization across distributed
  training nodes.

- **Seamless Kubernetes Integration**

Optimize GPU utilization and gang-scheduling for ML workloads by leveraging Kubernetes projects like
[Kueue](https://kueue.sigs.k8s.io/),
[Coscheduling](https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/pkg/coscheduling/README.md),
[Volcano](https://volcano.sh/en/) or [YuniKorn](https://yunikorn.apache.org/docs/).

## Next steps

Run your first Kubeflow TrainJob by following the
[Getting Started guide](/docs/components/trainer/getting-started/).



================================================
File: content/en/docs/components/trainer/contributor-guides/_index.md
================================================
+++
title = "Contributor Guides"
description = "Documentation for Kubeflow Trainer contributors"
weight = 60
+++

This doc is in progress...



================================================
File: content/en/docs/components/trainer/contributor-guides/community.md
================================================
+++
title = "Community Guide"
description = "How to get involved to Kubeflow Trainer community"
weight = 20
+++



================================================
File: content/en/docs/components/trainer/contributor-guides/contributing.md
================================================
+++
title = "Contributing Guide"
description = "How to contribute to Kubeflow Trainer project"
weight = 10
+++

This doc is in progress...




================================================
File: content/en/docs/components/trainer/legacy-v1/_index.md
================================================
+++
title = "Legacy Kubeflow Training Operator (v1)"
description = "Kubeflow Training Operator V1 Documentation"
weight = 999
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}



================================================
File: content/en/docs/components/trainer/legacy-v1/getting-started.md
================================================
+++
title = "Getting Started"
description = "Get started with the Training Operator"
weight = 30
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This guide describes how to get started with the Training Operator and run a few simple examples.

## Prerequisites

You need to install the following components to run examples:

- The Training Operator control plane [installed](/docs/components/trainer/legacy-v1/installation/#installing-the-control-plane).
- The Training Python SDK [installed](/docs/components/trainer/legacy-v1/installation/#installing-the-python-sdk).

## Getting Started with PyTorchJob

You can create your first Training Operator distributed PyTorchJob using the Python SDK. Define the
training function that implements end-to-end model training. Each Worker will execute this
function on the appropriate Kubernetes Pod. Usually, this function contains logic to
download dataset, create model, and train the model.

The Training Operator will automatically set `WORLD_SIZE` and `RANK` for the appropriate PyTorchJob
worker to perform [PyTorch Distributed Data Parallel (DDP)](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).

If you install the Training Operator as part of the Kubeflow Platform, you can open a new
[Kubeflow Notebook](/docs/components/notebooks/quickstart-guide/) to run this script. If you
install the Training Operator standalone, make sure that you
[configure local `kubeconfig`](https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#programmatic-access-to-the-api)
to access your Kubernetes cluster where you installed the Training Operator.

```python
def train_func():
    import torch
    import torch.nn.functional as F
    from torch.utils.data import DistributedSampler
    from torchvision import datasets, transforms
    import torch.distributed as dist

    # [1] Setup PyTorch DDP. Distributed environment will be set automatically by Training Operator.
    dist.init_process_group(backend="nccl")
    Distributor = torch.nn.parallel.DistributedDataParallel
    local_rank = int(os.getenv("LOCAL_RANK", 0))
    print(
        "Distributed Training for WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}".format(
            dist.get_world_size(),
            dist.get_rank(),
            local_rank,
        )
    )

    # [2] Create PyTorch CNN Model.
    class Net(torch.nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)
            self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)
            self.fc1 = torch.nn.Linear(4 * 4 * 50, 500)
            self.fc2 = torch.nn.Linear(500, 10)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2, 2)
            x = x.view(-1, 4 * 4 * 50)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return F.log_softmax(x, dim=1)

    # [3] Attach model to the correct GPU device and distributor.
    device = torch.device(f"cuda:{local_rank}")
    model = Net().to(device)
    model = Distributor(model)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

    # [4] Setup FashionMNIST dataloader and distribute data across PyTorchJob workers.
    dataset = datasets.FashionMNIST(
        "./data",
        download=True,
        train=True,
        transform=transforms.Compose([transforms.ToTensor()]),
    )
    train_loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=128,
        sampler=DistributedSampler(dataset),
    )

    # [5] Start model Training.
    for epoch in range(3):
        for batch_idx, (data, target) in enumerate(train_loader):
            # Attach Tensors to the device.
            data = data.to(device)
            target = target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()
            if batch_idx % 10 == 0 and dist.get_rank() == 0:
                print(
                    "Train Epoch: {} [{}/{} ({:.0f}%)]\tloss={:.4f}".format(
                        epoch,
                        batch_idx * len(data),
                        len(train_loader.dataset),
                        100.0 * batch_idx / len(train_loader),
                        loss.item(),
                    )
                )


from kubeflow.training import TrainingClient

# Start PyTorchJob with 3 Workers and 1 GPU per Worker (e.g. multi-node, multi-worker job).
TrainingClient().create_job(
    name="pytorch-ddp",
    train_func=train_func,
    num_procs_per_worker="auto",
    num_workers=3,
    resources_per_worker={"gpu": "1"},
)
```

## Getting Started with TFJob

Similar to the PyTorchJob example, you can use the Python SDK to create your first distributed
TensorFlow job. Run the following script to create TFJob with pre-created Docker image:
`docker.io/kubeflow/tf-mnist-with-summaries:latest` that contains
[distributed TensorFlow code](https://github.com/kubeflow/training-operator/tree/e6b4300f9dfebb5c2a3269641c828add367688ee/examples/tensorflow/mnist_with_summaries):

```python
from kubeflow.training import TrainingClient

TrainingClient().create_job(
    name="tensorflow-dist",
    job_kind="TFJob",
    base_image="docker.io/kubeflow/tf-mnist-with-summaries:latest",
    num_workers=3,
)
```

Run the following API to get logs from your TFJob:

```python
TrainingClient().get_job_logs(
    name="tensorflow-dist",
    job_kind="TFJob",
    follow=True,
)
```

## Next steps

- Run the [FashionMNIST example](https://github.com/kubeflow/training-operator/blob/release-1.9/examples/pytorch/image-classification/Train-CNN-with-FashionMNIST.ipynb) with using Training Operator Python SDK.

- Learn more about [the PyTorchJob APIs](/docs/components/trainer/legacy-v1/user-guides/pytorch/).



================================================
File: content/en/docs/components/trainer/legacy-v1/installation.md
================================================
+++
title = "Installation"
description = "How to install the Training Operator"
weight = 20
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This guide describes how to install the Training Operator on your Kubernetes cluster.
The Training Operator is a lightweight Kubernetes controller that orchestrates the
appropriate Kubernetes workloads to perform distributed ML training and fine-tuning.

## Prerequisites

These are the minimal requirements to install the Training Operator:

- Kubernetes >= 1.28
- `kubectl` >= 1.28
- Python >= 3.7

## Installing the Training Operator

You need to install the Training Operator control plane and Python SDK to create training jobs.

### Installing the Control Plane

You can skip these steps if you have already
[installed Kubeflow platform](https://www.kubeflow.org/docs/started/installing-kubeflow/)
using manifests or package distributions. The Kubeflow platform includes the Training Operator.

You can install the Training Operator as a standalone component.

Run the following command to install the stable release of the Training Operator control plane: `v1.8.1`

```shell
kubectl apply --server-side -k "github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=v1.8.1"
```

Run the following command to install the latest changes of Training Operator control plane:

```shell
kubectl apply --server-side -k "github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=master"
```

After installing it, you can verify that Training Operator controller is running as follows:

```shell
$ kubectl get pods -n kubeflow

NAME                                             READY   STATUS    RESTARTS   AGE
training-operator-658c68d697-46zmn               1/1     Running   0          90s
```

Run this command to check installed Kubernetes CRDs for each supported ML framework:

```shell
$ kubectl get crd

mpijobs.kubeflow.org                                     2023-06-09T00:31:07Z
mxjobs.kubeflow.org                                      2023-06-09T00:31:05Z
paddlejobs.kubeflow.org                                  2023-06-09T00:31:09Z
pytorchjobs.kubeflow.org                                 2023-06-09T00:31:06Z
tfjobs.kubeflow.org                                      2023-06-09T00:31:04Z
xgboostjobs.kubeflow.org                                 2023-06-09T00:31:04Z
```

### Installing the Python SDK

The Training Operator [implements a Python SDK](https://pypi.org/project/kubeflow-training/)
to simplify creation of distributed training and fine-tuning jobs.

Run the following command to install the latest stable release of the Training SDK:

```shell
pip install -U kubeflow-training
```

Run the following command to install the latest changes of Training SDK:

```shell
pip install git+https://github.com/kubeflow/training-operator.git@master#subdirectory=sdk/python
```

Otherwise, you can also install the Training SDK using the specific GitHub commit, for example:

```shell
pip install git+https://github.com/kubeflow/training-operator.git@7345e33b333ba5084127efe027774dd7bed8f6e6#subdirectory=sdk/python
```

#### Install the Python SDK with Fine-Tuning Capabilities

If you want to use the `train` API for LLM fine-tuning with the Training Operator, install the Python SDK
with the additional packages from HuggingFace:

```shell
pip install -U "kubeflow-training[huggingface]"
```

## Next steps

Run your first Training Operator Job by following the [Getting Started guide](/docs/components/trainer/legacy-v1/getting-started/).



================================================
File: content/en/docs/components/trainer/legacy-v1/overview.md
================================================
+++
title = "Overview"
description = "An overview of the Training Operator"
weight = 10
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

## What is the Training Operator

The Training Operator is a Kubernetes-native project for fine-tuning and scalable
distributed training of machine learning (ML) models created with different ML frameworks such as
PyTorch, TensorFlow, XGBoost, JAX, and others.

You can integrate other ML libraries such as [HuggingFace](https://huggingface.co),
[DeepSpeed](https://github.com/microsoft/DeepSpeed), or [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
with the Training Operator to orchestrate their ML training on Kubernetes.

The Training Operator allows you to use Kubernetes workloads to effectively train your large models
via Kubernetes Custom Resources APIs or using the Training Operator Python SDK.

The Training Operator implements a centralized Kubernetes controller to orchestrate distributed training jobs.

You can run high-performance computing (HPC) tasks with the Training Operator and MPIJob since it
supports running Message Passing Interface (MPI) on Kubernetes which is heavily used for HPC.
The Training Operator implements the V1 API version of MPI Operator. For the MPI Operator V2 version,
please follow [this guide](/docs/components/trainer/legacy-v1/user-guides/mpi/) to install MPI Operator V2.

<img src="/docs/components/trainer/legacy-v1/images/training-operator-overview.drawio.svg"
  alt="Training Operator Overview"
  class="mt-3 mb-3">

The Training Operator is responsible for scheduling the appropriate Kubernetes workloads to implement
various distributed training strategies for different ML frameworks.

## Why use the Training Operator

The Training Operator addresses the Model Training and Model Fine-Tuning steps in the AI/ML
lifecycle as shown in diagram below:

<img src="/docs/components/trainer/legacy-v1/images/ml-lifecycle-training-operator.drawio.svg"
  alt="AI/ML Lifecycle Training Operator"
  class="mt-3 mb-3">

- **The Training Operator simplifies the ability to run distributed training and fine-tuning.**

You can easily scale their model training from single machine to large-scale distributed
Kubernetes cluster using APIs and interfaces provided by Training Operator.

- **The Training Operator is extensible and portable.**

You can deploy the Training Operator on any cloud where you have Kubernetes cluster and you can
integrate their own ML frameworks written in any programming languages with Training Operator.

- **The Training Operator is integrated with the Kubernetes ecosystem.**

You can leverage Kubernetes advanced scheduling techniques such as Kueue, Volcano, and YuniKorn
with the Training Operator to optimize cost savings for your ML training resources.

## Custom Resources for ML Frameworks

To perform distributed training the Training Operator implements the following
[Custom Resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
for each ML framework:

| ML Framework | Custom Resource                                                       |
| ------------ | --------------------------------------------------------------------- |
| PyTorch      | [PyTorchJob](/docs/components/trainer/legacy-v1/user-guides/pytorch/) |
| TensorFlow   | [TFJob](/docs/components/trainer/legacy-v1/user-guides/tensorflow/)   |
| XGBoost      | [XGBoostJob](/docs/components/trainer/legacy-v1/user-guides/xgboost/) |
| MPI          | [MPIJob](/docs/components/trainer/legacy-v1/user-guides/mpi/)         |
| PaddlePaddle | [PaddleJob](/docs/components/trainer/legacy-v1/user-guides/paddle/)   |
| JAX          | [JAXJob](/docs/components/trainer/legacy-v1/user-guides/jax/)         |

## Next steps

- Follow [the installation guide](/docs/components/trainer/legacy-v1/installation/) to deploy the Training Operator.

- Run examples from [getting started guide](/docs/components/trainer/legacy-v1/getting-started/).



================================================
File: content/en/docs/components/trainer/legacy-v1/explanation/_index.md
================================================
+++
title = "Explanation"
description = "Explanation for Training Operator Features"
weight = 60
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}



================================================
File: content/en/docs/components/trainer/legacy-v1/explanation/fine-tuning.md
================================================
+++
title = "LLM Fine-Tuning with the Training Operator"
description = "Why the Training Operator needs the fine-tuning API"
weight = 10
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page explains how the [Training Operator fine-tuning API](/docs/components/trainer/legacy-v1/user-guides/fine-tuning)
fits into the Kubeflow ecosystem.

In the rapidly evolving landscape of machine learning (ML) and artificial intelligence (AI),
the ability to fine-tune pre-trained models represents a significant leap towards achieving custom
solutions with less effort and time. Fine-tuning allows practitioners to adapt large language models
(LLMs) like BERT or GPT to their specific needs by training these models on custom datasets.
This process maintains the model's architecture and learned parameters while making it more relevant
to particular applications. Whether you're working in natural language processing (NLP),
image classification, or another ML domain, fine-tuning can drastically improve performance and
applicability of pre-existing models to new datasets and problems.

## Why does the Training Operator's Fine-Tuning API Matter ?

The introduction of the Fine-Tuning API in the Training Operator is a game-changer for ML practitioners
operating within the Kubernetes ecosystem. Historically, the Training Operator has streamlined the
orchestration of ML workloads on Kubernetes, making distributed training more accessible. However,
fine-tuning tasks often require extensive manual intervention, including the configuration of
training environments and the distribution of data across nodes. The Fine-Tuning API aims to simplify
this process, offering an easy-to-use Python interface that abstracts away the complexity involved
in setting up and executing fine-tuning tasks on distributed systems.

## The Rationale Behind Kubeflow's Fine-Tuning API

Implementing the Fine-Tuning API within the Training Operator is a logical step in enhancing the platform's
capabilities. By providing this API, Training Operator not only simplifies the user experience for
ML practitioners but also leverages its existing infrastructure for distributed training.
This approach aligns with Kubeflow's mission to democratize distributed ML training, making it more
accessible and less cumbersome for users. The API facilitates a seamless transition from model
development to deployment, supporting the fine-tuning of LLMs on custom datasets without the need
for extensive manual setup or specialized knowledge of Kubernetes internals.

## Roles and Interests

Different user personas can benefit from this feature:

- **MLOps Engineers:** Can leverage this API to automate and streamline the setup and execution of
  fine-tuning tasks, reducing operational overhead.

- **Data Scientists:** Can focus more on model experimentation and less on the logistical aspects of
  distributed training, speeding up the iteration cycle.

- **Business Owners:** Can expect quicker turnaround times for tailored ML solutions, enabling faster
  response to market needs or operational challenges.

- **Platform Engineers:** Can utilize this API to better operationalize the ML toolkit, ensuring
  scalability and efficiency in managing ML workflows.

## Next Steps

- Understand [the architecture behind `train` API](/docs/components/trainer/legacy-v1/reference/fine-tuning).




================================================
File: content/en/docs/components/trainer/legacy-v1/reference/_index.md
================================================
+++
title = "Reference"
description = "Reference docs for the Training Operator"
weight = 50
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}



================================================
File: content/en/docs/components/trainer/legacy-v1/reference/architecture.md
================================================
+++
title = "Architecture"
description = "The Training Operator Architecture"
weight = 10
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

## What is the Training Operator Architecture?

The original design was drafted in April 2021 and is [available here for reference](https://docs.google.com/document/d/1x1JPDQfDMIbnoQRftDH1IzGU0qvHGSU4W6Jl4rJLPhI/).
The goal was to provide a unified Kubernetes operator that supports multiple
machine learning/deep learning frameworks. This was done by having a "Frontend"
operator that decomposes the job into different configurable Kubernetes
components (e.g., Role, PodTemplate, Fault-Tolerance, etc.),
watches all Role Customer Resources, and manages pod performance.
The dedicated "Backend" operator was not implemented and instead
consolidated to the "Frontend" operator.

The benefits of this approach were:

1. Shared testing and release infrastructure
2. Unlocked production grade features like manifests and metadata support
3. Simpler Kubeflow releases
4. A Single Source of Truth (SSOT) for other Kubeflow components to interact with

The V1 Training Operator architecture diagram can be seen in the diagram below:

<img src="/docs/components/trainer/legacy-v1/images/training-operator-v1-architecture.drawio.svg"
  alt="Training Operator V1 Architecture"
  class="mt-3 mb-3">

The diagram displays PyTorchJob and its configured communication methods but it
is worth mentioning that each framework can have its own appraoch(es) to
communicating across pods. Additionally, each framework can have its own set of
configurable resources.

As a concrete example, PyTorch has several
[Communication Backends](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)
available, see the [source code documentation for the full list](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group).
).



================================================
File: content/en/docs/components/trainer/legacy-v1/reference/distributed-training.md
================================================
+++
title = "Distributed Training with the Training Operator"
description = "How the Training Operator performs distributed training on Kubernetes"
weight = 10
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page shows different distributed strategies that can be used by the Training Operator.

## Distributed Training for PyTorch

This diagram shows how the Training Operator creates PyTorch workers for the
[ring all-reduce algorithm](https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/).

<img src="/docs/components/trainer/legacy-v1/images/distributed-pytorchjob.drawio.svg"
  alt="Distributed PyTorchJob"
  class="mt-3 mb-3">

You are responsible for writing the training code using native
[PyTorch Distributed APIs](https://pytorch.org/tutorials/beginner/dist_overview.html)
and creating a PyTorchJob with the required number of workers and GPUs using the Training Operator Python SDK.
Then, the Training Operator creates Kubernetes pods with the appropriate environment variables for the
[`torchrun`](https://pytorch.org/docs/stable/elastic/run.html) CLI to start the distributed
PyTorch training job.

At the end of the ring all-reduce algorithm gradients are synchronized
in every worker (`g1, g2, g3, g4`) and the model is trained.

You can define various distributed strategies supported by PyTorch in your training code
(e.g. [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html)), and the Training Operator will set
the appropriate environment variables for `torchrun`.

## Distributed Training for TensorFlow

This diagram shows how the Training Operator creates the TensorFlow parameter server (PS) and workers for
[PS distributed training](https://www.tensorflow.org/tutorials/distribute/parameter_server_training).

<img src="/docs/components/trainer/legacy-v1/images/distributed-tfjob.drawio.svg"
  alt="Distributed TFJob"
  class="mt-3 mb-3">

You are responsible for writing the training code using native
[TensorFlow Distributed APIs](https://www.tensorflow.org/guide/distributed_training) and creating a
TFJob with the required number of PSs, workers, and GPUs using the Training Operator Python SDK.
Then, the Training Operator creates Kubernetes pods with the appropriate environment variables for
[`TF_CONFIG`](https://www.tensorflow.org/guide/distributed_training#setting_up_the_tf_config_environment_variable)
to start the distributed TensorFlow training job.

The Parameter server splits training data for every worker and averages model weights based on gradients
produced by every worker.

You can define various [distributed strategies supported by TensorFlow](https://www.tensorflow.org/guide/distributed_training#types_of_strategies)
in your training code, and the Training Operator will set the appropriate environment
variables for `TF_CONFIG`.



================================================
File: content/en/docs/components/trainer/legacy-v1/reference/fine-tuning.md
================================================
+++
title = "LLM Fine-Tuning with Training Operator"
description = "How Training Operator performs fine-tuning on Kubernetes"
weight = 10
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page shows how Training Operator implements the
[API to fine-tune LLMs](/docs/components/trainer/legacy-v1/user-guides/fine-tuning).

## Architecture

In the following diagram you can see how `train` Python API works:

<img src="/docs/components/trainer/legacy-v1/images/fine-tune-llm-api.drawio.svg"
  alt="Fine-Tune API for LLMs"
  class="mt-3 mb-3">

- Once user executes `train` API, Training Operator creates PyTorchJob with appropriate resources
  to fine-tune LLM.

- Storage initializer InitContainer is added to the PyTorchJob worker 0 to download
  pre-trained model and dataset with provided parameters.

- PVC with [`ReadOnlyMany` access mode](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)
  it attached to each PyTorchJob worker to distribute model and dataset across Pods. **Note**: Your
  Kubernetes cluster must support volumes with `ReadOnlyMany` access mode, otherwise you can use a
  single PyTorchJob worker.

- Every PyTorchJob worker runs LLM Trainer that fine-tunes model using provided parameters.

Training Operator implements `train` API with these pre-created components:

### Model Provider

Model provider downloads pre-trained model. Currently, Training Operator supports
[HuggingFace model provider](https://github.com/kubeflow/training-operator/blob/6ce4d57d699a76c3d043917bd0902c931f14080f/sdk/python/kubeflow/storage_initializer/hugging_face.py#L56)
that downloads model from HuggingFace Hub.

You can implement your own model provider by using [this abstract base class](https://github.com/kubeflow/training-operator/blob/6ce4d57d699a76c3d043917bd0902c931f14080f/sdk/python/kubeflow/storage_initializer/abstract_model_provider.py#L4)

### Dataset Provider

Dataset provider downloads dataset. Currently, Training Operator supports
[AWS S3](https://github.com/kubeflow/training-operator/blob/6ce4d57d699a76c3d043917bd0902c931f14080f/sdk/python/kubeflow/storage_initializer/s3.py#L37)
and [HuggingFace](https://github.com/kubeflow/training-operator/blob/6ce4d57d699a76c3d043917bd0902c931f14080f/sdk/python/kubeflow/storage_initializer/hugging_face.py#L92)
dataset providers.

You can implement your own dataset provider by using [this abstract base class](https://github.com/kubeflow/training-operator/blob/6ce4d57d699a76c3d043917bd0902c931f14080f/sdk/python/kubeflow/storage_initializer/abstract_dataset_provider.py)

### LLM Trainer

Trainer implements training loop to fine-tune LLM. Currently, Training Operator supports
[HuggingFace trainer](https://github.com/kubeflow/training-operator/blob/6ce4d57d699a76c3d043917bd0902c931f14080f/sdk/python/kubeflow/trainer/hf_llm_training.py#L118-L139)
to fine-tune LLMs.

You can implement your own trainer for other ML use-cases such as image classification,
voice recognition, etc.



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/_index.md
================================================
+++
title = "User Guides"
description = "User guides for Training Operator"
weight = 40
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/fine-tuning.md
================================================
+++
title = "How to Fine-Tune LLMs with Kubeflow"
description = "Overview of the LLM fine-tuning API in the Training Operator"
weight = 10
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page describes how to use a [`train` API from the Training Python SDK](https://github.com/kubeflow/training-operator/blob/release-1.9/sdk/python/kubeflow/training/api/training_client.py#L95)
that simplifies the ability to fine-tune LLMs with distributed PyTorchJob workers.

If you want to learn more about how the fine-tuning API fits in the Kubeflow ecosystem, head to
the [explanation guide](/docs/components/trainer/legacy-v1/explanation/fine-tuning).

## Prerequisites

You need to install the Training Python SDK [with fine-tuning support](/docs/components/trainer/legacy-v1/installation/#install-the-python-sdk-with-fine-tuning-capabilities)
to run this API.

## How to use the Fine-Tuning API?

You need to provide the following parameters to use the `train` API:

- Pre-trained model parameters.
- Dataset parameters.
- Trainer parameters.
- Number of PyTorch workers and resources per workers.

For example, you can use the `train` API to fine-tune the BERT model using the Yelp Review dataset
from HuggingFace Hub with the code below:

```python
import transformers
from peft import LoraConfig

from kubeflow.training import TrainingClient
from kubeflow.storage_initializer.hugging_face import (
    HuggingFaceModelParams,
    HuggingFaceTrainerParams,
    HuggingFaceDatasetParams,
)

TrainingClient().train(
    name="fine-tune-bert",
    # BERT model URI and type of Transformer to train it.
    model_provider_parameters=HuggingFaceModelParams(
        model_uri="hf://google-bert/bert-base-cased",
        transformer_type=transformers.AutoModelForSequenceClassification,
    ),
    # Use 3000 samples from Yelp dataset.
    dataset_provider_parameters=HuggingFaceDatasetParams(
        repo_id="yelp_review_full",
        split="train[:3000]",
    ),
    # Specify HuggingFace Trainer parameters. In this example, we will skip evaluation and model checkpoints.
    trainer_parameters=HuggingFaceTrainerParams(
        training_parameters=transformers.TrainingArguments(
            output_dir="test_trainer",
            save_strategy="no",
            evaluation_strategy="no",
            do_eval=False,
            disable_tqdm=True,
            log_level="info",
        ),
        # Set LoRA config to reduce number of trainable model parameters.
        lora_config=LoraConfig(
            r=8,
            lora_alpha=8,
            lora_dropout=0.1,
            bias="none",
        ),
    ),
    num_workers=4, # nnodes parameter for torchrun command.
    num_procs_per_worker=2, # nproc-per-node parameter for torchrun command.
    resources_per_worker={
        "gpu": 2,
        "cpu": 5,
        "memory": "10G",
    },
)
```

After you execute `train`, the Training Operator will orchestrate the appropriate PyTorchJob resources
to fine-tune the LLM.

## Using custom images with Fine-Tuning API

Platform engineers can customize the storage initializer and trainer images by setting the `STORAGE_INITIALIZER_IMAGE` and `TRAINER_TRANSFORMER_IMAGE` environment variables before executing the `train` command.

For example: In your python code, set the env vars before executing `train`:

```python
...
os.environ['STORAGE_INITIALIZER_IMAGE'] = 'docker.io/<username>/<custom-storage-initiailizer_image>'
os.environ['TRAINER_TRANSFORMER_IMAGE'] = 'docker.io/<username>/<custom-trainer_transformer_image>'

TrainingClient().train(...)
```

## Next Steps

- Run the example to [fine-tune the TinyLlama LLM](https://github.com/kubeflow/training-operator/blob/release-1.9/examples/pytorch/language-modeling/train_api_hf_dataset.ipynb)

- Check this example to compare the `create_job` and the `train` Python API for
  [fine-tuning BERT LLM](https://github.com/kubeflow/training-operator/blob/release-1.9/examples/pytorch/text-classification/Fine-Tune-BERT-LLM.ipynb).

- Understand [the architecture behind `train` API](/docs/components/trainer/legacy-v1/reference/fine-tuning).



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/jax.md
================================================
+++
title = "JAX Training (JAXJob)"
description = "Using JAXJob to train a model with JAX"
weight = 60
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page describes `JAXJob` for training a machine learning model with [JAX](https://jax.readthedocs.io/en/latest/).

The `JAXJob` is a Kubernetes
[custom resource](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
to run JAX training jobs on Kubernetes. The Kubeflow implementation of
the `JAXJob` is in the [`training-operator`](https://github.com/kubeflow/training-operator).

The current custom resource for JAX has been tested to run multiple processes on CPUs using [gloo](https://github.com/facebookincubator/gloo) for communication between CPUs. Worker with replica 0 is recognized as a JAX coordinator. Process 0 will start a JAX coordinator service exposed via the IP address of process 0 in your cluster, together with a port available on that process, to which the other processes in the cluster will connect. We are looking for user feedback to run JAXJob on GPUs and TPUs.

## Creating a JAX training job

You can create a training job by defining a `JAXJob` config file. See the manifests for the [simple JAXJob example](https://github.com/kubeflow/training-operator/blob/release-1.9/examples/jax/cpu-demo/demo.yaml).
You may change the Job config file based on your requirements.

Deploy the `JAXJob` resource to start training:

```
kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/jax/cpu-demo/demo.yaml
```

You should now be able to see the created pods matching the specified number of replicas.

```
kubectl get pods -n kubeflow -l training.kubeflow.org/job-name=jaxjob-simple
```

Distributed computation takes several minutes on a CPU cluster. Logs can be inspected to see its progress.

```
PODNAME=$(kubectl get pods -l training.kubeflow.org/job-name=jaxjob-simple,training.kubeflow.org/replica-type=worker,training.kubeflow.org/replica-index=0 -o name -n kubeflow)
kubectl logs -f ${PODNAME} -n kubeflow
```

```
I1016 14:30:28.956959 139643066051456 distributed.py:106] Starting JAX distributed service on [::]:6666
I1016 14:30:28.959352 139643066051456 distributed.py:119] Connecting to JAX distributed service on jaxjob-simple-worker-0:6666
I1016 14:30:30.633651 139643066051456 xla_bridge.py:895] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I1016 14:30:30.638316 139643066051456 xla_bridge.py:895] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
JAX process 0/1 initialized on jaxjob-simple-worker-0
JAX global devices:[CpuDevice(id=0), CpuDevice(id=131072)]
JAX local devices:[CpuDevice(id=0)]
JAX device count:2
JAX local device count:1
[2.]
```

## Monitoring a JAXJob

```
kubectl get -o yaml jaxjobs jaxjob-simple -n kubeflow
```

See the status section to monitor the job status. Here is sample output when the job is successfully completed.

```yaml
apiVersion: kubeflow.org/v1
kind: JAXJob
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"kubeflow.org/v1","kind":"JAXJob","metadata":{"annotations":{},"name":"jaxjob-simple","namespace":"kubeflow"},"spec":{"jaxReplicaSpecs":{"Worker":{"replicas":2,"restartPolicy":"OnFailure","template":{"spec":{"containers":[{"command":["python3","train.py"],"image":"docker.io/kubeflow/jaxjob-simple:latest","imagePullPolicy":"Always","name":"jax"}]}}}}}}
  creationTimestamp: "2024-09-22T20:07:59Z"
  generation: 1
  name: jaxjob-simple
  namespace: kubeflow
  resourceVersion: "1972"
  uid: eb20c874-44fc-459b-b9a8-09f5c3ff46d3
spec:
  jaxReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - command:
                - python3
                - train.py
              image: docker.io/kubeflow/jaxjob-simple:latest
              imagePullPolicy: Always
              name: jax
status:
  completionTime: "2024-09-22T20:11:34Z"
  conditions:
    - lastTransitionTime: "2024-09-22T20:07:59Z"
      lastUpdateTime: "2024-09-22T20:07:59Z"
      message: JAXJob jaxjob-simple is created.
      reason: JAXJobCreated
      status: "True"
      type: Created
    - lastTransitionTime: "2024-09-22T20:11:28Z"
      lastUpdateTime: "2024-09-22T20:11:28Z"
      message: JAXJob kubeflow/jaxjob-simple is running.
      reason: JAXJobRunning
      status: "False"
      type: Running
    - lastTransitionTime: "2024-09-22T20:11:34Z"
      lastUpdateTime: "2024-09-22T20:11:34Z"
      message: JAXJob kubeflow/jaxjob-simple successfully completed.
      reason: JAXJobSucceeded
      status: "True"
      type: Succeeded
  replicaStatuses:
    Worker:
      selector: training.kubeflow.org/job-name=jaxjob-simple,training.kubeflow.org/operator-name=jaxjob-controller,training.kubeflow.org/replica-type=worker
      succeeded: 2
  startTime: "2024-09-22T20:07:59Z"
```



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/job-scheduling.md
================================================
+++
title = "Job Scheduling"
description = "How to schedule a job with gang-scheduling"
weight = 70
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This guide describes how to use [Kueue](https://kueue.sigs.k8s.io/),
[Volcano Scheduler](https://github.com/volcano-sh/volcano) and
[Scheduler Plugins with coscheduling](https://github.com/kubernetes-sigs/scheduler-plugins/blob/2502825c671063af5b2aa78a1d34b24917f2def4/pkg/coscheduling/README.md)
to support gang-scheduling in Kubeflow, to allow jobs to run multiple pods at the same time.

## Running jobs with gang-scheduling

The Training Operator and the MPI Operator support running jobs with gang-scheduling using Kueue, Volcano Scheduler,
and Scheduler Plugins with coscheduling.

### Using Kueue with Training Operator Jobs

Follow [this guide to learn](https://kueue.sigs.k8s.io/docs/tasks/run/kubeflow/) how to use Kueue
with Training Operator Jobs and manage queues for your ML training jobs

### Scheduler Plugins with coscheduling

You have to install the Scheduler Plugins with coscheduling in your cluster first as the default scheduler or a secondary scheduler of Kubernetes and
configure the operator to select the scheduler name for gang-scheduling in the following:

- training-operator

```diff
...
    spec:
      containers:
        - command:
            - /manager
+           - --gang-scheduler-name=scheduler-plugins
          image: kubeflow/training-operator
          name: training-operator
...
```

- mpi-operator (installed scheduler-plugins as a default scheduler)

```diff
...
    spec:
      containers:
      - args:
+       - --gang-scheduling=default-scheduler
        - -alsologtostderr
        - --lock-namespace=mpi-operator
        image: mpioperator/mpi-operator:0.4.0
        name: mpi-operator
...
```

- mpi-operator (installed scheduler-plugins as a secondary scheduler)

```diff
...
    spec:
      containers:
      - args:
+       - --gang-scheduling=scheduler-plugins-scheduler
        - -alsologtostderr
        - --lock-namespace=mpi-operator
        image: mpioperator/mpi-operator:0.4.0
        name: mpi-operator
...
```

- Follow the [instructions in the kubernetes-sigs/scheduler-plugins repository](https://github.com/kubernetes-sigs/scheduler-plugins/blob/2502825c671063af5b2aa78a1d34b24917f2def4/doc/install.md#install-release-v0249-and-use-coscheduling)
  to install the Scheduler Plugins with coscheduling.

**Note:** The Scheduler Plugins and operator in Kubeflow achieve gang-scheduling by using [PodGroup](https://github.com/kubernetes-sigs/scheduler-plugins/blob/2502825c671063af5b2aa78a1d34b24917f2def4/pkg/coscheduling/README.md#podgroup).
The Operator will create the PodGroup of the job automatically.

If you install the Scheduler Plugins in your cluster as a secondary scheduler,
you need to specify the scheduler name in the CustomJob resources (e.g., TFJob), for example:

```diff
apiVersion: "kubeflow.org/v1"
kind: TFJob
metadata:
  name: tfjob-simple
  namespace: kubeflow
spec:
  tfReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
+         schedulerName: scheduler-plugins-scheduler
          containers:
            - name: tensorflow
              image: kubeflow/tf-mnist-with-summaries:latest
              command:
                - "python"
                - "/var/tf_mnist/mnist_with_summaries.py"
```

If you install the Scheduler Plugins as a default scheduler, you don't need to specify the scheduler name in CustomJob resources (e.g., TFJob).

### Volcano Scheduler

You have to install volcano scheduler in your cluster first as a secondary scheduler of Kubernetes and
configure the operator to select the scheduler name for gang-scheduling in the following:

- training-operator

```diff
...
    spec:
      containers:
        - command:
            - /manager
+           - --gang-scheduler-name=volcano
          image: kubeflow/training-operator
          name: training-operator
...
```

- mpi-operator

```diff
...
    spec:
      containers:
      - args:
+       - --gang-scheduling=volcano
        - -alsologtostderr
        - --lock-namespace=mpi-operator
        image: mpioperator/mpi-operator:0.4.0
        name: mpi-operator
...
```

- Follow the [instructions in the volcano repository](https://github.com/volcano-sh/volcano) to install Volcano.

**Note:** Volcano scheduler and the operator in Kubeflow achieve gang-scheduling by using [PodGroup](https://volcano.sh/en/docs/podgroup/).
Operator will create the PodGroup of the job automatically.

The yaml to use volcano scheduler to schedule your job as a gang is the same as non-gang-scheduler, for example:

```yaml
apiVersion: "kubeflow.org/v1beta1"
kind: "TFJob"
metadata:
  name: "tfjob-gang-scheduling"
spec:
  tfReplicaSpecs:
    Worker:
      replicas: 1
      template:
        spec:
          containers:
            - args:
                - python
                - tf_cnn_benchmarks.py
                - --batch_size=32
                - --model=resnet50
                - --variable_update=parameter_server
                - --flush_stdout=true
                - --num_gpus=1
                - --local_parameter_device=cpu
                - --device=gpu
                - --data_format=NHWC
              image: gcr.io/kubeflow/tf-benchmarks-gpu:v20171202-bdab599-dirty-284af3
              name: tensorflow
              resources:
                limits:
                  nvidia.com/gpu: 1
              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
    PS:
      replicas: 1
      template:
        spec:
          containers:
            - args:
                - python
                - tf_cnn_benchmarks.py
                - --batch_size=32
                - --model=resnet50
                - --variable_update=parameter_server
                - --flush_stdout=true
                - --num_gpus=1
                - --local_parameter_device=cpu
                - --device=cpu
                - --data_format=NHWC
              image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3
              name: tensorflow
              resources:
                limits:
                  cpu: "1"
              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
```

## About gang-scheduling

When using Volcano Scheduler or the Scheduler Plugins with coscheduling to apply gang-scheduling,
a job can run only if there are enough resources for all the pods of the job.
Otherwise, all of the pods will be in a pending state waiting for enough resources.
For example, if a job requiring N pods is created and there are only enough resources to schedule N-2 pods,
then N pods of the job will stay pending.

**Note:** when under high workloads, if a pod of the job dies when the job is still running,
it might give other pods a chance to occupy the resources and cause deadlock.

## Troubleshooting

If you keep getting problems related to RBAC in your volcano scheduler.

You can try to add the following rules into your clusterrole of scheduler used by the volcano scheduler.

```
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
```



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md
================================================
+++
title = "MPI Training (MPIJob)"
description = "Instructions for using MPI for training"
weight = 70
+++

{{% beta-status
  feedbacklink="https://github.com/kubeflow/mpi-operator/issues" %}}

This guide walks you through using MPI for training.

The MPI Operator, `MPIJob`, makes it easy to run allreduce-style distributed training on Kubernetes. Please check out [this blog post](https://medium.com/kubeflow/introduction-to-kubeflow-mpi-operator-and-industry-adoption-296d5f2e6edc) for an introduction to MPI Operator and its industry adoption.

**Note**: `MPIJob` doesn’t work in a user namespace by default because of
Istio [automatic sidecar injection](https://istio.io/v1.3/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection).
In order to get it running, it needs annotation `sidecar.istio.io/inject: "false"`
to disable it for either the `MPIJob` pods or namespace.
To view an example of how to add this annotation to your `yaml` file,
see the [`TFJob` documentation](/docs/components/trainer/legacy-v1/user-guides/tensorflow/).

## Installation

You can deploy the operator with default settings by running the following commands:

```shell
git clone https://github.com/kubeflow/mpi-operator
cd mpi-operator
kubectl apply -f deploy/v2beta1/mpi-operator.yaml
```

Alternatively, follow the [getting started guide](https://www.kubeflow.org/docs/started/installing-kubeflow/) to deploy Kubeflow.

An alpha version of MPI support was introduced with Kubeflow 0.2.0. You must be using a version of Kubeflow newer than 0.2.0.

You can check whether the MPI Job custom resource is installed via:

```
kubectl get crd
```

The output should include `mpijobs.kubeflow.org` like the following:

```
NAME                                       AGE
...
mpijobs.kubeflow.org                       4d
...
```

If it is not included, you can add it as follows using [kustomize](https://github.com/kubernetes-sigs/kustomize):

```bash
git clone https://github.com/kubeflow/mpi-operator
cd mpi-operator
kustomize build manifests/overlays/kubeflow | kubectl apply -f -
```

Note that since Kubernetes v1.14, `kustomize` became a subcommand in `kubectl` so you can also run the following command instead:

Since Kubernetes v1.21, you can use:

```bash
kubectl apply -k manifests/overlays/kubeflow
```

```bash
kubectl kustomize base | kubectl apply -f -
```

## Creating an MPI Job

You can create an MPI job by defining an `MPIJob` config file. See [TensorFlow benchmark example](https://github.com/kubeflow/mpi-operator/blob/master/examples/v2beta1/tensorflow-benchmarks/tensorflow-benchmarks.yaml) config file for launching a multi-node TensorFlow benchmark training job. You may change the config file based on your requirements.

```
cat examples/v2beta1/tensorflow-benchmarks/tensorflow-benchmarks.yaml
```

Deploy the `MPIJob` resource to start training:

```
kubectl apply -f examples/v2beta1/tensorflow-benchmarks/tensorflow-benchmarks.yaml
```

## Scheduling Policy

The MPI Operator supports the [gang-scheduling](/docs/components/trainer/legacy-v1/user-guides/job-scheduling/#running-jobs-with-gang-scheduling).
If you want to modify the PodGroup parameters, you can configure in the following:

```diff
apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: tensorflow-benchmarks
spec:
  slotsPerWorker: 1
  runPolicy:
    cleanPodPolicy: Running
+   schedulingPolicy:
+     minAvailable: 10
+     queue: test-queue
+     minResources:
+       cpu: 3000m
+     priorityClass: high
+     scheduleTimeoutSeconds: 180
  mpiReplicaSpecs:
...
```

In addition, those fields are passed to the PodGroup for the volcano or the coscheduling plugin according to the following:

- `.spec.runPolicy.schedulingPolicy.minAvailable` defines the minimal number of members to run the PodGroup and is passed to `.spec.minMember`.
  When using this field, you must ensure the application supports resizing (e.g., Elastic Horovod).
- `.spec.runPolicy.schedulingPolicy.queue` defines the queue name to allocate resource for the PodGroup. However, iff you use the volcano as a gang scheduler, this is passed to `.spec.queue`.
- `.spec.runPolicy.schedulingPolicy.minResources` defines the minimal resources of members to run the PodGroup and is passed to `.spec.minResources`.
- `.spec.runPolicy.schedulingPolicy.priorityClass` defines the PodGroup's PriorityClass. However, iff you use the volcano as a gang scheduler, this is passed to `.spec.priorityClassName`.
- `.spec.runPolicy.schedulingPolicy.scheduleTimeutSeconds` defines the maximal time of members to wait before run the PodGroup.
  However, iff you use the coscheduling plugin as a gang scheduler, this is passed to `.spec.scheduleTimeutSeconds`.

Also, if you don't set the fields, the MPI Operator populates them based on the following:

volcano:

- `.spec.runPolicy.schedulingPolicy.minAvailable`: The number of a launcher and workers.
- `.spec.runPolicy.schedulingPolicy.queue`: A value of the `scheduling.volcano.sh/group-name` in `.spec.annotations`.
- `.spec.runPolicy.schedulingPolicy.minResources`: Nothing is set.
- `.spec.runPolicy.schedulingPolicy.priorityClass`: It uses the priorityClass for the launcher. If one for the launcher doesn't set, it uses one for the workers.

scheduler-plugins:

- `.spec.runPolicy.schedulingPolicy.minAvailable`: The number of a launcher and workers.
- `.spec.runPolicy.schedulingPolicy.minResources`: The sum of resources defined in all containers.
  However, if the number of replicas (`.spec.mpiReplicaSpecs[Launcher].replicas` + `.spec.mpiReplicaSpecs[Worker].replicas`) is more of minMembers,
  it reorders replicas according to each priorityClass setting in `podSpec.priorityClassName` and then resources with a priority less than minMember will not be added to minResources.
  Note that it doesn't account for the priorityClass specified in podSpec.priorityClassName if the priorityClass doesn't exist in the cluster when it reorders replicas.
- `.spec.runPolicy.schedulingPolicy.scheduleTimeutSeconds`: 0

## Monitoring an MPI Job

Once the `MPIJob` resource is created, you should now be able to see the created pods matching the specified number of GPUs. You can also monitor the job status from the status section. Here is sample output when the job is successfully completed.

```
kubectl get -o yaml mpijobs tensorflow-benchmarks
```

```
apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  creationTimestamp: "2019-07-09T22:15:51Z"
  generation: 1
  name: tensorflow-benchmarks
  namespace: default
  resourceVersion: "5645868"
  selfLink: /apis/kubeflow.org/v1alpha2/namespaces/default/mpijobs/tensorflow-benchmarks
  uid: 1c5b470f-a297-11e9-964d-88d7f67c6e6d
spec:
  runPolicy:
    cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          containers:
          - command:
            - mpirun
            - --allow-run-as-root
            - -np
            - "2"
            - -bind-to
            - none
            - -map-by
            - slot
            - -x
            - NCCL_DEBUG=INFO
            - -x
            - LD_LIBRARY_PATH
            - -x
            - PATH
            - -mca
            - pml
            - ob1
            - -mca
            - btl
            - ^openib
            - python
            - scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py
            - --model=resnet101
            - --batch_size=64
            - --variable_update=horovod
            image: mpioperator/tensorflow-benchmarks:latest
            name: tensorflow-benchmarks
    Worker:
      replicas: 1
      template:
        spec:
          containers:
          - image: mpioperator/tensorflow-benchmarks:latest
            name: tensorflow-benchmarks
            resources:
              limits:
                nvidia.com/gpu: 2
  slotsPerWorker: 2
status:
  completionTime: "2019-07-09T22:17:06Z"
  conditions:
  - lastTransitionTime: "2019-07-09T22:15:51Z"
    lastUpdateTime: "2019-07-09T22:15:51Z"
    message: MPIJob default/tensorflow-benchmarks is created.
    reason: MPIJobCreated
    status: "True"
    type: Created
  - lastTransitionTime: "2019-07-09T22:15:54Z"
    lastUpdateTime: "2019-07-09T22:15:54Z"
    message: MPIJob default/tensorflow-benchmarks is running.
    reason: MPIJobRunning
    status: "False"
    type: Running
  - lastTransitionTime: "2019-07-09T22:17:06Z"
    lastUpdateTime: "2019-07-09T22:17:06Z"
    message: MPIJob default/tensorflow-benchmarks successfully completed.
    reason: MPIJobSucceeded
    status: "True"
    type: Succeeded
  replicaStatuses:
    Launcher:
      succeeded: 1
    Worker: {}
  startTime: "2019-07-09T22:15:51Z"
```

Training should run for 100 steps and takes a few minutes on a GPU cluster. You can inspect the logs to see the training progress. When the job starts, access the logs from the `launcher` pod:

```
PODNAME=$(kubectl get pods -l mpi_job_name=tensorflow-benchmarks,mpi_role_type=launcher -o name)
kubectl logs -f ${PODNAME}
```

```
TensorFlow:  1.14
Model:       resnet101
Dataset:     imagenet (synthetic)
Mode:        training
SingleSess:  False
Batch size:  128 global
             64 per device
Num batches: 100
Num epochs:  0.01
Devices:     ['horovod/gpu:0', 'horovod/gpu:1']
NUMA bind:   False
Data format: NCHW
Optimizer:   sgd
Variables:   horovod

...

40	images/sec: 154.4 +/- 0.7 (jitter = 4.0)	8.280
40	images/sec: 154.4 +/- 0.7 (jitter = 4.1)	8.482
50	images/sec: 154.8 +/- 0.6 (jitter = 4.0)	8.397
50	images/sec: 154.8 +/- 0.6 (jitter = 4.2)	8.450
60	images/sec: 154.5 +/- 0.5 (jitter = 4.1)	8.321
60	images/sec: 154.5 +/- 0.5 (jitter = 4.4)	8.349
70	images/sec: 154.5 +/- 0.5 (jitter = 4.0)	8.433
70	images/sec: 154.5 +/- 0.5 (jitter = 4.4)	8.430
80	images/sec: 154.8 +/- 0.4 (jitter = 3.6)	8.199
80	images/sec: 154.8 +/- 0.4 (jitter = 3.8)	8.404
90	images/sec: 154.6 +/- 0.4 (jitter = 3.7)	8.418
90	images/sec: 154.6 +/- 0.4 (jitter = 3.6)	8.459
100	images/sec: 154.2 +/- 0.4 (jitter = 4.0)	8.372
100	images/sec: 154.2 +/- 0.4 (jitter = 4.0)	8.542
----------------------------------------------------------------
total images/sec: 308.27
```

For a sample that uses Intel MPI, see:

```bash
cat examples/pi/pi-intel.yaml
```

## Exposed Metrics

| Metric name                        | Metric type | Description                          | Labels                                                                      |
| ---------------------------------- | ----------- | ------------------------------------ | --------------------------------------------------------------------------- |
| mpi_operator_jobs_created_total    | Counter     | Counts number of MPI jobs created    |                                                                             |
| mpi_operator_jobs_successful_total | Counter     | Counts number of MPI jobs successful |                                                                             |
| mpi_operator_jobs_failed_total     | Counter     | Counts number of MPI jobs failed     |                                                                             |
| mpi_operator_job_info              | Gauge       | Information about MPIJob             | `launcher`=&lt;launcher-pod-name&gt; <br> `namespace`=&lt;job-namespace&gt; |

### Join Metrics

With [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics), one can join metrics by labels.
For example `kube_pod_info * on(pod,namespace) group_left label_replace(mpi_operator_job_infos, "pod", "$0", "launcher", ".*")`

## Docker Images

We push Docker images of the [mpioperator on Dockerhub](https://hub.docker.com/u/mpioperator) for every release.
You can use the following Dockerfile to build the image yourself:

- [mpi-operator](https://github.com/kubeflow/mpi-operator/blob/master/Dockerfile)

Alternative, you can build the image using make:

```bash
make RELEASE_VERSION=dev IMAGE_NAME=registry.example.com/mpi-operator images
```

This will produce an image with the tag `registry.example.com/mpi-operator:dev`.

## Contributing

Learn more in [CONTRIBUTING](https://github.com/kubeflow/mpi-operator/blob/master/CONTRIBUTING.md).



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/paddle.md
================================================
+++
title = "PaddlePaddle Training (PaddleJob)"
description = "Using PaddleJob to train a model with PaddlePaddle"
weight = 30
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page describes the `PaddleJob` for training a machine learning model with [PaddlePaddle](https://www.paddlepaddle.org.cn/).

The `PaddleJob` is a Kubernetes
[custom resource](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
to run PaddlePaddle training jobs on Kubernetes. The Kubeflow implementation of
the `PaddleJob` is in the [`training-operator`](https://github.com/kubeflow/training-operator).

**Note**: The `PaddleJob` doesn’t work in a user namespace by default because of
Istio [automatic sidecar injection](https://istio.io/v1.3/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection).
In order to get it running, it needs annotation `sidecar.istio.io/inject: "false"`
to disable it for either the `PaddleJob` pods or namespace.
To view an example of how to add this annotation to your `yaml` file,
see the [`TFJob` documentation](/docs/components/trainer/legacy-v1/user-guides/tensorflow/).

## Creating a PaddlePaddle training job

You can create a training job by defining a `PaddleJob` config file. See the manifests for the [distributed example](https://github.com/kubeflow/training-operator/blob/release-1.9/examples/paddlepaddle/simple-cpu.yaml).
You may change the config file based on your requirements.

Deploy the `PaddleJob` resource to start training:

```
kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/paddlepaddle/simple-cpu.yaml
```

You should now be able to see the created pods matching the specified number of replicas.

```
kubectl get pods -l job-name=paddle-simple-cpu -n kubeflow
```

Training takes several minutes on a cpu cluster. Logs can be inspected to see its training progress.

```
PODNAME=$(kubectl get pods -l job-name=paddle-simple-cpu,replica-type=worker,replica-index=0 -o name -n kubeflow)
kubectl logs -f ${PODNAME} -n kubeflow
```

## Monitoring a PaddleJob

```
kubectl get -o yaml paddlejobs paddle-simple-cpu -n kubeflow
```

See the status section to monitor the job status. Here is sample output when the job is successfully completed.

```
apiVersion: kubeflow.org/v1
kind: PaddleJob
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"kubeflow.org/v1","kind":"PaddleJob","metadata":{"annotations":{},"name":"paddle-simple-cpu","namespace":"kubeflow"},"spec":{"paddleReplicaSpecs":{"Worker":{"replicas":2,"restartPolicy":"OnFailure","template":{"spec":{"containers":[{"args":["-m","paddle.distributed.launch","run_check"],"command":["python"],"image":"registry.baidubce.com/paddlepaddle/paddle:2.4.0rc0-cpu","imagePullPolicy":"Always","name":"paddle","ports":[{"containerPort":37777,"name":"master"}]}]}}}}}}
  creationTimestamp: "2022-10-24T03:47:45Z"
  generation: 3
  name: paddle-simple-cpu
  namespace: kubeflow
  resourceVersion: "266235056"
  selfLink: /apis/kubeflow.org/v1/namespaces/kubeflow/paddlejobs/paddle-simple-cpu
  uid: 7ef4f92f-0ed4-4a35-b10a-562b79538cc6
spec:
  paddleReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
          containers:
          - args:
            - -m
            - paddle.distributed.launch
            - run_check
            command:
            - python
            image: registry.baidubce.com/paddlepaddle/paddle:2.4.0rc0-cpu
            imagePullPolicy: Always
            name: paddle
            ports:
            - containerPort: 37777
              name: master
              protocol: TCP
status:
  completionTime: "2022-10-24T04:04:43Z"
  conditions:
  - lastTransitionTime: "2022-10-24T03:47:45Z"
    lastUpdateTime: "2022-10-24T03:47:45Z"
    message: PaddleJob paddle-simple-cpu is created.
    reason: PaddleJobCreated
    status: "True"
    type: Created
  - lastTransitionTime: "2022-10-24T04:04:28Z"
    lastUpdateTime: "2022-10-24T04:04:28Z"
    message: PaddleJob kubeflow/paddle-simple-cpu is running.
    reason: JobRunning
    status: "False"
    type: Running
  - lastTransitionTime: "2022-10-24T04:04:43Z"
    lastUpdateTime: "2022-10-24T04:04:43Z"
    message: PaddleJob kubeflow/paddle-simple-cpu successfully completed.
    reason: JobSucceeded
    status: "True"
    type: Succeeded
  replicaStatuses:
    Worker:
      labelSelector:
        matchLabels:
          group-name: kubeflow.org
          job-name: paddle-simple-cpu
          training.kubeflow.org/job-name: paddle-simple-cpu
          training.kubeflow.org/operator-name: paddlejob-controller
          training.kubeflow.org/replica-type: Worker
      succeeded: 2
  startTime: "2022-10-24T03:47:45Z"
```



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/prometheus.md
================================================
+++
title = "Prometheus Monitoring"
description = "Prometheus Metrics for the Training Operator"
weight = 70
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This guide explains how to monitor Kubeflow training jobs using Prometheus metrics. The Training Operator exposes these metrics, providing essential insights into the status of distributed machine learning workloads.

{{< note >}}
Metrics are only generated in response to specific events. For example, job creation metrics will only appear after a job has been created. If a metric is not visible, it may be because the corresponding event has not occurred yet.
{{< /note >}}

## Prometheus Metrics for Training Operator

The Training Operator includes a built-in `/metrics` endpoint exposes Prometheus metrics. This feature is enabled by default and requires no additional configuration for basic use.

### Configuring Metrics Port

By default, metrics are exposed on port 8080 and can be scraped from any IP address.

If you want to change the default port for metrics exporting and limit which IP address can scrape the metrics, simply add the `metrics-bind-address` argument.

**For example**:

```yaml
# deployment.yaml for the Training Operator
spec:
    containers:
    - command:
        - /manager
        image: kubeflow/training-operator
        name: training-operator
        ports:
        - containerPort: 8080
        - containerPort: 9443
            name: webhook-server
            protocol: TCP
        args:
        - "--metrics-bind-address=192.168.1.100:8082"
```

**Explanation:**

`--metrics-bind-address=192.168.1.100:8082` specifies that metrics are now available on **port 8082**, restricted to the IP address **192.168.1.100**. Alternatively, you can bind the metrics to all interfaces by using **0.0.0.0:8082**.

### Accessing the Metrics

The method to access these metrics may vary depending on your Kubernetes setup and environment. For example, use the following command for local environments:

```
kubectl port-forward -n kubeflow deployment/training-operator 8080:8080
```

Then you'll see metrics in this format via `http://localhost:8080/metrics`:

```
# HELP training_operator_jobs_created_total Counts number of jobs created
# TYPE training_operator_jobs_created_total counter
training_operator_jobs_created_total{framework="tensorflow",job_namespace="kubeflow"} 7
```

## List of Job Metrics

| Metric name                               | Description                     | Labels                   |
| ----------------------------------------- | ------------------------------- | ------------------------ |
| `training_operator_jobs_created_total`    | Total number of jobs created    | `namespace`, `framework` |
| `training_operator_jobs_deleted_total`    | Total number of jobs deleted    | `namespace`, `framework` |
| `training_operator_jobs_successful_total` | Total number of successful jobs | `namespace`, `framework` |
| `training_operator_jobs_failed_total`     | Total number of failed jobs     | `namespace`, `framework` |
| `training_operator_jobs_restarted_total`  | Total number of restarted jobs  | `namespace`, `framework` |

Labels information can be interpreted as follows:
| Label name | Description |
|------------------------------------|---------|--------------------------|
| `namespace` | The Kubernetes namespace where the job is running |
| `framework` | The machine learning framework used (e.g. TensorFlow,PyTorch) |



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/pytorch.md
================================================
+++
title = "PyTorch Training (PyTorchJob)"
description = "Using PyTorchJob to train a model with PyTorch"
weight = 20
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page describes `PyTorchJob` for training a machine learning model with [PyTorch](https://pytorch.org/).

The `PyTorchJob` is a Kubernetes
[custom resource](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
to run PyTorch training jobs on Kubernetes. The Kubeflow implementation of
the `PyTorchJob` is in the [`training-operator`](https://github.com/kubeflow/training-operator).

**Note**: `PyTorchJob` doesn’t work in a user namespace by default because of
Istio [automatic sidecar injection](https://istio.io/v1.3/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection).
In order to get it running, it needs the annotation `sidecar.istio.io/inject: "false"`
to disable it for either `PyTorchJob` pods or the namespace.
To view an example of how to add this annotation to your `yaml` file,
see the [`TFJob` documentation](/docs/components/trainer/legacy-v1/user-guides/tensorflow/).

## Creating a PyTorch training job

You can create a training job by defining a `PyTorchJob` config file. See the manifests for the [distributed MNIST example](https://github.com/kubeflow/training-operator/blob/release-1.9/examples/pytorch/simple.yaml).
You may change the config file based on your requirements.

Deploy the `PyTorchJob` resource to start training:

```
kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/pytorch/simple.yaml
```

You should now be able to see the created pods matching the specified number of replicas.

```
kubectl get pods -l training.kubeflow.org/job-name=pytorch-simple -n kubeflow
```

Training takes 5-10 minutes on a cpu cluster. Logs can be inspected to see its training progress.

```
PODNAME=$(kubectl get pods -l training.kubeflow.org/job-name=pytorch-simple,training.kubeflow.org/replica-type=master,training.kubeflow.org/replica-index=0 -o name -n kubeflow)
kubectl logs -f ${PODNAME} -n kubeflow
```

## Monitoring a PyTorchJob

```
kubectl get -o yaml pytorchjobs pytorch-simple -n kubeflow
```

See the status section to monitor the job status. Here is sample output when the job is successfully completed.

```yaml
apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  clusterName: ""
  creationTimestamp: 2018-12-16T21:39:09Z
  generation: 1
  name: pytorch-tcp-dist-mnist
  namespace: default
  resourceVersion: "15532"
  selfLink: /apis/kubeflow.org/v1/namespaces/default/pytorchjobs/pytorch-tcp-dist-mnist
  uid: 059391e8-017b-11e9-bf13-06afd8f55a5c
spec:
  cleanPodPolicy: None
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
            - image: gcr.io/kubeflow-ci/pytorch-dist-mnist_test:1.0
              name: pytorch
              ports:
                - containerPort: 23456
                  name: pytorchjob-port
              resources: {}
    Worker:
      replicas: 3
      restartPolicy: OnFailure
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
            - image: gcr.io/kubeflow-ci/pytorch-dist-mnist_test:1.0
              name: pytorch
              ports:
                - containerPort: 23456
                  name: pytorchjob-port
              resources: {}
status:
  completionTime: 2018-12-16T21:43:27Z
  conditions:
    - lastTransitionTime: 2018-12-16T21:39:09Z
      lastUpdateTime: 2018-12-16T21:39:09Z
      message: PyTorchJob pytorch-tcp-dist-mnist is created.
      reason: PyTorchJobCreated
      status: "True"
      type: Created
    - lastTransitionTime: 2018-12-16T21:39:09Z
      lastUpdateTime: 2018-12-16T21:40:45Z
      message: PyTorchJob pytorch-tcp-dist-mnist is running.
      reason: PyTorchJobRunning
      status: "False"
      type: Running
    - lastTransitionTime: 2018-12-16T21:39:09Z
      lastUpdateTime: 2018-12-16T21:43:27Z
      message: PyTorchJob pytorch-tcp-dist-mnist is successfully completed.
      reason: PyTorchJobSucceeded
      status: "True"
      type: Succeeded
  replicaStatuses:
    Master: {}
    Worker: {}
  startTime: 2018-12-16T21:40:45Z
```

## Next steps

- Learn about [distributed training](/docs/components/trainer/legacy-v1/reference/distributed-training/) in the Training Operator.

- See how to [run a job with gang-scheduling](/docs/components/trainer/legacy-v1/user-guides/job-scheduling#running-jobs-with-gang-scheduling).



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md
================================================
+++
title = "TensorFlow Training (TFJob)"
description = "Using TFJob to train a model with TensorFlow"
weight = 10
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page describes `TFJob` for training a machine learning model with [TensorFlow](https://www.tensorflow.org/).

## What is TFJob?

`TFJob` is a Kubernetes
[custom resource](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
to run TensorFlow training jobs on Kubernetes. The Kubeflow implementation of
`TFJob` is in the [`training-operator`](https://github.com/kubeflow/training-operator).

**Note**: `TFJob` doesn't work in a user namespace by default because of Istio
[automatic sidecar injection](https://istio.io/v1.3/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection).
In order to get `TFJob` running, it needs the annotation `sidecar.istio.io/inject: "false"`
to disable it for `TFJob` pods.

A `TFJob` is a resource with a YAML representation like the one below (edit to use the container image and command for your own training code):

```yaml
apiVersion: kubeflow.org/v1
kind: TFJob
metadata:
  generateName: tfjob
  namespace: your-user-namespace
spec:
  tfReplicaSpecs:
    PS:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          containers:
            - name: tensorflow
              image: gcr.io/your-project/your-image
              command:
                - python
                - -m
                - trainer.task
                - --batch_size=32
                - --training_steps=1000
    Worker:
      replicas: 3
      restartPolicy: OnFailure
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          containers:
            - name: tensorflow
              image: gcr.io/your-project/your-image
              command:
                - python
                - -m
                - trainer.task
                - --batch_size=32
                - --training_steps=1000
```

If you want to give your `TFJob` pods access to credentials secrets, such as the Google Cloud credentials
automatically created when you do a GKE-based Kubeflow installation, you can mount and use a secret like this:

```yaml
apiVersion: kubeflow.org/v1
kind: TFJob
metadata:
  generateName: tfjob
  namespace: your-user-namespace
spec:
  tfReplicaSpecs:
    PS:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          containers:
            - name: tensorflow
              image: gcr.io/your-project/your-image
              command:
                - python
                - -m
                - trainer.task
                - --batch_size=32
                - --training_steps=1000
              env:
                - name: GOOGLE_APPLICATION_CREDENTIALS
                  value: "/etc/secrets/user-gcp-sa.json"
              volumeMounts:
                - name: sa
                  mountPath: "/etc/secrets"
                  readOnly: true
          volumes:
            - name: sa
              secret:
                secretName: user-gcp-sa
    Worker:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          containers:
            - name: tensorflow
              image: gcr.io/your-project/your-image
              command:
                - python
                - -m
                - trainer.task
                - --batch_size=32
                - --training_steps=1000
              env:
                - name: GOOGLE_APPLICATION_CREDENTIALS
                  value: "/etc/secrets/user-gcp-sa.json"
              volumeMounts:
                - name: sa
                  mountPath: "/etc/secrets"
                  readOnly: true
          volumes:
            - name: sa
              secret:
                secretName: user-gcp-sa
```

If you are not familiar with Kubernetes resources please refer to the page [Understanding Kubernetes Objects](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/).

What makes `TFJob` different from built in [controllers](https://kubernetes.io/docs/concepts/workloads/controllers/) is that the `TFJob` spec is designed to manage
[distributed TensorFlow training jobs](https://www.tensorflow.org/guide/distributed_training).

A distributed TensorFlow job typically contains 0 or more of the following processes

- **Chief** The chief is responsible for orchestrating training and performing tasks
  like checkpointing the model.
- **Ps** The ps are parameter servers; these servers provide a distributed data store
  for the model parameters.
- **Worker** The workers do the actual work of training the model. In some cases,
  worker 0 might also act as the chief.
- **Evaluator** The evaluators can be used to compute evaluation metrics as the model
  is trained.

The field **tfReplicaSpecs** in `TFJob` spec contains a map from the type of
replica (as listed above) to the **TFReplicaSpec** for that replica. **TFReplicaSpec**
consists of 3 fields

- **replicas** The number of replicas of this type to spawn for this `TFJob`.
- **template** A [PodTemplateSpec](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#podtemplatespec-v1-core) that describes the pod to create
  for each replica.

  - **The pod must include a container named `tensorflow`**.

- **restartPolicy** Determines whether pods will be restarted when they exit. The
  allowed values are as follows

  - **Always** means the pod will always be restarted. This policy is good
    for parameter servers since they never exit and should always be restarted
    in the event of failure.
  - **OnFailure** means the pod will be restarted if the pod exits due to failure.

    - A non-zero exit code indicates a failure.
    - An exit code of 0 indicates success and the pod will not be restarted.
    - This policy is good for chief and workers.

  - **ExitCode** means the restart behavior is dependent on the exit code of
    the `tensorflow` container as follows:

    - Exit code `0` indicates the process completed successfully and will
      not be restarted.

    - The following exit codes indicate a permanent error and the container
      will not be restarted:

      - `1`: general errors
      - `2`: misuse of shell builtins
      - `126`: command invoked cannot execute
      - `127`: command not found
      - `128`: invalid argument to exit
      - `139`: container terminated by SIGSEGV (invalid memory reference)

    - The following exit codes indicate a retryable error and the container
      will be restarted:

      - `130`: container terminated by SIGINT (keyboard Control-C)
      - `137`: container received a SIGKILL
      - `143`: container received a SIGTERM

    - Exit code `138` corresponds to SIGUSR1 and is reserved for
      user-specified retryable errors.

    - Other exit codes are undefined and there is no guarantee about the
      behavior.

    For background information on exit codes, see the [GNU guide to
    termination signals](https://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html)
    and the [Linux Documentation
    Project](https://tldp.org/LDP/abs/html/exitcodes.html).

  - **Never** means pods that terminate will never be restarted. This policy
    should rarely be used because Kubernetes will terminate pods for any number
    of reasons (e.g. node becomes unhealthy) and this will prevent the job from
    recovering.

## Running the Mnist example

See the manifests for the [distributed MNIST example](https://github.com/kubeflow/training-operator/blob/release-1.9/examples/tensorflow/simple.yaml). You may change the config file based on your requirements.

Deploy the `TFJob` resource to start training:

```
kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/tensorflow/simple.yaml
```

Monitor the job (see the [detailed guide below](#monitoring-your-job)):

```
kubectl -n kubeflow get tfjob tfjob-simple -o yaml
```

Delete it

```
kubectl -n kubeflow delete tfjob tfjob-simple
```

## Customizing the TFJob

Typically you can change the following values in the `TFJob` yaml file:

1. Change the image to point to the docker image containing your code
1. Change the number and types of replicas
1. Change the resources (requests and limits) assigned to each resource
1. Set any environment variables

   - For example, you might need to configure various environment variables to talk to datastores like GCS or S3

1. Attach PVs if you want to use PVs for storage.

## Using GPUs

To use GPUs your cluster must be configured to use GPUs.

- Nodes must have GPUs attached.
- The Kubernetes cluster must recognize the `nvidia.com/gpu` resource type.
- GPU drivers must be installed on the cluster.
- For more information:
  - [Kubernetes instructions for scheduling GPUs](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)
  - [GKE instructions](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus)
  - [EKS instructions](https://docs.aws.amazon.com/eks/latest/userguide/gpu-ami.html)

To attach GPUs specify the GPU resource on the container in the replicas
that should contain the GPUs; for example.

```yaml
apiVersion: "kubeflow.org/v1"
kind: "TFJob"
metadata:
  name: "tf-smoke-gpu"
spec:
  tfReplicaSpecs:
    PS:
      replicas: 1
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
            - args:
                - python
                - tf_cnn_benchmarks.py
                - --batch_size=32
                - --model=resnet50
                - --variable_update=parameter_server
                - --flush_stdout=true
                - --num_gpus=1
                - --local_parameter_device=cpu
                - --device=cpu
                - --data_format=NHWC
              image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3
              name: tensorflow
              ports:
                - containerPort: 2222
                  name: tfjob-port
              resources:
                limits:
                  cpu: "1"
              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
    Worker:
      replicas: 1
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
            - args:
                - python
                - tf_cnn_benchmarks.py
                - --batch_size=32
                - --model=resnet50
                - --variable_update=parameter_server
                - --flush_stdout=true
                - --num_gpus=1
                - --local_parameter_device=cpu
                - --device=gpu
                - --data_format=NHWC
              image: gcr.io/kubeflow/tf-benchmarks-gpu:v20171202-bdab599-dirty-284af3
              name: tensorflow
              ports:
                - containerPort: 2222
                  name: tfjob-port
              resources:
                limits:
                  nvidia.com/gpu: 1
              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
```

Follow TensorFlow's [instructions](https://www.tensorflow.org/guide/gpu)
for using GPUs.

## Monitoring your job

To get the status of your job

```bash
kubectl -n kubeflow get -o yaml tfjobs tfjob-simple
```

Here is sample output for an example job

```yaml
apiVersion: kubeflow.org/v1
kind: TFJob
metadata:
  creationTimestamp: "2021-09-06T11:48:09Z"
  generation: 1
  name: tfjob-simple
  namespace: kubeflow
  resourceVersion: "5764004"
  selfLink: /apis/kubeflow.org/v1/namespaces/kubeflow/tfjobs/tfjob-simple
  uid: 3a67a9a9-cb89-4c1f-a189-f49f0b581e29
spec:
  tfReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - command:
                - python
                - /var/tf_mnist/mnist_with_summaries.py
              image: gcr.io/kubeflow-ci/tf-mnist-with-summaries:1.0
              name: tensorflow
status:
  completionTime: "2021-09-06T11:49:30Z"
  conditions:
    - lastTransitionTime: "2021-09-06T11:48:09Z"
      lastUpdateTime: "2021-09-06T11:48:09Z"
      message: TFJob tfjob-simple is created.
      reason: TFJobCreated
      status: "True"
      type: Created
    - lastTransitionTime: "2021-09-06T11:48:12Z"
      lastUpdateTime: "2021-09-06T11:48:12Z"
      message: TFJob kubeflow/tfjob-simple is running.
      reason: TFJobRunning
      status: "False"
      type: Running
    - lastTransitionTime: "2021-09-06T11:49:30Z"
      lastUpdateTime: "2021-09-06T11:49:30Z"
      message: TFJob kubeflow/tfjob-simple successfully completed.
      reason: TFJobSucceeded
      status: "True"
      type: Succeeded
  replicaStatuses:
    Worker:
      succeeded: 2
  startTime: "2021-09-06T11:48:10Z"
```

### Conditions

A `TFJob` has a `TFJobStatus`, which has an array of `TFJobConditions` through which the `TFJob` has or has not passed.
Each element of the `TFJobCondition` array has six possible fields:

- The **lastUpdateTime** field provides the last time this condition was updated.
- The **lastTransitionTime** field provides the last time the condition transitioned from one status to another.
- The **message** field is a human readable message indicating details about the transition.
- The **reason** field is a unique, one-word, CamelCase reason for the condition's
  last transition.
- The **status** field is a string with possible values "True", "False", and "Unknown".
- The **type** field is a string with the following possible values:
  - **TFJobCreated** means the `TFJob` has been accepted by the system,
    but one or more of the pods/services has not been started.
  - **TFJobRunning** means all sub-resources (e.g. services/pods) of this `TFJob`
    have been successfully scheduled and launched and the job is running.
  - **TFJobRestarting** means one or more sub-resources (e.g. services/pods)
    of this `TFJob` had a problem and is being restarted.
  - **TFJobSucceeded** means the job completed successfully.
  - **TFJobFailed** means the job has failed.

Success or failure of a job is determined as follows

- If a job has a **chief**, success or failure is determined by the status
  of the chief.
- If a job has no chief, success or failure is determined by the workers.
- In both cases, the `TFJob` succeeds if the process being monitored exits
  with exit code 0.
- In the case of non-zero exit code, the behavior is determined by the restartPolicy
  for the replica.
- If the restartPolicy allows for restarts then the process will just be restarted and the `TFJob` will continue to execute.
  - For the restartPolicy ExitCode the behavior is exit code dependent.
  - If the restartPolicy doesn't allow restarts a non-zero exit code is considered
    a permanent failure and the job is marked failed.

### tfReplicaStatuses

tfReplicaStatuses provides a map indicating the number of pods for each
replica in a given state. There are three possible states

- **Active** is the number of currently running pods.
- **Succeeded** is the number of pods that completed successfully.
- **Failed** is the number of pods that completed with an error.

### Events

During execution, `TFJob` will emit events to indicate whats happening such
as the creation/deletion of pods and services. Kubernetes doesn't retain
events older than 1 hour by default. To see recent events for a job run

```
kubectl -n kubeflow describe tfjobs tfjob-simple
```

which will produce output like

```
Name:         tfjob-simple
Namespace:    kubeflow
Labels:       <none>
Annotations:  <none>
API Version:  kubeflow.org/v1
Kind:         TFJob
Metadata:
  Creation Timestamp:  2021-09-06T11:48:09Z
  Generation:          1
  Managed Fields:
    API Version:  kubeflow.org/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        .:
        f:tfReplicaSpecs:
          .:
          f:Worker:
            .:
            f:replicas:
            f:restartPolicy:
            f:template:
              .:
              f:spec:
    Manager:      kubectl-create
    Operation:    Update
    Time:         2021-09-06T11:48:09Z
    API Version:  kubeflow.org/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        f:runPolicy:
          .:
          f:cleanPodPolicy:
        f:successPolicy:
        f:tfReplicaSpecs:
          f:Worker:
            f:template:
              f:metadata:
              f:spec:
                f:containers:
      f:status:
        .:
        f:completionTime:
        f:conditions:
        f:replicaStatuses:
          .:
          f:Worker:
            .:
            f:succeeded:
        f:startTime:
    Manager:         manager
    Operation:       Update
    Time:            2021-09-06T11:49:30Z
  Resource Version:  5764004
  Self Link:         /apis/kubeflow.org/v1/namespaces/kubeflow/tfjobs/tfjob-simple
  UID:               3a67a9a9-cb89-4c1f-a189-f49f0b581e29
Spec:
  Tf Replica Specs:
    Worker:
      Replicas:        2
      Restart Policy:  OnFailure
      Template:
        Spec:
          Containers:
            Command:
              python
              /var/tf_mnist/mnist_with_summaries.py
            Image:  gcr.io/kubeflow-ci/tf-mnist-with-summaries:1.0
            Name:   tensorflow
Status:
  Completion Time:  2021-09-06T11:49:30Z
  Conditions:
    Last Transition Time:  2021-09-06T11:48:09Z
    Last Update Time:      2021-09-06T11:48:09Z
    Message:               TFJob tfjob-simple is created.
    Reason:                TFJobCreated
    Status:                True
    Type:                  Created
    Last Transition Time:  2021-09-06T11:48:12Z
    Last Update Time:      2021-09-06T11:48:12Z
    Message:               TFJob kubeflow/tfjob-simple is running.
    Reason:                TFJobRunning
    Status:                False
    Type:                  Running
    Last Transition Time:  2021-09-06T11:49:30Z
    Last Update Time:      2021-09-06T11:49:30Z
    Message:               TFJob kubeflow/tfjob-simple successfully completed.
    Reason:                TFJobSucceeded
    Status:                True
    Type:                  Succeeded
  Replica Statuses:
    Worker:
      Succeeded:  2
  Start Time:     2021-09-06T11:48:10Z
Events:
  Type    Reason                   Age                    From              Message
  ----    ------                   ----                   ----              -------
  Normal  SuccessfulCreatePod      7m9s                   tfjob-controller  Created pod: tfjob-simple-worker-0
  Normal  SuccessfulCreatePod      7m8s                   tfjob-controller  Created pod: tfjob-simple-worker-1
  Normal  SuccessfulCreateService  7m8s                   tfjob-controller  Created service: tfjob-simple-worker-0
  Normal  SuccessfulCreateService  7m8s                   tfjob-controller  Created service: tfjob-simple-worker-1
  Normal  ExitedWithCode           5m48s (x3 over 5m48s)  tfjob-controller  Pod: kubeflow.tfjob-simple-worker-1 exited with code 0
  Normal  ExitedWithCode           5m48s                  tfjob-controller  Pod: kubeflow.tfjob-simple-worker-0 exited with code 0
  Normal  TFJobSucceeded           5m48s                  tfjob-controller  TFJob kubeflow/tfjob-simple successfully completed.
```

Here the events indicate that the pods and services were successfully created.

## TensorFlow Logs

Logging follows standard K8s logging practices.

You can use kubectl to get standard output/error for any pods
that haven't been **deleted**.

First find the pod created by the job controller for the replica of
interest. Pods will be named

```
${JOBNAME}-${REPLICA-TYPE}-${INDEX}
```

Once you've identified your pod you can get the logs using kubectl.

```
kubectl logs ${PODNAME}
```

The **CleanPodPolicy** in the `TFJob` spec controls deletion of pods when a job terminates.
The policy can be one of the following values

- The **Running** policy means that only pods still running when a job completes
  (e.g. parameter servers) will be deleted immediately; completed pods will
  not be deleted so that the logs will be preserved. This is the default value.
- The **All** policy means all pods even completed pods will be deleted immediately
  when the job finishes.
- The **None** policy means that no pods will be deleted when the job completes.

If your cluster takes advantage of Kubernetes
[cluster logging](https://kubernetes.io/docs/concepts/cluster-administration/logging/)
then your logs may also be shipped to an appropriate data store for
further analysis.

### Stackdriver on GKE

Using the Stackdriver UI you can use a query like

```
resource.type="k8s_container"
resource.labels.cluster_name="${CLUSTER}"
metadata.userLabels.job-name="${JOB_NAME}"
metadata.userLabels.replica-type="${TYPE}"
metadata.userLabels.replica-index="${INDEX}"
```

Alternatively using gcloud

```
QUERY="resource.type=\"k8s_container\" "
QUERY="${QUERY} resource.labels.cluster_name=\"${CLUSTER}\" "
QUERY="${QUERY} metadata.userLabels.job-name=\"${JOB_NAME}\" "
QUERY="${QUERY} metadata.userLabels.replica-type=\"${TYPE}\" "
QUERY="${QUERY} metadata.userLabels.replica-index=\"${INDEX}\" "
gcloud --project=${PROJECT} logging read  \
     --freshness=24h \
     --order asc  ${QUERY}
```

## Troubleshooting

Here are some steps to follow to troubleshoot your job

1. Is a status present for your job? Run the command

   ```
   kubectl -n ${USER_NAMESPACE} get tfjobs -o yaml ${JOB_NAME}
   ```

   - **USER_NAMESPACE** is the namespace created for your user profile.

   - If the resulting output doesn't include a status for your job then this typically
     indicates the job spec is invalid.

   - If the `TFJob` spec is invalid there should be a log message in the tf operator logs

     ```
     kubectl -n ${KUBEFLOW_NAMESPACE} logs `kubectl get pods --selector=name=tf-job-operator -o jsonpath='{.items[0].metadata.name}'`
     ```

     - **KUBEFLOW_NAMESPACE** Is the namespace you deployed the `TFJob` operator in.

1. Check the events for your job to see if the pods were created

   - There are a number of ways to get the events; if your job is less than **1 hour old**
     then you can do

     ```
     kubectl -n ${USER_NAMESPACE} describe tfjobs ${JOB_NAME}
     ```

   - The bottom of the output should include a list of events emitted by the job; e.g.

     ```yaml
     Events:
      Type    Reason                   Age   From              Message
      ----    ------                   ----  ----              -------
      Normal  SuccessfulCreatePod      90s   tfjob-controller  Created pod: tfjob2-worker-0
      Normal  SuccessfulCreatePod      90s   tfjob-controller  Created pod: tfjob2-ps-0
      Normal  SuccessfulCreateService  90s   tfjob-controller  Created service: tfjob2-worker-0
      Normal  SuccessfulCreateService  90s   tfjob-controller  Created service: tfjob2-ps-0
     ```

   - Kubernetes only preserves events for **1 hour** (see [kubernetes/kubernetes#52521](https://github.com/kubernetes/kubernetes/issues/52521))

     - Depending on your cluster setup events might be persisted to external storage and accessible for longer periods
     - On GKE events are persisted in stackdriver and can be accessed using the instructions in the previous section.

   - If the pods and services aren't being created then this suggests the `TFJob` isn't being processed; common causes are

     - The `TFJob` spec is invalid (see above)
     - The `TFJob` operator isn't running

1. Check the events for the pods to ensure they are scheduled.

   - There are a number of ways to get the events; if your pod is less than **1 hour old**
     then you can do

     ```
      kubectl -n ${USER_NAMESPACE} describe pods ${POD_NAME}
     ```

   - The bottom of the output should contain events like the following

     ```
     Events:
     Type    Reason                 Age   From                                                  Message
     ----    ------                 ----  ----                                                  -------
     Normal  Scheduled              18s   default-scheduler                                     Successfully assigned tfjob2-ps-0 to gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt
     Normal  SuccessfulMountVolume  17s   kubelet, gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt  MountVolume.SetUp succeeded for volume "default-token-h8rnv"
     Normal  Pulled                 17s   kubelet, gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt  Container image "gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3" already present on machine
     Normal  Created                17s   kubelet, gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt  Created container
     Normal  Started                16s   kubelet, gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt  Started container
     ```

   - Some common problems that can prevent a container from starting are
     - Insufficient resources to schedule the pod
     - The pod tries to mount a volume (or secret) that doesn't exist or is unavailable
     - The docker image doesn't exist or can't be accessed (e.g due to permission issues)

1. If the containers start; check the logs of the containers following the instructions
   in the previous section.

## Next steps

- Learn about [distributed training](/docs/components/trainer/legacy-v1/reference/distributed-training/) in Training Operator.

- See how to [run a job with gang-scheduling](/docs/components/trainer/legacy-v1/user-guides/job-scheduling/#running-jobs-with-gang-scheduling).



================================================
File: content/en/docs/components/trainer/legacy-v1/user-guides/xgboost.md
================================================
+++
title = "XGBoost Training (XGBoostJob)"
description = "Using XGBoostJob to train a model with XGBoost"
weight = 50
+++

{{% alert title="Old Version" color="warning" %}}
This page is about **Kubeflow Training Operator V1**, for the latest information check
[the Kubeflow Trainer V2 documentation](/docs/components/trainer).

Follow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).
{{% /alert %}}

This page describes the `XGBoostJob` for training a machine learning model with [XGBoost](https://github.com/dmlc/xgboost).

`XGBoostJob` is a Kubernetes
[custom resource](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
to run XGBoost training jobs on Kubernetes. The Kubeflow implementation of
`XGBoostJob` is in the [`training-operator`](https://github.com/kubeflow/training-operator).

**Note**: `XGBoostJob` doesn’t work in a user namespace by default because of
Istio [automatic sidecar injection](https://istio.io/v1.3/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection).
In order to get it running, it needs the annotation `sidecar.istio.io/inject: "false"`
to disable it for either the `PyTorchJob` pods or namespace.
To view an example of how to add this annotation to your `yaml` file,
see the [`XGBoostJob` documentation](/docs/components/trainer/legacy-v1/user-guides/tensorflow/).

## Creating a XGBoost training job

You can create a training job by defining an `XGboostJob` config file. See the
manifests for the [IRIS example](https://github.com/kubeflow/training-operator/blob/release-1.9/examples/xgboost/xgboostjob.yaml).
You may change the config file based on your requirements. E.g.: add `CleanPodPolicy`
in Spec to `None` to retain pods after job termination.

```
cat xgboostjob.yaml
```

Deploy the `XGBoostJob` resource to start training:

```
kubectl create -f xgboostjob.yaml
```

You should now be able to see the created pods matching the specified number of replicas.

```
kubectl get pods -l job-name=xgboost-dist-iris-test-train
```

Training takes 5-10 minutes on a cpu cluster. Logs can be inspected to see its training progress.

```
PODNAME=$(kubectl get pods -l job-name=xgboost-dist-iris-test-train,replica-type=master,replica-index=0 -o name)
kubectl logs -f ${PODNAME}
```

## Monitoring a XGBoostJob

```
kubectl get -o yaml xgboostjobs xgboost-dist-iris-test-train
```

See the status section to monitor the job status. Here is sample output when the job is successfully completed.

```yaml
apiVersion: kubeflow.org/v1
kind: XGBoostJob
metadata:
  creationTimestamp: "2021-09-06T18:34:06Z"
  generation: 1
  name: xgboost-dist-iris-test-train
  namespace: default
  resourceVersion: "5844304"
  selfLink: /apis/kubeflow.org/v1/namespaces/default/xgboostjobs/xgboost-dist-iris-test-train
  uid: a1ea6675-3cb5-482b-95dd-68b2c99b8adc
spec:
  runPolicy:
    cleanPodPolicy: None
  xgbReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        spec:
          containers:
            - args:
                - --job_type=Train
                - --xgboost_parameter=objective:multi:softprob,num_class:3
                - --n_estimators=10
                - --learning_rate=0.1
                - --model_path=/tmp/xgboost-model
                - --model_storage_type=local
              image: docker.io/merlintang/xgboost-dist-iris:1.1
              imagePullPolicy: Always
              name: xgboost
              ports:
                - containerPort: 9991
                  name: xgboostjob-port
                  protocol: TCP
    Worker:
      replicas: 2
      restartPolicy: ExitCode
      template:
        spec:
          containers:
            - args:
                - --job_type=Train
                - --xgboost_parameter="objective:multi:softprob,num_class:3"
                - --n_estimators=10
                - --learning_rate=0.1
              image: docker.io/merlintang/xgboost-dist-iris:1.1
              imagePullPolicy: Always
              name: xgboost
              ports:
                - containerPort: 9991
                  name: xgboostjob-port
                  protocol: TCP
status:
  completionTime: "2021-09-06T18:34:23Z"
  conditions:
    - lastTransitionTime: "2021-09-06T18:34:06Z"
      lastUpdateTime: "2021-09-06T18:34:06Z"
      message: xgboostJob xgboost-dist-iris-test-train is created.
      reason: XGBoostJobCreated
      status: "True"
      type: Created
    - lastTransitionTime: "2021-09-06T18:34:06Z"
      lastUpdateTime: "2021-09-06T18:34:06Z"
      message: XGBoostJob xgboost-dist-iris-test-train is running.
      reason: XGBoostJobRunning
      status: "False"
      type: Running
    - lastTransitionTime: "2021-09-06T18:34:23Z"
      lastUpdateTime: "2021-09-06T18:34:23Z"
      message: XGBoostJob xgboost-dist-iris-test-train is successfully completed.
      reason: XGBoostJobSucceeded
      status: "True"
      type: Succeeded
  replicaStatuses:
    Master:
      succeeded: 1
    Worker:
      succeeded: 2
```



================================================
File: content/en/docs/components/trainer/operator-guides/_index.md
================================================
+++
title = "Operator Guides"
description = "Documentation for cluster operators of Kubeflow Trainer"
weight = 50
+++



================================================
File: content/en/docs/components/trainer/operator-guides/installation.md
================================================
+++
title = "Installation"
description = "How to install Kubeflow Trainer control plane"
weight = 10
+++

This guide describes how to install Kubeflow Trainer control plane on a Kubernetes cluster.

You can skip these steps if [the Kubeflow platform](https://www.kubeflow.org/docs/started/installing-kubeflow/)
is already deployed using manifests or package distributions, as it includes Kubeflow Trainer by default.

## Prerequisites

These are the minimal requirements to install Kubeflow Trainer control plane:

- Kubernetes >= 1.28
- `kubectl` >= 1.28

{{% alert title="Tip" color="primary" %}}
If you don't have Kubernetes cluster, you can quickly create one locally using [Kind](https://kind.sigs.k8s.io/docs/user/quick-start#installing-with-a-package-manager):

```bash
kind create cluster # or minikube start
```

{{% /alert %}}

## Installing the Kubeflow Trainer Controller Manager

Run the following command to deploy the Kubeflow Trainer controller manager:

```bash
kubectl apply --server-side -k "https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=master"
```

Ensure that the JobSet and Trainer controller manager pods are running:

```bash
$ kubectl get pods -n kubeflow-system

NAME                                                  READY   STATUS    RESTARTS   AGE
jobset-controller-manager-54968bd57b-88dk4            2/2     Running   0          65s
kubeflow-trainer-controller-manager-cc6468559-dblnw   1/1     Running   0          65s
```

## Installing the Kubeflow Training Runtimes

Run the following command to deploy the Kubeflow Training Runtimes:

```bash
kubectl apply --server-side -k "https://github.com/kubeflow/trainer.git/manifests/overlays/runtimes?ref=master"
```

## Next Steps

- How to [migrate from Kubeflow Training Operator V1](/docs/components/trainer/operator-guides/migration).



================================================
File: content/en/docs/components/trainer/operator-guides/migration.md
================================================
+++
title = "Migrating to Kubeflow Trainer V2"
description = "How to migrate to the new Kubeflow Trainer V2."
weight = 20
+++

## Overview

Kubeflow Trainer is a significant update to the Kubeflow Training Operator project.

The key features introduced by Kubeflow Trainer are:

- The new CRDs: TrainJob, TrainingRuntime, and ClusterTrainingRuntime APIs. These APIs enable the
  creation of templates for distributed model training and LLM fine-tuning. It abstracts the
  Kubernetes complexities, providing more intuitive experience for data scientists and ML engineers.

- The Kubeflow Python SDK: to further enhance ML user experience and to provide seamless integration
  with Kubeflow Trainer APIs.

- Custom dataset and model initializer: to streamline assets initialization across distributed
  training nodes and to reduce GPU cost by offloading I/O tasks to CPU workloads.

- Enhanced MPI support: featuring MPI-Operator V2 features with SSH-based optimization to boost
  MPI performance.

## Migration Paths

TODO (andreyvelich): Add docs for migration.



================================================
File: content/en/docs/components/trainer/user-guides/_index.md
================================================
+++
title = "User Guides"
description = "Documentation for ML users of Kubeflow Trainer"
weight = 40
+++



================================================
File: content/en/docs/components/trainer/user-guides/pytorch.md
================================================
+++
title = "PyTorch Guide"
description = "How to develop PyTorch models with Kubeflow Trainer"
weight = 10
+++

This doc is in progress...



================================================
File: content/en/docs/distributions/_index.md
================================================
+++
title = "Distributions"
description = "Distributions of Kubeflow"
weight = 40
+++


================================================
File: content/en/docs/distributions/list.md
================================================
+++
title = "List of Kubeflow Distributions"
description = "A list showing packaged distributions of Kubeflow"
weight = 10
manualLinkRelref = "../started/installing-kubeflow.md#packaged-distributions"
icon = "fa-solid fa-arrow-up-right-from-square"
+++

We maintain a list showing __packaged distributions of Kubeflow__ on the [Installing Kubeflow](/docs/started/installing-kubeflow/#packaged-distributions) page.


================================================
File: content/en/docs/external-add-ons/_index.md
================================================
+++
title = "External Add-Ons"
description = "Externally developed projects that integrate with Kubeflow"
weight = 30
+++

{{% alert title="Ownership of External Add-Ons" color="dark" %}}
These add-ons are <strong>not owned or maintained</strong> by the Kubeflow project, they are developed and supported by their respective maintainers.
{{% /alert %}}


================================================
File: content/en/docs/external-add-ons/elyra/_index.md
================================================
+++
title = "Elyra"
description = "Elyra | JupyterLab UI for Kubeflow Pipelines"
weight = 30
+++


================================================
File: content/en/docs/external-add-ons/elyra/github.md
================================================
+++
title = "GitHub Repository"
description = "LINK | GitHub repository for Elyra"
weight = 999
manualLink = "https://github.com/elyra-ai/elyra"
icon = "fa-brands fa-github"
+++

Elyra is developed in the [`elyra-ai/elyra`](https://github.com/elyra-ai/elyra) repository.


================================================
File: content/en/docs/external-add-ons/elyra/introduction.md
================================================
+++
title = "Introduction"
description = "A brief introduction to Elyra"
weight = 10
+++

## What is Elyra?

[Elyra](https://elyra.readthedocs.io/en/stable/index.html) is an [open-source](https://github.com/elyra-ai/elyra) tool to reduce model development life cycle complexities. 
Elyra is a _JupyterLab extension_ that provides a _visual pipeline editor_ to enable low-code creation of pipelines that can be executed with Kubeflow Pipelines.

The following screenshot shows an example of a Pipeline created with Elyra:

<img src="/docs/external-add-ons/elyra/images/elyra-pipeline-screenshot.png"
     alt="An example of a Pipeline created with Elyra"
     class="p-2"></img>

## How to use Elyra with Kubeflow?

Elyra can be used with Kubeflow to create and run Pipelines in a Kubeflow environment.

You may create a [custom Kubeflow Notebook image](/docs/components/notebooks/container-images/#custom-images) based on any of our pre-built Jupyter Notebook images and install Elyra in it.
The Elyra project has an [example in their documentation](https://elyra.readthedocs.io/en/stable/recipes/using-elyra-with-kubeflow-notebook-server.html) and a [`Dockerfile`](https://github.com/elyra-ai/elyra/blob/main/etc/docker/kubeflow/Dockerfile) that you can use as a reference.

{{% alert title="Elyra and JupyterLab 4.0" color="warning" %}}
Elyra [`3.15.0`](https://github.com/elyra-ai/elyra/releases/tag/v3.15.0) may not properly support JupyterLab 4.0, which has been included in the default Kubeflow Notebook images since Kubeflow 1.9.0.
{{% /alert %}}

## Next steps

- Visit the <a href="https://github.com/elyra-ai/elyra" target="_blank">Elyra GitHub Repository</a>
- <a href="https://elyra.readthedocs.io/en/stable/recipes/using-elyra-with-kubeflow-notebook-server.html" target="_blank">Use Elyra in Kubeflow Notebooks</a>



================================================
File: content/en/docs/external-add-ons/elyra/website.md
================================================
+++
title = "Elyra Website"
description = "LINK | Elyra Documentation Website"
weight = 998
manualLink = "https://elyra.readthedocs.io/"
icon = "fa-solid fa-arrow-up-right-from-square"
+++

Elyra has its own documentation website hosted at [`elyra.readthedocs.io/`](https://elyra.readthedocs.io/).



================================================
File: content/en/docs/external-add-ons/feast/_index.md
================================================
+++
title = "Feast"
description = "Feast | Feature Store"
weight = 20
+++



================================================
File: content/en/docs/external-add-ons/feast/github.md
================================================
+++
title = "GitHub Repository"
description = "LINK | GitHub repository for Feast"
weight = 999
manualLink = "https://github.com/feast-dev/feast"
icon = "fa-brands fa-github"
+++

Feast is developed in the [`feast-dev/feast`](https://github.com/feast-dev/feast) repository.


================================================
File: content/en/docs/external-add-ons/feast/introduction.md
================================================
+++
title = "Introduction"
description = "A brief introduction to Feast and Feature Stores"
weight = 10
+++

## What is Feast?

[Feast](https://docs.feast.dev/) is an [open-source](https://github.com/feast-dev/feast) feature store that helps teams operate ML systems at scale by allowing them to define, manage, validate, and serve features to models in production. 

The following diagram shows the architecture of Feast:

<img src="/docs/external-add-ons/feast/images/feast-architecture.png" 
     alt="Feast architecture diagram"
     class="p-2"></img>

Feast provides the following functionality:

- __Load streaming, batch, and request-time data__: Feast is built to be able to ingest data from a variety of bounded or unbounded sources. 
  Feast allows users to ingest data from api-calls, streams, object stores, databases, or notebooks.
  Data that is ingested into Feast can be persisted in both online store and historical stores, which in turn is used for the creation of training datasets and serving features to online systems.

- __Standardized definitions__: Feast becomes the single source of truth for all feature definitions and data within an organization.
  Teams are able to capture documentation, metadata, and metrics about features. 
  This allows teams to communicate clearly about features, test feature data, and determine if a feature is both safe and relevant to their use cases.

- __Historical serving__: Features that are persisted in Feast can be retrieved through its feature serving APIs to produce training datasets. 
  Feast is able to produce massive training datasets that are agnostics of the data source that was used to ingest the data originally. 
  Feast is also able to ensure point-in-time correctness when joining these data sources, which in turn ensures the quality and consistency of features reaching models.

- __Online serving__: Feast exposes low latency serving APIs for all data that has been ingested into the system. 
  This allows all production ML systems to use Feast as the primary data source when looking up real-time features.

- __Consistency between training and serving__: Feast provides a consistent view of feature data through the use of a unified ingestion layer, unified serving API and canonical feature references. 
  By building ML systems on feature references, teams abstract away the underlying data infrastructure and make it possible to safely move models between training and serving without a drop in data consistency.

- __Feature sharing and reuse__: Feast provides a discovery and metadata API that allows teams to track, share, and reuse features across projects. 
  Feast also provides a light-weight web UI to expose this metadata.
  Feast also decouples the process of creating features from the process of consumption, meaning teams that start new projects can begin by simply consuming features that already exist in the store, instead of starting from scratch.

- __Statistics and validation__: Feast allows for the generation of statistics based on features within the systems.
  Feast has compatibility with TFDV, meaning statistics that are generated by Feast can be validated using TFDV.
  Feast also allows teams to capture TFDV schemas as part of feature definitions, allowing domain experts to define data properties that can be used for validating these features in other production settings like training, ingestion, or serving.

### What is a feature store?

Feature stores are systems that reduce challenges faced by ML teams when productionizing features.

Some of the key challenges that feature stores help to address include:

- __Feature sharing and reuse__: Engineering features is one of the most time-consuming activities in building an end-to-end ML system, yet many teams continue to develop features in silos. 
  This leads to a high amount of re-development and duplication of work across teams and projects.

- __Serving features at scale__: Models need data that can come from a variety of sources, including api-calls, event streams, data lakes, warehouses, or notebooks.
  ML teams need to be able to store and serve all these data sources to their models in a performant and reliable way.
  The challenge is scalable production of massive datasets of features for model training, and providing access to real-time feature data at low latency and high throughput in serving.

- __Consistency between training and serving__: The separation between data scientists and engineering teams often lead to the re-development of feature transformations when moving from training to online serving. 
  Inconsistencies that arise due to discrepancies between training and serving implementations frequently leads to a drop in model performance in production.

- __Point-in-time correctness__:  General purpose data systems are not built with ML use cases in mind and by extension don't provide point-in-time correct lookups of feature data. 
  Without a point-in-time correct view of data, models are trained on datasets that are not representative of what is found in production, leading to a drop in accuracy.

- __Data quality and validation__: Features are business critical inputs to ML systems. Teams need to be confident in the quality of data that is served in production and need to be able to react when there is any drift in the underlying data.

## How to use Feast with Kubeflow?

Feast can be run on the same Kubernetes cluster as Kubeflow, and may be used to serve features for models that are trained in [Kubeflow Pipelines](/docs/components/notebooks/overview/) and deployed with [KServe](/docs/external-add-ons/kserve/introduction/).

### Requirements

- A Kubernetes cluster with [Kubeflow installed](/docs/started/installing-kubeflow/)
- A database to use as an [offline store](https://docs.feast.dev/reference/offline-stores/overview) _(BigQuery, Snowflake, Redshift, etc.)_
- A database to use as an [online store](https://docs.feast.dev/reference/online-stores/overview) _(Redis, Datastore, DynamoDB, etc.)_
- A bucket _(S3, GCS, Minio, etc.)_ or SQL Database _(Postgres, MySQL, etc.)_ to use as the [feature registry](https://docs.feast.dev/getting-started/concepts/registry)
- A workflow engine _(Airflow, Kubeflow Pipelines, etc.)_ to [materialize data](https://docs.feast.dev/getting-started/concepts/data-ingestion) and run other Feast jobs

### Installation

To use Feast with Kubeflow, please follow the following steps

1. [__Install the Feast Python package__](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/install-feast) 
1. [__Create a feature repository__](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/create-a-feature-repository)
1. [__Deploy your feature store__](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/deploy-a-feature-store)
1. [__Create a training dataset__](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/build-a-training-dataset)
1. [__Load features into the online store__](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/load-data-into-the-online-store)
1. [__Read features from the online store__](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/read-features-from-the-online-store)

Please their [production usage](https://docs.feast.dev/how-to-guides/running-feast-in-production) guide for best practices when running Feast in production.

### Using Feast APIs

Once Feast is installed within the same Kubernetes cluster as Kubeflow, users can access its APIs directly without any additional steps.

Feast provides the following categories of APIs:

- __Feature definition and management__: 
    - Feast provides both a [Python SDK](https://docs.feast.dev/getting-started/quickstart) and [CLI](https://docs.feast.dev/reference/feast-cli-commands) for interacting with Feast Core. 
    - Feast Core allows users to define and register features and entities and their associated metadata and schemas.
    - The Python SDK is typically used from within a Jupyter notebook by end users to administer Feast, but ML teams may opt to version control feature specifications in order to follow a GitOps based approach.
- __Model training__: 
   - The Feast Python SDK can be used to trigger the [creation of training datasets](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/build-a-training-dataset). 
   - The most natural place to use this SDK is to create a training dataset as part of a [Kubeflow Pipeline](/docs/components/pipelines/overview/) prior to model training.
- __Model serving__: 
   - The Feast Python SDK can also be used for [online feature retrieval](https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws/read-features-from-the-online-store).
     This client is used to retrieve feature values for inference with model-serving-systems like [KServe](/docs/external-add-ons/kserve/introduction/).

Please see the Feast [tutorials page](https://docs.feast.dev/tutorials/tutorials-overview) for more information on using Feast.


================================================
File: content/en/docs/external-add-ons/feast/website.md
================================================
+++
title = "Feast Website"
description = "LINK | Feast Documentation Website"
weight = 998
manualLink = "https://docs.feast.dev/"
icon = "fa-solid fa-arrow-up-right-from-square"
+++

Feast has its own documentation website hosted at [`docs.feast.dev`](https://docs.feast.dev/).



================================================
File: content/en/docs/external-add-ons/kserve/_index.md
================================================
+++
title = "KServe"
description = "KServe | Serverless Inferencing on Kubernetes"
weight = 10
+++



================================================
File: content/en/docs/external-add-ons/kserve/github.md
================================================
+++
title = "GitHub Repository"
description = "LINK | GitHub repository for KServe"
weight = 999
manualLink = "https://github.com/kserve/kserve"
icon = "fa-brands fa-github"
+++

KServe is developed in the [`kserve/kserve`](https://github.com/kserve/kserve) repository.


================================================
File: content/en/docs/external-add-ons/kserve/introduction.md
================================================
+++
title = "Introduction"
description = "A brief introduction to KServe"
weight = 1
+++

{{% alert title="KFServing is now KServe" color="dark" %}}
_KFServing_ was renamed to _KServe_ in September 2021, when the `kubeflow/kfserving` GitHub repository was transferred to the independent [KServe GitHub organization](https://github.com/kserve).

To learn about migrating from KFServing to KServe, see the [migration guide](https://kserve.github.io/website/0.13/admin/migration/) and the [blog post](https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition.html).
{{% /alert %}}

## What is KServe?

[KServe](https://kserve.github.io/website/latest/) is an [open-source](https://github.com/kserve/kserve) project that enables serverless inferencing on Kubernetes.
KServe provides performant, high abstraction interfaces for common machine learning (ML) frameworks like TensorFlow, XGBoost, scikit-learn, PyTorch, and ONNX to solve production model serving use cases.

The following diagram shows the architecture of KServe:

<img src="/docs/external-add-ons/kserve/pics/kserve-architecture.png"
     alt="KServe architecture diagram"
     class="p-2"></img>

KServe provides the following functionality:
 
- Provide a Kubernetes [Custom Resource Definition](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) for serving ML models on arbitrary frameworks.
- Encapsulate the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU autoscaling, scale to zero, and canary rollouts to your ML deployments.
- Enable a simple, pluggable, and complete story for your production ML inference server by providing prediction, pre-processing, post-processing and explainability out of the box.

## How to use KServe with Kubeflow?

Kubeflow provides Kustomize installation files [in the `kubeflow/manifests` repo](https://github.com/kubeflow/manifests/tree/master/contrib/kserve) with each Kubeflow release.
However, these files may not be up-to-date with the latest KServe release.
See [KServe on Kubeflow with Istio-Dex](https://github.com/KServe/KServe/tree/master/docs/samples/istio-dex) from the `KServe/KServe` repository for the latest installation instructions.

Kubeflow also provides the [models web app](/docs/external-add-ons/kserve/webapp/) to manage your deployed model endpoints with a web interface.


================================================
File: content/en/docs/external-add-ons/kserve/webapp.md
================================================
+++
title = "Models Web App"
description = "An overview of the KServe Models Web App"
weight = 2
+++

## What is the KServe Models Web App?

The [KServe Models Web App](https://github.com/kserve/models-web-app) is a component of KServe that provides a user-friendly way to handle the lifecycle of `InferenceService` CRs in a Kubeflow cluster.
This allows users to create, delete, and inspect the state of their Model Servers without needing to interact with Kubernetes resources directly.

## Install as part of Kubeflow

The web app has been [included as a part of the `kubeflow/manifests`](https://github.com/kubeflow/manifests/tree/master/contrib/kserve/models-web-app) since Kubeflow 1.5 release.

It is exposed as a manu link in the Kubeflow Central Dashboard by default.

## Install Standalone

Refer to the [`kserve/models-web-app`](https://github.com/kserve/models-web-app/#development) repository for standalone installation instructions.

In this case all the resources of the web app will be installed in the `kserve` namespace. 
Users can access the web app either via the `knative-ingress-gateway.knative-serving` Istio Ingress Gateway or by port-forwarding the backend.

To port-forward the backend, you can use the following commands:

```bash
# set the following ENV vars in the app's Deployment
kubectl edit -n kserve deployments.apps kserve-models-web-app
# APP_PREFIX: /
# APP_DISABLE_AUTH: "True"
# APP_SECURE_COOKIES: "False"

# expose the app under localhost:5000
kubectl port-forward -n kserve svc/kserve-models-web-app 5000:80
```

The following is a list of ENV var that can configure different aspects of the application.

| ENV Var | Default value | Description |
| - | - | - |
| APP_PREFIX | "/models" | Controls the app's prefix, by setting the [base-url](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base) element |
| APP_DISABLE_AUTH | "False" | Controls whether the app should use SubjectAccessReviews to ensure the user is authorized to perform an action |
| APP_SECURE_COOKIES | "True" | Controls whether the app should use [Secure](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie#Secure) CSRF cookies. By default the app expects to be exposed with https |
| CSRF_SAMESITE | "Strict" | Controls the [SameSite value](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie#SameSite) of the CSRF cookie |
| USERID_HEADER | "kubeflow-userid" | Header in each request that will contain the username of the logged in user |
| USERID_PREFIX | "" | Prefix to remove from the `USERID_HEADER` value to extract the logged in user name |

## Usage

The web app currently works with `v1beta1` versions of `InferenceService` objects.

The web app exposes information from the underlying Knative resources, like Conditions from the Knative Configurations, Route and Revisions as well as live logs from the Model server pod.

### Listing

The main page of the app provides a list of all the InferenceServices that are deployed in the selected Namespace. 
The frontend periodically polls the backend for the latest state of InferenceServices.

<img src="/docs/external-add-ons/kserve/pics/webapp-list.png" alt="Models web app main page" class="mt-3 mb-3 p-3 border border-info rounded"></img>

### Creating

The page for creating a new InferenceService. 
The user can paste the YAML object of the InferenceService they wish to create.

Note that the backend will override the provided `.metadata.namespace` field of the submitted object, to prevent users from trying to create InferenceServices in other namespaces.

<img src="/docs/external-add-ons/kserve/pics/webapp-new.png" alt="Models web app create page" class="mt-3 mb-3 p-3 border border-info rounded"></img>

### Deleting

Users can delete an existing InferenceService by clicking on the <i class="fas fa-trash"></i> icon next to an InferenceService, in the main page that lists all the namespaced resources.

{{% alert title="Note" color="info" %}}
The backend is using [foreground cascading deletion](https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#foreground-cascading-deletion) when deleting an InferenceService.
This means that the InferenceService CR will be deleted from the K8s API Server only once the underlying resources have been deleted.
{{% /alert %}}

### Inspecting

Users can click on the name of an InferenceService, from the main page, and view a more detailed summary of the CR's state.

<img src="/docs/external-add-ons/kserve/pics/webapp-overview.png" alt="Models web app overview page" class="mt-3 mb-3 p-3 border border-info rounded"></img>

{{% alert title="Note" color="info" %}}
To gather the logs the backend will:

1. Filter all the pods that have a `serving.knative.dev/revision` label
2. Get the logs from the `kserve-container`
{{% /alert %}}

## Metrics

As mentioned in the above sections the web app allows users to inspect the metrics from the InferenceService.
This tab will __not__ be enabled by default.
In order to expose it the users will need to install Grafana and Prometheus.

Currently, the frontend is expecting to find a Grafana exposed in the `/grafana` prefix.
This Grafana instance will need to have specific dashboards in order for the app to embed them in iframes. 
We are working on making this more generic to allow people to expose their own graphs.

You can install Grafana and Prometheus, for the web app to consume, by installing:

1. the `monitoring-core.yaml` and `monitoring-metrics-prometheus.yaml` files from the [Knative 0.18 release](https://github.com/knative/serving/releases/tag/v0.18.0)
2. the following yaml files for exposing Grafana outside the cluster, by allowing __anonymous access__

{{< blocks/tabs name="grafana-installation-yamls" >}}
{{{< blocks/tab name="ConfigMap" codelang="yaml" >}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-custom-config
  namespace: knative-monitoring
  labels:
    serving.knative.dev/release: "v0.11.0"
data:
  custom.ini: |
    # You can customize Grafana via changing the context of this field.
    [auth.anonymous]
    # enable anonymous access
    enabled = true
    [security]
    allow_embedding = true
    [server]
    root_url = "/grafana"
    serve_from_sub_path = true
{{< /blocks/tab >}}
{{< blocks/tab name="VirtualService" codelang="yaml" >}}
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: grafana
  namespace: knative-monitoring
spec:
  gateways:
  - kubeflow/kubeflow-gateway
  hosts:
  - '*'
  http:
  - match:
    - uri:
        prefix: /grafana/
    route:
    - destination:
        host: grafana.knative-monitoring.svc.cluster.local
        port:
          number: 30802
{{< /blocks/tab >}}}
{{< blocks/tab name="AuthorizationPolicy" codelang="yaml" >}}
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: models-web-app
  namespace: kubeflow
spec:
  action: ALLOW
  rules:
  - from:
    - source:
        principals:
        - cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account
  selector:
    matchLabels:
      kustomize.component: kserve-models-web-app
      app.kubernetes.io/component: kserve-models-web-app
{{< /blocks/tab >}}}
{{< /blocks/tabs >}}

{{% alert title="Note" color="info" %}}
If you installed the app in the _standalone_ mode then you will need to instead use the __knative-serving/knative-ingress-gateway__ Ingress Gateway and deploy the AuthorizationPolicy in the __kserve__ namespace instead.
{{% /alert %}}

After applying these YAMLs, based on your installation mode, and ensuring the Grafana instance is exposed under `/grafana` the web app will show the `METRICS` tab.

<img src="/docs/external-add-ons/kserve/pics/webapp-metrics.png" alt="Models web app metrics page" class="mt-3 mb-3 p-3 border border-info rounded"></img>


## Architecture

The web app includes the following resources:

- A `Deployment` for running the backend server, and serving the static frontend files
- A `Service` for configuring the incluster network traffic
- A `ServiceAccount` and `ClusterRole{Binding}` to give the necessary permissions to the web app's Pod
- A `VirtualService` for exposing the app via the cluster's Istio Ingress Gateway

### SubjectAccessReviews

The web app has a mechanism for performing authentication and authorization checks, to ensure that user actions are compliant with the cluster's RBAC, which is only enabled in the _kubeflow_ manifests of the app. 
This mechanism can be toggled by leveraging the `APP_DISABLE_AUTH: "True" | "False"` ENV Var.

This mechanism is only enabled in the _kubeflow_ manifests since in a Kubeflow installation all requests that end up in the web app's Pod will also contain a custom header that denotes the user. 
In a Kubeflow installation there's an authentication component in front of the cluster that ensures only logged in users can access the cluster's services. 
In the standalone mode such a component might not always be deployed.

The web app will be using the value from this custom header to extract the name of the [K8s user](https://kubernetes.io/docs/reference/access-authn-authz/authentication/) that made the request.
Then it will create a [SubjectAccessReview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/#determine-whether-a-request-is-allowed-or-denied) to check if the user has permissions to perform the specific action, for example deleting an InferenceService in a namespace.

{{% alert title="Tip" color="info" %}}
If you are port-forwarding the app via __kubectl port-forward__ then you will need to set __APP_DISABLE_AUTH="True"__ in the web app's Deployment. 
When port-forwarding the authentication header will not be set, which will result in the web app raising __401__ errors.
{{% /alert %}}

### Namespace selection

Both in _standalone_ and in _kubeflow_ setups the user needs to be able to select a Namespace in order to interact with the InferenceServices in it.

In _standalone_ mode the web app will show a dropdown that will show all the namespaces to the user and allow them to select any of them. 
The backend will make a LIST request to the API Server to get all the namespaces. 
In this case the only authorization check that takes place is in the K8s API Server that ensures the [web app Pod's ServiceAccount](https://github.com/kserve/models-web-app/blob/release-0.7/config/base/rbac.yaml) has permissions to list namespaces.

In _kubeflow_ mode the [Central Dashboard](/docs/components/central-dash/overview/) is responsible for the Namespace selection.
Once the user selects a namespace then the Dashboard will inform the iframed Models web app about the newly selected namespace.
The Models web app itself won't expose a dropdown namespace selector in this mode.



================================================
File: content/en/docs/external-add-ons/kserve/website.md
================================================
+++
title = "KServe Website"
description = "LINK | KServe Documentation Website"
weight = 998
manualLink = "https://kserve.github.io/website/latest/"
icon = "fa-solid fa-arrow-up-right-from-square"
+++

KServe has its own documentation website hosted at [`kserve.github.io/website/`](https://kserve.github.io/website/).








================================================
File: content/en/docs/releases/OWNERS
================================================
approvers:
  - thesuperzapper
  - zijianjoy
reviewers:
  - thesuperzapper
  - zijianjoy



================================================
File: content/en/docs/releases/_index.md
================================================
+++
title = "Releases"
description = "Information about past and future Kubeflow releases"
weight = 100
+++



================================================
File: content/en/docs/releases/kubeflow-0.6.md
================================================
+++
title = "Kubeflow 0.6"
description = "Information about the Kubeflow 0.6 release"
weight = 106
+++

## Kubeflow 0.6.1

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2019-07-31
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://medium.com/kubeflow/kubeflow-v0-6-a-robust-foundation-for-artifact-tracking-data-versioning-multi-user-support-9896d329412c">Kubeflow 0.6 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=fiFk5FB7il8">Kubeflow 0.6 Release Feature Review</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-06">Kubeflow 0.6 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v0.6.1">v0.6.1</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v0.6-branch">v0.6-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Zhenghui Wang (<a href="https://github.com/zhenghuiwang">@zhenghuiwang</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

## Kubeflow 0.6.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2019-07-19
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://medium.com/kubeflow/kubeflow-v0-6-a-robust-foundation-for-artifact-tracking-data-versioning-multi-user-support-9896d329412c">Kubeflow 0.6 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=fiFk5FB7il8">Kubeflow 0.6 Release Feature Review</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-06">Kubeflow 0.6 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v0.6.0">v0.6.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v0.6-branch">v0.6-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Kunming Qu (<a href="https://github.com/kunmingg">@kunmingg</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-0.7.md
================================================
+++
title = "Kubeflow 0.7"
description = "Information about the Kubeflow 0.7 release"
weight = 105
+++

## Kubeflow 0.7.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2019-10-17
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://medium.com/kubeflow/kubeflow-v0-7-delivers-beta-functionality-in-the-leadup-to-v1-0-1e63036c07b8">Kubeflow 0.7 Release Announcement</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-07">Kubeflow 0.7 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v0.7.0-rc.2">v0.7.0-rc.2</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v0.7-branch">v0.7-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Richard Liu (<a href="https://github.com/richardsliu">@richardsliu</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-1.0.md
================================================
+++
title = "Kubeflow 1.0"
description = "Information about the Kubeflow 1.0 release"
weight = 104
+++

## Kubeflow 1.0.2

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2020-04-18
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/releases/2020/03/02/kubeflow-1-0-cloud-native-ml-for-everyone.html">Kubeflow 1.0 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=L6wG_mpRWe8">Kubeflow 1.0 Release Community Webinar</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-10">Kubeflow 1.0 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.0.2">v1.0.2</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.0-branch">v1.0-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b>Richard Liu (<a href="https://github.com/richardsliu">@richardsliu</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

## Kubeflow 1.0.1

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2020-03-19
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/releases/2020/03/02/kubeflow-1-0-cloud-native-ml-for-everyone.html">Kubeflow 1.0 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=L6wG_mpRWe8">Kubeflow 1.0 Release Community Webinar</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-10">Kubeflow 1.0 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.0.1">v1.0.1</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.0-branch">v1.0-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b>Richard Liu (<a href="https://github.com/richardsliu">@richardsliu</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

## Kubeflow 1.0.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2020-02-20
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/releases/2020/03/02/kubeflow-1-0-cloud-native-ml-for-everyone.html">Kubeflow 1.0 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=L6wG_mpRWe8">Kubeflow 1.0 Release Community Webinar</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-10">Kubeflow 1.0 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.0.0">v1.0.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.0-branch">v1.0-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Jeremy Lewi (<a href="https://github.com/jlewi">@jlewi</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-1.1.md
================================================
+++
title = "Kubeflow 1.1"
description = "Information about the Kubeflow 1.1 release"
weight = 103
+++

## Kubeflow 1.1.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2020-07-31
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/release/official/2020/07/31/kubeflow-1.1-blog-post.html">Kubeflow 1.1 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=kd-mWl1cq48">Kubeflow 1.1 Community Release Update</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-11-features-release-date-late-june-2020">Kubeflow 1.1 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.1.0">v1.1.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.1-branch">v1.1-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Jeremy Lewi (<a href="https://github.com/jlewi">@jlewi</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-1.2.md
================================================
+++
title = "Kubeflow 1.2"
description = "Information about the Kubeflow 1.2 release"
weight = 102
+++

## Kubeflow 1.2.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2020-11-18
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/release/official/2020/11/18/kubeflow-1.2-blog-post.html">Kubeflow 1.2 Release Announcement</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-12-features-release-date-november-2020">Kubeflow 1.2 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.2.0">v1.2.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.2-branch">v1.2-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Jiaxin Shan (<a href="https://github.com/Jeffwan">@Jeffwan</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-1.3.md
================================================
+++
title = "Kubeflow 1.3"
description = "Information about the Kubeflow 1.3 release"
weight = 101
+++

## Kubeflow 1.3.1

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2021-08-04
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.3-release/">Kubeflow 1.3 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=RjN55bQ4kh4">Kubeflow 1.3 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-13-features-released-april-2021">Kubeflow 1.3 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.3.1">v1.3.1</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.3-branch">v1.3-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Yannis Zarkadas (<a href="https://github.com/yanniszark">@yanniszark</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.11.1">v0.11.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/admission-webhook">v1.3.1-rc.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/centraldashboard">v1.3.1-rc.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/crud-web-apps/jupyter">v1.3.1-rc.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/access-management">v1.3.1-rc.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/notebook-controller">v1.3.1-rc.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/profile-controller">v1.3.1-rc.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/notebook-controller">v1.3.1-rc.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/crud-web-apps/volumes">v1.3.1-rc.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.1-rc.0/components/crud-web-apps/tensorboards">v1.3.1-rc.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/1.5.1">v1.5.1</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v0.8.0">v0.8.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KFServing</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.5.0">v0.5.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="5" class="align-middle">Training Working Group</td>
        <td>MPI Operator</td>
        <td>
          <a href="https://github.com/kubeflow/mpi-operator/releases/tag/v0.2.3">v0.2.3</a>
        </td>
      </tr>
      <tr>
        <td>MXNet Operator</td>
        <td>
          <a href="https://github.com/kubeflow/mxnet-operator/releases/tag/v1.1.0">v1.1.0</a>
        </td>
      </tr>
      <tr>
        <td>PyTorch Operator</td>
        <td>
          <a href="https://github.com/kubeflow/pytorch-operator/releases/tag/v0.7.0">v0.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.1.0">v1.1.0</a>
        </td>
      </tr>
      <tr>
        <td>XGBoost Operator</td>
        <td>
          <a href="https://github.com/kubeflow/xgboost-operator/releases/tag/v0.2.0">v0.2.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

## Kubeflow 1.3.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2021-04-23
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.3-release/">Kubeflow 1.3 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=RjN55bQ4kh4">Kubeflow 1.3 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-13-features-released-april-2021">Kubeflow 1.3 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.3.0">v1.3.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.3-branch">v1.3-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Yannis Zarkadas (<a href="https://github.com/yanniszark">@yanniszark</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.11.0">v0.11.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/admission-webhook">v1.3.0-rc.1</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/centraldashboard">v1.3.0-rc.1</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/crud-web-apps/jupyter">v1.3.0-rc.1</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/access-management">v1.3.0-rc.1</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/notebook-controller">v1.3.0-rc.1</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/profile-controller">v1.3.0-rc.1</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/notebook-controller">v1.3.0-rc.1</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/crud-web-apps/volumes">v1.3.0-rc.1</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/crud-web-apps/tensorboards">v1.3.0-rc.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/1.5.0">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v0.8.0-rc0">v0.8.0-rc0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KFServing</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.5.0">v0.5.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="5" class="align-middle">Training Working Group</td>
        <td>MPI Operator</td>
        <td>
          <a href="https://github.com/kubeflow/mpi-operator/releases/tag/v0.2.3">v0.2.3</a>
        </td>
      </tr>
      <tr>
        <td>MXNet Operator</td>
        <td>
          <a href="https://github.com/kubeflow/mxnet-operator/releases/tag/v1.1.0">v1.1.0</a>
        </td>
      </tr>
      <tr>
        <td>PyTorch Operator</td>
        <td>
          <a href="https://github.com/kubeflow/pytorch-operator/releases/tag/v0.7.0">v0.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.1.0">v1.1.0</a>
        </td>
      </tr>
      <tr>
        <td>XGBoost Operator</td>
        <td>
          <a href="https://github.com/kubeflow/xgboost-operator/releases/tag/v0.2.0">v0.2.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-1.4.md
================================================
+++
title = "Kubeflow 1.4"
description = "Information about the Kubeflow 1.4 release"
weight = 100
+++

## Kubeflow 1.4.1

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2021-12-23
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.4-release/">Kubeflow 1.4 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=gG61gHw4J14">Kubeflow 1.4 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-141-release-delivered-december-2021">Kubeflow 1.4.1 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.4.1">v1.4.1</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.4-branch">v1.4-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> David van der Spek (<a href="https://github.com/DavidSpek">@DavidSpek</a>)
        <br>
        <b>Member:</b> Rui Vasconcelos (<a href="https://github.com/RFMVasconcelos">@RFMVasconcelos</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.12.0">v0.12.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/admission-webhook">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/centraldashboard">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/jupyter">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/access-management">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/notebook-controller">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/profile-controller">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/notebook-controller">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/volumes">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/tensorboards">v1.4.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/1.7.0">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v1.0.0">v1.0.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KFServing (KServe)</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.6.1">v0.6.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Training Working Group</td>
        <td>MPI Operator</td>
        <td>
          <a href="https://github.com/kubeflow/mpi-operator/releases/tag/v0.3.0">v0.3.0</a>
        </td>
      </tr>
      <tr>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.3.0">v1.3.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

## Kubeflow 1.4.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2021-10-12
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.4-release/">Kubeflow 1.4 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=gG61gHw4J14">Kubeflow 1.4 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-14-release-delivered-october-2021">Kubeflow 1.4 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.4.0">v1.4.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.4-branch">v1.4-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> David van der Spek (<a href="https://github.com/DavidSpek">@DavidSpek</a>)
        <br>
        <b>Member:</b> Rui Vasconcelos (<a href="https://github.com/RFMVasconcelos">@RFMVasconcelos</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.12.0">v0.12.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/admission-webhook">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/centraldashboard">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/jupyter">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/access-management">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/notebook-controller">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/profile-controller">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/notebook-controller">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/volumes">v1.4.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/tensorboards">v1.4.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/1.7.0">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v1.0.0">v1.0.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KFServing (KServe)</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.6.1">v0.6.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Training Working Group</td>
        <td>MPI Operator</td>
        <td>
          <a href="https://github.com/kubeflow/mpi-operator/releases/tag/v0.3.0">v0.3.0</a>
        </td>
      </tr>
      <tr>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.3.0">v1.3.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-1.5.md
================================================
+++
title = "Kubeflow 1.5"
description = "Information about the Kubeflow 1.5 release"
weight = 99
+++

## Kubeflow 1.5.1

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2022-06-15
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.5.1-release/">Kubeflow 1.5.1 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=QNNCM9Kq3Q0">Kubeflow 1.5 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-15-release-delivered-march-2022">Kubeflow 1.5 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.5.1">v1.5.1</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.5-branch">v1.5-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> Daniela Plascencia (<a href="https://github.com/DnPlas">@DnPlas</a>)
        <br>
        <b>Member:</b> Dominik Fleischmann (<a href="https://github.com/DomFleischmann">@DomFleischmann</a>)
        <br>
        <b>Member:</b> Kylie Travis (<a href="https://github.com/Bhakti087">@Bhakti087</a>)
        <br>
        <b>Member:</b> Mathew Wicks (<a href="https://github.com/thesuperzapper">@thesuperzapper</a>)
        <br>
        <b>Member:</b> Suraj Kota (<a href="https://github.com/surajkota">@surajkota</a>)
        <br>
        <b>Member:</b> Vedant Padwal (<a href="https://github.com/js-ts">@js-ts</a>)
        <br>
        <b>Product Manager:</b> Josh Bottum (<a href="https://github.com/jbottum">@jbottum</a>)
        <br>
        <b>Docs Lead:</b> Shannon Bradshaw (<a href="https://github.com/shannonbradshaw">@shannonbradshaw</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.13.0">v0.13.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/admission-webhook">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/centraldashboard">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/crud-web-apps/jupyter">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/access-management">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/notebook-controller">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/profile-controller">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/notebook-controller">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/crud-web-apps/volumes">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/crud-web-apps/tensorboards">v1.5.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/1.8.2">v1.8.2</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v1.1.1">v1.1.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KServe</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.7.0">v0.7.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Training Working Group</td>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.4.0">v1.4.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

### Dependency Versions (Manifests)

{{% alert title="Note" color="warning" %}}
This information is only for the manifests found in the <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a> repository, packaged distributions may have different requirements or supported versions.
{{% /alert %}}

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- Kubernetes -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.21</td>
        <td rowspan="1" class="align-middle">
          <i>Kubernetes 1.22 is NOT supported by Kubeflow 1.5, see <a href="https://github.com/kubeflow/kubeflow/issues/6353">kubeflow/kubeflow#6353</a> for more information.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Istio -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://istio.io/">Istio</a>
        </td>
        <td>1.11.0</td>
        <td rowspan="3" class="align-middle">
          <i>Other versions may work, but have not been validated by the <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Kubeflow Manifests Working Group</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- cert-manager  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://cert-manager.io/">cert-manager</a>
        </td>
        <td>1.5.0</td>
      </tr>
      <!-- ======================= -->
      <!-- dex  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://dexidp.io/">dex</a>
        </td>
        <td>2.22.0</td>
      </tr>
      <!-- ======================= -->
      <!-- Kustomize  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kustomize.io/">Kustomize</a>
        </td>
        <td>3.2.0</td>
        <td>
          <i>Newer versions of Kustomize are not currently supported, follow <a href="https://github.com/kubeflow/manifests/issues/1797">kubeflow/manifests#1797</a> for progress on this issue.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Serving -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/serving/">Knative Serving</a>
        </td>
        <td>0.22.1</td>
        <td rowspan="2" class="align-middle">
          <i>Knative is only needed when using the optional <a href="https://kserve.github.io/website/">KServe Component</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Eventing -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/eventing/">Knative Eventing</a>
        </td>
        <td>0.22.1</td>
      </tr>
  </tbody>
</table>
</div>

## Kubeflow 1.5.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2022-03-10
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.5-release/">Kubeflow 1.5 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=QNNCM9Kq3Q0">Kubeflow 1.5 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-15-release-delivered-march-2022">Kubeflow 1.5 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.5.0">v1.5.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.5-branch">v1.5-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> Daniela Plascencia (<a href="https://github.com/DnPlas">@DnPlas</a>)
        <br>
        <b>Member:</b> Dominik Fleischmann (<a href="https://github.com/DomFleischmann">@DomFleischmann</a>)
        <br>
        <b>Member:</b> Kylie Travis (<a href="https://github.com/Bhakti087">@Bhakti087</a>)
        <br>
        <b>Member:</b> Mathew Wicks (<a href="https://github.com/thesuperzapper">@thesuperzapper</a>)
        <br>
        <b>Member:</b> Suraj Kota (<a href="https://github.com/surajkota">@surajkota</a>)
        <br>
        <b>Member:</b> Vedant Padwal (<a href="https://github.com/js-ts">@js-ts</a>)
        <br>
        <b>Product Manager:</b> Josh Bottum (<a href="https://github.com/jbottum">@jbottum</a>)
        <br>
        <b>Docs Lead:</b> Shannon Bradshaw (<a href="https://github.com/shannonbradshaw">@shannonbradshaw</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.13.0">v0.13.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/admission-webhook">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/centraldashboard">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/crud-web-apps/jupyter">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/access-management">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/notebook-controller">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/profile-controller">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/notebook-controller">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/crud-web-apps/volumes">v1.5.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.5.0/components/crud-web-apps/tensorboards">v1.5.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/1.8.1">v1.8.1</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v1.1.1">v1.1.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KServe</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.7.0">v0.7.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Training Working Group</td>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.4.0">v1.4.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

### Dependency Versions (Manifests)

{{% alert title="Note" color="warning" %}}
This information is only for the manifests found in the <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a> repository, packaged distributions may have different requirements or supported versions.
{{% /alert %}}

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- Kubernetes -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.21</td>
        <td rowspan="1" class="align-middle">
          <i>Kubernetes 1.22 is NOT supported by Kubeflow 1.5, see <a href="https://github.com/kubeflow/kubeflow/issues/6353">kubeflow/kubeflow#6353</a> for more information.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Istio -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://istio.io/">Istio</a>
        </td>
        <td>1.11.0</td>
        <td rowspan="3" class="align-middle">
          <i>Other versions may work, but have not been validated by the <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Kubeflow Manifests Working Group</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- cert-manager  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://cert-manager.io/">cert-manager</a>
        </td>
        <td>1.5.0</td>
      </tr>
      <!-- ======================= -->
      <!-- dex  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://dexidp.io/">dex</a>
        </td>
        <td>2.22.0</td>
      </tr>
      <!-- ======================= -->
      <!-- Kustomize  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kustomize.io/">Kustomize</a>
        </td>
        <td>3.2.0</td>
        <td>
          <i>Newer versions of Kustomize are not currently supported, follow <a href="https://github.com/kubeflow/manifests/issues/1797">kubeflow/manifests#1797</a> for progress on this issue.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Serving -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/serving/">Knative Serving</a>
        </td>
        <td>0.22.1</td>
        <td rowspan="2" class="align-middle">
          <i>Knative is only needed when using the optional <a href="https://kserve.github.io/website/">KServe Component</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Eventing -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/eventing/">Knative Eventing</a>
        </td>
        <td>0.22.1</td>
      </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-1.6.md
================================================
+++
title = "Kubeflow 1.6"
description = "Information about the Kubeflow 1.6 release"
weight = 98
+++

## Kubeflow 1.6.1

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2022-10-10
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.6-release/">Kubeflow 1.6 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=RR1xSfnFGGI">Kubeflow 1.6 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-16-release-delivered-september-2022">Kubeflow 1.6 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.6.1">v1.6.1</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.6-branch">v1.6-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> Amber Graner (<a href="https://github.com/akgraner">@akgraner</a>)
        <br>
        <b>Member:</b> Daniela Plascencia (<a href="https://github.com/DnPlas">@DnPlas</a>)
        <br>
        <b>Member:</b> Dominik Fleischmann (<a href="https://github.com/DomFleischmann">@DomFleischmann</a>)
        <br>
        <b>Member:</b> Kartik Kalamadi (<a href="https://github.com/akartsky">@akartsky</a>)
        <br>
        <b>Member:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Maciek Stopa (<a href="https://github.com/mstopa">@mstopa</a>)
        <br>
        <b>Member:</b> Suraj Kota (<a href="https://github.com/surajkota">@surajkota</a>)
        <br>
        <b>Product Manager:</b> Josh Bottum (<a href="https://github.com/jbottum">@jbottum</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.14.0">v0.14.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/admission-webhook">v1.6.1</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/centraldashboard">v1.6.1</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/crud-web-apps/jupyter">v1.6.1</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/access-management">v1.6.1</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/notebook-controller">v1.6.1</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/profile-controller">v1.6.1</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/notebook-controller">v1.6.1</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/crud-web-apps/volumes">v1.6.1</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.1/components/crud-web-apps/tensorboards">v1.6.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/2.0.0-alpha.5">v2.0.0-alpha.5</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v1.2.1">v1.2.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KServe</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.8.0">v0.8.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Training Working Group</td>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.5.0">v1.5.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

### Dependency Versions (Manifests)

{{% alert title="Note" color="warning" %}}
This information is only for the manifests found in the <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a> repository, packaged distributions may have different requirements or supported versions.
{{% /alert %}}

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- Kubernetes -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.22</td>
        <td rowspan="4" class="align-middle">
          <i>Other versions may work, but have not been validated by the <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Kubeflow Manifests Working Group</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Istio -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://istio.io/">Istio</a>
        </td>
        <td>1.14.1</td>
      </tr>
      <!-- ======================= -->
      <!-- cert-manager  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://cert-manager.io/">cert-manager</a>
        </td>
        <td>1.5.0</td>
      </tr>
      <!-- ======================= -->
      <!-- dex  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://dexidp.io/">dex</a>
        </td>
        <td>2.31.2</td>
      </tr>
      <!-- ======================= -->
      <!-- Kustomize  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kustomize.io/">Kustomize</a>
        </td>
        <td>3.2.0</td>
        <td>
          <i>Newer versions of Kustomize are not currently supported, follow <a href="https://github.com/kubeflow/manifests/issues/1797">kubeflow/manifests#1797</a> for progress on this issue.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Serving -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/serving/">Knative Serving</a>
        </td>
        <td>1.2.5</td>
        <td rowspan="2" class="align-middle">
          <i>Knative is only needed when using the optional <a href="https://kserve.github.io/website/">KServe Component</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Eventing -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/eventing/">Knative Eventing</a>
        </td>
        <td>1.2.4</td>
      </tr>
  </tbody>
</table>
</div>

## Kubeflow 1.6.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2022-09-07
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.6-release/">Kubeflow 1.6 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=RR1xSfnFGGI">Kubeflow 1.6 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-16-release-delivered-september-2022">Kubeflow 1.6 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.6.0">v1.6.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.6-branch">v1.6-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> Amber Graner (<a href="https://github.com/akgraner">@akgraner</a>)
        <br>
        <b>Member:</b> Daniela Plascencia (<a href="https://github.com/DnPlas">@DnPlas</a>)
        <br>
        <b>Member:</b> Dominik Fleischmann (<a href="https://github.com/DomFleischmann">@DomFleischmann</a>)
        <br>
        <b>Member:</b> Kartik Kalamadi (<a href="https://github.com/akartsky">@akartsky</a>)
        <br>
        <b>Member:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Maciek Stopa (<a href="https://github.com/mstopa">@mstopa</a>)
        <br>
        <b>Member:</b> Suraj Kota (<a href="https://github.com/surajkota">@surajkota</a>)
        <br>
        <b>Product Manager:</b> Josh Bottum (<a href="https://github.com/jbottum">@jbottum</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.14.0">v0.14.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/admission-webhook">v1.6.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/centraldashboard">v1.6.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/crud-web-apps/jupyter">v1.6.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/access-management">v1.6.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/notebook-controller">v1.6.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/profile-controller">v1.6.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/notebook-controller">v1.6.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/crud-web-apps/volumes">v1.6.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.6.0/components/crud-web-apps/tensorboards">v1.6.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/2.0.0-alpha.3">v2.0.0-alpha.3</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v1.2.1">v1.2.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KServe</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.8.0">v0.8.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Training Working Group</td>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.5.0">v1.5.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

### Dependency Versions (Manifests)

{{% alert title="Note" color="warning" %}}
This information is only for the manifests found in the <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a> repository, packaged distributions may have different requirements or supported versions.
{{% /alert %}}

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- Kubernetes -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.22</td>
        <td rowspan="4" class="align-middle">
          <i>Other versions may work, but have not been validated by the <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Kubeflow Manifests Working Group</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Istio -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://istio.io/">Istio</a>
        </td>
        <td>1.14.1</td>
      </tr>
      <!-- ======================= -->
      <!-- cert-manager  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://cert-manager.io/">cert-manager</a>
        </td>
        <td>1.5.0</td>
      </tr>
      <!-- ======================= -->
      <!-- dex  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://dexidp.io/">dex</a>
        </td>
        <td>2.31.2</td>
      </tr>
      <!-- ======================= -->
      <!-- Kustomize  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kustomize.io/">Kustomize</a>
        </td>
        <td>3.2.0</td>
        <td>
          <i>Newer versions of Kustomize are not currently supported, follow <a href="https://github.com/kubeflow/manifests/issues/1797">kubeflow/manifests#1797</a> for progress on this issue.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Serving -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/serving/">Knative Serving</a>
        </td>
        <td>1.2.5</td>
        <td rowspan="2" class="align-middle">
          <i>Knative is only needed when using the optional <a href="https://kserve.github.io/website/">KServe Component</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Eventing -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/eventing/">Knative Eventing</a>
        </td>
        <td>1.2.4</td>
      </tr>
  </tbody>
</table>
</div>


================================================
File: content/en/docs/releases/kubeflow-1.7.md
================================================
+++
title = "Kubeflow 1.7"
description = "Information about the Kubeflow 1.7 release"
weight = 97
+++

## Kubeflow 1.7.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2023-03-29
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.7-release/">Kubeflow 1.7 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=CUQT-YccpR8">Kubeflow 1.7 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-17-release-planned-march-2023">Kubeflow 1.7 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.7.0">v1.7.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.7-branch">v1.7-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Dominik Fleischmann (<a href="https://github.com/DomFleischmann">@DomFleischmann</a>)
        <br>
        <b>Member:</b> Amber Graner (<a href="https://github.com/akgraner">@akgraner</a>)
        <br>
        <b>Member:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> Bozhao Yu (<a href="https://github.com/yubozhao">@yubozhao</a>)
        <br>
        <b>Member:</b> Daniela Plascencia (<a href="https://github.com/DnPlas">@DnPlas</a>)
        <br>
        <b>Member:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Suraj Kota (<a href="https://github.com/surajkota">@surajkota</a>)
        <br>
        <b>Product Manager:</b> Josh Bottum (<a href="https://github.com/jbottum">@jbottum</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.15.0">v0.15.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/admission-webhook">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/centraldashboard">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/crud-web-apps/jupyter">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/access-management">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/notebook-controller">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/profile-controller">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/notebook-controller">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/crud-web-apps/volumes">v1.7.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.7.0/components/crud-web-apps/tensorboards">v1.7.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/2.0.0-alpha.7">v2.0.0-alpha.7</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v1.5.1">v1.5.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KServe</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.10.0">v0.10.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Training Working Group</td>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.6.0">v1.6.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

### Dependency Versions (Manifests)

{{% alert title="Note" color="warning" %}}
This information is only for the manifests found in the <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a> repository, packaged distributions may have different requirements or supported versions.
{{% /alert %}}

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- Kubernetes -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.25/1.24</td>
        <td rowspan="4" class="align-middle">
          <i>Other versions may work, but have not been validated by the <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Kubeflow Manifests Working Group</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Istio -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://istio.io/">Istio</a>
        </td>
        <td>1.16.0</td>
      </tr>
      <!-- ======================= -->
      <!-- cert-manager  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://cert-manager.io/">cert-manager</a>
        </td>
        <td>1.10.1</td>
      </tr>
      <!-- ======================= -->
      <!-- dex  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://dexidp.io/">dex</a>
        </td>
        <td>2.31.2</td>
      </tr>
      <!-- ======================= -->
      <!-- Kustomize  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kustomize.io/">Kustomize</a>
        </td>
        <td>5.0.0</td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Serving -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/serving/">Knative Serving</a>
        </td>
        <td>1.8.1</td>
        <td rowspan="2" class="align-middle">
          <i>Knative is only needed when using the optional <a href="https://kserve.github.io/website/">KServe Component</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Eventing -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/eventing/">Knative Eventing</a>
        </td>
        <td>1.8.1</td>
      </tr>
  </tbody>
</table>
</div>



================================================
File: content/en/docs/releases/kubeflow-1.8.md
================================================
+++
title = "Kubeflow 1.8"
description = "Information about the Kubeflow 1.8 release"
weight = 96
+++

## Kubeflow 1.8.1

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2023-11-01
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.8-release/">Kubeflow 1.8 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=eUX9edNwYao">Kubeflow 1.8 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-18-release-planned-for-release-oct-2023">Kubeflow 1.8 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.8.1">v1.8.1</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.8-branch">v1.8-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Daniela Plascencia (<a href="https://github.com/DnPlas">@DnPlas</a>)
        <br>
        <b>Member:</b> Amber Graner (<a href="https://github.com/akgraner">@akgraner</a>)
        <br>
        <b>Member:</b> Julius von Kohout (<a href="https://github.com/juliusvonkohout">@juliusvonkohout</a>)
        <br>
        <b>Member:</b> Amber Graner (<a href="https://github.com/akgraner">@akgraner</a>)
        <br>
        <b>Member:</b> Ajay Nagar (<a href="https://github.com/nagar-ajay ">@nagar-ajay </a>)
        <br>
        <b>Member:</b> Noha Ihab (<a href="https://github.com/NohaIhab">@NohaIhab</a>)
        <br>
        <b>Member:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Eric Liu (<a href="https://github.com/helloericsf">@helloericsf</a>)
        <br>
        <b>Member:</b> David Cardozo (<a href="https://github.com/Davidnet">@Davidnet</a>)
        <br>
        <b>Product Manager:</b> Josh Bottum (<a href="https://github.com/jbottum">@jbottum</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.16.0">v0.16.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/admission-webhook">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/centraldashboard">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/crud-web-apps/jupyter">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/access-management">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/notebook-controller">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/profile-controller">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/notebook-controller">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/crud-web-apps/volumes">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/crud-web-apps/tensorboards">v1.8.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/2.0.5">2.0.5</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v2.0.5">v2.0.5</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KServe</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.11.2">v0.11.2</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Training Working Group</td>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.7.0">v1.7.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

### Dependency Versions (Manifests)

{{% alert title="Note" color="warning" %}}
This information is only for the manifests found in the <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a> repository, packaged distributions may have different requirements or supported versions.
{{% /alert %}}

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- Kubernetes -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.25/1.26</td>
        <td rowspan="4" class="align-middle">
          <i>Other versions may work, but have not been validated by the <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Kubeflow Manifests Working Group</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Istio -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://istio.io/">Istio</a>
        </td>
        <td>1.17.3</td>
      </tr>
      <!-- ======================= -->
      <!-- cert-manager  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://cert-manager.io/">cert-manager</a>
        </td>
        <td>1.12.2</td>
      </tr>
      <!-- ======================= -->
      <!-- dex  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://dexidp.io/">dex</a>
        </td>
        <td>2.36.0</td>
      </tr>
      <!-- ======================= -->
      <!-- Kustomize  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kustomize.io/">Kustomize</a>
        </td>
        <td>5.0.3</td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Serving -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/serving/">Knative Serving</a>
        </td>
        <td>1.10.2</td>
        <td rowspan="2" class="align-middle">
          <i>Knative is only needed when using the optional <a href="https://kserve.github.io/website/">KServe Component</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Eventing -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/eventing/">Knative Eventing</a>
        </td>
        <td>1.10.1</td>
      </tr>
  </tbody>
</table>
</div>

## Kubeflow 1.8.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2023-11-01
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.8-release/">Kubeflow 1.8 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=eUX9edNwYao">Kubeflow 1.8 Release Overview</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-18-release-planned-for-release-oct-2023">Kubeflow 1.8 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.8.0">v1.8.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.8-branch">v1.8-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Daniela Plascencia (<a href="https://github.com/DnPlas">@DnPlas</a>)
        <br>
        <b>Member:</b> Amber Graner (<a href="https://github.com/akgraner">@akgraner</a>)
        <br>
        <b>Member:</b> Julius von Kohout (<a href="https://github.com/juliusvonkohout">@juliusvonkohout</a>)
        <br>
        <b>Member:</b> Amber Graner (<a href="https://github.com/akgraner">@akgraner</a>)
        <br>
        <b>Member:</b> Ajay Nagar (<a href="https://github.com/nagar-ajay ">@nagar-ajay </a>)
        <br>
        <b>Member:</b> Noha Ihab (<a href="https://github.com/NohaIhab">@NohaIhab</a>)
        <br>
        <b>Member:</b> Anna Jung (<a href="https://github.com/annajung">@annajung</a>)
        <br>
        <b>Member:</b> Kimonas Sotirchos (<a href="https://github.com/kimwnasptd">@kimwnasptd</a>)
        <br>
        <b>Member:</b> Eric Liu (<a href="https://github.com/helloericsf">@helloericsf</a>)
        <br>
        <b>Member:</b> David Cardozo (<a href="https://github.com/Davidnet">@Davidnet</a>)
        <br>
        <b>Product Manager:</b> Josh Bottum (<a href="https://github.com/jbottum">@jbottum</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.16.0">v0.16.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/admission-webhook">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/centraldashboard">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/crud-web-apps/jupyter">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/access-management">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/notebook-controller">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/profile-controller">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/notebook-controller">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/crud-web-apps/volumes">v1.8.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.8.0/components/crud-web-apps/tensorboards">v1.8.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="2" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/2.0.3">2.0.3</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Pipelines Tekton</td>
        <td>
          <a href="https://github.com/kubeflow/kfp-tekton/releases/tag/v2.0.3">v2.0.3</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KServe</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.11.1">v0.11.1</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Training Working Group</td>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.7.0">v1.7.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

### Dependency Versions (Manifests)

{{% alert title="Note" color="warning" %}}
This information is only for the manifests found in the <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a> repository, packaged distributions may have different requirements or supported versions.
{{% /alert %}}

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- Kubernetes -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.25/1.26</td>
        <td rowspan="4" class="align-middle">
          <i>Other versions may work, but have not been validated by the <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Kubeflow Manifests Working Group</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Istio -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://istio.io/">Istio</a>
        </td>
        <td>1.17.3</td>
      </tr>
      <!-- ======================= -->
      <!-- cert-manager  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://cert-manager.io/">cert-manager</a>
        </td>
        <td>1.12.2</td>
      </tr>
      <!-- ======================= -->
      <!-- dex  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://dexidp.io/">dex</a>
        </td>
        <td>2.36.0</td>
      </tr>
      <!-- ======================= -->
      <!-- Kustomize  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kustomize.io/">Kustomize</a>
        </td>
        <td>5.0.3</td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Serving -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/serving/">Knative Serving</a>
        </td>
        <td>1.10.2</td>
        <td rowspan="2" class="align-middle">
          <i>Knative is only needed when using the optional <a href="https://kserve.github.io/website/">KServe Component</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Eventing -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/eventing/">Knative Eventing</a>
        </td>
        <td>1.10.1</td>
      </tr>
  </tbody>
</table>
</div>



================================================
File: content/en/docs/releases/kubeflow-1.9.md
================================================
+++
title = "Kubeflow 1.9"
description = "Information about the Kubeflow 1.9 release"
weight = 95
+++

## Kubeflow 1.9.0

<div class="table-responsive">
<table class="table table-bordered">
  <tbody>
    <tr>
      <th class="table-light">Release Date</th>
      <td>
        2024-07-22
      </td>
    </tr>
    <tr>
      <th class="table-light">Media</th>
      <td>
        <b>Blog:</b> 
          <a href="https://blog.kubeflow.org/kubeflow-1.9-release/">Kubeflow 1.9 Release Announcement</a>
        <br>
        <b>Video:</b> 
          <a href="https://www.youtube.com/watch?v=bzu2Qqv4Ij0">Kubeflow 1.9 Release Update</a>
        <br>
        <b>Roadmap:</b>
          <a href="https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-19-release-planned-for-release-jul-2024">Kubeflow 1.9 Features</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Manifests</th>
      <td>
        <b>Release:</b> 
          <a href="https://github.com/kubeflow/manifests/releases/tag/v1.9.0">v1.9.0</a>
        <br>
        <b>Branch:</b>
          <a href="https://github.com/kubeflow/manifests/tree/v1.9-branch">v1.9-branch</a>
      </td>
    </tr>
    <tr>
      <th class="table-light">Release Team</th>
      <td>
        <b>Lead:</b> Ricardo Martinelli de Oliveira (<a href="https://github.com/rimolive">@rimolive</a>)
        <br>
        <b>Member:</b> Ajay Nagar (<a href="https://github.com/nagar-ajay">@nagar-ajay</a>)
        <br>
        <b>Member:</b> Andrew Scribner (<a href="https://github.com/ca-scribner">@ca-scribner</a>)
        <br>
        <b>Member:</b> Diego Lovison (<a href="https://github.com/diegolovison">@diegolovison</a>)
        <br>
        <b>Member:</b> Helber Belmiro (<a href="https://github.com/hbelmiro">@hbelmiro</a>)
        <br>
        <b>Member:</b> Julius von Kohout (<a href="https://github.com/juliusvonkohout">@juliusvonkohout</a>)
        <br>
        <b>Member:</b> Milos Grubjesic (<a href="https://github.com/milosjava">@milosjava</a>)
        <br>
        <b>Product Manager:</b> Stefano Fioravanzo (<a href="https://github.com/StefanoFioravanzo">@StefanoFioravanzo</a>)
      </td>
    </tr>
  </tbody>
</table>
</div>

### Component Versions

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Maintainers</th>
        <th>Component Name</th>
        <th>Version</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- AutoML Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">AutoML Working Group</td>
        <td>Katib</td>
        <td>
          <a href="https://github.com/kubeflow/katib/releases/tag/v0.17.0">v0.17.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Notebooks Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="9" class="align-middle">Notebooks Working Group</td>
        <td>Admission Webhook (PodDefaults)</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/admission-webhook">v1.9.0</a>
        </td>
      </tr>
      <tr>
        <td>Central Dashboard</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/centraldashboard">v1.9.0</a>
        </td>
      </tr>
      <tr>
        <td>Jupyter Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/crud-web-apps/jupyter">v1.9.0</a>
        </td>
      </tr>
      <tr>
        <td>Kubeflow Access Management API</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/access-management">v1.9.0</a>
        </td>
      </tr>
      <tr>
        <td>Notebook Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/notebook-controller">v1.9.0</a>
        </td>
      </tr>
      <tr>
        <td>Profile Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/profile-controller">v1.9.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Controller</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/notebook-controller">v1.9.0</a>
        </td>
      </tr>
      <tr>
        <td>Tensorboard Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/crud-web-apps/volumes">v1.9.0</a>
        </td>
      </tr>
      <tr>
        <td>Volumes Web App</td>
        <td>
          <a href="https://github.com/kubeflow/kubeflow/tree/v1.9.0/components/crud-web-apps/tensorboards">v1.9.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Pipelines Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Pipelines Working Group</td>
        <td>Kubeflow Pipelines</td>
        <td>
          <a href="https://github.com/kubeflow/pipelines/releases/tag/2.2.0">v2.2.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Serving Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Serving Working Group</td>
        <td>KServe</td>
        <td>
          <a href="https://github.com/kserve/kserve/releases/tag/v0.13.0">v0.13.0</a>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Training Working Group -->
      <!-- ======================= -->
      <tr>
        <td rowspan="1" class="align-middle">Training Working Group</td>
        <td>Training Operator</td>
        <td>
          <a href="https://github.com/kubeflow/training-operator/releases/tag/v1.8.0">v1.8.0</a>
        </td>
      </tr>
  </tbody>
</table>
</div>

### Dependency Versions (Manifests)

{{% alert title="Note" color="warning" %}}
This information is only for the manifests found in the <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a> repository, packaged distributions may have different requirements or supported versions.
{{% /alert %}}

<div class="table-responsive">
<table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Dependency</th>
        <th>Validated or Included Version(s)</th>
        <th>Notes</th>
      </tr>
    </thead>
  <tbody>
      <!-- ======================= -->
      <!-- Kubernetes -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kubernetes.io/">Kubernetes</a>
        </td>
        <td>1.29</td>
        <td rowspan="4" class="align-middle">
          <i>Other versions may work, but have not been validated by the <a href="https://github.com/kubeflow/community/tree/master/wg-manifests">Kubeflow Manifests Working Group</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Istio -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://istio.io/">Istio</a>
        </td>
        <td>1.22.1</td>
      </tr>
      <!-- ======================= -->
      <!-- cert-manager  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://cert-manager.io/">cert-manager</a>
        </td>
        <td>1.14.5</td>
      </tr>
      <!-- ======================= -->
      <!-- dex  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://dexidp.io/">dex</a>
        </td>
        <td>2.39.1</td>
      </tr>
      <!-- ======================= -->
      <!-- Kustomize  -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://kustomize.io/">Kustomize</a>
        </td>
        <td>5.2.1</td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Serving -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/serving/">Knative Serving</a>
        </td>
        <td>1.12.4</td>
        <td rowspan="2" class="align-middle">
          <i>Knative is only needed when using the optional <a href="https://kserve.github.io/website/">KServe Component</a>.</i>
        </td>
      </tr>
      <!-- ======================= -->
      <!-- Knative Eventing -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://knative.dev/docs/eventing/">Knative Eventing</a>
        </td>
        <td>1.12.6</td>
      </tr>
      <!-- ======================= -->
      <!-- OAuth2-proxy -->
      <!-- ======================= -->
      <tr>
        <td>
          <a href="https://github.com/oauth2-proxy/oauth2-proxy/releases/tag/v7.6.0">OAuth2 Proxy</a>
        </td>
        <td>7.6.0</td>
      </tr>
  </tbody>
</table>
</div>



================================================
File: content/en/docs/started/OWNERS
================================================
approvers:
  - RFMVasconcelos
  - 8bitmp3

reviewers:
  - knkski
  - alfsuse



================================================
File: content/en/docs/started/_index.md
================================================
+++
title = "Getting Started"
description = "How to get started with Kubeflow"
weight = 20
+++



================================================
File: content/en/docs/started/architecture.md
================================================
+++
title = "Architecture"
description = "An overview of Kubeflow's architecture"
weight = 10
+++

This guide introduces Kubeflow ecosystem and explains how Kubeflow components fit in ML lifecycle.

Read [the introduction guide](/docs/started/introduction) to learn more about Kubeflow, standalone
Kubeflow components and Kubeflow Platform.

## Kubeflow Ecosystem

The following diagram gives an overview of the Kubeflow Ecosystem and how it relates to the wider
Kubernetes and AI/ML landscapes.

<img src="/docs/started/images/kubeflow-architecture.drawio.svg"
  alt="An architectural overview of Kubeflow on Kubernetes"
  class="mt-3 mb-3">

Kubeflow builds on [Kubernetes](https://kubernetes.io/) as a system for
deploying, scaling, and managing AI/ML infrastructure.

## Introducing the ML Lifecycle

When you develop and deploy an AI application, the ML lifecycle typically consists of
several stages. Developing an ML system is an iterative process.
You need to evaluate the output of various stages of the ML lifecycle, and apply
changes to the model and parameters when necessary to ensure the model keeps
producing the results you need.

The following diagram shows the ML lifecycle stages in sequence:

<img src="/docs/started/images/ml-lifecycle.drawio.svg"
  alt="ML Lifecycle"
  class="mt-3 mb-3">

Looking at the stages in more detail:

- In the _Data Preparation_ step you ingest raw data, perform feature engineering to extract ML
  features for the offline feature store, and prepare training data for model development.
  Usually, this step is associated with data processing tools such as Spark, Dask, Flink, or Ray.

- In the _Model Development_ step you choose an ML framework, develop your model architecture and
  explore the existing pre-trained models for fine-tuning like BERT or Llama.

- In the _Model Optimization_ step you can optimize your model hyperparameters and optimize your
  model with various AutoML algorithms such as neural architecture search and model compression.
  During model optimization you can store ML metadata in the _Model Registry_.

- In the _Model Training_ step you train or fine-tune your model on the large-scale
  compute environment. You should use a distributed training if single GPU can't handle your
  model size. The results of the model training is the trained model artifact that you
  can store in the _Model Registry_.

- In the _Model Serving_ step you serve your model artifact for online or batch inference. Your
  model may perform predictive or generative AI tasks depending on the use-case. During the model
  serving step you may use an online feature store to extract features. You monitor the model
  performance, and feed the results into your previous steps in the ML lifecycle.

### ML Lifecycle for Production and Development Phases

The ML lifecycle for AI applications may be conceptually split between _development_ and
_production_ phases, this diagram explores which stages fit into each phase:

<img src="/docs/started/images/ml-lifecycle-dev-prod.drawio.svg"
  alt="ML Lifecycle with Development and Production"
  class="mt-3 mb-3">

### Kubeflow Components in the ML Lifecycle

The next diagram shows how Kubeflow components are used for each stage in the ML lifecycle:

<img src="/docs/started/images/ml-lifecycle-kubeflow.drawio.svg"
  alt="Kubeflow Components in ML Lifecycle"
  class="mt-3 mb-3">

See the following links for more information about each Kubeflow component:

- [Kubeflow Spark Operator](https://github.com/kubeflow/spark-operator) can be used for data
  preparation and feature engineering step.

- [Kubeflow Notebooks](/docs/components/notebooks/) can be used for model development and interactive
  data science to experiment with your ML workflows.

- [Kubeflow Katib](/docs/components/katib/) can be used for model optimization and hyperparameter
  tuning using various AutoML algorithms.

- [Kubeflow Trainer](/docs/components/trainer/) can be used for large-scale distributed
  training or LLM fine-tuning.

- [Kubeflow Model Registry](/docs/components/model-registry/) can be used to store ML metadata,
  model artifacts, and preparing models for production serving.

- [KServe](https://kserve.github.io/website/master/) can be used for online and batch inference
  in the model serving step.

- [Feast](https://feast.dev/) can be used as a feature store and to manage offline and online
  features.

- [Kubeflow Pipelines](/docs/components/pipelines/) can be used to build, deploy, and manage each
  step in the ML lifecycle.

You can use most Kubeflow components as
[standalone tools](/docs/started/introduction/#what-are-standalone-kubeflow-components) and
integrate them into your existing AI/ML Platform, or you can deploy the full
[Kubeflow Platform](/docs/started/introduction/#what-is-kubeflow-platform) to get all Kubeflow
components for an end-to-end ML lifecycle.

## Kubeflow Interfaces

This section introduces the interfaces that you can use to interact with
Kubeflow and to build and run your ML workflows on Kubeflow.

### Kubeflow User Interface (UI)

The Kubeflow Central Dashboard looks like this:

<img src="/docs/images/dashboard/homepage.png" 
     alt="Kubeflow Central Dashboard - Homepage" 
     class="mt-3 mb-3 border border-info rounded">
</img>

The Kubeflow Platform includes [Kubeflow Central Dashboard](/docs/components/central-dash/overview/)
which acts as a hub for your ML platform and tools by exposing the UIs of components running in the
cluster.

### Kubeflow APIs and SDKs

<!--
TODO (andreyvelich): Add reference docs once this issue is implemented: https://github.com/kubeflow/katib/issues/2081
-->

Various components of Kubeflow offer APIs and Python SDKs.

See the following sets of reference documentation:

- [Pipelines reference docs](/docs/components/pipelines/reference/) for the Kubeflow
  Pipelines API and SDK, including the Kubeflow Pipelines domain-specific
  language (DSL).
- [Kubeflow Python SDK](https://github.com/kubeflow/trainer/blob/master/sdk/kubeflow/trainer/api/trainer_client.py)
  to interact with Kubeflow Trainer APIs and to manage TrainJobs.
- [Katib Python SDK](https://github.com/kubeflow/katib/blob/086093fed72610c227e3ae1b4044f27afa940852/sdk/python/v1beta1/kubeflow/katib/api/katib_client.py)
  to manage Katib hyperparameter tuning Experiments using Python APIs.

## Next steps

- Follow [Installing Kubeflow](/docs/started/installing-kubeflow/) to set up your environment and install Kubeflow.



================================================
File: content/en/docs/started/introduction.md
================================================
+++
title = "Introduction"
description = "An introduction to Kubeflow"
weight = 1
+++

## What is Kubeflow

Kubeflow is a community and ecosystem of open-source projects to address each stage in the
[machine learning (ML) lifecycle](/docs/started/architecture/#kubeflow-components-in-the-ml-lifecycle)
with support for best-in-class open source
[tools and frameworks](/docs/started/architecture/#kubeflow-ecosystem). Kubeflow makes AI/ML
on Kubernetes simple, portable, and scalable.

Whether you’re a researcher, data scientist, ML engineer, or a team of developers, Kubeflow offers
modular and scalable tools that cater to all aspects of the ML lifecycle: from building ML models to
deploying them to production for AI applications.

## What are Standalone Kubeflow Components

The Kubeflow ecosystem is composed of multiple open-source projects that address different aspects
of the ML lifecycle. Many of these projects are designed to be usable both within the
Kubeflow Platform and independently. These Kubeflow components can be installed standalone on a
Kubernetes cluster. It provides flexibility to users who may not require the full Kubeflow Platform
capabilities but wish to leverage specific ML functionalities such as model training or model serving.

## What is Kubeflow Platform

The Kubeflow Platform refers to the full suite of Kubeflow components bundled together with
additional integration and management tools. Using Kubeflow as a platform means deploying a
comprehensive ML toolkit for the entire ML lifecycle.

In addition to the standalone Kubeflow components, the Kubeflow Platform includes

- [Kubeflow Notebooks](/docs/components/notebooks/overview) for interactive data exploration and
  model development.
- [Central Dashboard](/docs/components/central-dash/overview/) for easy navigation and management
  with [Kubeflow Profiles](/docs/components/central-dash/profiles/) for access control.
- Additional tooling for data management (PVC Viewer), visualization (TensorBoards), and more.

The Kubeflow Platform can be installed via
[Packaged Distributions](/docs/started/installing-kubeflow/#packaged-distributions) or
[Kubeflow Manifests](/docs/started/installing-kubeflow/#kubeflow-manifests).

## Kubeflow Overview Diagram

The following diagram shows the main Kubeflow components to cover each stage of the ML lifecycle
on top of Kubernetes.

<img src="/docs/started/images/kubeflow-intro-diagram.drawio.svg"
  alt="Kubeflow overview"
  class="mt-3 mb-3">

Read the [architecture overview](/docs/started/architecture/) to learn about the Kubeflow ecosystem
and to see how Kubeflow components fit in ML lifecycle.

## Kubeflow Video Introduction

Watch the following video which provides an introduction to Kubeflow.

{{< youtube id="cTZArDgbIWw" title="Introduction to Kubeflow">}}

## The Kubeflow mission

Our goal is to make scaling machine learning (ML) models and deploying them to
production as simple as possible, by letting Kubernetes do what it's great at:

- Easy, repeatable, portable deployments on a diverse infrastructure
  (for example, experimenting on a laptop, then moving to an on-premises
  cluster or to the cloud)
- Deploying and managing loosely-coupled microservices
- Scaling based on demand

Because ML practitioners use a diverse set of tools, one of the key goals is to
customize the stack based on user requirements (within reason) and let the
system take care of the "boring stuff". While we have started with a narrow set
of technologies, we are working with many different projects to include
additional tooling.

Ultimately, we want to have a set of simple manifests that give you an easy to
use ML stack _anywhere_ Kubernetes is already running, and that can self
configure based on the cluster it deploys into.

## History

Kubeflow started as an open sourcing of the way Google ran [TensorFlow](https://www.tensorflow.org/) internally, based on a pipeline called [TensorFlow Extended](https://www.tensorflow.org/tfx/).
It began as just a simpler way to run TensorFlow jobs on Kubernetes, but has since expanded to be a multi-architecture, multi-cloud framework for running end-to-end machine learning workflows.

The [Kubeflow logo represents](https://github.com/kubeflow/kubeflow/issues/187#issuecomment-375194419) the letters `K` and `F` inside the heptagon of the Kubernetes logo, which represent two communities: `Kubernetes` (cloud-native) and `flow` (Machine Learning). In this context, `flow` is not only indicating `TensorFlow`, but also all ML frameworks which make use of Dataflow Graph as the normal form for model/algorithm implementation.

## Roadmaps

To see what's coming up in future versions of Kubeflow, refer to the [Kubeflow roadmap](https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md).

The following components also have roadmaps:

- [Kubeflow Pipelines](https://github.com/kubeflow/pipelines/blob/master/ROADMAP.md)
- [KServe](https://github.com/kserve/kserve/blob/master/ROADMAP.md)
- [Katib](https://github.com/kubeflow/katib/blob/master/ROADMAP.md)
- [Training Operator](https://github.com/kubeflow/training-operator/blob/master/ROADMAP.md)

## Getting involved

There are many ways to contribute to Kubeflow, and we welcome contributions!

Read the [contributor's guide](/docs/about/contributing/) to get started on the code, and learn about the community on the [community page](/docs/about/community/).

## Next Steps

- Follow [the installation guide](/docs/started/installing-kubeflow) to deploy standalone
  Kubeflow components or Kubeflow Platform.



================================================
File: content/en/docs/started/kubeflow-examples.md
================================================
+++
title = "Examples"
description = "Examples that demonstrate machine learning with Kubeflow"
weight = 99
+++

{{% alert title="Warning" color="warning" %}}
Some examples in [kubeflow/examples](https://github.com/kubeflow/examples) repository have not been tested with newer versions of Kubeflow. Please refer to the README of your chosen example.
{{% /alert %}}


{{% blocks/sample-section title="MNIST image classification"
  kfctl="v1.0.0"
  url="https://github.com/kubeflow/examples/tree/master/mnist"
  api="https://api.github.com/repos/kubeflow/examples/commits?path=mnist&sha=master" %}}
Train and serve an image classification model using the MNIST dataset.
This tutorial takes the form of a Jupyter notebook running in your Kubeflow
cluster.
You can choose to deploy Kubeflow and train the model on various clouds, 
including Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud, 
Microsoft Azure, and on-premises. Serve the model with TensorFlow Serving.
{{% /blocks/sample-section %}}

{{% blocks/sample-section title="Financial time series"
  kfctl="v0.7"
  url="https://github.com/kubeflow/examples/tree/master/financial_time_series"
  api="https://api.github.com/repos/kubeflow/examples/commits?path=financial_time_series&sha=master" %}}
Train and serve a model for financial time series analysis using TensorFlow on
Google Cloud Platform (GCP). Use the Kubeflow Pipelines SDK to automate the 
workflow.
{{% /blocks/sample-section %}}

## Next steps

Work through one of the 
[Kubeflow Pipelines samples](/docs/components/pipelines/legacy-v1/tutorials/build-pipeline/).



================================================
File: content/en/docs/started/support.md
================================================
+++
title = "Get Support"
description = "Where to get support for Kubeflow"
weight = 80
+++

This page describes the Kubeflow resources and support options available when you encounter a problem, have a question, or want to make a suggestion about Kubeflow.

<a id="application-status"></a>
## Component Status

Please make youself familiar with the [structure of Kubeflow](https://www.kubeflow.org/docs/started/introduction/#what-is-kubeflow) first. 
When you deploy Kubeflow to a Kubernetes cluster, your deployment includes multiple projects. Note that project versioning is independent from Kubeflow Platform version. Each project should meet certain [criteria](https://github.com/kubeflow/community/blob/master/guidelines/application_requirements.md) regarding stability, upgradability, logging, monitoring, and security (PodSecurityStandards restricted, network policies, and integration tests for authentication and authorization).

Component status indicators:

* **Stable**: The application complies with most of the criteria and is considered stable for this release.
* **Beta**: The application is progressing towards meeting most criteria.
* **Alpha**: The application is in an early development or integration stage.

<a id="levels-of-support"></a>
## Levels of Support

1. The Kubeflow community provides best-effort support for stable components.
2. You can also request commercial support from a company or freelancer listed below.

<a id="community-support"></a>
## Support from the Kubeflow community

Kubeflow has an active and helpful community of users and contributors. 
The Kubeflow community provides support on a best-effort basis for stable and beta
applications. If you need commercial support, please check the sections below.

**Best-effort support** means that there's no formal agreement or
commitment to solve a problem but the community appreciates the
importance of addressing the problem as soon as possible. The community commits
to helping you diagnose and address the problem if all of the following requirements are satisfied:

* The cause falls within the technical framework that Kubeflow controls. For
  example, the Kubeflow community may not be able to help if the problem is 
  caused by a specific network configuration within your organization.
* Community members can reproduce the problem.
* The reporter can assist with troubleshooting.

You can ask questions and make suggestions in the following places:

* **Slack** for online chat and messaging, see [Slack workspace and channels](/docs/about/community/#kubeflow-slack-channels).
* **GitHub discussions** per repository, e.g. [here](https://github.com/kubeflow/manifests/discussions)
* **Kubeflow discuss** for email-based group discussion. Join the
  [kubeflow-discuss](/docs/about/community/#kubeflow-mailing-list)
  group.
* **Kubeflow documentation** for overviews and how-to guides. In particular,
  refer to the following documents when troubleshooting a problem:
  * [Kubeflow installation and setup](/docs/started/installing-kubeflow/)
  * [Kubeflow components](/docs/components/)

* **Kubeflow issue trackers** for known issues, questions, and feature requests.
  Search the open issues to see if someone else has already logged the problem 
  that you are encountering and learn about any workarounds. If no one
  has logged your problem, create a new issue to describe the problem.

    Each Kubeflow component has its own issue tracker within the [Kubeflow
    organization on GitHub](https://github.com/kubeflow). To get you started,
    here are the primary issue trackers:
  * [Kubeflow Spark Operator](https://github.com/kubeflow/spark-operator/issues)
  * [Kubeflow Pipelines](https://github.com/kubeflow/pipelines/issues)
  * [Kubeflow Katib](https://github.com/kubeflow/katib/issues)
  * [Kubeflow Trainer](https://github.com/kubeflow/trainer/issues)
  * [Kubeflow Notebooks](https://github.com/kubeflow/notebooks/issues)
  * [Kubeflow Model Registry](https://github.com/kubeflow/model-registry/issues)
  * [Kubeflow Dashboard](https://github.com/kubeflow/dashboard/issues)
  * [Kubeflow Kserve](https://github.com/kserve/kserve/issues)
  * [Kubeflow Platform / Manifests](https://github.com/kubeflow/manifests/issues)
  * [Kubeflow Website](https://github.com/kubeflow/website/issues)

<a id="provider-support"></a>
## Support from commercial providers in the Kubeflow Ecosystem

We want to promote commercial companies and idividuals that contribute back to the open source project.
Below is a table of organizations that contribute to Kubeflow and offer commercial support:

| Provider                      | Support Link                                                                                                                                                    |
|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Aranui Solutions              | [Aranui Solutions](https://www.aranui.solutions/services)                                                                                                       |
| Canonical                     | [Ubuntu Kubeflow](https://ubuntu.com/kubeflow#get-in-touch)                                                                                                     |
| Freelancer Julius von Kohout  | [LinkedIn](https://de.linkedin.com/in/juliusvonkohout/), [Slack](https://cloud-native.slack.com/team/U06LW431SJF), [GitHub](https://github.com/juliusvonkohout) |
| Red Hat                       | [Red Hat](https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-ai)                                                                        |
| Other Providers               | Please reach out to the Kubeflow Steering Committee with proof of significant contributions to the Kubeflow open source project                                 |

<a id="cloud-support"></a>
If you are using a managed offer from a cloud provider for Kubeflow, then the cloud
provider may be able to help you diagnose and solve a problem.

## Getting Involved

You can participate in Kubeflow by contributing funding, code, documentation, use cases or by joining community meetings. For more information, see the [Kubeflow Community page](/docs/about/community/).

## Stay Updated

Keep up with Kubeflow news:
* The [community page](https://www.kubeflow.org/docs/about/community/) with Slack channels, regular meetings and other guidelines.
* The [Kubeflow Blog](https://blog.kubeflow.org/) for release announcements, events, and tutorials.
* [Kubeflow on Twitter](https://twitter.com/kubeflow) for technical tips.
* Release notes for detailed updates on each Kubeflow application.




================================================
File: content/en/docs/started/installing-kubeflow/get_new_releases.sh
================================================
#!/usr/bin/env bash

set -euo pipefail

THIS_SCRIPT_PATH=$(cd "$(dirname "$0")" && pwd)
cd "$THIS_SCRIPT_PATH"

GITHUB_ORGANIZATION="kubeflow"
GITHUB_REPOSITORY="manifests"

# ensure bash version 4.4+
if [[ ${BASH_VERSINFO[0]} -lt 4 || (${BASH_VERSINFO[0]} -eq 4 && ${BASH_VERSINFO[1]} -lt 4) ]]; then
  echo ">>> ERROR: Bash version 4.4+ is required to run this script, current version: '${BASH_VERSION}'"
  exit 1
fi

# ensure 'jq' is installed
if [[ -z "$(command -v jq)" ]]; then
  echo ">>> ERROR: 'jq' must be installed to run this script"
  exit 1
fi

#######################################
# Latest release
#######################################

# fetch the latest release from the GitHub API
GITHUB_API_URL="https://api.github.com/repos/${GITHUB_ORGANIZATION}/${GITHUB_REPOSITORY}/releases/latest"
echo ">>> Fetching latest release from: ${GITHUB_API_URL}"
latest_release_json=$(curl -sSfL "$GITHUB_API_URL")

# get the latest release details
latest_release_tag=$(echo "$latest_release_json" | jq -r '.tag_name')
latest_release_url=$(echo "$latest_release_json" | jq -r '.html_url')
latest_commit_date=$(echo "$latest_release_json" | jq -r '.created_at')
latest_publish_date=$(echo "$latest_release_json" | jq -r '.published_at')

# create the latest release file
latest_release_file="./release-info/latest.json"
echo ">>> Updating latest release file: ${latest_release_file}"
cat > "$latest_release_file" <<EOF
{
  "tag": "${latest_release_tag}",
  "url": "${latest_release_url}",
  "commit_date": "${latest_commit_date}",
  "publish_date": "${latest_publish_date}"
}
EOF

#######################################
# All releases
#######################################

# fetch the releases from the GitHub API
GITHUB_API_URL="https://api.github.com/repos/${GITHUB_ORGANIZATION}/${GITHUB_REPOSITORY}/releases?per_page=100"
echo ">>> Fetching releases from: ${GITHUB_API_URL}"
releases_json=$(curl -sSfL "$GITHUB_API_URL")

# get the list of releases
releases_list=$(echo "$releases_json" | jq -c '.[]?')

# for each release, update its .json file in this folder
IFS=$'\n'
for release_json in $releases_list; do

  # skip pre-releases and drafts
  is_pre_release=$(echo "$release_json" | jq -r '.prerelease')
  is_draft=$(echo "$release_json" | jq -r '.draft')
  if [[ "$is_pre_release" == "true" || "$is_draft" == "true" ]]; then
    continue
  fi

  # get the release details
  release_tag=$(echo "$release_json" | jq -r '.tag_name')
  release_url=$(echo "$release_json" | jq -r '.html_url')
  commit_date=$(echo "$release_json" | jq -r '.created_at')
  publish_date=$(echo "$release_json" | jq -r '.published_at')

  # create the release file
  release_file="./release-info/${release_tag}.json"
  echo ">>> Updating release file: ${release_file}"
  cat > "$release_file" <<EOF
{
  "tag": "${release_tag}",
  "url": "${release_url}",
  "commit_date": "${commit_date}",
  "publish_date": "${publish_date}"
}
EOF

done


================================================
File: content/en/docs/started/installing-kubeflow/index.md
================================================
+++
title = "Installing Kubeflow"
description = "Deployment options for Kubeflow"
weight = 20

+++

This guide describes how to install standalone Kubeflow components or Kubeflow Platform using package
distributions or Kubeflow manifests.

Read [the introduction guide](/docs/started/introduction) to learn more about Kubeflow, standalone
Kubeflow components and Kubeflow Platform.

## Installation Methods

You can install Kubeflow using one of these methods:

- [**Standalone Kubeflow Components**](#standalone-kubeflow-components)
- [**Kubeflow Platform**](#kubeflow-platform)

## Standalone Kubeflow Components

Some components in the [Kubeflow ecosystem](/docs/started/architecture/#kubeflow-ecosystem) may be
deployed as standalone services, without the need to install the full Kubeflow Platform. You might
integrate these services as part of your existing AI/ML platform or use them independently.

These components are a quick and easy method to get started with the Kubeflow ecosystem. They
provide flexibility to users who may not require the capabilities of a full Kubeflow Platform.

The following table lists Kubeflow components that may be deployed in a standalone mode. It also
lists their associated GitHub repository and
corresponding [ML lifecycle stage](/docs/started/architecture/#kubeflow-components-in-the-ml-lifecycle).

<div class="table-responsive distributions-table">
  <table class="table table-bordered">
    <thead>
      <tr>
        <th>Component</th>
        <th>ML Lifecycle Stage</th>
        <th>Source Code</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>
         <a href="https://kserve.github.io/website/master/admin/serverless/serverless">
            KServe
          </a>
        </td>
        <td>
          Model Serving
        </td>
        <td>
          <a href="https://github.com/kserve/kserve">
            <code>kserve/kserve</code>
          </a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="/docs/components/katib/installation/#installing-katib">
            Kubeflow Katib
          </a>
        </td>
        <td>
          Model Optimization and AutoML
        </td>
        <td>
          <a href="https://github.com/kubeflow/katib">
            <code>kubeflow/katib</code>
          </a>
        </td>
      </tr>
      <tr>
        <td>
         <a href="/docs/components/model-registry/installation/#installing-model-registry">
            Kubeflow Model Registry
          </a>
        </td>
        <td>
          Model Registry
        </td>
        <td>
          <a href="https://github.com/kubeflow/model-registry">
            <code>kubeflow/model-registry</code>
          </a>
        </td>
      </tr>
      <tr>
        <td>
         <a href="/docs/components/trainer/legacy-v1/user-guides/mpi/#installation">
            Kubeflow MPI Operator
          </a>
        </td>
        <td>
          All-Reduce Model Training
        </td>
        <td>
          <a href="https://github.com/kubeflow/mpi-operator">
            <code>kubeflow/mpi-operator</code>
          </a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="/docs/components/pipelines/operator-guides/installation/">
            Kubeflow Pipelines
          </a>
        </td>
        <td>
          ML Workflows and Schedules
        </td>
        <td>
          <a href="https://github.com/kubeflow/pipelines">
            <code>kubeflow/pipelines</code>
          </a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="/docs/components/spark-operator/getting-started#installation">
            Kubeflow Spark Operator
          </a>
        </td>
        <td>
          Data Preparation
        </td>
        <td>
          <a href="https://github.com/kubeflow/spark-operator">
            <code>kubeflow/spark-operator</code>
          </a>
        </td>
      </tr>
      <tr>
        <td>
          <a href="/docs/components/trainer/getting-started">
            Kubeflow Trainer
          </a>
        </td>
        <td>
          Model Training and LLMs Fine-Tuning
        </td>
        <td>
          <a href="https://github.com/kubeflow/training-operator">
            <code>kubeflow/training-operator</code>
          </a>
        </td>
      </tr>
    </tbody>
  </table>
</div>

## Kubeflow Platform

You can use one of the following methods to install the [Kubeflow Platform](/docs/started/introduction/#what-is-kubeflow-platform)
and get the full suite of Kubeflow components bundled together with additional tools.

### Packaged Distributions

Packaged distributions are maintained by various organizations and typically aim to provide
a simplified installation and management experience for your **Kubeflow Platform**. 
Some can be deployed on multiple [Kubernetes distributions](https://kubernetes.io/partners/#conformance),
while others target a specific platform (e.g. EKS or GKE).

{{% alert title="" color="dark" %}}
Packaged distributions are developed and supported by their respective maintainers.
The Kubeflow community <strong>does not endorse or certify</strong> any specific distribution.
{{% /alert %}}

The following table lists distributions which are <em>maintained</em> by their respective maintainers:

<div class="table-responsive distributions-table">
  <table class="table table-bordered">
    <thead>
      <tr>
        <th>Maintainer
          <br><small>Distribution Name</small>
        </th>
        <th>Kubeflow Version</th>
        <th>Target Platform</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>
          Amazon Web Services
        </td>
        <td>
          {{< kf-version-notice >}}{{% aws/latest-version %}}{{< /kf-version-notice >}}
          <sup><a href="https://github.com/awslabs/kubeflow-manifests/releases">[release notes]</a></sup>
        </td>
        <td>
          Amazon Elastic Kubernetes Service (EKS)
        </td>
        <td>
          <a href="https://awslabs.github.io/kubeflow-manifests">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          Aranui Solutions
            <br><small>deployKF</small>
        </td>
        <td>
          {{< kf-version-notice >}}{{% deploykf/latest-version %}}{{< /kf-version-notice >}}
          <sup><a href="https://www.deploykf.org/releases/tool-versions/#kubeflow-ecosystem">[version matrix]</a></sup>
        </td>
        <td>
          Multiple
          <sup><a href="https://www.deploykf.org/guides/getting-started/#kubernetes-cluster">[list]</a></sup>
        </td>
        <td>
          <a href="https://www.deploykf.org/">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          Canonical
            <br><small>Charmed Kubeflow</small>
        </td>
        <td>
          {{< kf-version-notice >}}{{% canonical/latest-version %}}{{< /kf-version-notice >}}
          <sup><a href="https://charmed-kubeflow.io/docs/release-notes">[release notes]</a></sup>
        </td>
        <td>
          Multiple
        </td>
        <td>
          <a href="https://charmed-kubeflow.io/">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          Google Cloud
        </td>
        <td>
          {{< kf-version-notice >}}{{% gke/latest-version %}}{{< /kf-version-notice >}}
          <sup><a href="https://googlecloudplatform.github.io/kubeflow-gke-docs/docs/changelog/">[release notes]</a></sup>
        </td>
        <td>
          Google Kubernetes Engine (GKE)
        </td>
        <td>
          <a href="https://googlecloudplatform.github.io/kubeflow-gke-docs">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          IBM Cloud
        </td>
        <td>
          {{< kf-version-notice >}}{{% iks/latest-version %}}{{< /kf-version-notice >}}
          <sup><a href="https://github.com/IBM/manifests/releases">[release notes]</a></sup>
        </td>
        <td>
          IBM Cloud Kubernetes Service (IKS)
        </td>
        <td>
          <a href="https://ibm.github.io/manifests/">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          Microsoft Azure
        </td>
        <td>
          {{< kf-version-notice >}}{{% azure/latest-version %}}{{< /kf-version-notice >}}
          <sup><a href="https://github.com/Azure/kubeflow-aks/releases">[release notes]</a></sup>
        </td>
        <td>
          Azure Kubernetes Service (AKS)
        </td>
        <td>
          <a href="https://azure.github.io/kubeflow-aks/main">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          Nutanix
        </td>
        <td>
          {{< kf-version-notice >}}{{% nutanix/latest-version %}}{{< /kf-version-notice >}}
        </td>
        <td>
          Nutanix Kubernetes Engine
        </td>
        <td>
          <a href="https://nutanix.github.io/kubeflow-manifests">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          QBO
        </td>
        <td>
          {{< kf-version-notice >}}{{% qbo/latest-version %}}{{< /kf-version-notice >}}
          <sup><a href="https://github.com/alexeadem/qbo-ce/blob/main/CHANGELOG.md">[release notes]</a></sup>
        </td>
        <td>
          QBO Kubernetes Engine (QKE)
        </td>
         <td>
          <a href="https://docs.qbo.io/#/qke?id=kubeflow">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          Red Hat
            <br><small>Open Data Hub</small>
        </td>
        <td>
          {{< kf-version-notice >}}{{% redhat/latest-version %}}{{< /kf-version-notice >}}
        </td>
        <td>
          OpenShift
        </td>
        <td>
          <a href="https://github.com/opendatahub-io/manifests">Website</a>
        </td>
      </tr>
      <tr>
        <td>
          VMware
        </td>
        <td>
          {{< kf-version-notice >}}{{% vmware/latest-version %}}{{< /kf-version-notice >}}
        </td>
        <td>
          VMware vSphere
        </td>
        <td>
          <a href="https://vmware.github.io/vSphere-machine-learning-extension/">Website</a>
        </td>
      </tr>
    </tbody>
  </table>
</div>

### Kubeflow Manifests

The Kubeflow manifests are a collection of community maintained manifests to install Kubeflow in popular Kubernetes clusters such as Kind (locally), Minikube (locally), Rancher, EKS, AKS, GKE.
They are aggregated by the Manifests Working Group and are intended to be
used by users with Kubernetes knowledge and as the base of packaged distributions.

Kubeflow Manifests contain all Kubeflow Components, Kubeflow Central Dashboard, and other Kubeflow
applications that comprise the **Kubeflow Platform**. This installation is helpful when you want to
try out the end-to-end Kubeflow Platform capabilities.
 
If you want a stable / conservative experience we recommend to use the [latest stable release](https://github.com/kubeflow/manifests/releases): 
- [**Kubeflow 1.9:**](/docs/releases/kubeflow-1.9/)
  - [`v1.9.1`](https://github.com/kubeflow/manifests/tree/v1.9.1#installation)

You can also install the master branch of [`kubeflow/manifests`](https://github.com/kubeflow/manifests) by following the instructions [here](https://github.com/kubeflow/manifests?tab=readme-ov-file#installation) and provide us feedback.

## Next steps

- Review our [introduction to Kubeflow](/docs/started/introduction/).
- Explore the [architecture of Kubeflow](/docs/started/architecture).
- Learn more about the [components of Kubeflow](/docs/components/).



================================================
File: content/en/docs/started/installing-kubeflow/release-info/latest.json
================================================
{
  "tag": "v1.9.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.9.1",
  "commit_date": "2024-10-24T20:57:31Z",
  "publish_date": "2024-10-28T14:09:47Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v0.6.0.json
================================================
{
  "tag": "v0.6.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v0.6.0",
  "commit_date": "2019-07-18T20:34:19Z",
  "publish_date": "2019-07-18T21:24:06Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v0.6.1.json
================================================
{
  "tag": "v0.6.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v0.6.1",
  "commit_date": "2019-07-26T22:45:58Z",
  "publish_date": "2019-07-30T21:52:06Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v0.6.2.json
================================================
{
  "tag": "v0.6.2",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v0.6.2",
  "commit_date": "2019-08-24T19:58:23Z",
  "publish_date": "2019-08-28T02:45:15Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.0.1.json
================================================
{
  "tag": "v1.0.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.0.1",
  "commit_date": "2020-03-18T19:58:42Z",
  "publish_date": "2020-03-18T20:15:26Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.0.2.json
================================================
{
  "tag": "v1.0.2",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.0.2",
  "commit_date": "2020-04-16T23:17:27Z",
  "publish_date": "2020-04-17T22:00:20Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.1.0.json
================================================
{
  "tag": "v1.1.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.1.0",
  "commit_date": "2020-07-26T00:07:36Z",
  "publish_date": "2020-11-09T08:16:45Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.2.0.json
================================================
{
  "tag": "v1.2.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.2.0",
  "commit_date": "2020-11-20T17:12:51Z",
  "publish_date": "2020-11-20T19:29:14Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.3.0.json
================================================
{
  "tag": "v1.3.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.3.0",
  "commit_date": "2021-04-21T22:48:42Z",
  "publish_date": "2021-06-02T21:53:55Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.3.1.json
================================================
{
  "tag": "v1.3.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.3.1",
  "commit_date": "2021-07-20T09:49:15Z",
  "publish_date": "2021-08-03T14:14:30Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.4.0.json
================================================
{
  "tag": "v1.4.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.4.0",
  "commit_date": "2021-10-08T17:55:24Z",
  "publish_date": "2021-10-12T03:15:04Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.4.1.json
================================================
{
  "tag": "v1.4.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.4.1",
  "commit_date": "2021-12-22T16:25:41Z",
  "publish_date": "2021-12-23T22:13:01Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.5.0.json
================================================
{
  "tag": "v1.5.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.5.0",
  "commit_date": "2022-03-10T17:01:57Z",
  "publish_date": "2022-03-10T17:32:42Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.5.1.json
================================================
{
  "tag": "v1.5.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.5.1",
  "commit_date": "2022-06-15T18:35:53Z",
  "publish_date": "2022-06-15T18:44:13Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.6.0.json
================================================
{
  "tag": "v1.6.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.6.0",
  "commit_date": "2022-09-07T17:41:26Z",
  "publish_date": "2022-09-07T18:31:34Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.6.1.json
================================================
{
  "tag": "v1.6.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.6.1",
  "commit_date": "2022-10-10T15:03:58Z",
  "publish_date": "2022-10-10T15:31:34Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.7.0.json
================================================
{
  "tag": "v1.7.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.7.0",
  "commit_date": "2023-03-29T15:17:36Z",
  "publish_date": "2023-03-29T15:47:56Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.8.0.json
================================================
{
  "tag": "v1.8.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.8.0",
  "commit_date": "2023-11-01T16:15:57Z",
  "publish_date": "2023-11-01T16:22:22Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.8.1.json
================================================
{
  "tag": "v1.8.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.8.1",
  "commit_date": "2024-03-04T12:04:42Z",
  "publish_date": "2024-04-08T16:53:00Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.9.0.json
================================================
{
  "tag": "v1.9.0",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.9.0",
  "commit_date": "2024-07-22T11:30:01Z",
  "publish_date": "2024-07-22T11:39:31Z"
}



================================================
File: content/en/docs/started/installing-kubeflow/release-info/v1.9.1.json
================================================
{
  "tag": "v1.9.1",
  "url": "https://github.com/kubeflow/manifests/releases/tag/v1.9.1",
  "commit_date": "2024-10-24T20:57:31Z",
  "publish_date": "2024-10-28T14:09:47Z"
}



================================================
File: content/en/docs/videos/taxi_custom_visualization.webm
================================================
[Non-text file]


================================================
File: content/en/docs/videos/tfdv_example_with_taxi_pipeline.webm
================================================
[Non-text file]


================================================
File: content/en/events/OWNERS
================================================
approvers:
  - akgraner
  - jbottum
  - thesuperzapper


================================================
File: content/en/events/_index.md
================================================
+++
title = "Kubeflow Events"
description = "Kubeflow Community Events"

[[cascade]]
type = "docs"
+++


================================================
File: content/en/events/past-events/_index.md
================================================
+++
title = "Past Events"
description = "Past Kubeflow events"
weight = 200
+++



================================================
File: content/en/events/past-events/2023/_index.md
================================================
+++
title = "2023"
description = "Events from 2023"
+++



================================================
File: content/en/events/past-events/2023/kubeflow-summit-2023.md
================================================
+++
title = "Kubeflow Summit 2023"
description = "October 6th, 2023 - Irving, TX, USA - Virtual Attendance Available"
icon = "fa-solid fa-calendar-day"

#
# NOTE: to avoid 404 when we move events to the "/past-events/", 
#       we explicitly set the URL here so it doesn't change
#
url = "/events/kubeflow-summit-2023/"
+++

---

## About the Event

The __Kubeflow Summit 2023__ will be held in on __6 October__ at the __Irving Convention Center in Irving Texas__.

Join us for the Kubeflow Summit 2023, an exciting event dedicated to all things Kubeflow! 
This year, we are thrilled to offer an in-person experience that will bring together experts, enthusiasts, and beginners alike.
Don't miss out on this incredible opportunity to expand your knowledge and network with the Kubeflow community. 

Register now and secure your spot at the Kubeflow Summit 2023!

## Event Details

<div class="table-responsive">
  <table class="table table-bordered">
    <tr class="thead-light">
      <th>
        Date
      </th>
      <td>
        October 6th, 2023
      </td>
    </tr>
    <tr class="thead-light">
      <th>
        Time
      </th>
      <td>
        7:30 AM - 5:30 PM CDT
      </td>
    </tr>
    <tr class="thead-light">
      <th>
        Location
      </th>
      <td>
        <a href="https://maps.app.goo.gl/Xnf4Y1ffVLRiPNGR9">Irving Convention Center at Las Colinas, Irving, TX, USA</a>
      </td>
    </tr>
    <tr class="thead-light">
      <th>
        Cost
      </th>
      <td>
        <strong>FREE</strong>, but registration is required.
      </td>
    </tr>
    <tr class="thead-light">
      <th>
        Registration
      </th>
      <td>
        <a href="https://www.eventbrite.com/e/kubeflow-summit-2023-virtual-registration-tickets-726298186427">
          <button class="btn btn-warning py-2 px-3 mx-3 my-3">Register to Attend<br>(VIRTUAL)</button>
        </a>
        <a href="https://www.eventbrite.com/e/kubeflow-summit-2023-in-person-registration-tickets-726236511957">
          <button class="btn btn-warning py-2 px-3 mx-3 my-3">Register to Attend<br>(IN-PERSON)</button>
        </a>
      </td>
    </tr>
  </table>
</div>

## Registration

The 2023 Kubeflow Summit event is __FREE__ to attend, but registration is required.

This year you may attend either _in-person_ or _virtually_:

- [__REGISTER HERE - Attend IN-PERSON__](https://www.eventbrite.com/e/kubeflow-summit-2023-in-person-registration-tickets-726236511957)
- [__REGISTER HERE - Attend VIRTUALLY__](https://www.eventbrite.com/e/kubeflow-summit-2023-virtual-registration-tickets-726298186427)

{{% alert title="" color="info" %}}
If you register to attend virtually, you will be sent more information a few days before the event on how access the talks and participate in the conversation.
{{% /alert %}}

## Speakers

We are excited to announce the following speakers will be presenting at the Kubeflow Summit 2023:

<div class="container">
  <div class="row">
    <div class="col-auto mb-3">
      {{< card title="Josh Bottom" 
               subtitle="Kubeflow Steering Committee">}}
      {{< /card >}}
      {{< card title="Oswaldo Gomez" 
               subtitle="Roche">}}
      {{< /card >}}
      {{< card title="Omri Shiv" 
               subtitle="Roblox">}}
      {{< /card >}}
      {{< card title="Krzysztof Romanowski" 
               subtitle="Roche">}}
      {{< /card >}}
      {{< card title="Vaibhav Jain" 
               subtitle="Red Hat">}}
      {{< /card >}}
      {{< card title="Amber Graner" 
               subtitle="Kubeflow Community Manager">}}
      {{< /card >}}
      {{< card title="Mathew Wicks" 
               subtitle="Kubeflow Community Manager & Notebooks Lead">}}
      {{< /card >}}
      {{< card title="Diana Atanasova" 
               subtitle="Kubeflow Security Team">}}
      {{< /card >}}
      {{< card title="Julius von Kohout" 
               subtitle="Kubeflow Security Team">}}
      {{< /card >}}
      {{< card title="Johnu George" 
               subtitle="Kubeflow Training Lead">}}
      {{< /card >}}
      {{< card title="Andrey Velichkevich" 
               subtitle="Kubeflow AutoML Lead">}}
      {{< /card >}}
      {{< card title="James Lui" 
               subtitle="Kubeflow Pipelines Lead">}}
      {{< /card >}}
      {{< card title="Kimonas Sotirchos" 
               subtitle="Kubeflow Notebooks & Manifests Lead">}}
      {{< /card >}}
      {{< card title="Dan Sun" 
               subtitle="KServe Lead">}}
      {{< /card >}}
      {{< card title="Jooho Lee" 
               subtitle="Red Hat">}}
      {{< /card >}}
      {{< card title="Qi Liu" 
               subtitle="VMWare">}}
      {{< /card >}}
      {{< card title="Michal Hucko" 
               subtitle="Canonical">}}
      {{< /card >}}
      {{< card title="Vendant Mahabaleshwarkar" 
               subtitle="Open Data Hub">}}
      {{< /card >}}
      {{< card title="Tommy Li" 
               subtitle="IBM">}}
      {{< /card >}}
      {{< card title="Ajay Tyagi" 
               subtitle="DKube">}}
      {{< /card >}}
      {{< card title="Ricardo Rocha" 
               subtitle="CERN">}}
      {{< /card >}}
      {{< card title="Roy Budhaditya" 
               subtitle="Deloitte">}}
      {{< /card >}}
      {{< card title="Prerit Shah" 
               subtitle="Equinor">}}
      {{< /card >}}
    </div>
  </div>
</div>

## Agenda

Get ready to dive into the world of Kubeflow, the open-source machine learning platform built on Kubernetes. 
Our summit will feature engaging sessions, hands-on workshops, and networking opportunities to connect with like-minded individuals.

Discover the latest advancements in Kubeflow, learn from industry leaders, and gain insights into real-world use cases. Whether you're a developer, data scientist, or IT professional, this event is designed to inspire and empower you.

| Start Time | End Time | Speaker                             | Session                                                                                  |
|------------|----------|-------------------------------------|------------------------------------------------------------------------------------------|
| 7:30 AM    | 8:15 AM  | -                                   | Registration Open                                                                        |
| 8:15 AM    | 8:25 AM  | Josh Bottom                         | Welcome & Opening Remarks                                                                |
| 8:25 AM    | 8:30 AM  | Ricardo Rocha                       | USER STORY: Kubeflow at CERN                                                             |
| 8:30 AM    | 9:00 AM  | Oswaldo Gomez                       | Simplifying Machine Learning deployments through Cloud Native Buildpacks and KServe      |
| 9:00 AM    | 9:30 AM  | Omri Shiv                           | The Journey to Supporting 60 Million DAUs starts by supporting 200                       |
| 9:30 AM    | 9:45 AM  | Krzysztof Romanowski                | Integrating oauth2-proxy into Istio Service Mesh for Seamless Authentication in Kubeflow |
| 9:45 AM    | 9:50 AM  | Roy Budhaditya                      | USER STORY: Kubeflow at Deloitte                                                         |
| 9:55 AM    | 10:00 AM | Prerit Shah                         | USER STORY: Kubeflow at Equinor                                                          |
| -          | -        | -                                   | -                                                                                        |
| 10:00 AM   | 10:15 AM | -                                   | Break                                                                                    |
| -          | -        | -                                   | -                                                                                        |
| 10:15 AM   | 10:35 PM | Diana Atanasova & Julius von Kohout | Kubeflow Security Team Update                                                            |
| 10:35 AM   | 10:55 AM | Johnu George                        | WG Update: Training                                                                      |
| 10:55 AM   | 11:15 AM | Andrey Velichkevich                 | WG Update: AutoML                                                                        |
| 11:15 AM   | 11:35 AM | James Lui                           | WG Update: Pipelines                                                                     |
| 11:35 AM   | 11:55 AM | Kimonas Sotirchos                   | WG Update: Notebooks & Manifests                                                         |
| 11:55 AM   | 12:15 PM | Dan Sun                             | KServe Update                                                                            |
| -          | -        | -                                   | -                                                                                        |
| 12:15 PM   | 1:30 PM  | -                                   | Lunch                                                                                    |
| -          | -        | -                                   | -                                                                                        |
| 1:30 PM    | 1:40 PM  | -                                   | Ignite Style Lightning Talks (5 minutes each)                                            |
| 1:40 PM    | 1:45 PM  | Josh Bottom                         | Kubeflow Steering Committee Update                                                       |
| 1:45 PM    | 2:00 PM  | Mathew Wicks                        | deployKF: A Better Way to Deploy Kubeflow and More                                       |
| 2:00 PM    | 2:30 PM  | Jooho Lee                           | Scale Your Models to Zero with Knative and Kserve                                        |
| 2:30 PM    | 3:00 PM  | Qi Liu                              | Platform to Enable AI workload for Multi-Cloud with Hardware accelerations               |
| -          | -        | -                                   | -                                                                                        |
| 3:00 PM    | 3:15 PM  | -                                   | Break                                                                                    |
| -          | -        | -                                   | -                                                                                        |
| 3:15 PM    | 3:45 PM  | Michal Hucko                        | How to use Kubeflow with MLflow                                                          |
| 3:45 PM    | 4:15 PM  | Vendant Mahabaleshwarkar            | Monitoring the performance of your deployed models using OpenDataHub                     |
| 4:15 PM    | 4:45 PM  | Tommy Li                            | Tekton Optimizations for Kubeflow Pipelines 2.0: Challenges and Benefits                 |
| 4:45 PM    | 5:15 PM  | Ajay Tyagi                          | Scaling your Kubeflow Implementation Enterprise Wide, from tens to hundreds of users     |
| 5:15 PM    | 5:30 PM  | Josh Bottom                         | Closing Remarks                                                                          |



================================================
File: content/en/events/past-events/2023/watch--kubeflow-summit-2023.md
================================================
+++
title = "Watch: Kubeflow Summit 2023"

manualLink = "https://www.youtube.com/playlist?list=PL2gwy7BdKoGdrkYIWGeAdKi9ntfxq8FYt"
icon = "fa-brands fa-youtube"
+++


================================================
File: content/en/events/past-events/2024/_index.md
================================================
+++
title = "2024"
description = "Events from 2024"
+++



================================================
File: content/en/events/past-events/2024/gsoc-2024.md
================================================
+++
title = "Google Summer of Code 2024"
description = "Google Summer of Code 2024"
icon = "fa-solid fa-calendar-day"

#
# NOTE: to avoid 404 when we move events to the "/past-events/", 
#       we explicitly set the URL here so it doesn't change
#
url = "/events/gsoc-2024/"
+++

---

Kubeflow Community is excited to announce that we have been [selected](https://summerofcode.withgoogle.com/programs/2024/organizations/kubeflow) as organization to participate in [**Google Summer of Code 2024**](https://buildyourfuture.withgoogle.com/programs/summer-of-code). This page aims to help you participate in the Kubeflow organization for GSoC 2024.

## How can I participate in Kubeflow GSoC?

Please go to [Google Summer of Code 2024](https://buildyourfuture.withgoogle.com/programs/summer-of-code) and sign up as a student. Next, look at the [projects below](#project-ideas-for-2024-gsoc) to decide which ones you are interested in. Note, that you must submit your proposals through the GSoC website and your proposal must be selected to participate.

Contributor applications open **March 18th, 2024** and close on **April 2nd, 2024**. For more information, see the GSoC website and/or reach out to the GSoC organizers. Please only contact mentors about projects, not the program itself.

### Slack

Please [**Join the Kubeflow Slack**](https://www.kubeflow.org/docs/about/community/#kubeflow-slack-channels).

Please **do not** reach out privately to mentors, instead, start a thread in the [`#gsoc-participants`](https://kubeflow.slack.com/archives/C06LW6Z3RA6) channel so others can see the response. Be kind to our mentors, please search to see if your question has already been answered.

### Meetings

You may wish to attend the next community meeting for the group that is leading your chosen project, please see the calendar below for more information.

<style>
#calendar-container {
   overflow: auto;
}
</style>
<div id="calendar-container"></div>
<script type="text/javascript">
const timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
const calender_src_list = [
  // Kubeflow Community
  "kubeflow.org_7l5vnbn8suj2se10sen81d9428%40group.calendar.google.com",
  // KServe Community
  "4fqdmu5fp4l0bgdlf4lm1atnsl2j4612%40import.calendar.google.com",
];
let calender_src = calender_src_list.map(src => `&src=${src}&color=%23A79B8E`).join('');
const html = `<iframe src="https://calendar.google.com/calendar/embed?ctz=${timezone}&height=600&wkst=1&bgcolor=%23ffffff&showPrint=0&showDate=1&mode=AGENDA&showTitle=0${calender_src}" style="border:solid 1px #777" width="800" height="600" frameborder="0" scrolling="no"></iframe>`;
document.getElementById('calendar-container').innerHTML = html;
</script>

## What if my proposal is not chosen?

Please understand that not everyone can be selected for Google Summer of Code (GSoC), there are many possible candidates for each project.

However, we still want to encourage you to participate in the Kubeflow project! Get started by attending [working group meetings](/docs/about/community/#kubeflow-community-meetings) for components you want to help with, and reading our [contributing guide](/docs/about/contributing/).

# Project Ideas for 2024 GSoC

### Project 1: Kubeflow Notebooks 2.0

Kubeflow Notebook is a widely used component of Kubeflow that allows Data Scientists and ML Engineers to run web-based IDEs (JupyterLab, VSCode, RStudio) on Kubernetes clusters.

There is currently an effort to create the next major version of Kubeflow Notebooks.

The main idea is to change the Kubeflow Notebook CRD so that it is no longer just a wrapper around a Kubernetes PodSpec.

This foundational change enables users to:

- Update existing notebooks after spawning, to change their “pod config” (CPU/GPU/RAM), “volumes” (storage), and “image” (what packages are installed) from options that are defined by their admin.
- Make spawning notebooks less confusing for end-users. Pod configs stop being about specific parts of the PodSpec (e.g. tolerations, requests, limits), and become a drop-down list of user-friendly names (e.g. “Big GPU Notebook - A100 - 128GB”), similar to cloud “instance types”.
- Give admins more control over how workspaces are spawned, and the lifecycle of the “options” which are available to users. For example, admins can now “redirect” existing image/pod configs to new ones, but delay the application of these updates until the next pod restart (during which, the interface will display a warning to users that a change is pending).
- Support new web-based IDEs without needing to specifically integrate with them. Cluster admins can define a custom “kind” for their internal app, or even make “flavors” of existing apps (like Jupyter and VSCode) with the packages and pod-sizes required for specific teams in their organization.

You would be part of the larger effort, and involved in one or more code deliverables:

- See Kubeflow Notebooks docs: [https://www.kubeflow.org/docs/components/notebooks/overview/](https://www.kubeflow.org/docs/components/notebooks/overview/)
- See Kubeflow Notebooks 2.0 GitHub proposal: [https://github.com/kubeflow/kubeflow/issues/7156](https://github.com/kubeflow/kubeflow/issues/7156)
- See Kubeflow Notebooks 2.0 design document: [https://docs.google.com/document/d/1_zk06zebbaTBdJ8TdU07Ibky25hqHGARXjVcsp2qEnU/edit](https://docs.google.com/document/d/1_zk06zebbaTBdJ8TdU07Ibky25hqHGARXjVcsp2qEnU/edit)

**Skills required:** Kubernetes Controllers (Golang - Kubebuilder) AND/OR Web Development (JS - Angular, Python - Flask)

**Difficulty:** medium/high

**Length:** 350 hrs

**Mentors:** Mathew Wicks, Kimonas Sitorchos, Julius von Kohout

**Component:** Notebooks

---

---

### Project 2: Rootless Kubeflow Container Images (Istio Ambient Mesh)

Kubeflow uses Istio as a service mesh, which by default requires “root level” network permissions for its init-containers. We want to reduce the number of privileged containers required to run Kubeflow, so are investigating using the Istio CNI, and eventually the Istio Ambient mesh.

You would be involved in testing and investigating the impacts of these changes, and helping push the integration forwards.

See the proposal for more information: [https://github.com/kubeflow/manifests/blob/master/proposals/20200913-rootlessKubeflow.md](https://github.com/kubeflow/manifests/blob/master/proposals/20200913-rootlessKubeflow.md)

**Skills required:** Istio, Kubernetes, YAML

**Difficulty:** medium

**Length:** 175 hrs

**Mentors:** Kimonas Sitorchos, Julius von Kohout

**Component:** Notebooks

---

---

### Project 3: Triage and Categorize Kubeflow GitHub Issues & PRs

The Kubeflow project needs help to triage, categorize, and highlight important Issues/PRs from the [https://github.com/kubeflow/kubeflow](https://github.com/kubeflow/kubeflow) GitHub repo. There are around 200 open Issues and 200 open PRs, in addition to many Issues/PRs that have been lost to time (closed automatically due to inactivity).

Specifically, your goal would be to:

- Decide which Issues/PRs are still relevant
- Categorize Issues/PRs by type
- De-duplicate multiple Issues for the same request
- Suggest which ones are the most important.
- Help find “good first issues” for new members:
  - [https://github.com/kubeflow/manifests/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22](https://github.com/kubeflow/manifests/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)
- Review which PRs are likely safe to merge (especially dependabot ones)

**Skills required:** GitHub, Kubernetes, YAML, Python, GO, JS

**Difficulty:** medium

**Length:** 175 hrs

**Mentors:** Mathew Wicks, Kimonas Sitorchos, Julius von Kohout

**Component:** Notebooks/General

---

---

### Project 4: Implement LLM Tuning API for Katib

Recently, we implemented [a new `train` Python SDK API](https://github.com/kubeflow/training-operator/blob/master/docs/proposals/train_api_proposal.md) in Kubeflow Training Operator to easily fine-tune LLMs on multiple GPUs with predefined datasets provider, model provider, and HuggingFace trainer.

To continue our roadmap around LLMOps in Kubeflow, we want to give user functionality to tune HyperParameters of LLMs using simple Python SDK APIs. It requires making appropriate changes to the Katib Python SDK which allows users to set model, dataset, and HyperParameters that they want to optimize for LLM.

**Skills required:** Kubernetes, YAML, Python

**Difficulty:** medium

**Length:** 350 hrs

**Mentors:** Andrey Velichkevich, Johnu George, Yuan (Terry) Tang, Yuki Iwai

**Component:** Katib

---

---

### Project 5: Support Distributed Jax for Training Operator

Open issue: [https://github.com/kubeflow/training-operator/issues/1619](https://github.com/kubeflow/training-operator/issues/1619)

We want to integrate Jax in Training Operator to run distributed training and fine-tuning jobs on Kubernetes using the Jax ML framework. We need to create a new Kubernetes Custom Resource for Jax (e.g. JaxJob) and update the Training Operator controller to support it. Potentially, we can integrate Jax with the Training Operator Python SDK to give Data Scientists simple APIs to create JaxJob on Kubernetes.

**Skills required:** Kubernetes, Go, YAML, Python

**Difficulty:** medium

**Length:** 350 hrs

**Mentors:** Andrey Velichkevich, Johnu George, Yuan (Terry) Tang, Yuki Iwai

**Component:** Training Operator

---

---

### Project 6: Push-based metrics collection for Katib

Open issue: [https://github.com/kubeflow/katib/issues/577](https://github.com/kubeflow/katib/issues/577).

Katib implements Metrics Collector as a sidecar container to collect training metrics from the Trials once training is complete. This Metrics Collector waits until the training container is complete and parses training logs to get appropriate metrics like accuracy or loss to get evaluation results for the HyperParameter tuning algorithm.

Sometimes the container sidecar approach might not work for users. For example, if their Trial resources executor doesn’t support sidecar containers. For such use-cases, we want to implement a new API to the Katib Python SDK to allow users to push metrics directly from their training scripts to the Katib DB.

**Skills required:** Kubernetes, Go, YAML, Python

**Difficulty:** medium

**Length:** 175 hrs

**Mentors:** Andrey Velichkevich, Johnu George, Yuan (Terry) Tang, Yuki Iwai

**Component:** Katib

---

---

### Project 7: Automate docs generation for Kubeflow Python SDKs

Open issue: [https://github.com/kubeflow/katib/issues/2081](https://github.com/kubeflow/katib/issues/2081)

Training Operator and Katib SDKs have [a valid docstring](https://github.com/kubeflow/training-operator/blob/0b6a30cd348e101506b53a1a176e4a7aec6e9f09/sdk/python/kubeflow/training/api/training_client.py#L50-L74) for each public API that users are running. We want to automatically generate documentation for Kubeflow users from these docstrings, so users don’t need to read source code to understand APIs parameters.

**Skills required:** Python

**Difficulty**: medium

**Length:** 90 hrs

**Mentors:** Andrey Velichkevich, Johnu George, Shivay Lamba, Yuan (Terry) Tang, Yuki Iwai

**Component:** Katib/Training Operator

---

---

### Project 8: Support various parameter distributions like log-uniform in Katib

Open issue: [https://github.com/kubeflow/katib/pull/2059](https://github.com/kubeflow/katib/pull/2059)

We need to enhance Katib Experiment APIs to support various parameter distributions like uniform, log-uniform, qlog-uniform to make Katib more native to other HyperParameter tuning frameworks like Hyperopt. Currently, Katib supports only uniform distribution of integer, float, and categorical HyperParameters.

**Skills required:** Kubernetes, Python, Go, YAML

**Difficulty:** medium

**Length:** 350 hrs

**Mentors:** Andrey Velichkevich, Johnu George, Yuan (Terry) Tang, Yuki Iwai

**Component:** Katib

---

---

### Project 9: PostgreSQL integration in Kubeflow Pipelines

Open issue: [https://github.com/kubeflow/pipelines/issues/9813](https://github.com/kubeflow/pipelines/issues/9813)

Kubeflow Pipelines must store information about pipelines, experiments, runs, and artifacts in a database. Currently, the only database it supports is MySQL/MariaDB.

We plan to support PostgreSQL as an alternative to MySQL/MariaDB so users will be able to reuse existing databases, and PostgreSQL will be a good use case for supporting multiple databases.

**Skills required:** Kubernetes, Python, Go, YAML

**Difficulty:** medium

**Length:** 175 hrs

**Mentors:** Ricardo Martinelli, Shivay Lamba

**Component:** Pipelines

---

---

### Project 10: Enhancing KF Model Registry Python client for seamless ML imports from alternative registries

We aim to extend the capabilities of the KF Model Registry Python client by enabling smooth imports from various machine learning registries. While import from HuggingFace is already implemented (and can be used as a basis) we seek to integrate support for MLFlow, and other popular registry formats.

**Skills required:** Python, ML model serialization formats, YAML, Kubernetes/Kubeflow as a plus

**Difficulty:** medium

**Length:** 175 hrs

**Mentors:** Matteo Mortari, Andrea Lamparelli

**Component:** Model Registry

---

---



================================================
File: content/en/events/past-events/2024/watch--kubeflow-summit-2024.md
================================================
+++
title = "Watch: Kubeflow Summit 2024"

manualLink = "https://www.youtube.com/playlist?list=PLj6h78yzYM2Nk-8Zyjaefz9yFJ-NxC-qn"
icon = "fa-brands fa-youtube"
+++


================================================
File: content/en/events/past-events/2024/watch--kubernetes-ai-day-2024.md
================================================
+++
title = "Watch: Cloud Native & Kubernetes AI Day 2024"

manualLink = "https://www.youtube.com/playlist?list=PLj6h78yzYM2Mvqk_mNejD7kbe3tldxxsr"
icon = "fa-brands fa-youtube"
+++


================================================
File: content/en/events/upcoming-events/_index.md
================================================
+++
title = "Future Events"
description = "Future Kubeflow events"
weight = 100
+++



================================================
File: content/en/events/upcoming-events/gsoc-2025.md
================================================
+++
title = "Google Summer of Code 2025"
description = "Google Summer of Code 2025"
icon = "fa-regular fa-calendar-day"

#
# NOTE: to avoid 404 when we move events to the "/past-events/", 
#       we explicitly set the URL here so it doesn't change
#
url = "/events/gsoc-2025/"
+++

---

The Kubeflow Community plans to participate in [**Google Summer of Code 2025**](https://summerofcode.withgoogle.com/).
This page aims to help you participate in GSoC 2025 with Kubeflow.

{{% alert title="Note" color="info" %}}
While Kubeflow participated in [GSoC 2024](/events/gsoc-2024/), we are currently awaiting final confirmation of our participation in GSoC 2025.
Google will announce the final list of accepted organizations on **February 27, 2025**.
{{% /alert %}}

## What is GSoC?

Google Summer of Code (GSoC) is a global program that offers students [stipends](https://developers.google.com/open-source/gsoc/help/student-stipends) for working on open-source projects during the summer.

For more information, see the [GSoC FAQ](https://developers.google.com/open-source/gsoc/faq) and watch the video below:

<div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><iframe src="https://www.youtube.com/embed/93oj6b7d3VI?rel=0" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;" allowfullscreen scrolling="no" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;"></iframe></div>

## How can I participate?

Thank you for your interest in participating in GSoC with Kubeflow!

Please carefully read the following information to learn how to participate in GSoC with Kubeflow.

### Key Dates

Here are the key dates for GSoC 2025, the [full timeline](https://developers.google.com/open-source/gsoc/timeline) is available on the GSoC website:

| Event                            | Date                 |
| -------------------------------- | -------------------- |
| **Applications Open**            | March 24 @ 18:00 UTC |
| **Applications Deadline**        | April 8 @ 18:00 UTC  |
| **Accepted Proposals Announced** | May 8                |
| **Community Bonding**            | May 8 - June 1       |
| **Coding Begins**                | June 2               |
| **Midterm Evaluations**          | July 14 - 18         |
| **Coding Ends**                  | September 1          |
| **Final Evaluations**            | September 1 - 8      |

### Eligibility

To participate in GSoC with Kubeflow, you **must** meet the GSoC [eligibility requirements](https://developers.google.com/open-source/gsoc/faq#what_are_the_eligibility_requirements_for_participation):

- Be at least 18 years old at time of registration.
- Be a student or an [open source beginner](https://developers.google.com/open-source/gsoc/faq#how_do_i_know_if_i_am_considered_a_beginner_in_open_source_development).
- Be eligible to work in their country of residence during duration of program.
- Be a resident of a country not currently embargoed by the United States.

### Steps

1. Sign up as a student on the [GSoC website](https://summerofcode.withgoogle.com/).
2. Join the [Kubeflow Slack](/docs/about/community/#kubeflow-slack-channels):
   - **NOTE:** please **do not** reach out privately to mentors, instead, start a thread in the [`#kubeflow-contributors`](https://cloud-native.slack.com/archives/C0742LBR5BM) channel so others can see the response.
3. Learn about Kubeflow:
   - Read the [Introduction to Kubeflow](/docs/started/introduction/)
   - Review the [Architecture Overview](/docs/started/architecture/)
   - Consider [trying out Kubeflow](/docs/started/installing-kubeflow/) (not required, can be challenging)
4. Review the [project ideas](#project-ideas) to decide which ones you are interested in:
   - You may wish to attend the next [community meeting](/docs/about/community/#kubeflow-community-calendar) for the group that is leading your chosen project.
   - **NOTE:** while we recommend you submit a proposal based on the project ideas, you can also submit a proposal with your own idea.
5. Submit a proposal through the [GSoC website](https://summerofcode.withgoogle.com/) between **March 24th** and **April 8th**.
6. Wait for the results to be announced on **May 8th**.

## Project Ideas

### Project 1: Kubeflow Platform Enhancements

**Components:** Kubeflow Manifests, Kubeflow Dashboard, Kubeflow Notebooks, Kubeflow Pipelines

**Possible Mentors:** [`@juliusvonkohout`](https://github.com/juliusvonkohout), [`@thesuperzapper`](https://github.com/thesuperzapper)

**Difficulty:** Hard

**Size:** 350 hours

**Possible Projects:**

- Pipelines: productionize the [SeaweedFS PoC](https://github.com/kubeflow/manifests/tree/master/experimental/seaweedfs) as secure minio replacement
- Pipelines: isolate artifacts per namespace/profile/user using only one bucket ([`kubeflow/pipelines#4649`](https://github.com/kubeflow/pipelines/issues/4649))
- Notebooks/Dashboard: migrate code to kubeflow/dashboard and kubeflow/notebooks ([`kubeflow/kubeflow#7549`](https://github.com/kubeflow/kubeflow/issues/7549))
- Dashboard: work on the Central Dashboard angular rewrite ([`kubeflow/dashboard#38`](https://github.com/kubeflow/dashboard/issues/38))
- Dashboard: support using groups for auth ([`kubeflow/manifests#2910`](https://github.com/kubeflow/manifests/issues/2910#issuecomment-2468745862))
- Manifests: improve scripts and CI/CD in kubeflow/manifests, including matrix calls to test multiple Kubernetes versions simultaneously

**Skills Required/Preferred:**

- GitHub and GitHub Actions
- containers and Kubernetes knowledge
- Experience with Python, Go and JavaScript frameworks

---

### Project 2: Kserve Models Web App

**Components:** KServe

**Possible Mentors:** [`@juliusvonkohout`](https://github.com/juliusvonkohout), [`@varodrig`](https://github.com/varodrig), [`@Griffin-Sullivan`](https://github.com/Griffin-Sullivan)

**Difficulty:** Medium

**Size:** 175 hours

**Goals:**

- Reviving and updating the [`kserve/kserve-models-web`](https://github.com/kserve/models-web-app) application.
  - Clean up and merge the open issues and PRs
  - Implement a better CI/CD pipeline.
  - Potentially migrate the application to `kubeflow/kserve-model-ui`
  - Add features for editing, regression testing, and monitoring/metrics support.
  - Synchronize with kserve 0.14+ changes.

**Skills Required/Preferred:**

- GitHub Actions
- containers and Kubernetes knowledge
- JavaScript frameworks

---

---

### Project 3: Istio CNI and Ambient Mesh

**Components:** Kubeflow Manifests

**Possible Mentors:** [`@juliusvonkohout`](https://github.com/juliusvonkohout), [`@kimwnasptd`](https://github.com/kimwnasptd)

**Difficulty:** Medium

**Size:** 175 hours

**Goals:**

- Secure our service mesh with istio-cni by default ([`kubeflow/manifests#2907`](https://github.com/kubeflow/manifests/pull/2907))
- Provide an out-of-box option for istio-ambient mesh ([`kubeflow/manifests#2676`](https://github.com/kubeflow/manifests/issues/2676))
  - Controllers to create HTTPRoute and AuthorizationPolicies, that align with way-point proxies
  - Manifests to also have a flavour of HTTPRoute and updated AuthorizationPolicies
- Secure Kserve by default ([`kubeflow/manifests#2811`](https://github.com/kubeflow/manifests/issues/2811))
- Rootless Kubeflow ([`kubeflow/manifests#2528`](https://github.com/kubeflow/manifests/issues/2528))

**Skills Required/Preferred:**

- GitHub and GitHub Actions
- Kubernetes and networking
- Istio, Kustomize

---

---

### Project 4: Deploying Kubeflow with Helm

**Components:** Kubeflow Manifests, Kubeflow Pipelines, Kubeflow Trainer, Kubeflow Katib, Kubeflow Spark Operator, Kubeflow Model Registry

**Possible Mentors:** [`@chasecadet`](https://github.com/chasecadet), [`@varodrig`](https://github.com/varodrig), [`@juliusvonkohout`](https://github.com/juliusvonkohout)

**Difficulty:** Medium

**Size:** 350 hours

**Goals:**

- To extend our userbase and satisfy the requirement for a helm chart that many users and companies have voiced, a community-driven Helm chart is being developed for Kubeflow v1.10.x.
- Work with Kubeflow components maintainers and kubeflow/manifests to support the creation of Helm charts for a full Kubeflow deployment with similar functionality as the current kustomize manifests for the Kubeflow 1.10.x release.
- Investigate possible systems to automatically generate or maintain charts based on the existing kustomize manifests, such that we have a single source of truth.

**Skills Required/Preferred:**

- Container and Kubernetes knowledge
- Helm (especially templating and chart creation)
- Kustomize (not strictly required, but a plus)

---

---

### Project 5: JupyterLab Plugin for Kubeflow

**Components:** Kubeflow Notebooks, Kubeflow Pipelines

**Possible Mentors:** [`@ederign`](https://github.com/ederign), [`@StefanoFioravanzo`](https://github.com/StefanoFioravanzo)

**Difficulty:** Medium

**Size:** 350 hours

**Goals:**

- Work with the new IDE Working Group (name pending - [`kubeflow/community#808`](https://github.com/kubeflow/community/issues/808)) to create a JupyterLab plugin for Kubeflow
- Modernizing and/or consolidating [Elyra](https://github.com/elyra-ai/elyra), [Kale](https://github.com/kubeflow-kale/kale), and [Jupyter Scheduler](https://github.com/jupyter-server/jupyter-scheduler) into a single plugin for Kubeflow
- Eventually, the plugin will likely integrate with:
  - Kubeflow Pipelines (priority)
  - Kubeflow Notebooks
  - Kubeflow Model Registry
  - Kubeflow Training Operator
  - and more

**Skills Required/Preferred:**

- Python for backend development and API integration
- JavaScript/TypeScript for frontend development
- Modern UI frameworks (e.g., React, Jupyter widgets) is a plus
- Familiarity with Jupyter Notebook, JupyterLab
- Jupyter extension development experience is a plus

---

---

### Project 6: Batch Processing Gateway Integration

**Components:** Kubeflow Spark Operator

**Possible Mentors:** [`@Shekharrajak`](https://github.com/Shekharrajak),
[`@lresende`](https://github.com/lresende),
[`@yuchaoran2011`](https://github.com/yuchaoran2011),
[`@andreyvelich`](https://github.com/andreyvelich)

**Difficulty:** Hard

**Size:** 350 hours

**Goals:**

- Integrating the [Batch Processing Gateway (BPG)](https://github.com/apple/batch-processing-gateway) with Kubeflow for submitting, monitoring, and managing Spark applications across multiple clusters ([`kubeflow/spark-operator#2422`](https://github.com/kubeflow/spark-operator/issues/2422))
- Analyse, Design, Plan, and Execute Spark Job Execution Strategies:
  - Evaluate the trade-offs between running a Spark kernel directly within a Kubeflow Notebook versus leveraging the Batch Processing Gateway for job submission.
  - Assess the cloud-native design of Kubeflow SDK and Notebook environments to determine the optimal approach for Spark integration that maximizes efficiency, scalability, and usability.
  - Make a well-informed decision on whether to support Spark kernels within notebooks, use BPG, or implement a hybrid approach for an enhanced user experience.
- Automated Job Routing and Scalable Execution:
  - Implement dynamic workload routing using BPG to automatically distribute Spark jobs based on cluster load, resource availability, and workload priority.
  - Integrate with the Spark Operator to optimize resource allocation, minimize execution delays, and ensure efficient scaling for petabyte-scale machine learning and data processing workloads.
- Enhanced User API and Notebook Integration:
  - Develop a Python SDK for Kubeflow notebooks, enabling users to submit, manage, and monitor Spark jobs via BPG REST APIs for a lightweight, scalable solution.
  - Ensure a seamless user experience by providing intuitive APIs that abstract complex job management operations, making it easier for data scientists and ML engineers to experiment and iterate on workflows within
- Comprehensive Debugging and Performance Monitoring:
  - Enable full debugging capabilities by integrating Spark UI, logging, and monitoring tools into Kubeflow, allowing users to visualize Spark DAGs, tasks, and execution stages.
  - Implement centralized logging and Prometheus-based monitoring to provide real-time insights into Spark job performance across clusters.
  - Ensure users can efficiently analyze job execution, detect bottlenecks, and optimize data processing and ML workflows within Kubeflow.
  - Note: Most of the logging APIs must be leveraged out of the box from either BPG or Spark - but we need to document, showcase examples to user.
- Comprehensive documentation and user guides to assist users in leveraging the new features effectively.

**Skills Required/Preferred:**

- Proficiency in Python, Java and familiarity with developing SDKs.
- Experience with Kubernetes and managing containerized applications.
- Understanding of Apache Spark and its deployment on Kubernetes clusters.
- Familiarity with RESTful API development and integration.
- Experience with monitoring tools and logging frameworks is a plus.

---

---

### Project 7: GPU Testing for LLM Blueprints

**Components:** Kubeflow Trainer (Training Operator)

**Possible Mentors:** [`@andreyvelich`](https://github.com/andreyvelich), [`@varodrig`](https://github.com/varodrig)

**Difficulty:** Medium

**Size:** 350 hours

**Goals:**

- Explore using Self-Hosted Runners for GPU testing in Kubeflow Trainer ([`kubeflow/trainer#2432`](https://github.com/kubeflow/trainer/issues/2432))

**Skills Required/Preferred:**

- GitHub Actions
- Kubernetes
- PyTorch
- Python

---

---

### Project 8: Support JAX and TensorFlow Training Runtimes

**Components:** Kubeflow Trainer (Training Operator)

**Possible Mentors:** [`@Electronic-Waste`](https://github.com/Electronic-Waste),
[`@XshubhamX`](https://github.com/XshubhamX),
[`@andreyvelich`](https://github.com/andreyvelich)

**Difficulty:** Hard

**Size:** 350 hours

**Goals:**

- Add support TensorFlow as a training runtime in Kubeflow Trainer ([`kubeflow/trainer#2443`](https://github.com/kubeflow/trainer/issues/2443))
- Add support JAX as a training runtime in Kubeflow Trainer ([`kubeflow/trainer#2442`](https://github.com/kubeflow/trainer/issues/2442))

**Skills Required/Preferred:**

- Go
- Kubernetes
- JAX
- TensorFlow

---

---

### Project 9: Export Kubeflow Trainer Models to Kubeflow Model Registry

**Components:** Kubeflow Trainer (Training Operator), Kubeflow Model Registry

**Possible Mentors:** [`@tarilabs`](https://github.com/tarilabs), [`@franciscojavierarceo`](https://github.com/franciscojavierarceo)

**Difficulty:** Hard

**Size:** 350 hours

**Goals:**

- Integrate Kubeflow Trainer with Kubeflow Model Registry ([`kubeflow/trainer#2438`](https://github.com/kubeflow/trainer/issues/2438))
  - Trainer has implemented initializers for model and dataset, and will support model exporter in the future.
  - By supporting the model registry as one of the destinations of the exporter, Trainer will integrate with Kubeflow ecosystem more deeply.

**Skills Required/Preferred:**

- Kubernetes
- Go
- YAML
- Python

---

---

### Project 10: Support Volcano Scheduler in Kubeflow Trainer

**Components:** Kubeflow Trainer (Training Operator)

**Possible Mentors:** [`@Electronic-Waste`](https://github.com/Electronic-Waste), [`@rudeigerc`](https://github.com/rudeigerc)

**Difficulty:** Hard

**Size:** 350 hours

**Goals:**

- Integrate Volcano Scheduler with Kubeflow Trainer ([`kubeflow/trainer#2437`](https://github.com/kubeflow/trainer/issues/2437))
  - Currently, Trainer does not support Volcano for scheduling.
  - Since Volcano is a widely adopted scheduler for AI workloads, it could provide Trainer with more AI-specific scheduling capabilities if we integrate Volcano into Trainer

**Skills Required/Preferred:**

- Kubernetes
- Go
- Volcano

---

---

### Project 11: Support Postgres for Kubeflow Pipelines backend

**Components:** Kubeflow Pipelines

**Possible Mentors:** [`@rimolive`](https://github.com/rimolive), [`@shivaylamba`](https://github.com/shivaylamba)

**Difficulty:** Medium

**Size:** 175 hours

**Goals:**

- Implement support for PostgreSQL as an alternative to MySQL/MariaDB in Kubeflow Pipelines ([`kubeflow/pipelines#9813`](https://github.com/kubeflow/pipelines/issues/9813))
  - Kubeflow Pipelines must store information about pipelines, experiments, runs, and artifacts in a database. Currently, the only database it supports is MySQL/MariaDB.
  - We plan to support PostgreSQL as an alternative to MySQL/MariaDB so users will be able to reuse existing databases, and PostgreSQL will be a good use case for supporting multiple databases.

**Skills Required/Preferred:**

- Kubernetes
- Python
- Go
- YAML

### Project 12: Empowering Kubeflow Documentation with LLMs

**Components:** Kubeflow Website

**Possible Mentors:** [`@franciscojavierarceo`](https://github.com/franciscojavierarceo),
[`@chasecadet`](https://github.com/chasecadet),
[`@shravan-achar`](https://github.com/shravan-achar),
[`@Shekharrajak`](https://github.com/Shekharrajak),
[`@varodrig`](https://github.com/varodrig)

**Difficulty:** Hard

**Size:** 350 hours

**Goals:**

- Leverage LLMs to improve Kubeflow documentation: ([`kubeflow/website#4025`](https://github.com/kubeflow/website/issues/4025)).
  - Explore how other OSS communities leverage LLMs with the user documentation.
  - Explore possibilities to use LLMs to improve existing Kubeflow documentation or use LLMs to help
    with user questions.

**Skills Required/Preferred:**

- JavaScript
- Python
- HTML
- Netlify
- Hugo



================================================
File: content/en/events/upcoming-events/kubeflow-summit-2025.md
================================================
+++
title = "Kubeflow Summit 2025"

manualLink = "https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/co-located-events/kubeflow-summit/"
icon = "fa-solid fa-arrow-up-right-from-square"
+++


================================================
File: i18n/en.toml
================================================
[post_create_issue]
other = "Give page feedback"

[post_create_project_issue]
other = "Create project issue"

[print_entire_section]
other = "Print this section"


================================================
File: layouts/404.html
================================================
{{ define "main"}}
<div class="contain error-page">
  <h1>404 Page not found</h1>
  <ul>
    {{ range .Site.Menus.main }}
    <li>
      {{ $url := urls.Parse .URL }}
      {{ $baseurl := urls.Parse $.Site.Params.Baseurl }}
      <a
        href="{{ with .Page }}{{ .RelPermalink }}{{ else }}{{ .URL | relLangURL }}{{ end }}"
        {{ if ne $url.Host $baseurl.Host }}target="_blank"{{ end }}
      >
        {{ .Name }}
      </a>
    </li>
    {{ end }}
  </ul>
</div>
{{ end }}


================================================
File: layouts/sitemap.xml
================================================
{{ `<?xml version="1.0" encoding="UTF-8"?>` | safeHTML }}
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xhtml="http://www.w3.org/1999/xhtml">
  {{- range .Data.Pages }}
  <url>
    <loc>https://www.kubeflow.org{{ .Permalink }}</loc>{{ if not .Lastmod.IsZero }}
    <lastmod>{{ safeHTML ( .Lastmod.Format "2006-01-02T15:04:05-07:00" ) }}</lastmod>{{ end }}{{ with .Sitemap.ChangeFreq }}
    <changefreq>{{ . }}</changefreq>{{ end }}{{ if ge .Sitemap.Priority 0.0 }}
    <priority>{{ .Sitemap.Priority }}</priority>{{ end }}{{ if .IsTranslated }}{{ range .Translations }}
    <xhtml:link
                rel="alternate"
                hreflang="{{ .Lang }}"
                href="{{ .Permalink }}"
                />{{ end }}
    <xhtml:link
                rel="alternate"
                hreflang="{{ .Lang }}"
                href="{{ .Permalink }}"
                />{{ end }}
  </url>
  {{- end }}
</urlset>


================================================
File: layouts/_default/content.html
================================================
<div class="td-content">
	<h1>{{ .Title }}</h1>
	{{ with .Params.description }}<div class="lead">{{ . | markdownify }}</div>{{ end }}
	<header class="article-meta">
		{{ partial "taxonomy_terms_article_wrapper.html" . }}
		{{ if (and (not .Params.hide_readingtime) (.Site.Params.ui.readingtime.enable)) }}
			{{ partial "reading-time.html" . }}
		{{ end }}
	</header>    
	{{ .Content }}
	{{ if (and (not .Params.hide_feedback) (.Site.Params.ui.feedback.enable) (.Site.GoogleAnalytics)) }}
		{{ partial "feedback.html" . }}
		<br />
	{{ end }}
	{{ if (.Site.Params.DisqusShortname) }}
		<br />
		{{ partial "disqus-comment.html" . }}
	{{ end }}
	{{ partial "page-meta-lastmod.html" . }}
</div>



================================================
File: layouts/_default/_markup/render-codeblock-mermaid.html
================================================
<pre class="mermaid">
  {{- .Inner | safeHTML }}
</pre>
{{ .Page.Store.Set "hasMermaid" true }}


================================================
File: layouts/_internal/opengraph.html
================================================
<!-- Modified from: https://github.com/gohugoio/hugo/blob/v0.129.0/tpl/tplimpl/embedded/templates/opengraph.html -->
<!-- We remove the `og:image` meta tags because we generate them in `social_image_generator.html` -->
<!-- We remove `.Summary` from the description generator, because it usually crazy long, so the site description is better -->

<meta property="og:url" content="{{ .Permalink }}">

{{- with or site.Title site.Params.title | plainify }}
  <meta property="og:site_name" content="{{ . }}">
{{- end }}

{{- with or .Title site.Title site.Params.title | plainify }}
  <meta property="og:title" content="{{ . }}">
{{- end }}

{{- with or .Description site.Params.description | plainify | htmlUnescape | chomp }}
  <meta property="og:description" content="{{ . }}">
{{- end }}

{{- with or .Params.locale site.Language.LanguageCode }}
  <meta property="og:locale" content="{{ replace . `-` `_` }}">
{{- end }}

{{- if .IsPage }}
  <meta property="og:type" content="article">
  {{- with .Section }}
    <meta property="article:section" content="{{ . }}">
  {{- end }}
  {{- $ISO8601 := "2006-01-02T15:04:05-07:00" }}
  {{- with .PublishDate }}
    <meta property="article:published_time" {{ .Format $ISO8601 | printf "content=%q" | safeHTMLAttr }}>
  {{- end }}
  {{- with .Lastmod }}
    <meta property="article:modified_time" {{ .Format $ISO8601 | printf "content=%q" | safeHTMLAttr }}>
  {{- end }}
  {{- range .GetTerms "tags" | first 6 }}
    <meta property="article:tag" content="{{ .Page.Title | plainify }}">
  {{- end }}
{{- else }}
  <meta property="og:type" content="website">
{{- end }}

{{- with .Params.audio }}
  {{- range . | first 6  }}
    <meta property="og:audio" content="{{ . | absURL }}">
  {{- end }}
{{- end }}

{{- with .Params.videos }}
  {{- range . | first 6 }}
    <meta property="og:video" content="{{ . | absURL }}">
  {{- end }}
{{- end }}

{{- range .GetTerms "series" }}
  {{- range .Pages | first 7 }}
    {{- if ne $ . }}
      <meta property="og:see_also" content="{{ .Permalink }}">
    {{- end }}
  {{- end }}
{{- end }}

{{- with site.Params.social }}
  {{- if reflect.IsMap . }}
    {{- with .facebook_app_id }}
      <meta property="fb:app_id" content="{{ . }}">
    {{- else }}
      {{- with .facebook_admin }}
        <meta property="fb:admins" content="{{ . }}">
      {{- end }}
    {{- end }}
  {{- end }}
{{- end }}


================================================
File: layouts/_internal/twitter_cards.html
================================================
<!-- Modified from: https://github.com/gohugoio/hugo/blob/v0.129.0/tpl/tplimpl/embedded/templates/opengraph.html -->
<!-- We remove the `twitter:image` meta tags because we generate them in `social_image_generator.html` -->
<!-- We remove `.Summary` from the description generator, because it usually crazy long, so the site description is better -->

{{- with or .Title site.Title site.Params.title | plainify }}
  <meta name="twitter:title" content="{{ . }}">
{{- end }}

{{- with or .Description site.Params.description | plainify | htmlUnescape | chomp }}
  <meta name="twitter:description" content="{{ . }}">
{{- end }}

{{- $twitterSite := "" }}
{{- with site.Params.social }}
  {{- if reflect.IsMap . }}
    {{- with .twitter }}
      {{- $content := . }}
      {{- if not (strings.HasPrefix . "@") }}
        {{- $content = printf "@%v" . }}
      {{- end }}
      <meta name="twitter:site" content="{{ $content }}">
    {{- end }}
  {{- end }}
{{- end }}


================================================
File: layouts/docs/baseof.html
================================================
<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="{{ .Site.Language.Lang }}" class="no-js">
  <head>
    {{ partial "head.html" . }}
  </head>
  <body class="td-{{ .Kind }}{{ with .Page.Params.body_class }} {{ . }}{{ end }}">
    <header>
      {{ partial "navbar.html" . }}
    </header>
    <div class="container-fluid td-outer">
      <div class="container td-main">
        <div class="row flex-xl-nowrap td-main_inner">
          <aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
            {{ partial "sidebar.html" . }}
          </aside>
          <aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none">
            <div class="td-sidebar-toc__inner">
              {{ partial "page-meta-links.html" . }}
              {{ partial "toc.html" . }}
              {{ partial "taxonomy_terms_clouds.html" . }}
            </div>
          </aside>
          <main class="col-12 col-md-9 col-xl-8 px-4 pl-md-4 pr-md-5 pl-xl-5" role="main">
            {{ partial "version-banner.html" . }}
            {{ if not .Site.Params.ui.breadcrumb_disable }}{{ partial "breadcrumb.html" . }}{{ end }}
            {{ block "main" . }}{{ end }}
          </main>
        </div>
      </div>
      {{ partial "footer.html" . }}
    </div>
    {{ partial "scripts.html" . }}
  </body>
</html>


================================================
File: layouts/docs/list.html
================================================
{{ define "main" }}
<div class="td-content">
	<h1>{{ .Title }}</h1>
  {{ with .Params.description }}<div class="lead">{{ . | markdownify }}</div>{{ end }}
	<header class="article-meta">
		{{ partial "taxonomy_terms_article_wrapper.html" . }}
		{{ if (and (not .Params.hide_readingtime) (.Site.Params.ui.readingtime.enable)) }}
			{{ partial "reading-time.html" . }}
		{{ end }}
	</header>
	{{ .Content }}
        {{ partial "section-index.html" . }}
	{{ if (and (not .Params.hide_feedback) (.Site.Params.ui.feedback.enable) (.Site.GoogleAnalytics)) }}
		{{ partial "feedback.html" . }}
		<br />
	{{ end }}
	{{ if (.Site.DisqusShortname) }}
		<br />
		{{ partial "disqus-comment.html" . }}
	{{ end }}
	{{ partial "page-meta-lastmod.html" . }}
</div>
{{ end }}



================================================
File: layouts/partials/favicons.html
================================================
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png?v=2">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png?v=2">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png?v=2">
<link rel="manifest" href="/site.webmanifest?v=2">
<link rel="mask-icon" href="/safari-pinned-tab.svg?v=2" color="#4279f4">
<link rel="shortcut icon" href="/favicon.ico?v=2">
<meta name="msapplication-TileColor" content="#4279f4">
<meta name="theme-color" content="#4279f4">


================================================
File: layouts/partials/feedback.html
================================================
<style>
  .feedback--answer {
    display: inline-block;
  }
  .feedback--answer-no {
    margin-left: 1em;
  }
  .feedback--response {
    display: none;
    margin-top: 1em;
  }
  .feedback--response__visible {
    display: block;
  }
</style>
<div class="card mt-4 col-12 col-sm-7 d-print-none">
  <div class="card-body">
    <h3 class="card-title">Feedback</h3>
    <p class="card-text">Was this page helpful?</p>
    <button class="btn btn-primary feedback--answer feedback--answer-yes" style="width: 5rem;">Yes</button>
    <button class="btn btn-primary feedback--answer feedback--answer-no" style="width: 5rem;">No</button>
    <p class="feedback--response feedback--response-yes">
      Thank you for your feedback!
    </p>
    <p class="feedback--response feedback--response-no">
      We're sorry this page wasn't helpful.
      {{- if .File }}
      {{- $gh_repo := ($.Param "github_repo") }}
      {{- $issuesURL := printf "%s/issues/new?title=[Feedback]+%s" $gh_repo (safeURL .File.Path) }}
      If you have a moment, please <a href="{{ $issuesURL }}" target="_blank">share your feedback</a> so we can improve.
      {{- end }}
    </p>
  </div>
</div>
<script>
  const yesButton = document.querySelector('.feedback--answer-yes');
  const noButton = document.querySelector('.feedback--answer-no');
  const yesResponse = document.querySelector('.feedback--response-yes');
  const noResponse = document.querySelector('.feedback--response-no');
  const disableButtons = () => {
    yesButton.disabled = true;
    noButton.disabled = true;
  };
  const sendFeedback = (value) => {
    if (typeof ga !== 'function') return;
    const args = {
      command: 'send',
      hitType: 'event',
      category: 'Helpful',
      action: 'click',
      label: window.location.pathname,
      value: value
    };
    ga(args.command, args.hitType, args.category, args.action, args.label, args.value);
  };
  yesButton.addEventListener('click', () => {
    yesResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(1);
  });
  noButton.addEventListener('click', () => {
    noResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(0);
  });
</script>



================================================
File: layouts/partials/footer.html
================================================
{{ $links := .Site.Params.links }}
<footer class="bg-dark pt-3 row d-print-none" xmlns="http://www.w3.org/1999/html">
  <div class="container px-5">
    <div class="row">
      <div class="col-6 col-sm-2 text-xs-center order-sm-2">
        {{ with $links }}
        {{ with index . "user"}}
        {{ template "footer-links-block"  . }}
        {{ end }}
        {{ end }}
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        {{ with $links }}
        {{ with index . "developer"}}
        {{ template "footer-links-block"  . }}
        {{ end }}
        {{ end }}
      </div>
      <div class="col-12 col-sm-8 text-center order-sm-2">
        <div class="mx-auto col-md-4 px-0">
          <p>
            <small class="text-white">&copy; {{ now.Year }} {{ .Site.Params.copyright }}</small>
            <br>
            <small class="text-white">Documentation distributed under <a href="https://github.com/kubeflow/website/blob/master/LICENSE" target="_blank" rel="noopener">CC BY 4.0</a></small>
          </p>
          <p>
            <small class="text-white">The Linux Foundation® (TLF) has registered trademarks and uses trademarks. For a list of TLF trademarks, see <a href="{{ .Site.Params.trademark }}" target="_blank" rel="noopener">Trademark Usage</a>.</small>
          </p>
        {{ if .Page.IsHome }}
          <p>
            <small class="text-white">The R Logo is used under <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">CC BY-SA 4.0</a>.</small>
          </p>
        {{ end }}
          <p>
            <small class="text-white">Website <a href="{{ .Site.Params.privacy_policy }}" target="_blank" rel="noopener">Privacy Policy</a></small>
          </p>
	{{ if not .Site.Params.ui.footer_about_disable }}
		{{ with .Site.GetPage "about" }}<p class="mt-2"><a href="{{ .RelPermalink }}">{{ .Title }}</a></p>{{ end }}
	{{ end }}
        </div>
      </div>
    </div>
  </div>
</footer>
{{ define "footer-links-block" }}
<ul class="list-inline mb-0">
  {{ range . }}
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="{{ .name }}" aria-label="{{ .name }}">
    <a class="text-white" target="_blank" rel="noopener" href="{{ .url }}" aria-label="{{ .name }}">
      <i class="{{ .icon }}"></i>
    </a>
  </li>
  {{ end }}
</ul>
{{ end }}



================================================
File: layouts/partials/head.html
================================================
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
{{ hugo.Generator }}
{{ range .AlternativeOutputFormats -}}
<link rel="{{ .Rel }}" type="{{ .MediaType.Type }}" href="{{ .Permalink | safeURL }}">
{{ end -}}

{{ $outputFormat := partial "outputformat.html" . -}}
{{ if and hugo.IsProduction (ne $outputFormat "print") -}}
<meta name="robots" content="index, follow">
{{ else -}}
<meta name="robots" content="noindex, nofollow">
{{ end -}}

{{ partialCached "favicons.html" . }}
<title>
  {{- if .IsHome -}}
    {{ .Site.Title -}}
  {{ else -}}
    {{ with .Title }}{{ . }} | {{ end -}}
    {{ .Site.Title -}}
  {{ end -}}
</title>
<meta name="description" content="{{ template "partials/page-description.html" . }}">
{{ template "_internal/schema.html" . -}}

<!-- include our custom "social_image_generator" partial -->
{{ partial "social_image_generator" . }}

<!-- include our custom "seo_schema" partial -->
{{ partial "seo_schema" . }}

{{ partialCached "head-css.html" . "asdf" -}}
<script
  src="https://code.jquery.com/jquery-3.6.0.min.js"
  integrity="sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK"
  crossorigin="anonymous"></script>
{{ if .Site.Params.offlineSearch -}}
<script defer
  src="https://unpkg.com/lunr@2.3.9/lunr.min.js"
  integrity="sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli"
  crossorigin="anonymous"></script>
{{ end -}}

{{ if .Site.Params.prism_syntax_highlighting -}}
<link rel="stylesheet" href="{{ "css/prism.css" | relURL }}"/>
{{ end -}}

{{ if .Param "mathjax" }}
{{ partialCached "math.html" . }}
{{ end }}

{{ if .Store.Get "hasSwagger" }}
{{ partialCached "swaggerui.html" . }}
{{ end }}

{{ if .Store.Get "hasMermaid" }}
{{ partialCached "mermaid.html" . }}
{{ end }}

{{ if or (.Store.Get "hasPopper") (.Param "popper") }}
{{ partialCached "popper.html" . }}
{{ end }}

{{ partial "hooks/head-end.html" . -}}

{{/* To comply with GDPR, cookie consent scripts places in head-end must execute before Google Analytics is enabled */ -}}
{{ if hugo.IsProduction -}}
  {{ $enableGtagForUniversalAnalytics := not .Site.Params.disableGtagForUniversalAnalytics -}}
  {{ if (or $enableGtagForUniversalAnalytics (hasPrefix .Site.GoogleAnalytics "G-")) -}}
    {{ template "_internal/google_analytics_gtag.html" . -}}
  {{ else -}}
    {{ template "_internal/google_analytics_async.html" . -}}
  {{ end -}}
{{ end -}}



================================================
File: layouts/partials/math.html
================================================
<script async
  id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"
  integrity="sha384-AHAnt9ZhGeHIrydA1Kp1L7FN+2UosbF7RQg6C+9Is/a7kDpQ1684C2iH2VWil6r4"
  crossorigin="anonymous">
</script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  // block
      inlineMath: [['\\(', '\\)']]                  // inline
    }
  };
</script>


================================================
File: layouts/partials/mermaid.html
================================================
<script
  src="https://cdn.jsdelivr.net/npm/mermaid@10.9.0/dist/mermaid.min.js"
  integrity="sha384-6F4Ibv/ylL12O35KFWTeGTHuBKDz5L6yjKsgv3QHQ8s4NTqlDXq7kMlYXGs7MHFc"
  crossorigin="anonymous">
</script>
<script>
  mermaid.initialize({ startOnLoad: true });
</script>


================================================
File: layouts/partials/navbar-version-selector.html
================================================
<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	{{ .Site.Params.version_menu }}
</a>
<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
	{{ $path := "" }}
	{{ if .Site.Params.version_menu_pagelinks }}
		{{ $path = .Page.RelPermalink }}
	{{ end }}
	{{ range .Site.Params.versions }}
	<a class="dropdown-item" href="{{ .url }}{{ $path }}">{{ .version }}</a>
	{{ end }}
</div>



================================================
File: layouts/partials/navbar.html
================================================
{{ $cover := and
    (.HasShortcode "blocks/cover")
    (not .Site.Params.ui.navbar_translucent_over_cover_disable)
-}}

<nav class="js-navbar-scroll navbar navbar-expand-md navbar-dark
            {{- if $cover }} td-navbar-cover {{- end }} td-navbar">
<div class="container px-0">
  <a class="navbar-brand" href="{{ .Site.Home.RelPermalink }}">
    {{- /**/ -}}
    <span class="navbar-brand__logo navbar-logo">
      {{- if ne .Site.Params.ui.navbar_logo false -}}
        {{ with resources.Get "icons/logo.svg" -}}
          {{ ( . | minify).Content | safeHTML -}}
        {{ end -}}
      {{ end -}}
    </span>
    {{- /**/ -}}
    <span class="text-uppercase font-weight-bold">
      {{- .Site.Title -}}
    </span>
    {{- /**/ -}}
  </a>
  <div class="filler"></div>
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main_navbar" aria-controls="main_navbar" aria-expanded="false" aria-label="Toggle navigation">
		<span class="navbar-toggler-icon"></span>
	</button>
	<div class="collapse navbar-collapse ml-md-auto" id="main_navbar">
    <ul class="navbar-nav ml-auto pt-4 pt-md-0 my-2 my-md-1">
      {{ $p := . -}}
      {{ range .Site.Menus.main -}}
      {{ $longContent := gt (len .Name) 10 -}}
      <li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0 {{- if $longContent }} long-content {{- end }}">
        {{ $active := or ($p.IsMenuCurrent "main" .) ($p.HasMenuCurrent "main" .) -}}
        {{ with .Page }}{{ $active = or $active ( $.IsDescendant .) }}{{ end -}}
        {{ $pre := .Pre -}}
        {{ $post := .Post -}}
        {{ $url := urls.Parse .URL -}}
        {{ $baseurl := urls.Parse $.Site.Params.Baseurl -}}
        <a {{/**/ -}}
          class="nav-link {{- if $active }} active {{- end }}" {{/**/ -}}
          href="{{ with .Page }}{{ .RelPermalink }}{{ else }}{{ .URL | relLangURL }}{{ end }}"
          {{- if ne $url.Host $baseurl.Host }} target="_blank" {{- end -}}
        >
            {{- with .Pre }}{{ $pre }}{{ end -}}
            <span {{- if $active }} class="active" {{- end }}>
              {{- .Name -}}
            </span>
            {{- with .Post }}{{ $post }}{{ end -}}
        </a>
      </li>
      {{ end -}}
      {{ if .Site.Params.versions -}}
      <li class="nav-item dropdown mt-1 mt-lg-0">
        {{ partial "navbar-version-selector.html" . -}}
      </li>
      {{ end -}}
      {{ if (gt (len .Site.Home.Translations) 0) -}}
      <li class="nav-item dropdown mt-1 mt-lg-0 mr-2">
        {{ partial "navbar-lang-selector.html" . -}}
      </li>
      {{ end -}}
    </ul>
  </div>
</div>
</nav>


================================================
File: layouts/partials/page-meta-links.html
================================================
{{ if .File }}
{{ $pathFormatted := replace .File.Path "\\" "/" -}}
{{ $gh_repo := ($.Param "github_repo") -}}
{{ $gh_url := ($.Param "github_url") -}}
{{ $gh_subdir := ($.Param "github_subdir") -}}
{{ $gh_project_repo := ($.Param "github_project_repo") -}}
{{ $gh_branch := (default "main" ($.Param "github_branch")) -}}
<div class="td-page-meta pb-3">
{{ if $gh_url -}}
  {{ warnf "Warning: use of `github_url` is deprecated. For details see https://www.docsy.dev/docs/adding-content/repository-links/#github_url-optional" -}}
  <a href="{{ $gh_url }}" target="_blank"><i class="fa-solid fa-pen-to-square fa-fw"></i> {{ T "post_edit_this" }}</a>
{{ else if $gh_repo -}}
  {{ $gh_repo_path := printf "%s/content/%s" $gh_branch $pathFormatted -}}
  {{ if and ($gh_subdir) (.Site.Language.Lang) -}}
    {{ $gh_repo_path = printf "%s/%s/content/%s/%s" $gh_branch $gh_subdir ($.Site.Language.Lang) $pathFormatted -}}
  {{ else if .Site.Language.Lang -}}
    {{ $gh_repo_path = printf "%s/content/%s/%s" $gh_branch ($.Site.Language.Lang) $pathFormatted -}}
  {{ else if $gh_subdir -}}
    {{ $gh_repo_path = printf "%s/%s/content/%s" $gh_branch $gh_subdir $pathFormatted -}}
  {{ end -}}

  {{/* Adjust $gh_repo_path based on path_base_for_github_subdir */ -}}
  {{ $ghs_base := $.Param "path_base_for_github_subdir" -}}
  {{ $ghs_rename := "" -}}
  {{ if reflect.IsMap $ghs_base -}}
    {{ $ghs_rename = $ghs_base.to -}}
    {{ $ghs_base = $ghs_base.from -}}
  {{ end -}}
  {{ with $ghs_base -}}
    {{ $gh_repo_path = replaceRE . $ghs_rename $gh_repo_path -}}
  {{ end -}}

  {{ $viewURL := printf "%s/tree/%s" $gh_repo $gh_repo_path -}}
  {{ $editURL := printf "%s/edit/%s" $gh_repo $gh_repo_path -}}
  {{ $issuesURL := printf "%s/issues/new?title=[Feedback]+%s" $gh_repo (safeURL $.File.Path ) -}}
  {{ $newPageStub := resources.Get "stubs/new-page-template.md" -}}
  {{ $newPageQS := querify "value" $newPageStub.Content "filename" "change-me.md" | safeURL -}}
  {{ $newPageURL := printf "%s/new/%s?%s"  $gh_repo $gh_repo_path $newPageQS -}}

<!--  <a href="{{ $viewURL }}" class="td-page-meta&#45;&#45;view" target="_blank" rel="noopener"><i class="fa-solid fa-file-lines fa-fw"></i> {{ T "post_view_this" }}</a>-->
  <a href="{{ $editURL }}" class="td-page-meta--edit" target="_blank" rel="noopener"><i class="fa-solid fa-pen-to-square fa-fw"></i> {{ T "post_edit_this" }}</a>
<!--  <a href="{{ $newPageURL }}" class="td-page-meta&#45;&#45;child" target="_blank" rel="noopener"><i class="fa-solid fa-pen-to-square fa-fw"></i> {{ T "post_create_child_page" }}</a>-->
  <a href="{{ $issuesURL }}" class="td-page-meta--issue" target="_blank" rel="noopener"><i class="fa-solid fa-comment fa-fw"></i> {{ T "post_create_issue" }}</a>
  {{ with $gh_project_repo -}}
    {{ $project_issueURL := printf "%s/issues/new/choose" . -}}
<!--    <a href="{{ $project_issueURL }}" class="td-page-meta&#45;&#45;project-issue" target="_blank" rel="noopener"><i class="fa-solid fa-bug fa-fw"></i> {{ T "post_create_project_issue" }}</a>-->
  {{ end -}}

{{ end -}}
{{ with .CurrentSection.AlternativeOutputFormats.Get "print" -}}
  <a id="print" href="{{ .Permalink | safeURL }}"><i class="fa-solid fa-print fa-fw"></i> {{ T "print_entire_section" }}</a>
{{ end }}
</div>
{{ end -}}



================================================
File: layouts/partials/popper.html
================================================
<script
  src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"
  integrity="sha256-/ijcOLwFf26xEYAjW75FizKVo5tnTYiQddPZoLUHHZ8="
  crossorigin="anonymous">
</script>
<script>
  // Initialize tooltips
  $(function () {
    $('[data-toggle="tooltip"]').tooltip()
  })

  // Initialize popovers
  $(function () {
    $('[data-toggle="popover"]').popover({
      fallbackPlacement: ['right', 'bottom', 'top', 'left'],
    })
  })
</script>


================================================
File: layouts/partials/scripts.html
================================================
{{/* NOTE: we disable Docsy's default versions of these libraries, because they are very old. */ -}}
{{ $needKaTeX  := false -}}
{{ $needmhchem := false -}}
{{ $needmermaid := false -}}

{{ if .Site.Params.markmap.enable -}}
<style>
.markmap > svg {
  width: 100%;
  height: 300px;
}
</style>
<script>
window.markmap = {
  autoLoader: {
      manual: true,
      onReady() {
        const { autoLoader, builtInPlugins } = window.markmap;
        autoLoader.transformPlugins = builtInPlugins.filter(plugin => plugin.name !== 'prism');
      },
  },
};
</script>
<script src="https://cdn.jsdelivr.net/npm/markmap-autoloader"></script>
{{ end -}}

{{ if .Site.Params.plantuml.enable -}}
  <script src='{{ "js/deflate.js" | relURL }}'></script>
{{ end -}}

{{ if  $needKaTeX -}}
{{/* load stylesheet and scripts for KaTeX support */ -}}
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css"
      integrity="sha512-6VMVcy7XQNyarhVuiL50FzpgCFKsyTd6YO93aaQEyET+BNaWvj0IgKR86Bf6+AmWczxAcSnL+JGjo+iStgO1gQ==" crossorigin="anonymous">
  {{/* The loading of KaTeX is deferred to speed up page rendering */ -}}
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js"
        integrity="sha512-b9IKj4LCNrtCPBhceRcoYOHWW/S2q9fpl7iAJlyxYpykRj1SKM7FE9+E0NEnJ8g8ni47LBr2GuX9qzg/xeuwzQ=="
        crossorigin="anonymous">
</script>
  {{ if $needmhchem -}}
  {{/* To add support for displaying chemical equations and physical units, load the mhchem extension: */ -}}
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/mhchem.min.js"
        integrity="sha512-V1hl0fnOXW6Cdqe5ZVqtw8TBpJVpu3XRDRQti96j/04+tMarPrCdXEUE3UdfvfKYTpOn9DV4zEZBVr0HhDiuiQ=="
        crossorigin="anonymous">
</script>
  {{ end -}}
  {{/* To automatically render math in text elements, include the auto-render extension: */ -}}
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js"
       integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous"
       {{ printf "onload='renderMathInElement(%s, %s);'" (( $.Page.Site.Params.katex.html_dom_element | default "document.body" ) | safeJS ) ( printf "%s" ( $.Page.Site.Params.katex.options | jsonify )) | safeHTMLAttr }}>
</script>
{{ end -}}

{{ $jsBs := resources.Get "vendor/bootstrap/dist/js/bootstrap.bundle.js" -}}
{{ $jsBase := resources.Get "js/base.js" -}}
{{ $jsAnchor := resources.Get "js/anchor.js" -}}
{{ $jsSearch := resources.Get "js/search.js" | resources.ExecuteAsTemplate "js/search.js" .Site.Home -}}
{{ $jsMermaid := resources.Get "js/mermaid.js" | resources.ExecuteAsTemplate "js/mermaid.js" . -}}
{{ $jsMarkmap := resources.Get "js/markmap.js" | resources.ExecuteAsTemplate "js/markmap.js" . -}}
{{ $jsPlantuml := resources.Get "js/plantuml.js" | resources.ExecuteAsTemplate "js/plantuml.js" . -}}
{{ $jsDrawio := resources.Get "js/drawio.js" | resources.ExecuteAsTemplate "js/drawio.js" . -}}
{{ if .Site.Params.offlineSearch -}}
  {{ $jsSearch = resources.Get "js/offline-search.js" -}}
{{ end -}}

{{ $jsArray := slice $jsBs $jsBase $jsAnchor $jsSearch $jsPlantuml $jsMarkmap $jsDrawio -}}

{{ if $needmermaid -}}
{{ $jsArray = $jsArray | append $jsMermaid -}}
<script src="https://cdn.jsdelivr.net/npm/mermaid@9.2.2/dist/mermaid.min.js" integrity="sha512-IX+bU+wShHqfqaMHLMrtwi4nK6W/Z+QdZoL4kPNtRxI2wCLyHPMAdl3a43Fv1Foqv4AP+aiW6hg1dcrTt3xc+Q==" crossorigin="anonymous"></script>
{{ end -}}

{{ $js := $jsArray | resources.Concat "js/main.js" -}}
{{ if hugo.IsProduction -}}
  {{ $js := $js | minify | fingerprint -}}
  <script src="{{ $js.RelPermalink }}" integrity="{{ $js.Data.Integrity }}" crossorigin="anonymous"></script>
{{ else -}}
  <script src="{{ $js.RelPermalink }}"></script>
{{ end -}}

{{ if .Site.Params.prism_syntax_highlighting -}}
  <script src='{{ "js/prism.js" | relURL }}'></script>
{{ else if false -}}
  {{ $c2cJS := resources.Get "js/click-to-copy.js" -}}
  {{ if hugo.IsProduction -}}
    {{ $c2cJS = $c2cJS | minify | fingerprint -}}
  {{ end -}}
  <script defer src="{{ $c2cJS.RelPermalink }}" {{ with $c2cJS.Data.Integrity -}}
    integrity="{{ . }}" {{ end -}}
    crossorigin="anonymous"></script>
{{ end -}}

<script src='{{ "js/tabpane-persist.js" | relURL }}'></script>
{{ partial "hooks/body-end.html" . -}}



================================================
File: layouts/partials/seo_schema.html
================================================
{{- if and (not .IsHome) .File }}
<script type="application/ld+json">
[
  {
      "@context" : "http://schema.org",
      "@type" : "Article",
      "articleSection" : {{ .Section }},
      "name" : {{ .Title }},
      "headline" : {{ .Title }},
      {{- if .Description }}
      "description" : {{ .Description }},
      {{- else if .IsPage }}
      "description" : {{ .Summary }},
      {{- end }}
      "inLanguage" : "en-US",
      "copyrightHolder" : {{ .Site.Param "copyright" }},
      "copyrightYear" : {{ .Lastmod.Year }},
      "dateModified" : {{ .Lastmod }},
      "url" : {{ .Permalink }},
      "wordCount" : {{ .WordCount }},
      "keywords" : [ {{ if isset .Params "tags" }}{{ range .Params.tags }}"{{ . }}",{{ end }}{{ end }}"Kubeflow" ]
  }
  {{- if gt (len .Ancestors) 1 }},
  {
      "@context" : "http://schema.org",
      "@type" : "BreadcrumbList",
      "itemListElement" : [
          {{- $finalIndex := sub (len .Ancestors) 1 }}
          {{- range $index, $item := .Ancestors.Reverse }}
          {{- if gt $index 0 }}
          {
              "@type" : "ListItem",
              "position" : {{ $index }},
              "item" : {
                  "@id" : {{ $item.Permalink }},
                  "name" : {{ $item.Title }}
              }
          }{{ if ne $index $finalIndex }},{{ end }}
          {{- end }}
          {{- end }}
      ]
  }
  {{- end }}
]
</script>
{{- end }}


================================================
File: layouts/partials/sidebar-tree.html
================================================
{{/* We cache this partial for bigger sites and set the active class client side. */}}
{{ $sidebarCacheLimit := cond (isset .Site.Params.ui "sidebar_cache_limit") .Site.Params.ui.sidebar_cache_limit 2000 -}}
{{ $shouldDelayActive := ge (len .Site.Pages) $sidebarCacheLimit -}}
<div id="td-sidebar-menu" class="td-sidebar__inner{{ if $shouldDelayActive }} d-none{{ end }}">
  {{ if not .Site.Params.ui.sidebar_search_disable -}}
  <form class="td-sidebar__search d-flex align-items-center">
    {{ partial "search-input.html" . }}
    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-section-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>
  {{ else -}}
  <div id="content-mobile">
  <form class="td-sidebar__search d-flex align-items-center">
    {{ partial "search-input.html" . }}
    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-section-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>
  </div>
  <div id="content-desktop"></div>
  {{ end -}}
  <nav class="collapse td-sidebar-nav{{ if .Site.Params.ui.sidebar_menu_foldable }} foldable-nav{{ end }}" id="td-section-nav">
    {{ if  (gt (len .Site.Home.Translations) 0) -}}
    <div class="nav-item dropdown d-block d-lg-none">
      {{ partial "navbar-lang-selector.html" . }}
    </div>
    {{ end -}}
    {{ $navRoot := cond (and (ne .Params.toc_root true) (eq .Site.Home.Type "docs")) .Site.Home .FirstSection -}}
    {{ $ulNr := 0 -}}
    {{ $ulShow := cond (isset .Site.Params.ui "ul_show") .Site.Params.ui.ul_show 1 -}}
    {{ $sidebarMenuTruncate := cond (isset .Site.Params.ui "sidebar_menu_truncate") .Site.Params.ui.sidebar_menu_truncate 50 -}}
    <ul class="td-sidebar-nav__section ul-{{ $ulNr }}">
      {{ template "section-tree-nav-section" (dict "page" . "section" $navRoot "shouldDelayActive" $shouldDelayActive "sidebarMenuTruncate" $sidebarMenuTruncate "ulNr" $ulNr "ulShow" (add $ulShow 1)) }}
    </ul>
  </nav>
</div>
{{ define "section-tree-nav-section" -}}
{{ $s := .section -}}
{{ $p := .page -}}
{{ $shouldDelayActive := .shouldDelayActive -}}
{{ $sidebarMenuTruncate := .sidebarMenuTruncate -}}
{{ $treeRoot := cond (eq .ulNr 0) true false -}}
{{ $ulNr := .ulNr -}}
{{ $ulShow := .ulShow -}}
{{ $active := and (not $shouldDelayActive) (eq $s $p) -}}
{{ $activePath := and (not $shouldDelayActive) (or (eq $p $s) ($p.IsDescendant $s)) -}}
{{ $show := cond (or (lt $ulNr $ulShow) $activePath (and (not $shouldDelayActive) (eq $s.Parent $p.Parent)) (and (not $shouldDelayActive) (eq $s.Parent $p)) (not $p.Site.Params.ui.sidebar_menu_compact) (and (not $shouldDelayActive) ($p.IsDescendant $s.Parent))) true false -}}
{{ $mid := printf "m-%s" ($s.RelPermalink | anchorize) -}}
{{ $pages_tmp := where (union $s.Pages $s.Sections).ByWeight ".Params.toc_hide" "!=" true -}}
{{ $pages := $pages_tmp | first $sidebarMenuTruncate -}}
{{ $withChild := gt (len $pages) 0 -}}
{{ $manualLink := cond (isset $s.Params "manuallink") $s.Params.manualLink ( cond (isset $s.Params "manuallinkrelref") (relref $s $s.Params.manualLinkRelref) $s.RelPermalink) -}}
{{ $manualLinkTitle := cond (isset $s.Params "manuallinktitle") $s.Params.manualLinkTitle $s.Title -}}
<li class="td-sidebar-nav__section-title td-sidebar-nav__section{{ if $withChild }} with-child{{ else }} without-child{{ end }}{{ if $activePath }} active-path{{ end }}{{ if (not (or $show $p.Site.Params.ui.sidebar_menu_foldable )) }} collapse{{ end }}" id="{{ $mid }}-li">
  {{ if (and $p.Site.Params.ui.sidebar_menu_foldable (ge $ulNr 1)) -}}
  <input type="checkbox" id="{{ $mid }}-check"{{ if $activePath}} checked{{ end }}/>
  <label for="{{ $mid }}-check"><a href="{{ $manualLink }}"{{ if ne $s.LinkTitle $manualLinkTitle }} title="{{ $manualLinkTitle }}"{{ end }}{{ with $s.Params.manualLinkTarget }} target="{{ . }}"{{ if eq . "_blank" }} rel="noopener"{{ end }}{{ end }} class="align-left pl-0 {{ if $active}} active{{ end }} td-sidebar-link{{ if $s.IsPage }} td-sidebar-link__page{{ else }} td-sidebar-link__section{{ end }}{{ if $treeRoot }} tree-root{{ end }}" id="{{ $mid }}">{{ with $s.Params.Icon}}<i class="{{ . }}"></i>{{ end }}<span class="{{ if $active }}td-sidebar-nav-active-item{{ end }}">{{ $s.LinkTitle }}</span></a></label>
  {{ else -}}
  <a href="{{ $manualLink }}"{{ if ne $s.LinkTitle $manualLinkTitle }} title="{{ $manualLinkTitle }}"{{ end }}{{ with $s.Params.manualLinkTarget }} target="{{ . }}"{{ if eq . "_blank" }} rel="noopener"{{ end }}{{ end }} class="align-left pl-0{{ if $active}} active{{ end }} td-sidebar-link{{ if $s.IsPage }} td-sidebar-link__page{{ else }} td-sidebar-link__section{{ end }}{{ if $treeRoot }} tree-root{{ end }}" id="{{ $mid }}">{{ with $s.Params.Icon}}<i class="{{ . }}"></i>{{ end }}<span class="{{ if $active }}td-sidebar-nav-active-item{{ end }}">{{ $s.LinkTitle }}</span></a>
  {{- end }}
  {{- if $withChild }}
  {{- $ulNr := add $ulNr 1 }}
  <ul class="ul-{{ $ulNr }}{{ if (gt $ulNr 1)}} foldable{{end}}">
    {{ range $pages -}}
    {{ if (not (and (eq $s $p.Site.Home) (eq .Params.toc_root true))) -}}
    {{ template "section-tree-nav-section" (dict "page" $p "section" . "shouldDelayActive" $shouldDelayActive "sidebarMenuTruncate" $sidebarMenuTruncate "ulNr" $ulNr "ulShow" $ulShow) }}
    {{- end }}
    {{- end }}
  </ul>
  {{- end }}
</li>
{{- end }}


================================================
File: layouts/partials/social_image_generator.html
================================================
{{- if .IsHome }}
  <!-- NOTE: the homepage uses a hand-made social image -->
  {{- $img := resources.Get "/social/social_home.png" }}

  <!-- Fingerprint the image to get the hash -->
  {{- $img := $img | resources.Fingerprint "md5" }}

  <!-- OpenGraph metadata (used by Facebook, LinkedIn, etc.) -->
  <meta property="og:image" content="{{ $img.Permalink }}">
  <meta property="og:image:type" content="{{ $img.MediaType.Type }}">
  <meta property="og:image:width" content="{{ $img.Width }}">
  <meta property="og:image:height" content="{{ $img.Height }}">
  {{ template "_internal/opengraph.html" . }}

  <!-- Twitter Card metadata -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="{{ $img.Permalink }}">
  {{ template "_internal/twitter_cards.html" . }}

{{- else if not .File }}
  <!-- NOTE: we filter out any pages that are not actually files (only the 404 page should match this) -->

{{- else }}
  <!-- NOTE: all other pages use dynamically generated social images -->
  {{- $base := resources.Get "/social/social_base.png" }}
  {{- $font := resources.Get "/fonts/Inter-Medium.ttf"}}

  <!-- Generate the image text by appending all but the first two ancestors (which are the same for all) -->
  {{- $text := "" }}
  {{- range $index, $ancestor := .Page.Ancestors.Reverse }}
    {{- if gt $index 1 }}
      {{- $text = printf "%s%s ‣ " $text $ancestor.Title }}
    {{- end }}
  {{- end }}
  {{- $text = printf "%s%s" $text .Title }}

  <!-- We use Hugo's filter feature to apply text over a base PNG -->
  {{- $img_opts := dict
    "color" "#ffffff"
    "size" 64
    "linespacing" 2
    "x" 65
    "y" 320
    "font" $font
  }}
  {{- $img := $base.Filter (images.Text $text $img_opts) }}

  <!-- Rename the image to `social.png` and put it under the same path as the page html -->
  {{- $img = $img | resources.Copy (path.Join .Page.RelPermalink "social.png")}}

  <!-- Fingerprint the image to get the hash -->
  {{- $img := $img | resources.Fingerprint "md5" }}

  <!-- OpenGraph metadata (used by Facebook, LinkedIn, etc.) -->
  <meta property="og:image" content="{{ $img.Permalink }}">
  <meta property="og:image:type" content="{{ $img.MediaType.Type }}">
  <meta property="og:image:width" content="{{ $img.Width }}">
  <meta property="og:image:height" content="{{ $img.Height }}">
  {{ template "_internal/opengraph.html" . }}

  <!-- Twitter Card metadata -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="{{ $img.Permalink }}">
  {{ template "_internal/twitter_cards.html" . -}}
{{- end }}


================================================
File: layouts/partials/swaggerui.html
================================================
<!-- NOTE: when updating, don't forget to update the style version in `shortcodes/swaggerui-inline.html` -->
<script
  src="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.17.14/swagger-ui-bundle.js"
  integrity="sha384-wmyclcVGX/WhUkdkATwhaK1X1JtiNrr2EoYJ+diV3vj4v6OC5yCeSu+yW13SYJep"
  crossorigin="anonymous">
</script>
<script
  src="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.17.14/swagger-ui-standalone-preset.js"
  integrity="sha384-2YH8WDRaj7V2OqU/trsmzSagmk/E2SutiCsGkdgoQwC9pNUJV1u/141DHB6jgs8t"
  crossorigin="anonymous">
</script>
<!-- NOTE: we use the js-yaml package to parse the YAML/JSON files into JS objects so we can remove fields that cause swagger-ui to display unneeded UI elements -->
<script
  src="https://cdn.jsdelivr.net/npm/js-yaml@4.1.0/dist/js-yaml.min.js"
  integrity="sha256-Rdw90D3AegZwWiwpibjH9wkBPwS9U4bjJ51ORH8H69c="
  crossorigin="anonymous">
</script>


================================================
File: layouts/shortcodes/alpha-status.html
================================================
<div class="alert alert-warning" role="alert">
  <h4 class="alert-heading">Alpha</h4>
  This Kubeflow component has <b>alpha</b> status with limited support. See the
  <a href="/docs/started/support/#application-status">Kubeflow versioning policies</a>.
  The Kubeflow team is interested in your {{ if .Get "feedbacklink"}} {{ with .Get "feedbacklink" }} 
  <a href="{{ . | safeURL }}">feedback</a></h4>{{ end }} {{ else }}feedback{{ end }}
  about the usability of the feature.
</div>



================================================
File: layouts/shortcodes/beta-status.html
================================================
<div class="alert alert-warning" role="alert">
  <h4 class="alert-heading">Beta</h4>
  This Kubeflow component has <b>beta</b> status. See the
  <a href="/docs/started/support/#application-status">Kubeflow versioning policies</a>.
  The Kubeflow team is interested in your {{ if .Get "feedbacklink"}} {{ with .Get "feedbacklink" }} 
  <a href="{{ . | safeURL }}">feedback</a></h4>{{ end }} {{ else }}feedback{{ end }}
  about the usability of the feature.
</div>
  


================================================
File: layouts/shortcodes/config-file-anthos.html
================================================
kfctl_anthos.v1.0.2.yaml


================================================
File: layouts/shortcodes/config-file-gcp-basic-auth.html
================================================
kfctl_gcp_basic_auth.v1.0.2.yaml


================================================
File: layouts/shortcodes/config-file-gcp-iap.html
================================================
kfctl_gcp_iap.v1.0.2.yaml


================================================
File: layouts/shortcodes/config-file-ibm.html
================================================
kfctl_ibm.v1.1.0.yaml



================================================
File: layouts/shortcodes/config-file-istio-dex.html
================================================
kfctl_istio_dex.v1.2.0.yaml


================================================
File: layouts/shortcodes/config-file-k8s-istio.html
================================================
kfctl_k8s_istio.v1.2.0.yaml


================================================
File: layouts/shortcodes/config-file-openshift.html
================================================
kfctl_openshift.yaml


================================================
File: layouts/shortcodes/config-uri-anthos.html
================================================
https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_anthos.v1.0.2.yaml


================================================
File: layouts/shortcodes/config-uri-gcp-basic-auth.html
================================================
https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_gcp_basic_auth.v1.0.2.yaml


================================================
File: layouts/shortcodes/config-uri-gcp-iap.html
================================================
https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_gcp_iap.v1.0.2.yaml


================================================
File: layouts/shortcodes/config-uri-ibm.html
================================================
https://raw.githubusercontent.com/kubeflow/manifests/v1.1-branch/kfdef/kfctl_ibm.v1.1.0.yaml



================================================
File: layouts/shortcodes/config-uri-istio-dex.html
================================================
https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_istio_dex.v1.2.0.yaml


================================================
File: layouts/shortcodes/config-uri-k8s-istio.html
================================================
https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_k8s_istio.v1.2.0.yaml


================================================
File: layouts/shortcodes/config-uri-openshift.html
================================================
https://raw.githubusercontent.com/opendatahub-io/manifests/v0.7-branch-openshift/kfdef/kfctl_openshift.yaml


================================================
File: layouts/shortcodes/kf-deployment-ui-version.html
================================================
v1.0.0


================================================
File: layouts/shortcodes/kf-latest-version.html
================================================
v1.9.0



================================================
File: layouts/shortcodes/kf-version-notice.html
================================================
{{ .Page.Store.Set "hasPopper" true }}
{{- $version_full := replaceRE `\s` "" .Inner }}
{{- $version_short := replaceRE `^(\d+)\.(\d+)\.(\d+).*` `$1.$2` $version_full }}

{{- $latest_release_json_path := "release-info/latest.json" }}
{{- $latest_release_json := .Page.Resources.Get $latest_release_json_path }}
{{- if not $latest_release_json }}
  {{- errorf `Failed to get "latest" release info, no file at %q.` $latest_release_json_path }}
{{- end }}
{{- $latest_release := $latest_release_json | transform.Unmarshal }}
{{- $latest_release_full := $latest_release.tag | replaceRE `^v(\d+)\.(\d+)\.(\d+).*` `$1.$2.$3` }}
{{- $latest_release_major := $latest_release.tag | replaceRE `^v(\d+)\.(\d+)\.(\d+).*` `$1` | int  }}
{{- $latest_release_minor := $latest_release.tag | replaceRE `^v(\d+)\.(\d+)\.(\d+).*` `$2` | int  }}

{{- $this_release_json_path := printf "release-info/v%s.json" $version_full }}
{{- $this_release_json := .Page.Resources.Get $this_release_json_path }}
{{- if not $this_release_json }}
  {{- errorf "Failed to get release info for version %q, no file at %q.\n\n\n!!!! TIP: run the './content/en/docs/started/installing-kubeflow/get_new_releases.sh' to update Kubeflow release information !!!!\n\n\n" $version_full $this_release_json_path $version_full }}
{{- end }}
{{- $this_release := $this_release_json | transform.Unmarshal }}
{{- $this_release_major := $this_release.tag | replaceRE `^v(\d+)\.(\d+)\.(\d+).*` `$1` | int }}
{{- $this_release_minor := $this_release.tag | replaceRE `^v(\d+)\.(\d+)\.(\d+).*` `$2` | int }}
{{- $this_release_date := $this_release.publish_date | time.AsTime }}
{{- $this_release_date_string := $this_release_date | time.Format ":date_long" }}

{{- $this_version_lag := sub $latest_release_minor $this_release_minor }}
{{- $popup_content_prefix := printf "Kubeflow %s was initially released on <strong>%s</strong> and is <strong>%d</strong> minor versions behind the latest release." $version_full $this_release_date_string $this_version_lag }}
{{- $popup_content_suffix := "Distributions are supported by their maintainers. Contact the provider for information on support and updates." }}

{{- $button_style := "" }}
{{- $popup_icon := "" }}
{{- $popup_title := "" }}
{{- $popup_content :=  "" }}
{{- if eq $this_version_lag 0 }}
  {{- $button_style = "btn-outline-primary" }}
  {{- $popup_icon = "" }}
  {{- $popup_title = "Version Info" }}
  {{- $popup_content = printf `%s<br><br>%s` $popup_content_prefix $popup_content_suffix }}
{{- else if eq $this_version_lag 1 }}
  {{- $button_style = "btn-outline-primary" }}
  {{- $popup_icon = "" }}
  {{- $popup_title = "Version Info" }}
  {{- $popup_content = printf `%s<br><br>%s` $popup_content_prefix $popup_content_suffix }}
{{- else if eq $this_version_lag 2 }}
  {{- $button_style = "btn-outline-primary" }}
  {{- $popup_icon = "" }}
  {{- $popup_title = "Version Info" }}
  {{- $popup_content = printf `%s<br><br>%s` $popup_content_prefix $popup_content_suffix }}
{{- else }}
  {{- $button_style = "btn-outline-primary" }}
  {{- $popup_icon = "" }}
  {{- $popup_title = "Version Info" }}
  {{- $popup_content = printf `%s<br><br>%s` $popup_content_prefix $popup_content_suffix }}
{{- end }}

<button
  type="button"
  class="btn btn-sm {{ $button_style }} d-block d-lg-inline mb-2 mb-lg-0"
  data-container="body"
  data-placement="right"
  data-toggle="popover"
  data-trigger="hover focus"
  data-html="true"
  title="{{ $popup_title }}"
  data-content="{{ $popup_content }}"
>
  {{- if $popup_icon }}
  <i class="fas {{ $popup_icon }}"></i>
  {{- end }}
  <span>
    {{- if eq $this_version_lag 0 }}
    <strong>{{ $version_short }}</strong>
    {{- else }}
    {{ $version_short }}
    {{- end }}
  </span>
</button>


================================================
File: layouts/shortcodes/kfp-v2-keywords.html
================================================
<meta name="keywords" content="kfp, kfp sdk, kfp sdk v2, version 2">



================================================
File: layouts/shortcodes/kubernetes-incompatible-versions.html
================================================
1.16


================================================
File: layouts/shortcodes/kubernetes-min-version.html
================================================
1.11


================================================
File: layouts/shortcodes/kubernetes-tested-version.html
================================================
1.14


================================================
File: layouts/shortcodes/kustomize-min-version.html
================================================
2.0.3


================================================
File: layouts/shortcodes/needs-update.html
================================================
<div class="alert alert-warning" role="alert">
  <h4 class="alert-heading">Documentation Needs Update</h4>
  This page needs updating. The instructions don't work for the current version of Kubeflow.
</div>



================================================
File: layouts/shortcodes/no-index.html
================================================
<meta name="robots" content="noindex">



================================================
File: layouts/shortcodes/note.html
================================================
<!-- adapted from Docsy alert shortcode -->
{{- $_hugo_config := `{ "version": 1 }` -}}
{{ $color := "primary" }}
<div class="alert alert-{{- $color -}}" role="alert">
{{- with ( "Note" ) -}}<h4 class="alert-heading">{{- . | safeHTML -}}</h4>{{- end -}}
{{- if eq .Page.File.Ext "md" -}}
    {{- .Inner | markdownify -}}
{{- else -}}
    {{- .Inner | htmlUnescape | safeHTML -}}
{{- end -}}
</div>



================================================
File: layouts/shortcodes/oss-be-unsupported.html
================================================
<div class="alert alert-warning" role="alert">
  <h4 class="alert-heading">Not yet supported</h4>
  {{ if .Get "feature_name"}} {{ with .Get "feature_name" }} 
  {{ . }} {{ end }} {{ else }}This feature {{ end }} is not yet supported by the KFP orchestration backend, but may be supported by other orchestration backends.{{ if .Get "feature_name"}} {{ with .Get "gh_issue_link" }} You can track support for this feature via the <a href="{{ . | safeURL }}">GitHub issue</a></h4>. {{ end }} {{ else }} {{ end }}
</div>



================================================
File: layouts/shortcodes/params.html
================================================
{{- .Page.Param (.Get 0) -}}


================================================
File: layouts/shortcodes/pipelines-compatibility.html
================================================
<p><i>Due to <a href="https://github.com/kubeflow/pipelines/issues/1700">kubeflow/pipelines#1700</a>, the container builder in Kubeflow Pipelines currently prepares credentials for Google Cloud Platform (GCP) only. As a result, the container builder supports only Google Container Registry. However, you can store the container images on other registries, provided you set up the credentials correctly to fetch the image.</i></p>



================================================
File: layouts/shortcodes/stable-status.html
================================================
<div class="alert alert-primary" role="alert">
This Kubeflow component has <b>stable</b> status. See the
<a href="/docs/started/support/#application-status">Kubeflow versioning policies</a>.
</div>



================================================
File: layouts/shortcodes/swaggerui-inline.html
================================================
{{ .Page.Store.Set "hasSwagger" true }}

{{- $component_name := .Get "component_name" }}
{{- if not $component_name }}
  {{- errorf "Missing param 'component_name' on 'swaggerui-inline' shortcode for page: %q" .Page.File.Path }}
{{- end }}

{{- $default_input_url := .Get "default_input_url" }}
{{- if not $default_input_url }}
  {{- errorf "Missing param 'default_input_url' on 'swaggerui-inline' shortcode for page: %q" .Page.File.Path }}
{{- end }}

{{- $remote_url := replaceRE `\s` "" .Inner }}
{{- warnf "Getting remote resource %q for page: %q" $remote_url .Page.File.Path }}

{{- $swagger_file := resources.GetRemote $remote_url }}
{{- if not $swagger_file }}
  {{- errorf "Unable to get remote resource %q" $remote_url }}
{{- end }}

<div id="swagger-ui-container"></div>
<script>
  window.onload = function () {
    // we create a shadow DOM to encapsulate the swagger UI,
    // this avoids style conflicts with the rest of the page
    const shadowRoot = document.getElementById('swagger-ui-container').attachShadow({mode: 'open'});

    // add the swagger UI CSS
    const linkNode = document.createElement('link');
    linkNode.rel = 'stylesheet';
    linkNode.href = 'https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.17.14/swagger-ui.css';
    linkNode.integrity = 'sha384-wxLW6kwyHktdDGr6Pv1zgm/VGJh99lfUbzSn6HNHBENZlCN7W602k9VkGdxuFvPn';
    linkNode.crossOrigin = 'anonymous';
    shadowRoot.appendChild(linkNode);

    // add custom styles to the shadow DOM
    const styleNode = document.createElement('style');
    styleNode.textContent = `
      #swagger-ui-container_inner {
        @media (max-width: 768px) {
          overflow-wrap: anywhere;
        }
      }
      .swagger-ui .wrapper {
        padding: 0;
      }
    `;
    shadowRoot.appendChild(styleNode);

    // create an input div to encapsulate the input and description
    const inputDiv = document.createElement('div');
    inputDiv.style.display = 'flex';
    inputDiv.style.flexDirection = 'column';
    inputDiv.style.marginBottom = '1rem';
    shadowRoot.appendChild(inputDiv);

    // add a description for the input
    const descriptionNode = document.createElement('p');
    descriptionNode.textContent = 'Enter the base URL of your {{ $component_name }} API:';
    descriptionNode.style.marginBottom = '0.5rem';
    inputDiv.appendChild(descriptionNode);

    // create an input so users can change the base URL
    const validColor = '#f3ffef';
    const invalidColor = '#ffefef';
    const inputNode = document.createElement('input');
    inputNode.type = 'url';
    inputNode.defaultValue = {{ $default_input_url }};
    inputNode.style.width = '90%';
    inputNode.style.padding = '1rem';
    inputNode.style.border = '1px solid rgba(0, 0, 0, 0.125)';
    inputNode.style.backgroundColor = validColor;
    inputNode.disabled = true;
    inputDiv.appendChild(inputNode);

    // if the user changes the base url, ensure its valid
    inputNode.addEventListener('input', (event) => {
      var parsedURL;
      try {
        parsedURL = new URL(event.target.value);
        if (parsedURL.protocol !== 'http:' && parsedURL.protocol !== 'https:') {
          throw new Error('Invalid protocol');
        }
        event.target.style.backgroundColor = validColor;
        event.target.setCustomValidity('');
      } catch (error) {
        event.target.style.backgroundColor = invalidColor;
        event.target.setCustomValidity(error.message);
      }
      event.target.reportValidity();
    });

    // create the swagger UI container
    const swaggerNode = document.createElement('div');
    swaggerNode.id = 'swagger-ui-container_inner';
    shadowRoot.appendChild(swaggerNode);

    // define a request interceptor to update the base URL
    const requestInterceptor = (request) => {
      if (!request.loadSpec) {
        // get path from the request URL
        // NOTE: we remove the beginning slash which is always present
        requestURL = new URL(request.url);
        requestPath = requestURL.pathname.substring(1);

        // get the path of the user-provided base URL
        // NOTE: we remove the trailing slash, if present
        if (inputNode.validity.valid) {
          baseURL = new URL(inputNode.value);
        } else {
          baseURL = new URL(inputNode.defaultValue);
        }
        pathToAdd = baseURL.pathname;
        pathToAdd = pathToAdd.endsWith('/') ? pathToAdd.substring(0, pathToAdd.length - 1) : pathToAdd;

        // update the request URL
        requestURL.protocol = baseURL.protocol;
        requestURL.host = baseURL.host;
        requestURL.port = baseURL.port;
        requestURL.pathname = pathToAdd + '/' + requestPath;

        // update the request
        request.url = requestURL.toString();
      }
      return request;
    };

    // fetch the spec as a JavaScript object and store as `swaggerSpec` variable
    const request = new Request({{ $swagger_file.RelPermalink }});
    fetch(request)
      .then((response) => {
        if (!response.ok) {
          throw new Error('Failed to fetch the swagger spec');
        }
        return response.text();
      })
      .then((responseString) => {
        // parse the response as YAML (which is a superset of JSON)
        // NOTE: we dont use hugo to convert the YAML to JSON because it does not preserve the order of map keys
        //       and this can break swagger specs
        const swaggerSpec = jsyaml.load(responseString);

        // remove the info section from the spec
        // NOTE: this prevents having an extra heading and other info
        delete swaggerSpec.info;

        // remove the schemes section from the spec
        // NOTE: this prevents having a dropdown for HTTP/HTTPS
        delete swaggerSpec.schemes;

        // remove the servers section from the spec
        // NOTE: this prevents having a dropdown for servers
        delete swaggerSpec.servers;

        // remove the security sections from the spec
        // NOTE: this prevents having the "authorization" buttons
        delete swaggerSpec.security;
        delete swaggerSpec.securityDefinitions;
        if (swaggerSpec.components) {
          delete swaggerSpec.components.securitySchemes;
        }

        // add the base URL to the spec
        swaggerSpec.host = 'HOST_NAME';
        swaggerSpec.basePath = 'BASE_PATH';

        // initialize the swagger UI
        const ui = SwaggerUIBundle({
          spec: swaggerSpec,
          domNode: swaggerNode,
          presets: [SwaggerUIBundle.presets.apis],
          requestInterceptor: requestInterceptor,
          syntaxHighlight: {
            activated: true,
            theme: 'idea',
          }
        });

        // enable the input field
        inputNode.disabled = false;
      })
      .catch((error) => {
        console.error(error);
      });
  };
</script>


================================================
File: layouts/shortcodes/tf-serving-version.html
================================================
<a href="https://github.com/kubeflow/kubeflow/blob/master/kubeflow/tf-serving/tf-serving.libsonnet">v1</a>


================================================
File: layouts/shortcodes/aws/OWNERS
================================================
approvers:
  - surajkota
  - mbaijal
  - akartsky


================================================
File: layouts/shortcodes/aws/kfctl-aws.html
================================================
v1.2.0


================================================
File: layouts/shortcodes/aws/latest-version.html
================================================
1.7.0



================================================
File: layouts/shortcodes/azure/config-uri-azure-oidc.html
================================================
https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_azure_aad.v1.2.0.yaml



================================================
File: layouts/shortcodes/azure/config-uri-azure.html
================================================
https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_azure.v1.2.0.yaml



================================================
File: layouts/shortcodes/azure/latest-version.html
================================================
1.7.0


================================================
File: layouts/shortcodes/blocks/content-item.html
================================================
{{ $url_text := .Get "url_text" }}
<div class="col-lg-12 mb-5 mb-lg-0 ">
  {{ with .Get "title" }}<h4 class="h3 mt-3">{{ . }}</h4>{{ end }}

  <p class="text-muted">{{ .Get "date" }}</p>

  <p class="mb-0">{{ .Inner }}</p>
  {{ with .Get "url" }}<p><a href="{{ . }}">{{ with $url_text }}{{ $url_text }}{{ else }}{{ "Read" }}{{ end }}</a></p>{{ end }}
</div>



================================================
File: layouts/shortcodes/blocks/content-section.html
================================================
{{ $col_id := .Get "color" | default .Ordinal }}
{{ $height := .Get "height" | default "auto"  }}

<a id="td-block-{{ .Ordinal }}" class="td-offset-anchor"></a>
<section class="row td-box td-box--{{ $col_id }} td-box--gradient td-box--height-{{ $height }}">
	<div class="col">
    <h4 class="h2 mb-3 mt-5">{{ .Get "title" | markdownify }}</h4>
		<div class="row">
			{{ .Inner }}
		</div>
	</div>
</section>


================================================
File: layouts/shortcodes/blocks/link-down.html
================================================
{{ with .Parent }}
{{ $id := $.Get "id" | default "overview"  }}
{{ $color := $.Get "color" | default "blue" }}
<a class="btn btn-link text-{{ $color }}" href="#{{ $id }}"><i class="fa fa-chevron-circle-down" style="font-size: 400%"></i></a>
{{ else }}
{{ errorf "The link-down shortcode is supposed to be nested inside a shortcode"}}
{{ end }}


================================================
File: layouts/shortcodes/blocks/sample-section.html
================================================
{{ $api := .Get "api" }}
{{ $last_updated := "" }}
{{ $version := .Get "kfctl" }}
<div class="col-lg-12 mb-5 mb-lg-0 ">
  {{ with .Get "title" }}<h4 class="h3 mt-3">{{ . }}</h4>{{ end }}
  {{ with getJSON $api }} {{ $last_updated = (index (index (index (index . 0) "commit") "committer") "date") | dateFormat "2006/01/02" }}
  <p class="text-muted">{{ "Last update " }} {{ $last_updated }} {{ with $version }}{{ "Kubeflow " }}{{ . }}{{ end }}</p>{{ end }}
  <p class="mb-0">{{ .Inner }}</p>
  {{ with .Get "url" }}<p><a href="{{ . }}">{{ "Go to sample" }}</a></p>{{ end }}
</div>



================================================
File: layouts/shortcodes/blocks/tab.html
================================================
{{ if .Parent }}
	{{ $name := trim (.Get "name") " " }}
	{{ $include := trim (.Get "include") " "}}
	{{ $codelang := .Get "codelang" }}
	{{ if not (.Parent.Scratch.Get "tabs") }}
	{{ .Parent.Scratch.Set "tabs" slice }}
	{{ end }}
	{{ with .Inner }}
	{{ if $codelang }}
	{{ $.Parent.Scratch.Add "tabs" (dict "name" $name "content" (highlight . $codelang "") ) }}
	{{ else }}
	{{ $.Parent.Scratch.Add "tabs" (dict "name" $name "content" . ) }}
	{{ end }}
	{{ else }}
	{{ $.Parent.Scratch.Add "tabs" (dict "name" $name "include" $include "codelang" $codelang) }}
	{{ end }}
{{ else }}
	{{- errorf "[%s] %q: tab shortcode missing its parent" site.Language.Lang .Page.Path -}}
{{ end}}



================================================
File: layouts/shortcodes/blocks/tabs.html
================================================
{{- .Page.Scratch.Add "tabset-counter" 1 -}}
{{- $tab_set_id := .Get "name" | default (printf "tabset-%s-%d" (.Page.RelPermalink) (.Page.Scratch.Get "tabset-counter") ) | anchorize -}}
{{- $tabs := .Scratch.Get "tabs" -}}
{{- if .Inner -}}{{- /* We don't use the inner content, but Hugo will complain if we don't reference it. */ -}}{{- end -}}
<ul class="nav nav-tabs" id="{{ $tab_set_id }}" role="tablist">
	{{- range $i, $e := $tabs -}}
	  {{- $id := printf "%s-%d" $tab_set_id $i -}}
	  {{- if (eq $i 0) -}}
		<li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#{{ $id }}" role="tab" aria-controls="{{ $id }}" aria-selected="true">{{- trim .name " " -}}</a></li>
	  {{ else }}
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#{{ $id }}" role="tab" aria-controls="{{ $id }}">{{- trim .name " " -}}</a></li>
	  {{- end -}}
{{- end -}}
</ul>
<div class="tab-content" id="{{ $tab_set_id }}">
{{- range $i, $e := $tabs -}}
{{- $id := printf "%s-%d" $tab_set_id $i -}}
{{- if (eq $i 0) -}}
  <div id="{{ $id }}" class="tab-pane show active" role="tabpanel" aria-labelledby="{{ $id }}">
{{ else }}
  <div id="{{ $id }}" class="tab-pane" role="tabpanel" aria-labelledby="{{ $id }}">
{{ end }}
<p>
	{{- with .content -}}
		{{- . -}}
	{{- else -}}
		{{- if eq $.Page.BundleType "leaf" -}}
			{{- /* find the file somewhere inside the bundle. Note the use of double asterisk */ -}}
			{{- with $.Page.Resources.GetMatch (printf "**%s*" .include) -}}
				{{- if ne .ResourceType "page" -}}
				{{- /* Assume it is a file that needs code highlighting. */ -}}
				{{- $codelang := $e.codelang | default ( path.Ext .Name | strings.TrimPrefix ".") -}}
				{{- highlight .Content $codelang "" -}}
				{{- else -}}
					{{- .Content -}}
				{{- end -}}
			{{- end -}}
		{{- else -}}
		{{- $path := path.Join $.Page.File.Dir .include -}}
		{{- $page := site.GetPage "page" $path -}}
		{{- with $page -}}
			{{- .Content -}}
		{{- else -}}
		{{- errorf "[%s] tabs include not found for path %q" site.Language.Lang $path -}}
		{{- end -}}
		{{- end -}}
	{{- end -}}
</div>
{{- end -}}
</div>



================================================
File: layouts/shortcodes/canonical/OWNERS
================================================
approvers:
  - ca-scribner



================================================
File: layouts/shortcodes/canonical/latest-version.html
================================================
1.8.0


================================================
File: layouts/shortcodes/deploykf/OWNERS
================================================
approvers:
  - thesuperzapper



================================================
File: layouts/shortcodes/deploykf/latest-version.html
================================================
1.8.0


================================================
File: layouts/shortcodes/gke/OWNERS
================================================
approvers:
  - IronPan
  - zijianjoy
  - chensun
  - james-jwu



================================================
File: layouts/shortcodes/gke/latest-version.html
================================================
1.8.0


================================================
File: layouts/shortcodes/iks/OWNERS
================================================
approvers:
  - animeshsingh
  - Tomcli
reviewers:
  - animeshsingh
  - Tomcli
  - yhwang



================================================
File: layouts/shortcodes/iks/latest-version.html
================================================
1.8.0


================================================
File: layouts/shortcodes/model-registry/OWNERS
================================================
approvers:
  - ederign
  - rareddy
  - tarilabs
  - Tomcli



================================================
File: layouts/shortcodes/model-registry/latest-version.html
================================================
0.2.15


================================================
File: layouts/shortcodes/nutanix/OWNERS
================================================
approvers:
  - johnugeorge
  - deepak-muley
  - nagar-ajay

reviewers:
  - johnugeorge
  - deepak-muley
  - nagar-ajay



================================================
File: layouts/shortcodes/nutanix/latest-version.html
================================================
1.8.0


================================================
File: layouts/shortcodes/oracle/latest-version.html
================================================
1.6.0


================================================
File: layouts/shortcodes/pipelines/OWNERS
================================================
approvers:
  - chensun
  - IronPan
  - james-jwu
  - zijianjoy
reviewers:
  - chensun
  - zijianjoy



================================================
File: layouts/shortcodes/pipelines/latest-version.html
================================================
2.4.0


================================================
File: layouts/shortcodes/qbo/latest-version.html
================================================
1.8.0



================================================
File: layouts/shortcodes/redhat/OWNERS
================================================
approvers:
  - rimolive



================================================
File: layouts/shortcodes/redhat/latest-version.html
================================================
1.9.0


================================================
File: layouts/shortcodes/vmware/OWNERS
================================================
approvers:
  - liuqi
  - xujinheng



================================================
File: layouts/shortcodes/vmware/latest-version.html
================================================
1.6.1


================================================
File: scripts/add_outdated_banner.py
================================================
"""Mark outdated docs as requiring updating

This script finds all markdown files under `content/` that haven't been updated
recently (default 30 days) according to git, and adds a header warning that the
file is out of date.

Example usage:

    python scripts/add_outdated_banner.py --old-version 1.0 --new-version 1.1

"""

import argparse
import re
from datetime import datetime, timedelta
from pathlib import Path
from subprocess import check_output


WARNING = """

{{%% alert title="Out of date" color="warning" %%}}
This guide contains outdated information pertaining to Kubeflow %s. This guide
needs to be updated for Kubeflow %s.
{{%% /alert %%}}

"""

WARNING_REGEX = r"""

{{% alert title="Out of date".*{{% /alert %}}

"""


def main(warning_text: str, cutoff: timedelta):
    now = datetime.utcnow().astimezone()

    for md in Path("content/").rglob("*.md"):
        last_changed = check_output(
            ["git", "log", "-1", "--pretty=format:%ci", md]
        ).decode("utf-8")
        last_changed = datetime.strptime(last_changed, "%Y-%m-%d %H:%M:%S %z")

        # If the docs are recent, don't add the header
        if now - last_changed < cutoff:
            continue

        contents = md.read_text()

        # If the doc already has an out of date warning, replace it with the new one
        if re.search(WARNING_REGEX, contents, flags=re.MULTILINE | re.DOTALL):
            md.write_text(
                re.sub(
                    WARNING_REGEX,
                    warning_text,
                    contents,
                    flags=re.MULTILINE | re.DOTALL,
                )
            )
        # Otherwise, check to see if there's a front matter section delimited by `+++`. If so,
        # add the out of date warning directly afterwards.
        else:
            pluses = list(re.finditer(r"\+\+\+", contents))
            if len(pluses) == 2:
                p = pluses[1]

                # If there's only a front matter section and no actual content, don't bother
                # adding the warning.
                if contents[p.end():].strip():
                    md.write_text(contents[:p.end()] + warning_text + contents[p.end():])


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Mark old docs as requiring updating")

    parser.add_argument(
        "--old-version",
        required=True,
        help="The docs version that is considered out of date",
    )
    parser.add_argument("--new-version", required=True, help="The newest docs version")
    parser.add_argument(
        "--cutoff",
        default=30,
        type=int,
        help="Cutoff in days that a document must have been "
        "updated within to not be considered out of date",
    )
    args = parser.parse_args()

    main(WARNING % (args.old_version, args.new_version), timedelta(days=args.cutoff))



================================================
File: scripts/kfp_nb_to_md.sh
================================================
#!/usr/bin/env bash
# Copyright 2021 The Kubeflow Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Convert all Kubeflow Pipelines notebook documentation to markdown.
# This script is expected to be run from website root folder.
# cd kubeflow/website
# ./scripts/kfp_nb_to_md.sh

find content/en/docs/components/pipelines -name '*.ipynb' -exec python scripts/nb_to_md.py --notebook {} \;



================================================
File: scripts/nb_to_md.py
================================================
# This script creates and updates Markdown versions of Notebook files
# for publication on Kubeflow.org using Hugo/Docsy.
#
# Hugo Markdown files have a metadata section at the top of the page for the
# Front Matter. The Front Matter specifies the page Title, Description, and
# Weight. These values are used to generate the left side navigation, index
# pages, and some page content. 
#
# This script expects the Front Matter to be specified in the following format
# in the first cell of a Jupyter notebook:
#
# # {Title}
# > {Description}
#
# So, the Title is expected to be a Heading 1 and the Description is expected to
# immediately follow it as a Blockquote. Currently, there is no Weight in the
# notebook file.
#
# The script reads the Front Matter from the existing Markdown file,
# or initializes default values, and then overrides the Markdown file's
# Front Matter with values from the notebook. 
#
# * The Weight is always used from the Markdown file. If no Markdown file
#   exists, this will default to `DEFAULT_WEIGHT`. Which should push doc to
#   the end of the list. Edit the Markdown file to specify the correct Weight.
# * If no Title is specified in the notebook, the Markdown file's
#   front matter is used.
# * If the Title is specified in the notebook and the Description is not,
#   the notebook's Title is used and otherwise the Markdown file's front
#   matter is used.
#
# To run this script, type the following on the command line:
#   python3 scripts/nb_to_md.py --notebook /content/en/docs/path-to-notebook
#
# Input:
#   The path to the notebook to convert to Markdown as `--notebook` command
#   line flag.
#
# Output:
#   STDOUT returns the status of the conversion process.
#
# Dependencies:
#   This script depends on `absl`, `nbconvert`, `nbformat`, and `toml`. You
#   may need to install these dependencies using a command like the following:
#   pip3 install nbconvert

from pathlib import Path
import re
from typing import Tuple 

from absl import app
from absl import flags
from nbconvert.exporters import MarkdownExporter
import nbformat
import toml

FLAGS = flags.FLAGS

flags.DEFINE_string(
    'notebook',
    None,
    'Path to the notebook to publish. Should start with "content/en/docs"')

DEFAULT_WEIGHT = 900

class MarkdownFile:
  """Represents the Markdown version of a notebook."""
  
  
  def __init__(self, file_path):
      self.file_path = file_path
      
  def exists(self):
    """Indicates if the Markdown file exists."""
    return Path(self.file_path).exists()
  
  def parse_front_matter(self) -> Tuple[str, str, int]:
    """Parses Front Matter values from Markdown
    
      Returns
        A tuple containing the title, description, and weight.
    """
    
    # default values
    title = None
    description = None
    weight = DEFAULT_WEIGHT
    
    if self.exists():
      content = Path(self.file_path).read_text()

      # find the front matter section
      regexp = re.compile('\++\n(.*?)\++\n', re.S)
      m = regexp.match(content)
      front_matter_content =  m.group(1)

      # load the TOML
      front_matter = toml.loads(front_matter_content)

      if 'title' in front_matter:
        title = front_matter['title']

      if 'description' in front_matter:
        description = front_matter['description']

      if 'weight' in front_matter:
        weight = front_matter['weight']
    
    return title, description, weight

  def write_file(self, content: str):
    p = Path(self.file_path)
    p.write_text(content)
  
class NotebookFile:
  """Represents a Jupyter notebook file."""
  
  def __init__(self, file_path):
    self.file_path = file_path
    
  def exists(self):
    """Indicates if the notebook file exists."""
    return Path(self.file_path).exists()
  
  def get_markdown_file(self):
    p = Path(self.file_path)
    markdown_file_path = p.with_suffix('.md')
    
    return MarkdownFile(markdown_file_path)
  
  def parse_front_matter(self, content, markdown) -> Tuple[str, str, int, str]:
    """Gets the Front Matter for the updated notebook.
    Uses the Markdown Front Matter as the default values and overrides with 
    content from the notebook.
    
    Args:
      content: The notebook content converted to Markdown.
      markdown: An instance of MarkdownFile.
    
    Returns:
       A tuple containing the title, description, weight, and content without
       the Front Matter."""
    title, description, weight = markdown.parse_front_matter()
    
    content_idx = 0
  
    # find the title
    idx = content.find('\n')
    if idx:
      line = content[0:idx]
      if line.startswith("#"):
        title = line[1:].strip()
        content_idx = idx + 1

      # find the description
      descIdx = content.find('\n', idx + 1)
      if descIdx:
        line = content[idx + 1:descIdx]
        if line.startswith(">"):
          description = line[1:].strip()
          content_idx = descIdx + 1
    
    content = content[content_idx:]
    
    return title, description, weight, content
    
  def publish_markdown(self):
    """Updates the Markdown version of a Jupyter notebook file."""
    
    nb = self.get_clean_notebook();
    exporter = MarkdownExporter()
    (content, resources) = exporter.from_notebook_node(nb)
    
    markdown = self.get_markdown_file()
    
    # separate front matter from content
    title, description, weight, content = self.parse_front_matter(
        content, markdown)
    
    template = ('+++\n'
               'title = "{0}"\n'
               'description = "{1}"\n'
               'weight = {2}\n'
               '+++\n\n'
               '<!--\n' 
               'AUTOGENERATED FROM {4}\n'
               'PLEASE UPDATE THE JUPYTER NOTEBOOK AND REGENERATE THIS FILE'
               ' USING scripts/nb_to_md.py.'
               '-->\n\n'
               '<style>\n'
               '.notebook-links {{display: flex; margin: 1em 0;}}\n'
               '.notebook-links a {{padding: .75em; margin-right: .75em;'
               ' font-weight: bold;}}\n'
               'a.colab-link {{\n'
               'padding-left: 3.25em;\n'
               'background-image: url(/docs/images/logos/colab.ico);\n'
               'background-repeat: no-repeat;\n'
               'background-size: contain;\n'
               '}}\n'
               'a.github-link {{\n'
               'padding-left: 2.75em;\n'
               'background-image: url(/docs/images/logos/github.png);\n'
               'background-repeat: no-repeat;\n'
               'background-size: auto 75%;\n'
               'background-position: left center;\n'
               '}}\n'
               '</style>\n'
               '<div class="notebook-links">\n'
               '<a class="colab-link" href="https://colab.research.google.com/'
               'github/kubeflow/website/blob/master/{4}">Run in Google Colab'
               '</a>\n'
               '<a class="github-link" href="https://github.com/kubeflow/websi'
               'te/blob/master/{4}">View source on GitHub</a>\n'
               '</div>\n\n'
               '{3}'
               '\n\n'
               '<div class="notebook-links">\n'
               '<a class="colab-link" href="https://colab.research.google.com/'
               'github/kubeflow/website/blob/master/{4}">Run in Google Colab'
               '</a>\n'
               '<a class="github-link" href="https://github.com/kubeflow/websi'
               'te/blob/master/{4}">View source on GitHub</a>\n'
               '</div>')
    
    markdown.write_file(
        template.format(title, description, weight, content, self.file_path))

  def format_as_terminal(self, commands: str) -> str:
    """Formats a command block to indicate that it contains terminal commands.
    
    Args:
      commands: The command block to format.
    
    Returns:
      The reformatted command block.
    """
    
    lines = commands.split('\n')
    buffer = []
    for line in lines:
      if line.startswith('!'):
        line = '$ {}'.format(line[1:])
      buffer.append(line)
    return '\n'.join(buffer)
  
  def get_clean_notebook(self):
    """Cleans up formatting when converting notebook content to Markdown."""
    
    nb = nbformat.read(self.file_path, as_version=4)
    for cell in nb.cells:
      if cell.cell_type == 'code' and cell.source.find('!') != -1:
        cell.source = self.format_as_terminal(cell.source)
    return nb
  

def main(argv):
  """[nb_to_md.py] Publish Jupyter notebooks as a Kubeflow.org Markdown page"""
  
  if FLAGS.notebook is not None:
    notebook = NotebookFile(FLAGS.notebook)
    if notebook.exists():
      notebook.publish_markdown()
      print('Markdown content has been updated!')
    else:
      print(('Could not update Markdown content.'
             ' Notebook file was not found at "{}"').format(FLAGS.notebook))
  else:
    print(('Could not update Markdown content.'
           ' No notebook parameter was specified.'))

if __name__ == '__main__':
  app.run(main)


================================================
File: scripts/nb_to_md_test.py
================================================
from pathlib import Path
import unittest
from unittest.mock import Mock
from unittest.mock import patch

import nb_to_md


class MarkdownFileTest(unittest.TestCase):
    
    def test_parse_front_matter_default_values(self):
        with patch.object(nb_to_md.MarkdownFile, 'exists', return_value=False):
            markdown = nb_to_md.MarkdownFile('content/en/docs/foo.md')
            title, description, weight = markdown.parse_front_matter()
            self.assertEqual(weight, nb_to_md.DEFAULT_WEIGHT)
            self.assertIsNone(title)
            self.assertIsNone(description)
    
    def test_parse_front_matter_success(self):
        fake_title = "Building an example title"
        fake_description = "Learn more about building fake content"
        fake_weight = 42
        
        fake_content = """+++
title = "{}"
description = "{}"
weight = {}
+++

Lorem ipsum something or other.""".format(
            fake_title,
            fake_description,
            fake_weight)
        
        with patch.object(nb_to_md.MarkdownFile, 'exists', return_value=True):
            with patch.object(Path, 'read_text', return_value=fake_content):
                markdown = nb_to_md.MarkdownFile('content/en/docs/foo.md')
                title, description, weight = markdown.parse_front_matter()
                self.assertEqual(weight, fake_weight)
                self.assertEqual(title, fake_title)
                self.assertEqual(description, fake_description)
    
        
class NotebookFileTests(unittest.TestCase):

    def test_format_as_terminal_success(self):
        fake_command = """!pip install
        kfp
!pip install tfx"""
        
        notebook = nb_to_md.NotebookFile('content/en/docs/foo.ipynb')
        actual = notebook.format_as_terminal(fake_command)
        self.assertEqual(actual, fake_command.replace('!', '$ '))
    
    def test_format_as_terminal_no_changes(self):
        fake_command = """def foo(bar: str) -> str:
  if !bar.startswith('something'):
    # do something
  return bar"""
        
        notebook = nb_to_md.NotebookFile('content/en/docs/foo.ipynb')
        actual = notebook.format_as_terminal(fake_command)
        self.assertEqual(actual, fake_command)
    
    def test_parse_front_matter_success(self):
        fake_title = "Build test cases for doc tools"
        fake_description = "Learning how to build test cases"
        fake_page_content = '\nLorem ipsum dolor sit amet, something or other.'
        fake_content = """# {}
> {}
{}""".format(fake_title, fake_description, fake_page_content)
        with patch.object(nb_to_md.MarkdownFile,
                          'parse_front_matter',
                          return_value=(None, None, nb_to_md.DEFAULT_WEIGHT)):
            markdown = nb_to_md.MarkdownFile('content/en/docs/foo.md')
            notebook = nb_to_md.NotebookFile('content/en/docs/foo.ipynb')
            title, description, weight, content = notebook.parse_front_matter(
                fake_content, markdown)
            self.assertEqual(weight, nb_to_md.DEFAULT_WEIGHT)
            self.assertEqual(title, fake_title)
            self.assertEqual(description, fake_description)
            self.assertEqual(content, fake_page_content)
    
    def test_parse_front_matter_no_description(self):
        fake_title = "Build test cases for doc tools"
        fake_page_content = '\nLorem ipsum dolor sit amet, something or other.'
        fake_content = """# {}
{}""".format(fake_title,  fake_page_content)
        with patch.object(nb_to_md.MarkdownFile,
                          'parse_front_matter',
                          return_value=(None, None, nb_to_md.DEFAULT_WEIGHT)):
            markdown = nb_to_md.MarkdownFile('content/en/docs/foo.md')
            notebook = nb_to_md.NotebookFile('content/en/docs/foo.ipynb')
            title, description, weight, content = notebook.parse_front_matter(
                fake_content, markdown)
            self.assertEqual(weight, nb_to_md.DEFAULT_WEIGHT)
            self.assertEqual(title, fake_title)
            self.assertIsNone(description)
            self.assertEqual(content, fake_page_content)

    def test_parse_front_matter_no_front_matter(self):
        fake_page_content = 'Lorem ipsum dolor sit amet, something or other.'
        with patch.object(nb_to_md.MarkdownFile,
                          'parse_front_matter',
                          return_value=(None, None, nb_to_md.DEFAULT_WEIGHT)):
            markdown = nb_to_md.MarkdownFile('content/en/docs/foo.md')
            notebook = nb_to_md.NotebookFile('content/en/docs/foo.ipynb')
            title, description, weight, content = notebook.parse_front_matter(
                fake_page_content, markdown)
            self.assertEqual(weight, nb_to_md.DEFAULT_WEIGHT)
            self.assertIsNone(title)
            self.assertIsNone(description)
            self.assertEqual(content, fake_page_content)

    
if __name__ == "__main__":
    unittest.main()


================================================
File: scripts/validate-urls.py
================================================
# This script finds .md files under a directory and its subdirectories, extracts
# http/https URLs from .md files and validates them.
#
# This script can be run periodically on kubeflow/website source repository
# to find outdated URLs, which indicate possible outdated document sections.
#
# To run this script, type the following on the command line:
#   python3.8 validate-urls.py -d /path/to/kubeflow/website/content/docs
#
# Input:
#   The path of a directory that contains .md files as `-d` command line flag.
#
# Output:
#   STDOUT logs in the format of `<file>: <URL> , <status>` and a summary of all
#   invalid URLs at the end.
#
# Dependency:
#   You may need to install the `requests` Python package via command line:
#   python3.8 -m pip install requests

import argparse
import os
import re
import requests

parser = argparse.ArgumentParser(
    description='Validate all URLs in the kubeflow.org website'
)

parser.add_argument(
    '-d',
    '--dir',
    dest='input_dir',
    nargs='?',
    default='kubeflow/website/content',
    help=
    'Path to the doc content folder. (Default: %(default)s)',
)

# http/https URLs
HTTP_PATTERN = re.compile(
    'http[s]?://[a-zA-Z\-_?/*\.#\$][a-zA-Z0-9\-_?/*\.#%=\$]+')

# Patterns in this white list are considered valid.
WHITE_LIST = [
    re.compile('http[s]?://localhost'),
    re.compile('http[s]?://\.\.'), # https://......
    re.compile('https://path/to/component.yaml'),
    re.compile('https://github.com/kubeflow/kfctl/releases/tag')
]

def should_skip(url):
    for p in WHITE_LIST:
        if p.match(url):
            return True
    return False

def main():
    args = parser.parse_args()
    # find all md files under INPUT_DIR.
    files = []
    for (dirpath, dirname, filenames) in os.walk(args.input_dir):
        for f in filenames:
            if f.endswith(".md"):
                files.append(os.path.join(dirpath, f))

    urls = {}
    for file in files:
        with open(file, "r") as f:
            u = HTTP_PATTERN.findall(f.read())
            if u:
                urls[file[len(args.input_dir):]] = u

    problematic_urls = []
    for file, urls in urls.items():
        for url in urls:
            if should_skip(url):
                print(f"skipping {url} ")
                continue
            print(f"{file}: URL {url}",end='')
            try:
                r = requests.head(url)
                print(f" , Status {r.status_code}")
                if r.status_code >= 400 and r.status_code < 500:
                    problematic_urls.append((file, url, r.status_code))
            except Exception as e:
                print(e)
                problematic_urls.append((file, url, "FAIL"))
    print("\nSummary:\n")  
    for u in problematic_urls:
        print(f"|{u[0]} | {u[1]} | {u[2]}|")

if __name__ == "__main__":
    main()



================================================
File: static/browserconfig.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<browserconfig>
    <msapplication>
        <tile>
            <square150x150logo src="/mstile-150x150.png?v=2"/>
            <TileColor>#4279f4</TileColor>
        </tile>
    </msapplication>
</browserconfig>



================================================
File: static/google65401334ad4c38b1.html
================================================
google-site-verification: google65401334ad4c38b1.html


================================================
File: static/site.webmanifest
================================================
{
    "name": "",
    "short_name": "",
    "icons": [
        {
            "src": "/android-chrome-192x192.png?v=2",
            "sizes": "192x192",
            "type": "image/png"
        },
        {
            "src": "/android-chrome-512x512.png?v=2",
            "sizes": "512x512",
            "type": "image/png"
        }
    ],
    "theme_color": "#4279f4",
    "background_color": "#ffffff",
    "display": "standalone"
}





================================================
File: .github/issue_label_bot.yaml
================================================
# for https://mlbot.net a GitHub bot that automatically labels issues using Kubeflow.
label-alias:
  bug: 'kind/bug'
  feature: 'kind/feature'
  feature_request: 'kind/feature'  
  question: 'kind/question'



================================================
File: .github/pull_request_template.md
================================================
<!-- Add the component name to the PR's title. Example: pipelines: Fixed broken link in Getting Started with Kubeflow Pipelines -->


**Checklist:**
- [ ] You have [signed off your commits](https://www.kubeflow.org/docs/about/contributing/#sign-off-your-commits)
- [ ] Ensure you follow best practices from our guide. [Contributing](https://github.com/kubeflow/website/blob/master/content/en/docs/about/contributing.md). 
- [] You have included screenshots when changing the website style or adding a new page.


**Description of your changes:**


### Issue

<!--
 If this pull request resolves an open issue, please link the issue in the PR
 description so it will automatically close when the PR is merged.

 See the GitHub documentation for more details and other options:

 https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword
-->

Closes: #

<!--Additional Information:-->
### Labels
<!-- Please include labels below by uncommenting them to help us better review PRs -->

<!-- /area central-dashboard -->
<!-- /area katib -->
<!-- /area kserve -->
<!-- /area model-registry -->
<!-- /area notebooks -->
<!-- /area pipelines -->
<!-- /area spark-operator -->
<!-- /area trainer -->
<!-- /area gsoc -->
<!-- /area website -->
<!-- /area community -->
---




================================================
File: .github/ISSUE_TEMPLATE/BUG_REPORT.md
================================================
---
name: Bug Report
about: Report a bug encountered with Kubeflow Website
labels:
- kind/bug
title: "bug(<component>): <Bug Name>"
---
**This is a Bug Report**

<!-- Thanks for filing an issue! Before submitting, please fill in the following information. -->
<!-- See https://www.kubeflow.org/docs/about/contributing/ for guidance on writing an actionable issue description. -->

<!--Required Information-->
**Problem:**


**Proposed Solution:**


**Page to Update (provide the full path):**
https://kubeflow.org/...


<!--Component/Kubeflow Version:-->
**Component/Kubeflow Version:**


<!--Additional Information:-->
### Labels
<!-- Please include labels below by uncommenting them to help us better triage issues -->

<!-- /area central-dashboard -->
<!-- /area katib -->
<!-- /area kserve -->
<!-- /area model-registry -->
<!-- /area notebooks -->
<!-- /area pipelines -->
<!-- /area spark-operator -->
<!-- /area trainer -->
<!-- /area gsoc -->
<!-- /area website -->
<!-- /area community -->
---

<!-- Don't delete message below to encourage users to support your issue! -->
Impacted by this bug? Give it a 👍. 


================================================
File: .github/ISSUE_TEMPLATE/CHORE.md
================================================
---
name: 🧹 Chore
about: Create a Chore on Kubeflow Website
labels: 
- kind/chore
title: "chore(<component>): <Chore Name>"
---

## Chore description

<!-- Describe the chore details and why it's needed.  -->



<!--Component/Kubeflow Version:-->
**Component/Kubeflow Version:**

<!--Additional Information:-->
### Labels
<!-- Please include labels below by uncommenting them to help us better triage issues -->

<!-- /area central-dashboard -->
<!-- /area katib -->
<!-- /area kserve -->
<!-- /area model-registry -->
<!-- /area notebooks -->
<!-- /area pipelines -->
<!-- /area spark-operator -->
<!-- /area trainer -->
<!-- /area gsoc -->
<!-- /area website -->
<!-- /area community -->
---

<!-- Don't delete the message below to encourage users to support your issue! -->
Love this idea? Give it a 👍. 


================================================
File: .github/ISSUE_TEMPLATE/FEATURE_REQUEST.md
================================================
---
name: Feature Request
about: Suggest a/an feature/enhancement to the Kubeflow Website project
labels:
- kind/feature
title: "feature(<component>): <Feature Name>"
---
**This is a Feature Request**
<!-- Have a proposal about a component's functionality or feedback on a feature? Submit an issue on any of the kubeflow components on their associated repository https://github.com/kubeflow.

<!-- Please only use this template for submitting feature/enhancement requests related to the website -->
<!-- See https://www.kubeflow.org/docs/about/contributing/ for guidance on writing an actionable issue description. -->



**What would you like to be added**
<!-- Describe as precisely as possible how this feature/enhancement should work from the user perspective. What should be changed, etc. -->

**Why is this needed**


**Page to Update or indicate if it is a new page (provide the full path):**
https://kubeflow.org/...


<!--Component/Kubeflow Version:-->
**Component/Kubeflow Version:**

<!--Additional Information:-->
### Labels
<!-- Please include labels below by uncommenting them to help us better triage issues -->

<!-- /area central-dashboard -->
<!-- /area katib -->
<!-- /area kserve -->
<!-- /area model-registry -->
<!-- /area notebooks -->
<!-- /area pipelines -->
<!-- /area spark-operator -->
<!-- /area trainer -->
<!-- /area gsoc -->
<!-- /area website -->
<!-- /area community -->
---

**Comments**
<!-- Any additional related comments that might help. Drawings/mockups would be extremely helpful (if required). -->

<!-- Don't delete the message below to encourage users to support your issue! -->
Love this idea? Give it a 👍. 


================================================
File: .github/ISSUE_TEMPLATE/SUPPORT_REPORT.md
================================================
---
name: Support Request
about: Support request or question relating to Kubeflow Website project
labels:
- kind/support
---
**This is Support**

<!--
STOP -- PLEASE READ!

GitHub is not the right place for support requests.

If you're looking for help, post your question on any of the [Kubeflow Slack channels](https://www.kubeflow.org/docs/about/community/#kubeflow-slack-channels).



================================================
File: .github/workflows/pr_title_check.yaml
================================================
name: PR Title Validation

on:
  pull_request:
    types: [opened, edited, synchronize, reopened]

jobs:
  validate-pr-title:
    runs-on: ubuntu-latest
    steps:
      - name: Check PR Title Format
        env:
          PR_TITLE: ${{ github.event.pull_request.title }}
        run: |
          VALID_COMPONENTS="central-dashboard|katib|kserve|model-registry|notebooks|pipelines|spark-operator|trainer|gsoc|website|community"
          if [[ ! "$PR_TITLE" =~ ^($VALID_COMPONENTS):\ .+ ]]; then
            echo "❌ PR title does not follow the correct format: 'component_name: description'"
            echo "Component value must be one of these central-dashboard|katib|kserve|model-registry|notebooks|pipelines|spark-operator|trainer|gsoc|website|community"
            echo "Example: model-registry: Add model registry UI Installation instructions"
            exit 1
          fi
          echo "✅ PR title format is correct."



================================================
File: .github/workflows/stale.yaml
================================================
# This workflow warns and then closes issues and PRs that have had no activity for a specified amount of time.
#
# You can adjust the behavior by modifying this file.
# For more information, see:
# https://github.com/actions/stale
name: Mark stale issues and pull requests

on:
  schedule:
  - cron: '0 0 * * *' # Run every day at midnight

jobs:
  stale:
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write

    steps:
    - uses: actions/stale@v5
      with:
        repo-token: ${{ secrets.GITHUB_TOKEN }}
        days-before-stale: 90
        days-before-close: 21
        # The message that will be added as a comment to the issues
        # when the stale workflow marks it automatically as stale with a label.
        stale-issue-message: >
          This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
        # The message that will be added as a comment to the issues
        # when the stale workflow closes it automatically after being stale for too long.
        close-issue-message: >
          This issue has been automatically closed because it has not had recent activity. Please comment "/reopen" to reopen it.
        stale-issue-label: lifecycle/stale
        # Exclude them from being marked as stale
        exempt-issue-labels: lifecycle/frozen,enhancement,good first issue
        # The message that will be added as a comment to the pull requests
        # when the stale workflow marks it automatically as stale with a label.
        stale-pr-message: "This pull request has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. \n"
        # The message that will be added as a comment to the pull requests
        # when the stale workflow closes it automatically after being stale for too long.
        close-pr-message: "This pull request has been automatically closed because it has not had recent  activity.You can reopen the PR if you want. \n"
        stale-pr-label: lifecycle/stale
        # Exclude them from being marked as stale
        exempt-pr-labels: lifecycle/frozen,enhancement,good first issue
        # The issues or the pull requests with a milestone will not be marked as stale automatically
        exempt-all-milestones: true
        # Learn more about operations: https://github.com/actions/stale#operations-per-run.
        operations-per-run: 250



================================================
File: .github/workflows/triage_issues.yaml
================================================
# Define a GitHub action workflow to determine whether issues 
# should be added or removed from the Needs Triage Kanban board.
name: Check Triage Status of Issue
on:
  issues:
    types: [opened, closed, reopened, transferred, labeled, unlabeled]
    # Issue is created, Issue is closed, Issue added or removed from projects, Labels added/removed

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Update Kanban
        uses: kubeflow/code-intelligence/Issue_Triage/action@master
        with:
          # Letting input NEEDS_TRIAGE_PROJECT_CARD_ID use the default value
          ISSUE_NUMBER: ${{ github.event.issue.number }}
          GITHUB_PERSONAL_ACCESS_TOKEN: ${{ secrets.triage_projects_github_token }}



